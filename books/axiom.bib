@misc{Daly17,
  author = "Daly, Timothy and Botch, Mark",
  title = {{Axiom Developer Website}},
  link = "\url{http://axiom-developer.org}",
  year = "2017"
}

@book{Book00,
  author = "Axiom Authors",
  title = {{Volume 0: Axiom Jenks and Sutor}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol0.pdf}",
  publisher = "Axiom Project",
  year = "2016",
  keywords = "axiomref",
  beebe = "Jenks:2003:AVS"
}

@book{Book01,
  author = "Axiom Authors",
  title = {{Volume 1: Axiom Tutorial}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol1.pdf}",
  publisher = "Axiom Project",
  year = "2016"
}

@book{Book02,
  author = "Axiom Authors",
  title = {{Volume 2: Axiom Users Guide}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol2.pdf}",
  publisher = "Axiom Project",
  year = "2016",
  keywords = "axiomref",
  beebe = "Daly:2005:AVAb"
}

@book{Book03,
  author = "Axiom Authors",
  title = {{Volume 3: Axiom Programmers Guide}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol3.pdf}",
  publisher = "Axiom Project",
  year = "2016",
  keywords = "axiomref",
  beebe = "Daly:2005:AVAc"
}

@book{Book04,
  author = "Axiom Authors",
  title = {{Volume 4: Axiom Developers Guide}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol4.pdf}",
  publisher = "Axiom Project",
  year = "2016",
  keywords = "axiomref",
  beebe = "Daly:2005:AVAd"
}

@book{Book05,
  author = "Axiom Authors",
  title = {{Volume 5: Axiom Interpreter}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol5.pdf}",
  publisher = "Axiom Project",
  year = "2016",
  keywords = "axiomref",
  beebe = "Daly:2003:AVA",
}

@book{Book06,
  author = "Axiom Authors",
  title = {{Volume 6: Axiom Command}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol6.pdf}",
  publisher = "Axiom Project",
  year = "2016",
  keywords = "axiomref",
  beebe = "Daly:2005:AVAe"
}

@book{Book07,
  author = "Axiom Authors",
  title = {{Volume 7: Axiom Hyperdoc}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol7.pdf}",
  publisher = "Axiom Project",
  year = "2016",
  keywords = "axiomref",
  beebe = "Daly:2005:AVAj"
}

@book{Book71,
  author = "Axiom Authors",
  title = {{Volume 7.1: Axiom Hyperdoc Pages}},
  publisher = "Axiom Project",
  link = "\url{http://axiom-developer.org/axiom-website/bookvol7.1.pdf}",
  year = "2016"
}

@book{Book08,
  author = "Axiom Authors",
  title = {{Volume 8: Axiom Graphics}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol8.pdf}",
  publisher = "Axiom Project",
  year = "2016",
  keywords = "axiomref",
  beebe = "Daly:2005:AVAf"
}

@book{Book81,
  author = "Axiom Authors",
  title = {{Volume 8.1: Axiom Gallery}},
  publisher = "Axiom Project",
  link = "\url{http://axiom-developer.org/axiom-website/bookvol8.1.pdf}",
  year = "2016"
}

@book{Book09,
  author = "Axiom Authors",
  title = {{Volume 9: Axiom Compiler}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol9.pdf}",
  publisher = "Axiom Project",
  year = "2016",
  keywords = "axiomref",
  beebe = "Daly:2005:AVAg"
}

@book{Book91,
  author = "Axiom Authors",
  title = {{Volume 9.1: Axiom Compiler Details}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol9.1.pdf}",
  publisher = "Axiom Project",
  year = "2016"
}

@book{Book10,
  author = "Axiom Authors",
  title = {{Volume 10: Axiom Algebra: Implementation}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol10.pdf}",
  publisher = "Axiom Project",
  year = "2016",
  keywords = "axiomref",
  beebe = "Daly:2005:AVAh"
}

@book{Book101,
  author = "Axiom Authors",
  title = {{Volume 10.1: Axiom Algebra: Theory}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol10.1.pdf}",
  publisher = "Axiom Project",
  year = "2016"
}

@book{Book102,
  author = "Axiom Authors",
  title = {{Volume 10.2: Axiom Algebra: Categories}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol10.2.pdf}",
  publisher = "Axiom Project",
  year = "2016"
}

@book{Book103,
  author = "Axiom Authors",
  title = {{Volume 10.3: Axiom Algebra: Domains}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol10.3.pdf}",
  publisher = "Axiom Project",
  year = "2016"
}

@book{Book104,
  author = "Axiom Authors",
  title = {{Volume 10.4: Axiom Algebra: Packages}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol10.4.pdf}",
  publisher = "Axiom Project",
  year = "2016"
}

@book{Book105,
  author = "Axiom Authors",
  title = {{Volume 10.5: Axiom Algebra: Numerics}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol10.5.pdf}",
  publisher = "Axiom Project",
  year = "2016"
}

@book{Book11,
  author = "Axiom Authors",
  title = {{Volume 11: Axiom Browser}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol11.pdf}",
  publisher = "Axiom Project",
  year = "2016",
  keywords = "axiomref",
  beebe = "Portes:2007:AVA"
}

@book{Book12,
  author = "Axiom Authors",
  title = {{Volume 12: Axiom Crystal}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol12.pdf}",
  publisher = "Axiom Project",
  year = "2016",
  keywords = "axiomref",
  beebe = "Daly:2005:AVAi"
}

@book{Book13,
  author = "Axiom Authors",
  title = {{Volume 13: Proving Axiom Correct}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol13.pdf}",
  publisher = "Axiom Project",
  year = "2016"
}

@book{Book14,
  author = "Axiom Authors",
  title = {{Volume 14: Algorithms}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvol14.pdf}",
  publisher = "Axiom Project",
  year = "2016"
}

@book{Bookbib,
  author = "Axiom Authors",
  title = {{Volume Bibliography: Axiom Literature Citations}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvolbib.pdf}",
  publisher = "Axiom Project",
  year = "2016"
}

@book{Bookbug,
  author = "Axiom Authors",
  title = {{Volume BugList: Axiom Bugs}},
  link = "\url{http://axiom-developer.org/axiom-website/bookvolbug.pdf}",
  publisher = "Axiom Project",
  year = "2016"
}

@inproceedings{Abda86,
  author = "Abdali, S. Kamal and Cherry, Guy W. and Soiffer, Neil",
  title = {{A Smalltalk System for Algebraic Manipulation}},
  booktitle = "OOPSLA 86",
  pages = "277-293",
  year = "1986",
  abstract =
    "This paper describes the design of an algebra system Views
    implemented in Smalltalk. Views contains facilities for dynamic
    creation and manipulation of computational domains, for viewing these
    domains as various categories such as groups, rings, or fields, and
    for expressing algorithms generically at the level of categories. The
    design of Views has resulted in the addition of some new abstractions
    to Smalltalk that are quite useful in their own right. Parameterized
    classes provide a means for run-time creation of new classes that
    exhibit generally very similar behavior, differing only in minor ways
    that can be described by different instantiations of certain
    parameters. Categories allow the abstraction of the common behavior of
    classes that derives from the class objects and operations satisfying
    certain laws independently of the implementation of those objects and
    operations. Views allow the run-time association of classes with
    categories (and of categories with other categories), facilitating the
    use of code written for categories with quite different
    interpretations of operations. Together, categories and views provide
    an additional mechanism for code sharing that is richer than both
    single and multiple inheritance. The paper gives algebraic as well as
    non-algebraic examples of the above-mentioned features.",
  paper = "Abda86.pdf",
  keywords = "axiomref"
}

@book{Ahox86,
  author = "Aho, Alfred V. and Sethi, Ravi and Ullman, Jeffrey D.",
  title = {{Compilers: Principles, Techniques, and Tools}},
  year = "1986",
  publisher = "Addison-Wesley",
  isbn = "978-0201100884"
}

@misc{Bake16b,
  author = "Baker, Martin",
  title = {{add coerce from PermutationGroup to GroupPresentation}},
  link = "\url{https://groups.google.com/forum/?hl=en\#!topic/fricas-devel/EtLwgd2dWNU}",
  year = "2016"
}

@misc{Bake17,
  author = "Baker, Martin",
  title = {{Finite Group Implementation}},
  link = "\url{http://www.euclideanspace.com/prog/scratchpad/mycode/discrete/finiteGroup/}",
  year = "2017"
}

@book{Bare84,
  author = "Barendregt, H. P.",
  title = {{The Lambda Calculus: Its Syntax and Semantics}},
  publisher = "Elsevier Science",
  year = "1984"
}

@techreport{Baum95,
  author = "Baumgartner, Gerald and Stansifer, Ryan D.",
  title = {{A Proposal to Study Type Systems for Computer Algebra}},
  type = "technical report",
  institution = "RISC-LINZ",
  number = "90-07.0",
  year = "1995",
  abstract =
    "It is widely recognized that programming languages should oer
    features to help structure programs. To achieve this goal, languages
    like Ada , Modula-2 , object-oriented languages, and functional
    languages have been developed. The structuring techniques available
    so far (like modules, classes, parametric polymorphism) are still not
    enough or not appropriate for some application areas. In symbolic
    computation, in particular computer algebra, several problems occur
    that are dicult to handle with any existing programming
    language. Indeed, nearly all available computer algebra systems suer
    from the fact that the underlying programming language imposes too
    many restrictions.
    
    We propose to develop a language that combines the essential features
    from functional languages, object-oriented languages, and computer
    algebra systems in a semantically clean manner. Although intended for
    use in symbolic computation, this language should prove interesting as
    a general purpose programming language.  The main innovation will be
    the application of sophisticated type systems to the needs of computer
    algebra systems. We will demonstrate the capabilities of the language
    by using it to implement a small computer algebra library. This 
    implementation will be compared against a straightforward Lisp
    implementation and against existing computer algebra systems. Our
    development should have an impact both on the programming languages
    world and on the computer algebra world.",
  paper = "Baum95.pdf"
}

@techreport{Berg92,
  author = "Berger, Emery",
  title = {{FP + OOP = Haskell}},
  institution = "University of Texas",
  number = "TR-92-30",
  year = "1992",
  abstract =
    "The programming language Haskell adds object-oriented functionality
    (using a concept known as type classes) to a pure functional
    programming framework. This paper describes these extensions and
    analyzes its accomplishments as well as some problems."
}

@book{Birt80,
  author = "Birtwistle, Graham M.",
  title = {{Simula Begin}},
  year = "1980",
  publisher = "Chartwell-Bratt",
  isbn = "9780862380090"
}

@inproceedings{Brea89,
  author = "Breazu-Tannen, Val and Coquand, Thierry and Gunter, Carl A. and
            Scedrov, Andre",
  title = {{Inheritance and Explicit Coercion}},
  booktitle = "Logic in Computer Science",
  year = "1989",
  isbn = "0-8186-1954-6",
  abstract =
    "A method is presented for providing semantic interpretations for
    languages which feature inheritance in the framework of statically
    checked, rich type disciplines. The approach is illustrated by an
    extension of the language Fun of L. Cardelli and P. Wegner (1985),
    which is interpreted via a translation into an extended polymorphic
    lambda calculus. The approach interprets inheritances in Fun as
    coercion functions already definable in the target of the
    translation. Existing techniques in the theory of semantic domains can
    then be used to interpret the extended polymorphic lambda calculus,
    thus providing many models for the original language. The method
    allows the simultaneous modeling of parametric polymorphism, recursive
    types, and inheritance, which has been regarded as problematic because
    of the seemingly contradictory characteristics of inheritance and type
    recursion on higher types. The main difficulty in providing
    interpretations for explicit type disciplines featuring inheritance is
    identified. Since interpretations follow the type-checking
    derivations, coherence theorems are required, and the authors prove
    them for their semantic method.",
  paper = "Brea89.pdf"
}  

@article{Brea91,
  author = "Breazu-Tannen, Val and Coquand, Thierry and Gunter, Carl A. and
            Scedrov, Andre",
  title = {{Inheritance as Implicit Coercion}},
  journal = "Information and Computation",
  volume = "93",
  number = "1",
  year = "1991",
  pages = "172-221",
  abstract =
    "We present a method for providing semantic interpretations for
    languages with a type system featuring inheritance polymorphism. Our
    approach is illustrated on an extension of the language Fun of
    Cardelli and Wegner, which we interpret via a translation into an
    extended polymorphic lambda calculus. Our goal is to interpret
    inheritances in Fun via coercion functions which are definable in the
    target of the translation. Existing techniques in the theory of
    semantic domains can be then used to interpret the extended
    polymorphic lambda calculus, thus providing many models for the
    original language. This technique makes it possible to model a rich
    type discipline which includes parametric polymorphism and recursive
    types as well as inheritance. A central difficulty in providing
    interpretations for explicit type disciplines featuring inheritance in
    the sense discussed in this paper arises from the fact that programs
    can type-check in more than one way. Since interpretations follow the
    type-checking derivations, coherence theorems are required: that is,
    one must prove that the meaning of a program does not depend on the
    way it was type-checked. Proofs of such theorems for our proposed
    interpretation are the basic technical results of this
    paper. Interestingly, proving coherence in the presence of recursive
    types, variants, and abstract types forced us to reexamine fundamental
    equational properties that arise in proof theory (in the form of
    commutative reductions) and domain theory (in the form of strict
    vs. non-strict functions).",
  paper = "Brea91.pdf"
}  

@inproceedings{Bruc93,
  author = "Bruce, Kim B.",
  title = {{Safe type checking in a statically-typed object-oriented
           programming language}},
  booktitle = "POPL 93",
  year = "1993",
  isbn = "0-89791-560-7",
  pages = "285-298",
  abstract = 
    " In this paper we introduce a statically-typed, functional,
    object-oriented programming language, TOOPL, which supports classes,
    objects, methods, instance variable, subtypes, and inheritance. It has
    proved to be surprisingly difficult to design statically-typed
    object-oriented languages which are nearly as expressive as Smalltalk
    and yet have no holes in their typing systems. A particular problem
    with statically type checking object-oriented languages is determining
    whether a method provided in a superclass will continue to type check
    when inherited in a subclass. This program is solved in our language
    by providing type checking rules which guarantee that a method which
    type checks as part of a class will type check correctly in all legal
    subclasses in which it is inherited. This feature enables library
    providers to provide only the interfaces of classes with executables
    and still allow users to safely create subclasses. The design of TOOPL
    has been guided by an analysis of the semantics of the language, which
    is given in terms of a sufficiently rich model of the F-bounded
    second-order lambda calculus. This semantics supported the language
    design by providing a means of proving that the type-checking rules
    for the language are sound, ensuring that well-typed terms produce
    objects of the appropriate type. In particular, in a well-typed
    program it is impossible to send a message to an object which lacks a
    corresponding method.",
  paper = "Bruc93.pdf"
}

@inproceedings{Brea89a,
  author = "Breazu-Tannen, Val Gallier, Jean",
  title = {{Polymorphic Rewriting Concerves Algebraic Strong Normalization
           and Confluence}},
  booktitle = "Automata, Languages and Programming",
  pages = "137-150",
  year = "1989",
  abstract =
    "We study combinations of many-sorted algebraic term rewriting systems
    and polymorphic lambda term rewriting. Algebraic and lambda terms are
    mixed by adding the symbols of the algebraic signature to the
    polymorphic lambda calculus, as higher-order constants.
    
    We show that if a many-sorted algebraic rewrite system R is strongly
    normalizing (terminating, noetherian), then 
    $R+\beta+\nu+type-\beta+type-\nu$ rewriting of mixed terms is also
    strongly normalizing. We obtain this results using a technique which
    generalizes Girard's ``{candidats de reductibiliti\'e}'', introduced in
    the original proof of strong normalization for the polymorphic lambda
    calculus.
    
    We also show that if a many-sorted algebraic rewrite system $R$ has
    the Church-Rosser property (is confluent), then 
    $R+\beta+type-\beta+type-\nu$ rewriting of mixed terms has the 
    Church-Rosser property too. Combining the two results, we conclude
    that if $R$ is canonical (complete) on algebraic terms, then
    $R+\beta+type-\beta+type-\nu$  is canonical on mixed terms.
    
    $\nu$ reduction does not commute with algebraic reduction, in general.
    However, using long $\nu$-normal forms, we show that if $R$ is canonical
    then $R+\beta+type-\beta+type-\nu$ convertibility is still decidable.",
paper = "Brea89.pdf"
}

@book{Buch82,
  author = "Buchberger, Bruno and Collins, George Edwin and Loos, Rudiger",
  title = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  paper = "Buch82.pdf"
}

@techreport{Buch93,
  author = "Buchberger, Bruno and Collins, George E. and Encarnacion, Mark J.
            and Hong, Hoon and Johnson, Jeremy R. and Krandick, Werner and
            Loos, Rudiger and Mandache, Ana M. and Neubacher, Andreas and
            Vielhaber, Herbert",
  title = {{SACLIB 1.1 User's Guide}},
  year = "1993",
  institution = "Kurt Godel Institute",
  abstract =
    "This paper lists most of the algorithms provided by SACLIB and shows
    how to call them from C. There is also a brief explanation of the
    inner workings of the list processing and garbage collection
    facilities of SACLIB",
  paper = "Buch93.pdf"
}

@article{Buen91,
  author = "Buendgen, R. and Hagel, G. and Loos, R. and Seitz, S. and
            Simon, G. and Stuebner, R. and Weber, A.",
  title = {{SAC-2 in ALDES -- Ein Werkzeug fur dis Algorithmenforschung}},
  journal = "MathPAD 1",
  volume = "3",
  year = "1991",
  pages = "33-37"
}

@book{Bund93,
  author = "Bundgen, Reinhard",
  title = {{The ReDuX System Documentation}},
  year = "1993",
  publisher = "WSI"
}

@inproceedings{Bund93a,
  author = "Bundgen, Reinhard",
  title = {{Reduce the Redex $->$ ReDuX}},
  booktitle = "Proc. Rewriting Techniques and Applications 93",
  year = "1993",
  pages = "446-450",
  publisher = "Springer-Verlag",
  isbn = "3-540-56868-9"
}

@inproceedings{Butl90,
  author = "Butler, Greg and Cannon, John",
  title = {{The Design of Cayley -- A Language for Modern Algebra}},
  booktitle = "DISCO 1990",
  year = "1990",
  pages = "10-19",
  abstract =
    "Established practice in the domain of modern algebra has shaped the
    design of Cayley. The design has also been responsive to the needs of
    its users. The requirements of the users include consistency with
    common mathematical notation; appropriate data types such as sets,
    sequences, mappings, algebraic structures and elements; efficiency;
    extensibility; power of in-built functions and procedures for known
    algorithms; and access to common examples of algebraic structures. We
    discuss these influences on the design of Cayley's user language.",
  paper = "Butl90.pdf",
  keywords = "axiomref"
}

@misc{Cant16,
  author = "Cantrill, Bryan",
  title = {{Oral Tradition in Software Engineering}},
  link = "\url{https://www.youtube.com/watch?v=4PaWFYm0kEw}",
  year = "2016"
}

@article{Card85,
  author = "Cardelli, Luca and Wegner, Peter",
  title = {{On Understanding Types, Data Abstraction, and Polymorphism}},
  journal = "ACM Computing Surveys",
  volume = "17",
  number = "4",
  year = "1985",
  pages = "471-523",
  abstract =
    "Our objective is to understand the notion of type in programming
    languages, present a model of typed, polymorphic programming languages
    that reflects recent research in type theory, and examine the
    relevance of recent research to the design of practical programming
    languages. 

    Object-oriented languages provide both a framework and a
    motivation for exploring the interaction among the concepts of type,
    data abstraction, and polymorphism, since they extend the notion of
    type to data abstraction and since type inheritance is an important
    form of polymorphism. We develop a $\lambda$-calculus-based model for type
    systems that allows us to explore these interactions in a simple
    setting, unencumbered by complexities of production programming
    languages. 

    The evolution of languages from untyped universes to
    monomorphic and then polymorphic type systems is reviewed. Mechanisms
    for polymorphism such as overloading, coercion, subtyping, and
    parameterization are examined. A unifying framework for polymorphic
    type systems is developed in terms of the typed $\lambda$-calculus
    augmented to include binding of types by quantification as well as
    binding of values by abstraction. 

    The typed $\lambda$-calculus is
    augmented by universal quantification to model generic functions with
    type parameters, existential quantification and packaging (information
    hiding) to model abstract data types, and bounded quantification to
    model subtypes and type inheritance. In this way we obtain a simple
    and precise characterization of a powerful type system that includes
    abstract data types, parametric polymorphism, and multiple inheritance
    in a single consistent framework. The mechanisms for type checking for
    the augmented $\lambda$-calculus are discussed. 

    The augmented typed
    $\lambda$-calculus is used as a programming language for a variety of
    illustrative examples. We christen this language Fun because fun
    instead of $\lambda$ is the functional abstraction keyword and because it
    is pleasant to deal with. 

    Fun is mathematically simple and can serve
    as a basis for the design and implementation of real programming
    languages with type facilities that are more powerful and expressive
    than those of existing programming languages. In particular, it
    provides a basis for the design of strongly typed object-oriented
    languages",
  paper = "Card85.pdf",
  keywords = "printed"
}

@inproceedings{Card86,
  author = "Cardelli, Luca",
  title = {{Typechecking Dependent Types and Subtypes}},
  link = 
   "\url{http://lucacardelli.name/Papers/Dependent%20Typechecking.US.pdf}",
  booktitle = "Foundations of Logic and Functional Programming",
  year = "1996",
  journal = "LNCS",
  volume = "523",
  pages = "45-57",
  paper = "Card86.pdf"
}

@article{Card88,
  author = "Cardelli, Luca",
  title = {{A Semantics of Multiple Inheritance}},
  journal = "Information and Computation",
  volume = "76",
  number = "2-3",
  year = "1988",
  pages = "138-164",
  paper = "Card88.pdf"
}

@article{Card91,
  author = "Cardelli, Luca and Longo, Giuseppe",
  title = {{A Semantic Basis for Quest}},
  journal = "J. of Functional Programming",
  volume = "1",
  number = "4",
  pages = "417-458",
  year = "1991",
  abstract =
    "Quest is a programming language based on impredicative type
    quantifiers and subtyping within a three-level structure of kinds,
    types and type operators, and values.
    
    The semantics of Quest is rather challenging. In particular,
    difficulties arise when we try to model simultaneously features such
    as contravariant function spaces, record types, subtyping, recursive
    types and fixpoints.
    
    In this paper we describe in detail the type inference rules for
    Quest, and give them meaning using a partial equivalence relation
    model of types. Subtyping is interpreted as in previous work by Bruce
    and Longo (1989), but the interpretation of some aspects – namely
    subsumption, power kinds, and record subtyping – is novel. The latter
    is based on a new encoding of record types.
    
    We concentrate on modelling quantifiers and subtyping; recursion is
    the subject of current work.",
  paper = "Card91.pdf"
}

@book{Chan90,
  author = "Chang, C.C. and Keisler, H. Jerome",
  title = {{Model Theory}},
  publisher = "North Holland",
  year = "1990",
  comment = "Studics in Logic and the Foundations of Mathematics",
  volume = "73",
  abstract = 
    "Since the second edition of this book (1977), Model Theory has
    changed radically, and is now concerned with fields such as
    classification (or stability) theory, nonstandard analysis,
    model-theoretic algebra, recursive model theory, abstract model
    theory, and model theories for a host of nonfirst order logics. Model
    theoretic methods have also had a major impact on set theory,
    recursion theory, and proof theory.
    
    This new edition has been updated to take account of these changes,
    while preserving its usefulness as a first textbook in model
    theory. Whole new sections have been added, as well as new exercises
    and references. A number of updates, improvements and corrections have
    been made to the main text"
}

@book{Char91,
  author = "Char, Bruce and Geddes, Keith O. and Gonnet, Gaston H. and
            Leong, Benton and Monagan, Michael B. and Watt, Stephen M.",
  title = {{Maple V Language Reference Manual}},
  publisher = "Springer",
  year = "1991",
  isbn = "978-0-387-94124-0"
}

@book{Char91a,
  author = "Char, Bruce and Geddes, Keith O. and Gonnet, Gaston H. and
            Leong, Benton and Monagan, Michael B. and Watt, Stephen M.",
  title = {{Maple V Library Reference Manual}},
  publisher = "Springer",
  year = "1991",
  isbn = "978-1-4757-2133-1",
  abstract =
    "The design and implementation of the Maple system is an on-going
    project of the Symbolic Com­ putation Group at the University of
    Waterloo in Ontario, Canada. This manual corresponds with version V
    (roman numeral five) of the Maple system. The on-line help subsystem
    can be invoked from within a Maple session to view documentation on
    specific topics. In particular, the command ?updates points the user
    to documentation updates for each new version of Maple. The Maple
    project was first conceived in the autumn of 1980, growing out of
    discussions on the state of symbolic computation at the University of
    Waterloo. The authors wish to acknowledge many fruitful discussions
    with colleagues at the University of Waterloo, particularly Morven
    Gen­ tleman, Michael Malcolm, and Frank Tompa. It was recognized in
    these discussions that none ofthe locaIly-available systems for
    symbolic computation provided the facilities that should be expected
    for symbolic computation in modern computing environments. We
    concluded that since the basic design decisions for the then-current
    symbolic systems such as ALTRAN, CAMAL, REDUCE, and MACSYMA were based
    on 1960's computing technology, it would be wise to design a new
    system ``from scratch//. Thus we could take advantage of the software
    engineering technology which had become available in recent years, as
    well as drawing from the lessons of experience. Maple's basic features
    (elementary data structures, Input/output, arithmetic with numbers,
    and elementary simplification) are coded in a systems programming
    language for efficiency."
}

@inproceedings{Chen92,
  author = "Chen, Kung and Hudak, Paul and Odersky, Martin",
  title = {{Parametric Type Classes}},
  booktitle = "Proc. ACM Conf. on LISP and Functional Programming",
  year = "1992",
  pages = "170-181",
  abstract = 
    "We propose a generalization to Haskell's type classes where a class
    can have type parameters besides the placeholder variable. We show
    that this generalization is essential to represent container classes
    with overloaded data constructor and selector operations. We also show
    that the resulting type system has principal types and present
    unification and type reconstruction algorithms.",
  paper = "Chen92.pdf"
}

@techreport{Coll90,
  author = "Collins, George E. and Loos, Rudiger",
  title = {{Specification and Index of SAC-2 Algorithms}},
  institution = "Univ. of Tubingen",
  type = "technical report",
  year = "1990",
  number = "WSI-90-4"
}

@inproceedings{Como90,
  author = "Comon, Hubert",
  title = {{Equational Formulas in Order-sorted Algebras}},
  booktitle = "IICALP 90. Automata, Languages and Programming",
  year = "1990",
  pages = "674-688",
  abstract =
    "We propose a set of transformation rules for first order formulas
    whose atoms are either equations between terms or “sort constraints” t
    ε s where s is a regular tree language (or a sort in the algebraic
    specification community). This set of rules is proved to be correct,
    terminating and complete. This shows in particular that the first
    order theory of any rational tree language is decidable, extending the
    results of [Mal71,CL89,Mah88]. We also show how to apply our results
    to automatic inductive proofs in equational theories."
}

@article{Como91,
  author = "Comon, Hubert and Lugiez, D. and Schnoebelen, Ph.",
  title = {{A Rewrite-based Type Discipline for a Subset of Computer Algebra}},
  journal = "J. Symbolic Computation",
  volume = "11",
  number = "4",
  year = "1991",
  pages = "349-368",
  abstract =
    "This paper is concerned with the type structure of a system including
    polymorphism, type properties and subtypes. This type system
    originates from computer algebra but it is not intended to be the
    solution of all type problems in this area.
    
    Types (or sets of types) are denoted by terms in some order-sorted
    algebra. We consider a rewrite relation in this algebra, which is
    intended to express subtyping. The relations between the semantics and
    the axiomatization are investigated. It is shown that the problem of
    type inference is undecidable but a narrowing strategy for
    semi-decision procedures is described and studied.",
  paper = "Como91.pdf"
}

@article{Cool92,
  author = "Coolsaet, Kris",
  title = {{A Quick Introduction to the Programming Language MIKE}},
  journal = "Sigplan Notices",
  volume = "27",
  number = "6",
  year = "1992",
  pages = "37-48",
  abstract =
    "MIKE is a new programming language developed by the author as a base
    language for the development of algebraic and symbolic algorithms. It
    is a structured programming language with a MODULA-2-like syntax
    supporting special features such as transparent dynamic memory
    management, discriminated union types, operator overloading, data
    abstraction and parametrized types. This text gives an overview of the
    main features of the language as of version 2.0."
}

@inproceedings{Dama82,
  author = "Damas, Luis and Milner, Robin",
  title = {{Principal Type-schemes for Functional Programs}},
  booktitle = "POPL 82",
  pages = "207-212",
  year = "1982",
  isbn = "0-89798-065-6",
  paper = "Dama82.pdf"
}

@book{Davi94,
  author = "Davis, Martin D. and Sigal, Ron and Weyuker, Elaine J.",
  title = {{Computability, Complexity, and Languages: Fundamentals of
           Theoretical Computer Science}},
  publisher = "Academic Press",
  year = "1994",
  isbn = "978-0122063824"
}

@techreport{Ders89,
  author = "Dershowitz, Nachum and Jouannaud, Jean-Pierre",
  title = {{Rewrite Systems}},
  year = "1989",
  number = "478",
  institution = "Laboratoire de Recherche en Informatique",
  link = "\url{http://www.cs.tau.ac.il/~nachum/papers/survey-draft.pdf}",
  paper = "Ders89.pdf",
  keywords = "printed"
}

@book{Ehri85,
  author = "Ehrig, Hartmut and Mahr, Bernd",
  title = {{Fundamentals of Algebraic Specification 1: Equations and
           Initial Semantics}},
  publisher = "Springer Verlag",
  year = "1985",
  isbn = "978-0387137186"
}

@inproceedings{Elli89,
  author = "Elliott, Conal M.",
  title = {{Higher-order Unification with Dependent Function Types}},
  booktitle = "Rewriting Techniques and Applications",
  year = "1989",
  pages = "121-136",
  abstract =
    "Roughly fifteen years ago, Huet developed a complete semidecision
    algorithm for unification in the simply typed $\lambda$-calculus
    ($\lambda_\rightarrow$). In spite of the undecidability of this 
    problem, his algorithm is quite usable in practice. Since then, many
    important applications have come about in such areas as theorem
    proving, type inference, program transformation, and machine learning.
    
    Another development is the discovery that by enriching 
    $\lambda_\rightarrow$ to include {\sl dependent function types},
    the resulting calculus ($\lambda_\Pi$) forms the basis of a very
    elegant and expressive Logical Framework, encompassing the syntax,
    rules, and proofs for a wide class of logics.
    
    This paper presents an algorithm in the spirit of Huet's, for
    unification in $\lambda_\Pi$. This algorithm gives us the best
    of both worlds: the automation previously possible in
    $\lambda_\rightarrow$ and the greatly enriched expressive power of
    $\lambda_\Pi$. It can be used to considerable advantage in many
    of the current applications of Huet's algorithm, and has important
    new applications as well. These include automated and semi-automated
    theorem proving in encoded logics, and automatic type inference in a
    variety of encoded languages."
}

@article{Farm90,
  author = "Farmer, William M.",
  title = {{A Partial Functions Version of Church's Simple Theory of Types}},
  journal = "The Journal of Symbolic Logic",
  volume = "55",
  number = "3",
  year = "1990",
  pages = "1269-1291",
  abstract =
    "Church's simple theory of types is a system of higher-order logic in
     which functions are assumed to be total. We present in this paper a
     version of Church's system called PF in which functions may be
     partial. The semantics of PF, which is based on Henkin's
     general-models semantics, allows terms to be nondenoting but requires
     formulas to always denote a standard truth value. We prove that PF is
     complete with respect to its semantics. The reasoning mechanism in PF
     for partial functions corresponds closely to mathematical practice,
     and the formulation of PF adheres tightly to the framework of
     Church's system.",
  paper = "Farm90.pdf"
}

@article{Farm95,
  author = "Farmer, William M. and Guttman, J.D. and Thayer, F.J.",
  title = {{Contexts in Mathematical Reasoning and Computation}},
  journal = "J. of Symbolic Computation",
  volume = "19",
  pages = "201-216",
  year = "1995",
  abstract =
    "Contexts are sets of formulas used to manage the assumptions that
    arise in the course of a mathematical deduction or
    calculation. Although context-dependent reasoning is commonplace in
    informal mathematics, most contemporary symbolic computation systems
    do not utilize contexts in sophisticated ways. This paper describes
    some context-based techniques for symbolic computation, including
    techniques for reasoning about definedness, simplifying abstract
    algebraic expressions, and computing with theorems. All of these
    techniques are implemented in the IMPS Interactive Mathematical Proof
    System. The paper also proposes a general mathematics laboratory that
    combines the functionality of current symbolic computation systems
    with the facilities of a theorem proving system like IMPS.",
  paper = "Farm95.pdf",
  keywords = "printed"
}

@misc{Farm95a,
  author = "Farmer, William M. and Guttman, Joshua D. and
            F. Javier Thayer",
  title = {{The IMPS User's Manual}},
  link = "\url{http://imps.mcmaster.ca/manual/manual.html}",
  year = "1995"
}

@article{Faxe02,
  author = "Faxen, Karl-Filip",
  title = {{A Static Sematics for Haskell}},
  year = "2002",
  journal = "J. Functional Programming",
  volume = "12",
  number = "4-5",
  pages = "295-357",
  abstract = 
    "This paper gives a static semantics for Haskell 98, a non-strict
    purely functional programming language. The semantics formally speci
    es nearly all the details of the Haskell 98 type system, including the
    resolution of overloading, kind inference (including defaulting) and
    polymorphic recursion, the only major omission being a proper
    treatment of ambiguous overloading and its resolution. Overloading is
    translated into explicit dictionary passing, as in all current
    implementations of Haskell. The target language of this translation is
    a variant of the Girard-Reynolds polymorphic lambda calculus featuring
    higher order polymorphism and explicit type abstraction and
    application in the term language. Translated programs can thus still
    be type checked, although the implicit version of this system is
    impredicative. A surprising result of this formalization e ort is that
    the monomorphism restriction, when rendered in a system of inference
    rules, compromises the principal type property.",
  paper = "Faxe02.pdf"
}

@misc{Fija17,
  author = "Fijalkow, Nathanael",
  title = 
   {{Computing using the generators of a group: the Schreier-Sims' algorithm}},
  year = "2017",
  link = "\url{https://www.cs.ox.ac.uk/blogs/nathanael-fijalkow/2016/01/27/computing-using-the-generators-of-a-group/}"
}

@phdthesis{Fode83,
  author = "Foderaro, John K.",
  title = {{The Design of a Language for Algebraic Computation Systems}},
  school = "U.C. Berkeley, EECS Dept.",
  year = "1983",
  link = "\url{http://digitalassets.lib.berkeley.edu/techreports/ucb/text/CSD-83-160.pdf}",
  abstract =
    "This thesis describes the design of a language to support a
    mathematics-oriented symbolic algebra system. The language, which we
    have named NEWSPEAK, permits the complex interrelations of
    mathematical types, such as rings, fields and polynomials to be
    described. Functions can be written over the most general type that
    has the required operations and properties and the inherited by
    subtypes. All function calls are generic, with most function
    resolution done at compile time. Newspeak is type-safe, yet permits
    runtime creation of tyhpes.",
  paper = "Fode83.pdf",
  keywords = "axiomref"
}

@inproceedings{Frue91,
  author = "Fruehwirth, Thom and Shapiro, Ehud and Vardi, Moshe Y. and
            Yardeni, Eyal",
  title = {{Logic programs as types for logic programs}},
  booktitle = "Proc. Sixth Annual IEEE Symp. on Logic in Comp. Sci.",
  publisher = "IEEE",
  pages = "300-309",
  year = "1991",
  abstract = 
    "Type checking can be extremely useful to the program development process.
    Of particular interest are descriptive type systems, which let the
    programmer write programs without having to define or mention types.
    We consider here optimistic type systems for logic programs. In such
    systems types are conservative approximations to the success set of the
    program predicates. We propose the use of logic programs to describe
    types. We argue that this approach unifies the denotational and
    operational approaches to descriptive type systems and is simpler
    and more natural than previous approaches. We focus on the use of
    unary-predicate programs to describe types. We identify a proper class
    of unary-predicate programs and show that it is expressive enough to
    express several notions of types. We use an analogy with 2-way automata
    and a correspondence with alternating algorithms to obtain a complexity
    characterization of type inference and type checking. This
    characterization was facilitated by the use of logic programs to 
    represent types.",
  paper = "Frue91.pdf",
  keywords = "printed"
}

@article{Fuhx89,
  author = "Fuh, You-Chin and Mishra, Prateek",
  title = {{Polymorphic Subtype Inference -- Closing the Theory-Practice Gap}},
  journal = "Lecture Notes in Computer Science",
  volume = "352",
  year = "1989",
  pages = "167-183",
  paper = "Fuhx89.pdf"
}

@article{Fuhx90,
  author = "Fuh, You-Chin",
  title = {{Type Inference with Subtypes}},
  journal = "Theoretical Computer Science",
  volume = "73",
  number = "2",
  year = "1990",
  pages = "155-175",
  abstract =
    "We extend polymorphic type inference with a very general notion of
    subtype based on the concept of type transformation. This paper
    describes the following results. We prove the existence of (i)
    principal type property and (ii) syntactic completeness of the
    type-checker, for type inference with subtypes. This result is
    developed with only minimal assumptions on the underlying theory of
    subtypes. As a consequence, it can be used as the basis for type
    inference with a broad class of subtype theories. For a particular
    “structural” theory of subtypes, those engendered by inclusions
    between type constants only, we show that principal types are
    compactly expressible. This suggests that type inference for the
    structured theory of subtypes is feasible. We describe algorithms
    necessary for such a system. The main algorithm we develop is called
    MATCH, an extension to the classical unification algorithm. A proof of
    correctness for MATCH is given.",
  paper = "Fuhx90.pdf"
}

@misc{GAPx17,
  author = "The GAP Group",
  title = {{GAP - Reference Manual}},
  year = "2017",
  link = "\url{https://www.gap-system.org/Manuals/doc/ref/manual.pdf}"
}

@phdthesis{Gira72,
  author = "Girard, Jean-Yves",
  title = {Intrpr\'etation fontionelle et \'elimination des coupures de
           l'arithm\'etique d'orde sup\'erieur},
  school = {Universit\'e Paris VII},
  year = "1972"
}

@book{Gira89,
  author = "Girard, Jean-Yves",
  title = {{Proofs and Types}},
  publisher = "Cambridge University Press",
  year = "1989"
}  

@misc{Gode58,
  author = "Godel, Kurt",
  title = {\"Uber eine bisher noch nicht benutzte Erweiterung des Finiten
           Standpunktes},
  journal = "Dialectica 12",
  year = "1958",
  pages = "280-287"
}

@techreport{Gogu89,
  author = "Goguen, Joseph and Meseguer, Jose",
  title = {{Order-sorted Algebra I : Equational Deduction for Multiple
           Inheritance, Overloading, Exceptions, and Partial Operations}},
  type = "technical report",
  institution = "SRI International",
  year = "1989",
  number = "SRIR 89-10"
}

@article{Gogu92,
  author = "Goguen, Joseph and Meseguer, Jose",
  title = {{Order-sorted Algebra I : Equational Deduction for Multiple
           Inheritance, Overloading, Exceptions, and Partial Operations}},
  journal = "Theoretical Computer Science",
  volume = "105",
  number = "2",
  year = "1992",
  pages = "217-273",
  abstract =
    "This paper generalizes many-sorted algebra (MSA) to order-sorted
    algebra (OSA) by allowing a partial ordering relation on the set of
    sorts. This supports abstract data types with multiple inheritance (in
    roughly the sense of object-oriented programming), several forms of
    polymorphism and overloading, partial operations (as total on
    equationally defined subsorts), exception handling, and an operational
    semantics based on term rewriting. We give the basic algebraic
    constructions for OSA, including quotient, image, product and term
    algebra, and we prove their basic properties, including quotient,
    homomorphism, and initiality theorems. The paper's major mathematical
    results include a notion of OSA deduction, a completeness theorem for
    it, and an OSA Birkhoff variety theorem. We also develop conditional
    OSA, including initiality, completeness, and McKinsey-Malcev
    quasivariety theorems, and we reduce OSA to (conditional) MSA, which
    allows lifting many known MSA results to OSA. Retracts, which
    intuitively are left inverses to subsort inclusions, provide
    relatively inexpensive run-time error handling. We show that it is
    safe to add retracts to any OSA signature, in the sense that it gives
    rise to a conservative extension. A final section compares and
    contrasts many different approaches to OSA. This paper also includes
    several examples demonstrating the flexibility and applicability of
    OSA, including some standard benchmarks like stack and list, as well
    as a much more substantial example, the number hierarchy from the
    naturals up to the quaternions.",
  paper = "Gogu92.pdf"
}

@book{Gold83,
  author = "Goldberg, Adele and Robson, David",
  title = {{Smalltalk-80: The Language and Its Implementation}},
  publisher = "Addison-Wesley",
  year = "1983"
}

@article{Goll90,
  author = "Gollan, H. and Grabmeier, J.",
  title = {{Algorithms in Representation Theory and their
           Realization in the Computer Algebra System Scratchpad}},
  journal = "Bayreuther Mathematische Schriften",
  volume = "33",
  year = "1990",
  pages = "1-23",
  algebra = "\newline\refto{package REP1 RepresentationPackage1}",
  keywords = "axiomref"
}

@article{Gons71,
  author = "Gonshor, H.",
  title = {{Contributions to Genetic Algebras}},
  journal = "Proc. Edinburgh Mathmatical Society (Series 2)",
  volume = "17",
  number = "4",
  month = "December",
  year = "1971",
  issn = "1464-3839",
  pages = "289--298",
  doi = "10.1017/S0013091500009548",
  link = "\url{http://journals.cambridge.org/article_S0013091500009548}",
  algebra = "\newline\refto{domain ALGSC AlgebraGivenByStructuralConstants}",
  abstract =
    "Etherington introduced certain algebraic methods into the study of
    population genetics. It was noted that algebras arising in genetic
    systems tend to have certain abstract properties and that these can be
    used to give elegant proofs of some classical stability theorems in
    population genetics."
}

@misc{Gowe17,
  author = "Gowers, Timothy",
  title = {{Group actions II: the orbit-stabilizer theorem}},
  year = "2017",
  link = "\url{https://gowers.wordpress.com/2011/11/09/group-actions-ii-the-orbit-stabilizer-theorem/}"
}

@article{Grab87,
  author = "Grabmeier, Johannes and Kerber, Adalbert",
  title = {{The Evaluation of Irreducible Polynomial 
           Representations of the General Linear Groups
           and of the Unitary Groups over Fields of 
           Characteristic 0}},
  journal = "Acta Applicandae Mathematica",
  volume = "8",
  year = "1987",
  pages = "271-291",
  algebra = "\newline\refto{package REP1 RepresentationPackage1}",
  abstract = 
    "We describe an efficient method for the computer evaluation of the
    ordinary irreducible polynomial representations of general linear
    groups using an integral form of the ordinary irreducible
    representations of symmetric groups. In order to do this, we first
    give an algebraic explanation of D. E. Littlewood's modification of
    I. Schur's construction. Then we derive a formula for the entries of
    the representing matrices which is much more concise and adapted to
    the effective use of computer calculations. Finally, we describe how
    one obtains — using this time an orthogonal form of the ordinary
    irreducible representations of symmetric groups — a version which
    yields a unitary representation when it is restricted to the unitary
    subgroup. In this way we adapt D. B. Hunter's results which heavily
    rely on Littlewood's methods, and boson polynomials come into the play
    so that we also meet the needs of applications to physics.",
  keywords = "axiomref"
}

@book{Grie78,
  author = "Gries, David",
  title = {{Programming Methodology}},
  publisher = "Springer-Verlag",
  year = "1978"
}

@book{Grae79,
  author = "Graetzer, George",
  title = {{Universal Algebra}},
  publisher = "Springer",
  isbn = "978-0-387-77486-2",
  year = "1979",
  paper = "Grae79.pdf"
}

@article{Harp93,
  author = "Harper, Robert and Honsell, Furio and Plotkin, Gordon",
  title = {{A Framework for Defining Logics}},
  journal = "J. ACM",
  volume = "40",
  number = "1",
  year = "1993",
  pages = "143-184",
  abstract =
    "The Edinburgh Logical Framework (LF) provides a means to define (or
    present) logics. It is based on a general treatment of syntax, rules,
    and proofs by means of a typed $\lambda$-calculus with dependent
    types. Syntax is treated in a style similar to, but more general than,
    Martin-Lof's system of arities. The treatment of rules and proofs
    focuses on his notion of a judgment. Logics are represented in LF via
    a new principle, the judgments as types principle, whereby each
    judgment is identified with the type of its proofs. This allows for a
    smooth treatment of discharge and variable occurence conditions and
    leads to a uniform treatment of rules and proofs whereby rules are
    viewed as proofs of higher-order judgments and proof checking is
    reduced to type checking. The practical benefit of our treatment of
    formal systems is that logic-independent tools, such as proof editors
    and proof checkers, can be constructed.",
  paper = "Harp93.pdf"
}

@article{Harp93a,
  author = "Harper, Robert and Mitchell, John C.",
  title = {{On the Type Structure of Standard ML}},
  journal = "Transactions on Programming Languages and Systems",
  publisher = "ACM",
  volume = "15",
  number = "2",
  pages = "211-252",
  year = "1993",
  abstract =
    "Standard ML is a useful programming module facility. One notable
    feature of the core expression language of ML is that it is implictly
    typed: no explicit type information need be supplied by the
    programmer.  In contrast, the module language of ML is explicitly
    typed; in particular, the types of parameters in parametric
    modules must be supplied by the programmer. We study the type
    structure of Standard ML by giving an explicitly-typed,
    polymmorphic function calculus that captures many of the essential
    aspects of both the core and module language. In this setting,
    implicitly-type core language expressions are reguarded as a
    convenient short-hand for an explicitly-typed counterpart in our
    function calculus. In contrast to the Girard-Reynolds polymorphic
    calculus, our function calculus is predicative: the type system
    may be built up by induction on type levels. We show that, in a
    precise sense, the language becomes inconsistent if restrictions
    imposed by type levels are relaxed. More specifically, we prove
    that the important programming features of ML cannot be added to
    any impredicative language, such as the Girard-Reynolds calcus,
    without implicitly assuming a type of all types.",
  paper = "Harp93a.pdf"
}

@article{Hind69,
  author = "Hindley, R.",
  title = {{The Principal Type-Scheme of an Object in Combinatory Logic}},
  journal = "Trans. AMS",
  volume = "146",
  year = "1969",
  pages = "29-60",
  paper = "Hind69.pdf",
  keywords = "printed"
}

@article{Hodg95,
  author = "Hodges, Wilfrid",
  title = {{The Meaning of Specifications I: Domains and Initial Models}},
  journal = "Theoretical Computer Science",
  volume = "192",
  issue = "1",
  year = "1995",
  pages = "67-89",
  abstract = 
    "This is the first of a short series of papers intended to provide one
    common semantics for several different types of specification
    language, in order to allow comparison and translations. The
    underlying idea is that a specification describes the behaviour of a
    system, depending on parameters. We can represent this behaviour as a
    functor which acts on structures representing the parameters, and
    which yields a structure representing the behaviour. We characterise
    in domain-theoretic terms the class of functors which could in
    principle be specified and implemented; briefly, they are the functors
    which preserve directed colimits and whose restriction to finitely
    presented structures is recursively enumerable. We also characterise
    those functors which allow specification by initial semantics in
    universal Horn classes with finite vocabulary; these functors consist
    of a free functor (i.e. left adjoint of a forgetful functor) followed
    by a forgetful functor. The main result is that these two classes of
    functor are the same up to natural isomorphism.",
  paper = "Hodg95.pdf"
}

@techreport{Howe87,
  author = "Howe, Douglas J.",
  title = {{The Computational Behaviour of Girard's Paradox}},
  institution = "Cornell University",
  year = "1987",
  link = "\url{https://ecommons.cornell.edu/handle/1813/6660}",
  number = "TR 87-820",
  abstract = 
    "In their paper ``Type'' Is Not a Type, Meyer and Reinhold argued that
    serious pathologies can result when a type of all types is added to a
    programing language with dependent types. Central to their argument is
    the claim that by following the proof of Girard's paradox it is
    possible to construct in their calculus $\lambda^{\tau \tau}$ a term
    having a fixed-point property. Because of the tremendous amount of
    formal detail involved, they were unable to establish this claim. We
    have made use of the Nuprl proof development system in constructing a
    formal proof of Girard's paradox and analysing the resulting term. We
    can show that the term does not have the desired fixed-point property,
    but does have a weaker form of it that is sufficient to establish some
    of the results of Meyer and Reinhold. We believe that the method used
    here is in itself of some interest, representing a new kind of
    application of a computer to a problem in symbolic logic."
}

@misc{Howe69,
  author = "Howard, W.H.",
  title = {{The Formulae-as-Types Notion of Construction}},
  year = "1969",
  link = "\url{}",
  abstract =
    "The following consists of notes which were privately circulated in
    1969. Since they have been referred to a few times in the literature,
    it seems worth while to publish them. They have been rearranged for
    easier reading, and some inessential corrections have been made.
    
    The ultimate goal was to develop a notion of construction suitable for
    the interpretation of intuitionistic mathematics. The notion of
    construction developed in the notes is certainly too crude for that,
    so the use of the word construction is not very appropriate. However,
    the terminology has been kept in order to preserve the original title
    and also to preserve the character of the notes. The title has a
    second defect; namely, a type should be regarded as a abstract object
    whereas a formula is the name of a type.
    
    In Part I the ideas are illustrated for the intuitionistic
    propositional calculus and in Part II (page 6) they are applied to
    Heyting arithmetic.",
  paper = "Howe69.pdf"
}

@article{Huda92,
  author = "Hudak, Paul and Jones, Simon Peyton and Wadler, Philip and
            Boutel, Brian and Fairbairn, Jon and Fasel, Joseph and
            Guzman, Maria M. and Hammond, Kevin and Hughes, John and
            Johnsson, Thomas and Kieburtz, Dick and Nikhil, Rishiyur and
            Patrain, Will and Peterson, John",
  title = {{Report on the Programming Language Haskell, a non-strict
           functional language version 1.2}},
  journal = "ACM SIGPLAN Notices",
  volume = "27",
  number = "5",
  year = "1992",
  pages = "1-164",
  abstract =
    "Some half dozen persons have written technically on combinatory
    logic, and most of these, including ourselves, have published
    something erroneous. Since some of our fellow sinners are among the
    most careful and competent logicians on the contemporary scene, we
    regard this as evidence that the subject is refractory. Thus fullness
    of exposition is necessory for accurary; and excessive condensation
    would be false economy here, even more than it is ordinarily."
}

@misc{Huda99,
  author = "Hudak, Paul and Peterson, John and Fasel, Joseph H.",
  title = {{A Gentle Introduction to Haskell 98}},
  year = "1999",
  link = "\url{https://www.haskell.org/tutorial/haskell-98-tutorial.pdf}",
  paper = "Huda99.pdf"
}

@book{Huet91,
  author = "Huet, Gerard and Plotkin, G.",
  title = {{Logical Frameworks}},
  publisher = "Cambridge University",
  year = "1991"
}

@misc{Isab11,
  author = "Unknown",
  title = {{Isabell}},
  link = "\url{http://www.cl.cam.ac.uk/research/hvg/Isabelle/index.html}",
  year = "2011"
}

@article{Jame81,
  author = "James, G. and Kerber, A.",
  title = {{The Representation Theory of the Symmetric Group}},
  journal = "Encycl. of Math. and its Appl.",
  volume = "16",
  algebra = "\newline\refto{package REP1 RepresentationPackage1}",
  publisher = "Cambr. Univ. Press",
  year = "1981"
}

@book{Jone87,
  author = "Jones, Simon Peyton",
  title = {{The Implementation of Functional Programming Languages}},
  publisher = "Simon and Schuster",
  year = "1987",
  isbn = "0-13-453333-X",
  paper = "Jone87.pdf"
}

@book{Joua90,
  author = "Jouannaud, Jean-Pierre and Kirchner, Claude",
  title = {{Solving Equations in Abstract Algebras: A Rule-based Survey of
           Unification}},
  year = "1990",
  publisher = "Universite do Paris-Sud"
}

@inproceedings{Joua91,
  author = "Jouannaud, Jean Pierre and Okada, Mitsuhiro",
  title = {{A Computation Model for Executable Higher-order Algebraic
           Specification Languages}},
  booktitle = "Symposium on Logic in Computer Science",
  pages = "350-361",
  isbn = "081862230X",
  year = "1991",
  abstract =
    "The combination of (polymorphically) typed lambda-calculi with
    first-order as well as higher-order rewrite rules is considered. The
    need of such a combination for exploiting the benefits of
    algebraically defined data types within functional programming is
    demonstrated. A general modularity result, which allows as particular
    cases primitive recursive functionals of higher types, transfinite
    recursion of higher types, and inheritance for all types, is
    proved. The class of languages considered is first defined, and it is
    shown how to reduce the Church-Rosser and termination (also called
    strong normalization) properties of an algebraic functional language
    to a so-called principal lemma whose proof depends on the property to
    be proved and on the language considered. The proof of the principal
    lemma is then sketched for various languages. The results allows
    higher order rules defining the higher-order constants by a certain
    generalization of primitive recursion. A prototype of such primitive
    recursive definitions is provided by the definition of the map
    function for lists.",
  paper = "Joua91.pdf"
}

@incollection{Kalt83a,
  author = "Kaltofen, E.",
  title = {{Factorization of Polynomials}},
  booktitle = "Computer Algebra - Symbolic and Algebraic Computation",
  publisher = "ACM",
  pages = "95-113",
  year = "1983",
  abstract =
    "Algorithms for factoring polynomials in one or more variables over
    various coefficient domains are discussed. Special emphasis is given
    to finite fields, the integers, or algebraic extensions of the
    rationals, and to multivariate polynomials with integral coefficients. 
    In particular, various squarefree decomposition algorithms and Hensel 
    lifting techniques are analyzed. An attempt is made to establish a 
    complete historic trace for today’s methods. The exponential worst 
    case complexity nature of these algorithms receives attention.",
  paper = "Kalt83a.pdf"
}

@techreport{Kane90,
  author = "Kanellakis, Paris C. and Mairson, Harry G. and Mitchell, John C.",
  title = {{Unification and ML Type Reconstruction}},
  link = "\url{ftp://ftp.cs.brown.edu/pub/techreports/90/cs90-26.pdf}",
  institution = "Brown University",
  year = "1990",
  number = "CS-90-26",
  abstract =
    "We study the complexity of type reconstruction for a core fragment of
    ML with lambda abstraction, function application, and the polymorphic
    {\bf let} declaration. We derive exponential upper and lower bounds on
    recognizing the typable core ML expressions. Our primary technical
    tool is unification of succinctly represented type expressions. After
    observing that core ML expressions, of size $n$, can be typed in
    DTIME($s^n$), we exhibit two different families of programs whose
    principal types grow exponentially. We show how to exploit the
    expressiveness of the {\bf let}-polymorphism in these constructions to
    derive lower bounds on deciding typability: one leads naturally to
    NP-hardness and the other to DTIME($2^{n^k}$)-hardness for each integer
    $k\ge 1$. Our generic simulation of any exponential time Turing
    Machine by ML type reconstruction may be viewed as a nonstandard way
    of computing with types. Our worse-case lower bounds stand in contrast
    to practical experience, which suggests that commonly used algorithms
    for type reconstruction do not slow compilation substantially.",
  paper = "Kane90.pdf"
}

@inproceedings{Kfou88,
  author = "Kfoury, A.J. and Tiuryn, J. and Utzyczyn, P.",
  title = {{A Proper Extension of ML with an Effective Type-Assignment}},
  booktitle = "POPL 88",
  year = "1988",
  pages = "58-69",
  abstract = 
    "We extend the functional language ML by allowing the recursive calls
    to a function F on the right-hand side of its definition to be at
    different types, all generic instances of the (derived) type of F on
    the left-hand side of its definition. The original definition of ML
    does not allow this feature. This extension does not produce new types
    beyond the usual universal polymorphic types of ML and satisfies the
    properties already enjoyed by ML: the principal-type property and the
    effective type-assignment property.",
  paper = "Kfou88.pdf"
}

@article{Kfou93,
  author = "Kfoury, A. J. and Tiuryn, J. and Urzyczyn, P.",
  title = {{The Undecidability of the Semi-unification Problem}},
  journal = "Information and Computation",
  volume = "102",
  number = "1",
  year = "1993",
  pages = "83-101",
  abstract = 
    "The Semi-Unification Problem (SUP) is a natural generalization of
    both first-order unification and matching. The problem arises in
    various branches of computer science and logic. Although several
    special cases of SUP are known to be decidable, the problem in general
    has been open for several years. We show that SUP in general is
    undecidable, by reducing what we call the ``boundedness problem'' of
    Turing machines to SUP. The undecidability of this boundedness problem
    is established by a technique developed in the mid-1960s to prove
    related results about Turing machines.",
  paper = "Kfou93.pdf"
}

@inproceedings{Kife91,
  author = "Kifer, Michael and Wu, James",
  title = {{A First-order Theory of Types and Polymorphism in Logic
           Programming}},
  booktitle = "Proc Sixth Annual IEEE Symp. on Logic in Comp. Sci.",
  year = "1991",
  pages = "310-321",
  abstract =
    "A logic called typed predicate calculus (TPC) that gives declarative
    meaning to logic programs with type declarations and type inference is
    introduced. The proper interaction between parametric and inclusion
    varieties of polymorphism is achieved through a construct called type
    dependency, which is analogous to implication types but yields more
    natural and succinct specifications. Unlike other proposals where
    typing has extra-logical status, in TPC the notion of type-correctness
    has precise model-theoretic meaning that is independent of any
    specific type-checking or type-inference procedure. Moreover, many
    different approaches to typing that were proposed in the past can be
    studied and compared within the framework of TPC. Another novel
    feature of TPC is its reflexivity with respect to type declarations;
    in TPC, these declarations can be queried the same way as any other
    data. Type reflexivity is useful for browsing knowledge bases and,
    potentially, for debugging logic programs.",
  paper = "Kife91.pdf",
  keywords = "printed"
}

@book{Kirk89,
  author = "Kirkerud, Bjorn",
  title = {{Object-Oriented Programming With Simula}},
  year = "1989",
  series = "International Computer Science Series",
  publisher = "Addison-Wesley"
}

@techreport{Klop90,
  author = "Klop, J. W.",
  title = {{Term Rewriting Systems}},
  institution = "Stichting Methematisch Centrum",
  year = "1990",
  number = "CS-R9073",
  abstract =
    "Term Rewriting Systems play an important role in various areas, such
    as abstract data type specifications, implementations of functional
    programming languages and automated deduction. In this chapter we
    introduce several of the basic concepts and facts for
    TRSs. Specifically, we discuss Abstract Reduction Systems; general
    Term Rewriting Systems including an account of Knuth-Bendix completion
    and (E- )unification; orthogonal TRSs and reduction strategies;
    strongly sequential orthogonal TRS. Finally some extended rewrite
    formates are introduced: Conditional TRSs and Combinatory Reduction
    Systems. The emphasis throughout the paper is on providing information
    of a syntactic nature."
}

@book{Kowa63,
  author = "Kowalsky, Hans Joachim",
  title = {{Linear Algebra}},
  year = "1963",
  publisher = "Walter de Gruyter",
  comment = "(German)"
}

@book{Lang05,
  author = "Lang, Serge",
  title = {{Algebra}},
  publisher = "Springer",
  year = "2005",
  series = "Graduate Texts in Mathematics",
  isbn = "978-0387953854"
}

@InCollection{Laue82,
  author = "Lauer, M.",
  title = {{Computing by Homomorphic Images}},
  booktitle = "Computer Algebra: Symbolic and Algebraic Computation",
  pages = "139-168",
  year = "1982",
  publisher = "Springer",
  isbn = "978-3-211-81684-4",
  abstract =
    "After explaining the general technique of Computing by homomorphic
    images, the Chinese remainder algorithm and the Hensel lifting
    construction are treated extensively. Chinese remaindering is first
    presented in an abstract setting. Then the specialization to Euclidean
    domains, in particular $\mathbb{Z}$, $\mathbb{K}[y]$, and
    $\mathbb{Z}[y_1,\ldots,y_n]$ is considered. For both techniques,
    Chinese remaindering as well as the lifting algorithms, a complete
    computational example is presented and the most frequent application
    is discussed."
}

@inproceedings{Leis87,
  author = "Leiss, Hans",
  title = {{On Type Inference for Object-Oriented Programming Languages}},
  booktitle = "Int. Workshop on Computer Science Logic",
  year = "1987",
  pages = "151-172",
  abstract =
    "We present a type inference calculus for object-oriented programming
    languages. Explicit polymorphic types, subtypes and multiple
    inheritance are allowed. Class types are obtained by selection from
    record types, but not considered subtypes of record types. The subtype
    relation for class types reflects the (mathematically clean)
    properties of subclass relations in object-oriented programming to a
    better extend than previous systems did.
    
    Based on Mitchells models for type inference, a semantics for types is
    given where types are sets of values in a model of type-free lambda
    calculus. For the sublanguage without type quantifiers and subtype
    relation, automatic type inference is possible by extending Milners
    algorithm W to deal with a polymorphic fixed-point rule."
}

@article{Limo92,
  author = "Limongelli, C. and Temperini, M.",
  title = {{Abstract Specification of Structures and Methods in Symbolic
           Mathematical Computation}},
  journal = "Theoretical Computer Science",
  volume = "104",
  year = "1992",
  pages = "89-107",
  abstract = 
    "This paper describes a methodology based on the object-oriented
    programming paradigm, to support the design and implementation of a
    symbolic computation system. The requirements of the system are
    related to the specification and treatment of mathematical
    structures. This treatment is considered from both the numerical and
    the symbolic points of view. The resulting programming system should
    be able to support the formal definition of mathematical data
    structures and methods at their highest level of abstraction, to
    perform computations on instances created from such definitions, and
    to handle abstract data structures through the manipulation of their
    logical properties. Particular consideration is given to the
    correctness aspects. Some examples of convenient application of the
    proposed design methodology are presented.",
  paper = "Limo92.pdf"
}

@inproceedings{Linc92,
  author = "Lincoln, Patrick and Mitchell, John C.",
  title = {{Algorithmic Aspects of Type Inference with Subtypes}},
  booktitle = "POPL 92",
  pages = "293-304",
  year = "1992",
  abstract =
    "We study the complexity of type inference for programming languages
    with subtypes. There are three language variations that effect the
    problem: (i) basic functions may have polymorphic or more limited
    types, (ii) the subtype hierarchy may be fixed or vary as a result of
    subtype declarations within a program, and (iii) the subtype hierarchy
    may be an arbitrary partial order or may have a more restricted form,
    such as a tree or lattice. The naive algorithm for infering a most
    general polymorphic type, undervariable subtype hypotheses, requires
    deterministic exponential time. If we fix the subtype ordering, this
    upper bound grows to nondeterministic exponential time. We show that
    it is NP-hard to decide whether a lambda term has a type with respect
    to a fixed subtype hierarchy (involving only atomic type names). This
    lower bound applies to monomorphic or polymorphic languages. We give
    PSPACE upper bounds for deciding polymorphic typability if the subtype
    hierarchy has a lattice structure or the subtype hierarchy varies
    arbitrarily. We also give a polynomial time algorithm for the limited
    case where there are of no function constants and the type hierarchy
    is either variable or any fixed lattice.",
  paper = "Linc92.pdf"
}

@inproceedings{Lint10,
  author = "Linton, S. and Hammond, K. and Konovalov, A. and Al Zain, A.D.
            and Trinder, P. and Horn, P.",
  title = {{Easy Compostion of Symbolic Computation Software: A New Lingua
           Franca for Symbolic Computation}},
  booktitle = "Proc. ISSAC 2010",
  publisher = "ACM",
  year = "2010",
  pages = "339-346",
  abstract =
    "We present the results of the first four years of the European
    research project SCIEnce (www.symbolic-computation.org),
    which aims to provide key infrastructure for symbolic computation
    research.  A primary outcome of the project is that we have developed
    a new way of combining computer algebra systems using the Symbolic
    Computation Software Composability Protocol (SCSCP), in which both
    protocol messages and data are encoded in the OpenMath format.  We
    describe SCSCP middleware and APIs, outline some implementations for
    various Computer Algebra Systems (CAS), and show how SCSCP-compliant
    components may be combined to solve scientific problems that can not
    be solved within a single CAS, or may be organised into a system for
    distributed parallel computations.",
  paper = "Lint10.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Loos72,
  author = "Loos, Rudiger",
  title = {{Algebraic Algorithm Descriptions as Programs}},
  journal = "ACM SIGSAM Bulletin",
  volume = "23",
  year = "1972",
  pages = "16-24",
  abstract =
    "We propose methods for writing algebraic programs in an algebraic
    notation. We discuss the advantages of this approach and a specific
    example",
  paper = "Loos72.pdf"
}

@article{Loos76,
  author = "Loos, Rudiger",
  title = {{The Algorithm Description Language (ALDES) (report)}},
  journal = "ACM SIGSAM Bulletin",
  volume = "10",
  number = "1",
  year = "1976",
  pages = "14-38",
  abstract =
    "ALDES is a formalization of the method to describe algorithms used in
    Knuth's books. The largest documentation of algebraic algorithms,
    Collins' SAC system for Computer Algebra, is written in this
    language. In contrast to PASCAL it provides automatic storage
    deallocation. Compared to LISP equal emphasis was placed on efficiency
    of arithmetic, list processing, and array handling. To allow the
    programmer full control of efficiency all mechanisms of the system are
    accessible to him. Currently ALDES is available as a preprocessor to
    ANSI Fortran, using no additional primitives.",
  paper = "Loos76.pdf"
}

@article{Loos74,
  author = "Loos, Ruediger G. K.",
  title = {{Toward a Formal Implementation of Computer Algebra}},
  journal = "SIGSAM",
  volume = "8",
  number = "3",
  pages = "9-16",
  year = "1974",
  abstract =
    "We consider in this paper the task of synthesizing an algebraic
    system. Today the task is significantly simpler than in the pioneer
    days of symbol manipulation, mainly because of the work done by the
    pioneers in our area, but also because of the progress in other areas
    of Computer Science. There is now a considerable collection of
    algebraic algorithms at hand and a much better understanding of data
    structures and programming constructs than only a few years ago.",
  paper = "Loos74.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@book{Loos92,
  author = "Loos, Rudiger and Collins, George E.",
  title = {{Revised Report on the ALgorithm Language ALDES}},
  publisher = "Institut fur Informatik",
  year = "1992"
}

@book{Macl91,
  author = "MacLane, Saunders",
  title = {{Categories for the Working Mathematician}},
  publisher = "Springer",
  year = "1991",
  isbn = "0-387-98403-8",
  link = "\url{http://www.maths.ed.ac.uk/~aar/papers/maclanecat.pdf}",
  paper = "Macl91.pdf"
}

@book{Macl92,
  author = "MacLane, Saunders",
  title = {{Sheaves in Geometry and Logic: A First Introduction to Topos
           Theory}},
  year = "1992",
  isbn = "978-0-387-97710-2",
  publisher = "Springer"
}

@book{Mane76,
  author = "Manes, Ernest G.",
  title = {{Algebraic Theories}},
  publisher = "Springer",
  year = "1976",
  series = "Graduate Texts in Mathematics",
  isbn = "978-1-9860-1"
}

@book{Marc77,
  author = "Marcus, Daniel A.",
  title = {{Number Fields}},
  publisher = "Springer",
  year = "1977",
  isbn = "978-0387902791"
}

@inproceedings{Meye86,
  author = "Meyer, Albert R. and Reinhold, Mark B.",
  title = {{Type is not a type}},
  booktitle = "POPL 86",
  pages = "287-295",
  year = "1986",
  abstract =
    "A function has a dependent type when the type of its result
    depends upon the value of its argument. Dependent types originated in
    the type theory of intuitionistic mathematics and have reappeared
    independently in programming languages such as CLU, Pebble, and
    Russell. Some of these languages make the assumption that there exists
    a type-of-all-types which is its own type as well as the type
    of all other types. Girard proved that this approach is inconsistent
    from the perspective of intuitionistic logic. We apply Girard's
    techniques to establish that the type-of-all-types assumption creates
    serious pathologies from a programming perspective: a system using
    this assumption is inherently not normalizing, term equality is
    undecidable, and the resulting theory fails to be a conservative
    extension of the theory of the underlying base types. The failure of
    conservative extension means that classical reasoning about programs
    in such a system is not sound.",
}

@book{Meye88,
  author = "Meyer, Bertrand",
  title = {{Object-Oriented Software Construction}},
  year = "1988",
  publisher = "Prentice Hall",
  link = "\url{https://sophia.javeriana.edu.co/~cbustaca/docencia/POO-2016-01/documentos/Object%20Oriented%20Software%20Construction-Meyer.pdf}",
  paper = "Meye88.pdf"
}

@book{Miln90,
  author = "Milner, Robin and Torte, Mads and Harper, Robert",
  title = {{The Definition of Standard ML}},
  publisher = "Lab for Foundations of Computer Science, Univ. Edinburgh",
  link = "\url{http://sml-family.org/sml90-defn.pdf}",
  year = "1990",
  paper = "Miln90.pdf"
}

@book{Miln97,
  author = "Milner, Robin and Torte, Mads and Harper, Robert and
            MacQueen, David",
  title = {{The Definition of Standard ML}},
  publisher = "Lab for Foundations of Computer Science, Univ. Edinburgh",
  link = "\url{http://sml-family.org/sml97-defn.pdf}",
  year = "1997",
  paper = "Miln97.pdf",
  keywords = "printed"
}

@book{Miln91,
  author = "Milner, Robin and Torte, Mads",
  title = {{Commentary on Standard ML}},
  publisher = "Lab for Foundations of Computer Science, Univ. Edinburgh",
  link = "\url{https://pdfs.semanticscholar.org/d199/16cbbda01c06b6eafa0756416e8b6f15ff44.pdf}",
  year = "1991",
  paper = "Miln91.pdf",
  keywords = "printed"
}

@article{Mitc91,
  author = "Mitchell, John C.",
  title = {{TYpe Inference with Simple Subtypes}},
  journal = "J. of Functional Programming",
  volume = "1",
  number = "3",
  year = "1991",
  pages = "245-285",
  abstract =
    "Subtyping appears in a variety of programming languages, in the form
    of the ‘automatic coercion’ of integers to reals, Pascal subranges,
    and subtypes arising from class hierarchies in languages with
    inheritance. A general framework based on untyped lambda calculus
    provides a simple semantic model of subtyping and is used to
    demonstrate that an extension of Curry's type inference rules are
    semantically complete. An algorithm G for computing the most general
    typing associated with any given expression, and a restricted,
    optimized algorithm GA using only atomic subtyping hypotheses are
    developed. Both algorithms may be extended to insert type conversion
    functions at compile time or allow polymorphic function declarations
    as in ML.",
  paper = "Mitc91.pdf"
}

@InCollection{Mitc91a,
  author = "Mitchell, John C.",
  title = {{Type Systems for Programming Languages}},
  booktitle = "Handbook of Theoretical Computer Science (Vol B.)",
  pages = "365-458",
  year = "1991",
  publisher = "MIT Press",
  isbn = "0-444-88074-7"
}

@book{Monk76,
  author = "Monk, J. Donald",
  title = {{Mathematical Logic}},
  publisher = "Springer",
  year = "1976",
  isbn = "978-1-4684-9452-5"
}

@incollection{Nada92,
  author = "Nadathur, Gopalan and Pfenning, Frank",
  title = {{The Type System of a Higher-Order Logic Programming Language}},
  booktitle = "Types in Logic Programming",
  isbn = "9780262161312",
  publisher = "MIT Press",
  year = "1992"
}  

@inproceedings{Nipk91,
  author = "Nipkow, Tobias and Snelting, Gregor",
  title = {{Type Classes and Overloading Resolution via Order-Sorted
           Unification}},
  booktitle = "Proc 5th ACM Conf. Functional Prog. Lang. and Comp. Arch.",
  year = "1991",
  publisher = "Springer",
  journal = "LNCS",
  volume = "523",
  pages = "1-14",
  abstract =
    "We present a type inference algorithm for a Haskell-like language
    based on order-sorted unification. The language features polymorphism,
    overloading, type classes and multiple inheritance. Class and instance
    declarations give rise to an order-sorted algebra of types. Type
    inference essentially reduces to the Hindley/Milner algorithm where
    unification takes place in this order-sorted algebra of types. The
    theory of order-sorted unification provides simple sufficient
    conditions which ensure the existence of principal types. The
    semantics of the language is given by a translation into ordinary
    lambda-calculus. We prove the correctness of our type inference
    algorithm with respect to this semantics.",
  paper = "Nipk91.pdf"
}

@book{Odif92,
  author = "Odifreddi, Piergiorgio",
  title = {{Classical Recursion Theory: The Theory of Functions and Sets of
           Natural Numbers}},
  publisher = "Elsevier",
  year = "1992"
}

@article{Pate78,
  author = "Paterson, M. S.",
  title = {{Linear Unification}},
  journal = "J. Computer and System Sciences",
  volume = "16",
  number = "2",
  year = "1978",
  pages = "158-167",
  abstract =
    "A unification algorithm is described which tests a set of expressions
    for unifiability and which requires time and space which are only linear
    in the size of the input",
  paper = "Pate78.pdf"
}

@inproceedings{Pfen91,
  author = "Pfenning, Frank",
  title = {{Logic Programming in the LF Logical Framework}},
  booktitle = "Proc. First Workshop on Logical Frameworks",
  year = "1991",
  paper = "Pfen91.pdf"
}

@inproceedings{Pfen91a,
  author = "Pfenning, Frank",
  title =
    {{Unification and Anti-Unification in the Calculus of Constructions}},
  booktitle = "Logic in Computer Science 91",
  year = "1991",
  pages = "74-85",
  abstract =
    "We present algorithms for unification and anti-unification in the
    Calculus of Constructions, where occurrences of free variables (the
    variables subject to instantiation) are restricted to higher-order
    patterns, a notion investigated for the simply-typed $\lambda$-calculus 
    by Miller. Most general unifiers and least common anti-instances are
    shown to exist and are unique up to a simple equivalence.  The
    unification algorithm is used for logic program execution and type and
    term reconstruction in the current implementation of Elf and has
    shown itself to be practical.  The main application of the
    anti-unification algorithm we have in mind is that of proof
    generalization.",
  paper = "Pfen91a.pdf"
}

@book{Pfen92,
  author = "Pfenning, Frank",
  title = {{Types in Logic Programming}},
  isbn = "9780262161312",
  publisher = "MIT Press",
  year = "1992",
  abstract =
    "Types play an increasingly important role in logic programming, in
    language design as well as language implementation.  We present
    various views of types, their connection, and their role within the
    logic programming paradigm.  

    Among the basic views of types we find
    the so-called descriptive systems, where types describe properties of
    untyped logic programs, and prescriptive systems, where types are
    essential to the meaning of programs.  A typical application of
    descriptive types is the approximation of the meaning of a logic
    program as a subset of the Herbrand universe on which a predicate
    might be true.  The value of prescriptive systems lies primarily in
    program development, for example, through early detection of errors
    in programs which manifest themselves as type inconsistencies, or as
    added documentation for the intended and legal use of predicates.

    Central topics within these views are the problems of type inference
    and type reconstruction, respectively.  Type inference is a form of
    analysis of untyped logic programs, while type reconstruction attempts
    to fill in some omitted type information in typed logic programs and
    generalizes the problem of type checking.  Even though analogous
    problems arise in functional programming, algorithms addressing these
    problems are quite different in our setting.  

    Among the specific forms of types we discuss are simple types,
    recursive types, polymorphic types, and dependent types.  We also
    briefly touch upon subtypes and inheritance, and the role of types
    in module systems for logic programming languages."
}

@phdthesis{Pier91,
  author = "Pierce, Benjamin C.",
  title = {{Programming with Intersection Types and Bounded Polymorphism}},
  school = "Carnegie Mellon University",
  year = "1991",
  comment = "CMU-CS-91-205",
  abstract =
    "Intersection types and bounded quantification are complementary 
    mechanisms for extending the expressive power of statically typed 
    programming languages. They begin with a common framework: a simple, 
    typed language with higher-order functions and a notion of subtyping. 
    Intersection types extend this framework by giving every pair of types
    $\sigma$ and $\tau$ a greatest lower bound, $\sigma \land \tau$,
    corresponding intuitively to the intersection of the sets of values
    described by $\sigma$ and $\tau$. Bounded quantification extends the
    basic framework along a different axis by adding polymorphic functions
    that operate uniformly on all the subtypes of a given type. This thesis
    unifies and extends prior work on intersection types and bounded
    quantification, previously studied only in isolation, by investigating
    theoretical and practical aspects of a typed $\lambda$-calculus
    incorporating both.
    
    The practical utility of this calculus, called $F_\land$ is
    established by examples showing, for instance, that it allows a rich
    form of ``coherent overloading'' and supports an analog of abstract
    interpretation during typechecking; for example, the addition function
    is given a type showing that it maps pairs of positive inputs to a
    positive result, pairs of zero inputs to a zero result, etc. More
    familiar programming examples are presented in terms of an extention
    of Forsythe (an Algol-like language with intersection types),
    demonstrating how parametric polymorphism can be used to simplify and
    generalize Forsythe's design. We discuss the novel programming and
    debugging styles that arise in $F_\land$.
    
    We prove the correctness of a simple semi-decision procedure for the
    subtype relation and the partial correctness of an algorithm for
    synthesizing minimal types of $F_\land$ terms. Our main tool in this
    analysis is a notion of ``canonical types,'' which allows proofs to be
    factored so that intersections are handled separately from the other
    type constructors.
    
    A pair of negative results illustrates some subtle complexities of
    $F_\land$. First, the subtype relation of $F_\land$ is shown to be
    undecidable; in fact, even the sutype relation of pure second-order
    bounded quantification is undecidable, a surprising result in its own
    right. Second, the failure of an important technical property of the
    subtype relation -- the existence of least upper bounds -- indicates
    that typed semantic models of $F_\land$ will be more difficult to
    construct and analyze than the known typed models of intersection
    types. We propose, for future study, some simpler fragments of
    $F_\land$ that share most of its essential features, while recovering
    decidability and least upper bounds.
    
    We study the semantics of $F_\land$ from several points of view. An
    untyped model based on partial equivalence relations demonstrates the
    consistency of the typing rules and provides a simple interpolation
    for programs, where ``$\sigma$ is a subtype of $\tau$'' is read as
    ``$\sigma$ is a subset of $\tau$.'' More refined models can be
    obtained using a translation from $F_\land$ into the pure polymorphic
    $\lambda$-calculus; in these models, ``$\sigma$ is a subtype of
    $\tau$'' is interpreted by an explicit coercion function from $\sigma$
    to $\tau$.  The nonexistence of least upper bounds shows up here in
    the failure of known techniques for proving the coherence of the
    translation semantics.  Finally, an equational theory of equivalences
    between $F_\land$ terms is presented and its soundness for both styles
    of model is verified.",
  paper = "Pier91.pdf" 
}

@techreport{Pier91a,
  author = "Pierce, Benjamin C.",
  title = {{Bounded Quantification is Undecidable}},
  year = "1991",
  number = "CMU-CS-91-161",
  institution = "Carnegie Mellon University",
  link = "\url{http://repository.cmu.edu/cgi/viewcontent.cgi?article=3059}",
  abstract =
    "$F_\le$ is a typed $\lambda$-calculus with subtyping and bounded
    second-order polymorphism. First introduced by Cardelli and Wegner, it
    has been widely studied as a core calculus for type systems with
    subtyping.
    
    Curien and Ghelli proved the partial correctness of a recursive
    procedure for computing minimal types of $F_\le$ terms and showed
    that the termination of this procedure is equivalent to the
    termination of its major component, a procedure for checking the
    subtype relation between $F_\le$ types. Ghelli later claimed that
    this procedure is also guaranteed to terminate, but the discovery of a
    subtle bug in his proof led him recently to observe that, in fact,
    there are inputs on which the subtyping procedure diverges. This
    reopens the question of the decidability of subtyping and hence of
    typechecking.
    
    This question is settled here in the negative, using a reduction from
    the halting problem for two-counter Turing machines to show that the
    subtype relation of $F_\le$ is undecidable.",
  paper = "Pier91a.pdf"
}

@misc{Poiz85,
  author = "Poizat, B.",
  title = {{Cours de Th\'eorie des Mod\'eles}},
  comment = {Nur al-Mantiq wal-Ma'rifah, Villeurbanne, France},
  year = "1985"
}

@misc{Poly11a,
  author = "Unknown",
  title = {{Poly/ML}},
  link = "\url{http://www.polyml.org}",
  year = "2011"
}

@misc{QED94,
  author = "Anonymous",
  title = {{The QED Manifesto}},
  link = "\url{}",
  paper = "QED94.txt",
  keywords = "printed"
}

@InCollection{Rect89,
  author = "Rector, D. L.",
  title = {{Semantics in Algebraic Computation}},
  booktitle = "Computers and Mathematics",
  publisher = "Springer-Verlag",
  year = "1989",
  pages = "299-307",
  isbn = "0-387-97019-3",
  abstract =  
    "I am interested in symbolic computation for theoretical research in
    algebraic topology. Most algebraic computations in topology are hand
    calculations; that is, they can be accomplished by the researcher in
    times ranging from hours to weeks, and they are aimed at discovering
    general patterns rather than producing specific formulas understood in
    advance. Furthermore, the range of algebraic constucts used in such
    calculations is very wide.",
  keywords = "axiomref, printed"
}

@article{Reed97,
  author = "Reed, Mary Lynn",
  title = {{Algebraic Structure of Genetic Inheritance}},
  journal = "Bulletin of the American Mathematical Society",
  year = "1997",
  volume = "34",
  number = "2",
  month = "April",
  pages = "107--130",
  algebra = "\newline\refto{domain ALGSC AlgebraGivenByStructuralConstants}",
  link="\url{http://www.ams.org/bull/1997-34-02/S0273-0979-97-00712-X/S0273-0979-97-00712-X.pdf}",
  abstract =
   "In this paper we will explore the nonassociative algebraic structure
    that naturally ocurs as genetic informatin gets passed down through
    the generations. While modern understanding of genetic inheritance
    initiated with the theories of Charles Darwin, it was the Augustinian
    monk Gregor Mendel who began to uncover the mathematical nature of the
    subject. In fact, the symbolism Mendel used to describe his first
    results (e.g. see his 1866 paper {\sl Experiments in
    Plant-Hybridization} is quite algebraically suggestive. Seventy four
    years later, I.M.H. Etherington introduced the formal language of
    abstract algebra to the study of genetics in his series of seminal
    papers. In this paper we will discuss the concepts of genetics that
    suggest the underlying algebraic structure of inheritance, and we will
    give a brief overview of the algebras which arise in genetics and some
    of their basi properties and relationships. With the popularity of
    biologically motivated mathematics continuing to rise, we offer this
    survey article as another example of the breadth of mathematics that
    has biological significance. The most comprehensive reference for the
    mathematical research done in this area (through 1980) is
    Worz-Busekros.",
  paper = "Reed97.pdf"
}

@inproceedings{Remy89,
  author = "Remy, Didier",
  title = {{Typechecking Records and Variants in a Natural Extension of ML}},
  booktitle = "POPL 89",
  isbn = "978-0-89791-294-5",
  year = "1989",
  publisher = "ACM",
  link = "\url{https://www.cs.cmu.edu/~aldrich/courses/819/row.pdf}",
  abstract =
    "We describe an extension of ML with records where inheritance is
    given by ML generic polymorphism. All common operations on records but
    concatenation are supported, in particular, the free extension of
    records. Other operations such as renaming of fields are added.  The
    solution relies on an extension of ML, where the language of types is
    sorted and considered modulo equations, and on a record extension of
    types. The solution is simple and modular and the type inference
    algorithm is efficient in practice.",
  paper = "Remy89.pdf"
}

@inproceedings{Reyn74,
  author = "Reynolds, John C.",
  title = {{Towards a Theory of Type Structure}},
  booktitle = "Colloquim on Programming",
  year = "1974",
  pages = "408-425",
  paper = "Reyn74.pdf",
  keywords = "printed"
}

@inproceedings{Reyn80,
  author = "Reynolds, John C.",
  title = {{Using Category Theory to Design Implicit Conversions and
           Generic Operators}},
  booktitle = "Lecture Notes in Computer Science",
  year = "1980",
  abstract = 
    "A generalization of many-sorted algebras, called category-sorted
    algebras, is defined and applied to the language-design problem of
    avoiding anomalies in the interaction of implicit conversions and
    generic operators. The definition of a simple imperative language
    (without any binding mechanisms) is used as an example.",
  paper = "Reyn80.pdf, printed"
}

@inproceedings{Reyn84,
  author = "Reynolds, John C.",
  title = {{Polymorphism is not Set-theoretic}},
  booktitle = "Proc Semantics of Data Types",
  pages = "145-156",
  year = "1984",
  link = "\url{https://hal.inria.fr/inria-00076261/document}",
  abstract =
    "The polymorphic, or second-order, typed lambda calculus is an
    extension of the typed lambda calculus in which polymorphic functions
    can be defined. In this paper that the standard set-theoretic model of
    the ordinary typed lambda calculus cannot be extended to model this
    language extension.",
  paper = "Reyn84.pdf",
  keywords = "printed"
}

@inproceedings{Reyn91,
  author = "Reynolds, John C.",
  title = {{The Coherence of Languages with Intersection Types}},
  booktitle = "TACS 91",
  year = "1991",
  abstract =
    "When a programming language has a sufficiently rich type structure,
    there can be more than one proof of the same typing judgement;
    potentially this can lead to semantic ambiguity since the semantics of
    a typed language is a function of such proofs. When no such ambiguity
    arises, we say that the language is coherent. In this paper we prove
    the coherence of a class of lambda-calculus-based languages that use
    the intersection type discipline, including both a purely functional
    programming language and the Algol-like programming language Forsythe.",
  paper = "Reyn91.pdf"
}

@book{Robi96,
  author = "Robinson, J. S. Derek",
  title = {{A Course in the Theory of Groups}},
  year = "1996",
  series = "Graduate Texts in Mathematics",
  isbn = "978-1-4612-6443-9",
  publisher = "Springer"
}

@misc{Roll1691,
  author = "Rolle, Michel",
  title = {{Rolle's Therem}},
  year = "1691",
  link = "\url{https://en.wikipedia.org/wiki/Rolle%27s\_theorem}",
  abstract =
    "If a real-valued function $f$ is continuous on a proper closed interval
    $[a,b]$, differentiable on the open interval $(a,b)$, and $f(a)=f(b)$,
    then there exists at least one $c$ in the open interval $(a,b)$ such
    that $f^\prime(c)=0$."
}

@book{Ryde88,
  author = "Rydeheard, D. E. and Burstall, R. M.",
  title = {{Computational Category Theory}},
  publisher = "Prentice Hall",
  year = "1988",
  isbn = "978-0131627369"
}

@book{Schm89,
  author = "Schmidt-Schauss, M.",
  title = {{Computational Aspects of an Order-Sorted Logic with Term
           Declarations}},
  publisher = "Springer",
  isbn = "978-3-540-51705-4",
  year = "1989"
}

@misc{Scho24,
  author = "Schoenfinkel, M.",
  title = {{Uber die Bausteine der mathematischen Logik}},
  year = "1924",
  pages = "305-316"
}

@book{Schu72,
  author = "Schubert, Horst",
  title = {{Categories}},
  publisher = "Springer-Verlag",
  year = "1972"
}

@article{Siek89,
  author = "Siekmann, Jorg H.",
  title = {{Unification Theory}},
  journal = "Journal of Symbolic Computation",
  volume = "7",
  number = "3-4",
  year = "1989",
  pages = "207-274",
  abstract =
    "Most knowledge based systems in artificial intelligence (AI), with a
    commitment to asymbolic representation, support one basic operation:
    ``matching of descriptions''. This operation, called unification in work
    on deduction, is the ``addition-and-multiplication'' of AI-systems and
    is consequently often supported by special purpose hardware or by a
    fast instruction set on most AI-machines. Unification theory provides
    the formal framework for investigations into the properties of this
    operation. This article surveys what is presently known in unification
    theory and records its early history.",
  paper = "Siek89.pdf"
}

@article{Sims71,
  author = "Sims, Charles",
  title = {{Determining the Conjugacy Classes of a Permutation Group}},
  journal = "Computers in Algebra and Number Theory, SIAM-AMS Proc.",
  volume = "4",
  publisher = "American Math. Soc.",
  year = "1991",
  pages = "191--195",
  algebra = "\newline\refto{domain PERMGRP PermutationGroup}"
}

@article{Smol88,
  author = "Smolka, G.",
  title = {{Logic Programming with Polymorphically Order-sorted Types}},
  journal = "Lecture Notes in Computer Science",
  volume = "343",
  pages = "53-70",
  year = "1988"
}

@InCollection{Smol89,
  author = "Smolka, G. and Nutt, W. and Goguen, J. and Meseguer, J.",
  title = {{Order-sorted Equational Computation}},
  booktitle = "Resolution of Equations in Algebra Structures (Vol 2)",
  publisher = "Academic Press",
  pages = "297-367",
  year = "1989"
}

@phdthesis{Smol89a,
  author = "Smolka, G.",
  title = {{Logic Programming over Polymorphically Order-Sorted Types}},
  school = "Fachbereich Informatik, Universitat Kaiserslautern",
  year = "1989",
  paper = "Smol89a.pdf",
  keywords = "printed"
}

@misc{Stac17,
  author = "StackExchange",
  title = {{How do Gap generate the elements in permutation groups}},
  year = "2017",
  link = "\url{http://math.stackexchange.com/questions/1705277/how-do-gap-generate-the-elements-in-permutation-groups}"
}

@inproceedings{Stan88,
  author = "Stansifer, R.",
  title = {{Type Inference with Subtypes}},
  booktitle = "POPL 88",
  pages = "88-97",
  year = "1988",
  abstract =
    "We give an algorithm for type inference in a language with functions,
    records, and variant records. A similar language was studied by
    Cardelli who gave a type checking algorithm. This language is
    interesting because it captures aspects of object-oriented programming
    using subtype polymorphism. We give a type system for deriving types
    of expressions in the language and prove the type inference algorithm
    is sound, i.e., it returns a type derivable from the proof system. We
    also prove that the type the algorithm finds is a ``principal'' type,
    i.e., one which characterizes all others. The approach taken here is
    due to Milner for universal polymorphism. The result is a synthesis of
    subtype polymorphism and universal polymorphism.",
  paper = "Stan88.pdf"
}

@article{Stra00,
  author = "Strachey, Christopher",
  title = {{Fundamental Concepts in Programming Languages}},
  journal = "Higher-Order and Symbolic Computation",
  volume = "13",
  number = "1-2",
  pages = "11-49",
  year = "2000",
  abstract =
    "This paper forms the substance of a course of lectures given at the
    International Summer School in Computer Programming at Copenhagen in
    August, 1967. The lectures were originally given from notes and the
    paper was written after the course was finished. In spite of this, and
    only partly because of the shortage of time, the paper still retains
    many of the shortcomings of a lecture course. The chief of these are
    an uncertainty of aim—it is never quite clear what sort of audience
    there will be for such lectures—and an associated switching from
    formal to informal modes of presentation which may well be less
    acceptable in print than it is natural in the lecture room. For these
    (and other) faults, I apologise to the reader.
    
    There are numerous references throughout the course to CPL [1–3]. This
    is a programming language which has been under development since 1962
    at Cambridge and London and Oxford. It has served as a vehicle for
    research into both programming languages and the design of
    compilers. Partial implementations exist at Cambridge and London. The
    language is still evolving so that there is no definitive manual
    available yet. We hope to reach another resting point in its evolution
    quite soon and to produce a compiler and reference manuals for this
    version. The compiler will probably be written in such a way that it
    is relatively easyto transfer it to another machine, and in the first
    instance we hope to establish it on three or four machines more or
    less at the same time.
    
    The lack of a precise formulation for CPL should not cause much
    difficulty in this course, as we are primarily concerned with the
    ideas and concepts involved rather than with their precise
    representation in a programming language.",
  paper = "Stra00.pdf"
}

@article{Stra00a,
  author = "Strachey, Christopher and Wadsworth, Christopher",
  title = {{Continuations: A Mathematical Semantics for Handling Full Jumps}},
  journal = "Higher-Order and Symbolic Computation",
  volume = "13",
  pages = "135-152",
  year = "2000",
  abstract =
    "This paper describes a method of giving the mathematical
    semantics of programming languages which include the most general
    form of jumps.",
  paper = "Stra00a.pdf",
  keywords = "printed"
}

@book{Stro95,
  author = "Stroustrup, Bjarne",
  title = {{The C++ Programming Language (2nd Edition)}},
  publisher = "Addison-Wesley",
  year = "1995",
  isbn = "0-201-53992-6"
}

@article{Stur1829,
  author = "Sturm, Jacques Charles Francois",
  title = {{M\'emoire sur la r\'solution des \'equations num\'eriques}},
  journal = {Bulletin des Sciences de F\'erussac},
  year = "1829",
  volume = "11",
  pages = "419-425",
  link = "\url{https://en.wikipedia.org/wiki/Sturm%27s\_theorem}",
  abstract =
    "Let $p_0,\ldots,p_m$ be the Sturm chain of a square free polynomial $p$,
    and let $\sigma{\eta}$ denote the number of sign changes (ignoring zeros)
    in the sequence $p_0(\eta),p_1(\eta),p_2(\eta),\ldots,p_m(\eta)$.
    Sturms' theorem states that for two real numbers $a < b$, the number of
    distinct roots of $p$ in the half-open interval $(a,b]$ is
    $\sigma(a)-\sigma(b)$.
    
    A Sturm chain is a finite sequence of polynomials $p_0,p_1,\ldots,p_m$
    of decreasing degree with these following properties:
    \begin{itemize}
    \item $p_0=p$ is square free (no square factors, i.e. no repeated roots)
    \item if $p(\eta)=0$, then $sign(p_1(\eta))=sign(p^\prime(\eta))$
    \item if $p_i(\eta)=0$ for $0<i<m$ then 
    $sign(p_{i-1}(\eta))=-sign(p_{i+1}(\eta))$
    \item $p_m$ does not change its sign
    \end{itemize}"
}

@phdthesis{Szab82,
  author = "Szabo, P.",
  title = {{Unifikationstheorie erster Ordnung}},
  school = {Fakult\"at f\"ur Informatik, Universit\"at Karlsruhe},
  year = "1982"
}

@misc{Temp92,
  author = "Temperini, M.",
  title = {{Design and Implementation Methodologies for Symbolic
           Computation Systems}},
  year = "1992",
  comment = "Preprint"
}

@article{That91,
  author = "Thatte, Satish R.",
  title = {{Coercive Type Isomorphism}},
  journal = "LNCS",
  volume = "523",
  year = "1991",
  pages = "29-49",
  abstract =
    "There is a variety of situations in programming in which it is useful
    to think of two distinct types as representations of the same abstract
    structure.  However, language features which allow such relations to
    be effectively expressed at an abstract level are lacking.  We propose
    a generalization of ML-style type inference to deal effectively with
    this problem.  Under the generalization, the (normally free) algebra
    of type expressions is subjected to an equational theory generated by
    a finite set of user-specified equations that express
    interconvertibility relations between objects of ``equivalent'' types.
    Each type equation is accompanied by a pair of conversion functions
    that are (at least partial) inverses.  We show that so long as the
    equational theory satisfies a reasonably permissive syntactic
    constraint, the resulting type system admits a complete type 
    inference algorithm that produces unique principal types.  The main
    innovation required in type inference is the replacement of ordinary
    free unification by unification in the user-specified equational
    theory.  The syntactic constraint ensures that the latter is unitary,
    i.e., yields unique most general unifiers.  The proposed constraint is
    of independent interest as the first known syntactic
    characterization for a class of unitary theories.  Some of the
    applicatloils of the system are similar to those of Wadler's views
    [Wad87]. However, our system is considerably more general, and more
    orthogonal to the underlying language.",
  paper = "That91.pdf"
}

@article{Tiur90,
  author = "Tiuryn, J.",
  title = {{Type Inference Problems -- A Survey}},
  journal = "LNCS",
  volume = "452",
  pages = "105-120",
  year = "1990",
  paper = "Tiur90.pdf"
}

@inproceedings{Tiur92,
  author = "Tiuryn, J.",
  title = {{Subtype Inequalities}},
  booktitle = "Proc. Logic in Computer Science 92",
  year = "1992",
  pages = "308-315",
  abstract =
    "In this paper we study the complexity of the satisfiability problem
    for subtype inequalities in simple types. The naive algorithm which
    solves this problem runs in non-deterministic exponential time for
    every pre-defined poset of atomic subtypings. In this paper we show
    that over certain finite posets of atomic subtypings the
    satisfiability problem for subtype inequalities is PSPACE-hard.  On
    the other hand we prove that if the poset of atomic subtypings is a
    disjoint union of lattices, then the satisfiability problem for
    subtype inequalities is solvable in PTIME. This result covers the
    important special case of the unification problem which can be
    obtained when the atomic subtype relation is equality (in this case
    the poset is a union of one-element lattices).",
  paper = "Tiur92.pdf"
}

@book{Thom91,
  author = "Thompson, Simon",
  title = {{Type Theory and Functional Programming}},
  year = "1991",
  publisher = "Addison-Wesley",
  abstract =
    "This book explores the role of Martin-Lof's constructive type
    theory in computer programming. Th emain focus of the book is how
    the theory can be successfully applied in practice. Introductory
    sections provide the necessary background in logic, lambda
    calculus and constructive mathematics, and exercises and chapter
    summaries are included to reinforce understanding"
}

@article{Turn85,
  author = "Turner, D. A.",
  title = {{Miranda: A non-strict functional language with polymorphic types}},
  journal = "Lecture Notes in Computer Science",
  volume = "201",
  pages = "1-16",
  year = "1985",
  link = "\url{http://miranda.org.uk/nancy.html}",
  paper = "Turn85.pdf"
}

@article{Turn86,
  author = "Turner, D. A.",
  title = {{An Overview of Miranda}},
  journal = "SIGPLAN Notices",
  volume = "21",
  number = "12",
  pages = "158-166",
  year = "1986",
  link = "\url{http://miranda.org.uk/}",
  paper = "Turn86.pdf"
}

@article{Vajd09,
  author = "Vajda, Robert and Jebelean, Tudor and Buchberger, Bruno",
  title = {{Combining Logical and Algebraic Techniques for Matural
            Style Proving in Elementary Analysis}},
  journal = "Mathematics and Computers in SImulation",
  volume = "79",
  number = "8",
  pages = "2310-2316",
  year = "2009",
  link = 
    "\url{http://www.risc.jku.at/publications/download/risc_3320/acafin3.pdf}",
  abstract =
    "PCS (Proving-Computing-Solving) [Buchberber 2001] and S-Decomposition
    [Jebelean 2001] are strategies for handling proof problems by
    combining logic inference steps (e.g. modus ponens, Skolemization,
    instantiation) with rewriting steps (application of definitions) and
    solving procedures based on algebraic techniques (e.g. Groebner Bases,
    Cylindrical Algebraic Decomposition).
    
    If one formalizes the main notions of elementary analysis like
    continuity, convergence, etc., usually a sequence of alternating
    quantifier blocks pops up in the quantifier prefix of the
    corresponding formula. This makes the proof problems involving these
    notions not easy. S-Decomposition strategy is expecially suitable for
    property-preserving problems like continuity of sum, becuase it is
    designed for handling problems where the goal and the main assumptions
    have a similar structure. During proof deduction, existentially
    quantified goals and universal assumptions are handled by introducing
    metavariables, if no suitable ground instance is known in
    adva=nce. For finalizing proof attempts, the metavariables should be
    instantiated in such a way that they satisfy the cumulated algebraic
    constraints collected during the proof attempt. The instantiation
    problem is considered to be difficult in the logical
    calculus. Appropriate instances can be often found using quantifier
    elimination (QE) over real closed fields. In order to obtain witness
    terms we utilize the QE method based on cylindrical algebraic
    decomposition (CAD) [Collins 1975]. However, the QE method alone is
    not sufficient. One needs to pre-process the (closed, quantified)
    conjectured formula and post-process the resulting CAD-structure after
    the call of the QE algorithm.",
  paper = "Vajd09.pdf",
  keywords = "printed"
}

@techreport{Volp91,
  author = "Volpano, Dennis M. and Geoffrey S.",
  title = {{On the Complexity of ML Typability with Overloading}},
  institution = "Cornell University",
  year = "1991",
  number = "TR91-1210",
  abstract =
    "We examine the complexity of type checking in an ML-style type system
    that permits functions to be overloaded with different types. In
    particular, we consider the extension of the ML Type system proposed
    by Wadler and Blott in the appendix of [WB89], with global overloading
    only, that is, where the only overloading is that which exists in an
    initial type assumption set; no local overloading via over and inst
    expressions is allowed. It is shown that under a correct notion of
    well-typed terms, the problem of determining whether a term is well
    typed with respect to an assumption set in this system is
    undecidable. We then investigate limiting recursion in assumption
    sets, the source of the undecidability. Barring mutual recursion is
    considered, but this proves too weak, for the problem remains
    undecidable. Then we consider a limited form of recursion called
    parametric recursion. We show that although the problem becomes
    decidable under parametric recursion, it appears harder than
    conventional ML typability, which is complete for DEXPTIME [Mai90].",
  paper = "Volp91.pdf"
}

@article{Wald92,
  author = "Waldmann, Uwe",
  title = {{Semantics of Order-sorted Specifications}},
  journal = "Theoretical Computer Science",
  volume = "94",
  number = "1-2",
  year = "1992",
  pages = "1-35",
  abstract =
    "Order-sorted specifications (i.e. many-sorted specifications with
    subsort relations) have been proved to be a useful tool for the
    description of partially defined functions and error handling in
    abstract data types.
    
    Several definitions for order-sorted algebras have been proposed. In
    some papers an operator symbol, which may be multiply declared, is
    interpreted by a family of functions (“overloaded” algebras). In other
    papers it is always interpreted by a single function (“nonoverloaded”
    algebras). On the one hand, we try to demonstrate the differences
    between these two approaches with respect to equality, rewriting and
    completion; on the other hand, we prove that in fact both theories can
    be studied in parallel provided that certain notions are suitably
    defined.
    
    The overloaded approach differs from the many-sorted and the
    nonoverloaded one in that the overloaded term algebra is not
    necessarily initial. We give a decidable sufficient criterion for the
    initiality of the term algebra, which is less restrictive than
    GJM-regularity as proposed by Goguen, Jouannaud and Meseguer.
    
    Sort-decreasingness is an important property of rewrite systems since
    it ensures that confluence and Church-Rosser property are equivalent,
    that the overloaded and nonoverloaded rewrite relations agree, and
    that variable overlaps do not yield critical pairs. We prove that it
    is decidable whether or not a rewrite rule is sort-decreasing, even if
    the signature is not regular.
    
    Finally, we demonstrate that every overloaded completion procedure may
    also be used in the nonoverloaded world, but not conversely, and that
    specifications exist that can only be completed using the
    nonoverloaded semantics.",
  paper = "Wald92.pdf"
}

@inproceedings{Wand87,
  author = "Wand, Mitchell",
  title = {{Complete Type Inference for Simple Objects}},
  booktitle = "Symp. on Logic in Computer Science",
  year = "1987",
  pages = "22-25",
  abstract =
    "The problem of strong typing is considered for a model of
    object-oriented programming systems. These systems permit values which
    are records of other values, and in which fields inside these records
    are retrieved by name. A type system is proposed that permits
    classification of these kinds of values and programs by the type of
    their result, as is usual in strongly-typed programming languages. The
    type system has two important properties: it admits multiple
    inheritance, and it has a syntactically complete type inference system.",
  paper = "Wand87.pdf"
}

@inproceedings{Wand88,
  author = "Wand, Mitchell",
  title = {{Corrigendum: Complete Type Inference for Simple Objects}},
  booktitle = "Symp. on Logic in Computer Science",
  year = "1988",
  pages = "5-8",
  abstract =
    "An error has been pointed out in the author's paper (see Proc. 2nd
     IEEE Symp. on Logic in Computer Science, p 37-44 (1987)). It appears
     that there are programs without principal type schemes in the
     system in that paper."
}

@inproceedings{Wand89,
  author = "Wand, Mitchell",
  title = {{Type Inference for Record Concatenation and Multiple
           Inheritance}},
  booktitle = "Logic in Computer Science",
  year = "1989",
  isbn = "0-8186-1954-6",
  abstract =
    "The author shows that the type inference problem for a lambda
    calculus with records, including a record concatenation operator, is
    decidable. He shows that this calculus does not have principal types
    but does have finite complete sets of type, that is, for any term M in
    the calculus, there exists an effectively generable finite set of type
    schemes such that every typing for M is an instance of one of the
    schemes in the set. The author shows how a simple model of
    object-oriented programming, including hidden instance variables and
    multiple inheritance, may be coded in this calculus. The author
    concludes that type inference is decidable for object-oriented
    programs, even with multiple inheritance and classes as first-class
    values.",
  paper = "Wand89.pdf"
}

@article{Wand91,
  author = "Wand, Mitchell",
  title = {{Type Inference for Record Concatenation and Multiple
           Inheritance}},
  journal = "Information and Computation",
  volume = "93",
  issue = "1",
  year = "1991",
  pages = "1-15",
  abstract =
    "We show that the type inference problem for a lambda calculus with
    records, including a record concatenation operator, is decidable. We
    show that this calculus does not have principal types, but does have
    finite complete sets of types: that is, for any term M in the
    calculus, there exists an effectively generable finite set of type
    schemes such that every typing for M is an instance of one of the
    schemes in the set. We show how a simple model of object-oriented
    programming, including hidden instance variables and multiple
    inheritance, may be coded in this calculus. We conclude that type
    inference is decidable for object-oriented programs, even with
    multiple inheritance and classes as first-class values.",
  paper = "Wand91.pdf"
}

@techreport{Webe92b,
  author = "Weber, Andreas",
  title = {{Structuring the Type System of a Computer Algebra System}},
  link = "\url{http://cg.cs.uni-bonn.de/personal-pages/weber/publications/pdf/WeberA/Weber92a.pdf}",
  institution = "Wilhelm-Schickard-Institut fur Informatik",
  year = "1992",
  abstract = 
    "Most existing computer algebra systems are pure symbol manipulating
    systems without language support for the occuring types. This is
    mainly due to the fact taht the occurring types are much more
    complicated than in traditional programming languages. In the last
    decade the study of type systems has become an active area of
    research. We will give a proposal for a type system showing that
    several problems for a type system of a symbolic computation system
    can be solved by using results of this research. We will also provide
    a variety of examples which will show some of the problems that remain
    and that will require further research.",
  paper = "Webe92b.pdf",
  keywords = "axiomref, printed"
}

@inproceedings{Wirs82,
  author = "Wirsing, Martin and Broy, Manfred",
  title = {{An Analysis of Semantic Models for Algebraic Specifications}},
  booktitle = "Theoretical Foundations of Programming Methodology",
  year = "1982",
  publisher = "Springer",
  pages = "351-413",
  isbn = "978-94-009-7893-5",
  abstract =
    "Data structures, algorithms and programming languages can be
    described in a uniform implementation-independent way by axiomatic
    abstract data types i.e. by algebraic specifications defining
    abstractly the properties of objects and functions. Different semantic
    models such as initial and terminal algebras have been proposed in
    order to specify the meaning of such specifications -often involving a
    considerable amount of category theory. A more concrete semantics
    encompassing these different approaches is presented:

    Abstract data types are specified in hierarchies, employing
    ``primitive'' types on which other types are based. The semantics is
    defined to be the class of all partial heterogeneous algebras
    satisfying the axioms and respecting the hierarchy. The interpretation
    of a specification as its initial or terminal algebra is just a
    constraint on the underlying data. These constraints can be modified
    according to the specification goals. E.g. the data can be specified
    using total functions; for algorithms partial functions with
    syntactically checkable domains seem appropriate whereas for
    programming languages the general notion of partiality is needed,
    Model-theoretic and deduction-oriented conditions are developed which
    ensure properties leading to criteria for the soundness and complexity
    of specifications. These conditions are generalized to parameterized
    types, i.e. type procedures mapping types into types. Syntax and
    different semantics of parameter are defined and discussed. Criteria
    for proper parameterized specifications are developed. It is shown
    that the properties of proper specifications viz. of snowballing and
    impeccable types are preserved under application of parameterized
    types — finally guaranteeing that the composition of proper small
    specifications always leads to a proper large specification."
}

@InCollection{Wirs91,
  author = "Wirsing, Martin",
  title = {{Algebraic Specification}},
  booktitle = "Handbook of Theoretical Computer Science (Vol B)",
  publisher = "MIT Press",
  year = "1991",
  pages = "675-788",
  chapter = "13",
  isbn = "0-444-88074-7"
}

@book{Wolf91,
  author = "Wolfram, Stephen",
  title = {{Mathematica: A System for Doing Mathematics by Computer}},
  publisher = "Addison-Wesley",
  isbn = "978-0201515022",
  year = "1991"
}

@article{Worz80,
  author = {W\"orz-Busekros, A.},
  title = {{Algebra in Genetics}},
  publisher = "Springer-Verlag",
  journal = "Lecture Notes in Biomathematics",
  volume = "36",
  year = "1980",
  algebra = "\newline\refto{domain ALGSC AlgebraGivenByStructuralConstants}"
}

@misc{Wiki17a,
  author = "Wikipedia",
  title = {{Group Action}},
  year = "2017",
  link = "\url{https://en.wikipedia.org/wiki/Group\_action}",
  abstract =
    "In mathematics, an action of a group is a way of interpreting the
    elements of the group as ``acting'' on some space in a way that
    preserves the structure of that space. Common examples of spaces that
    groups act on are sets, vector spaces, and topological spaces. Actions
    of groups on vector stacces are called representations of the group."
}

@misc{Wiki17b,
  author = "Wikipedia",
  title = {{Strong generating set}},
  year = "2017",
  link = "\url{http://en.wikipedia.org/wiki/Strong\_generating\_set}",
  abstract =
    "In abstract algebra, especially in the area of group theory, a strong
    generating set of a permutation group is a generating set that clearly
    exhibits the permutation structure as described by a stabilizer
    chain. A stabilizer chain is a sequence of subgroups, each containing
    the next and each stabilizing one more point."
}

@misc{Wiki17c,
  author = "Wikipedia",
  title = {{Compound matrix}},
  year = "2017",
  link = "\url{https://en.wikipedia.org/wiki/Compound\_matrix}"
}

@book{Zari75,
  author = "Zariski, Oscar and Samuel, Pierre",
  title = {{Commutative Algebra}},
  Series = "Graduate Texts in Mathematics",
  year = "1975",
  publisher = "Springer-Verlag",
  isbn = "978-0387900896"
}

@Unpublished{Kalt01,
  author = "Kaltofen, E.",
  title = {{Algorithms for sparse and black box matrices
           over finite fields (Invited talk)}},
  note = "unknown",
  year = "2001",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/01/Ka01_Fq6.pdf}",
  keywords = "survey",
  abstract = "
    Sparse and structured matrices over finite fields occur in many
    settings.  Sparse linear systems arise in sieve-based integer factoring
    and discrete logarithm algorithms. Structured matrices arise in
    polynomial factoring algorithms; one example is the famous Q-matrix
    from Berlekamp's method.  Sparse diophantine linear problems, like
    computing the Smith canonical form of an integer matrix or computing
    an integer solution to a sparse linear system, are reduced via p-adic
    lifting to sparse matrix analysis over a finite field.

    In the past 10 years there has been substantial activity on the
    improvement of a solution proposed by Wiedemann in 1986. The main new
    ingredients are faster pre-conditioners, projections by an entire
    block of random vectors, Lanczos recurrences, and a connection to
    Kalman realizations of control theory. My talk surveys these
    developments and describe some major unresolved problems.",
  paper = "Kalt01.pdf"
}

@Article{Chen02,
  author = "Chen, L. and Eberly, W. and Kaltofen, E.
            and Saunders, B. D. and Turner, W. J. and Villard, G.",
  title = {{Efficient Matrix Preconditioners for Black Box Linear Algebra}},
  journal = "Linear Algebra and Applications",
  year = "2002",
  volume = "343--344",
  pages = "119--146",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/02/CEKSTV02.pdf}",
  abstract = "
    The main idea of the ``black box'' approach in exact linear algebra is
    to reduce matrix problems to the computation of minimum polynomials.
    In most cases preconditioning is necessary to obtain the desired
    result.  Here good preconditioners will be used to ensure geometrical
    / algebraic properties on matrices, rather than numerical ones, so we
    do not address a condition number. We offer a review of problems for
    which (algebraic) preconditioning is used, provide a bestiary of
    preconditioning problems, and discuss several preconditioner types to
    solve these problems.  We present new conditioners, including
    conditioners to preserve low displacement rank for Toeplitz-like
    matrices. We also provide new analyses of preconditioner performance
    and results on the relations among preconditioning problems and with
    linear algebra problems.  Thus, improvements are offered for the
    efficiency and applicability of preconditioners. The focus is on
    linear algebra problems over finite fields, but most results are valid
    for entries from arbitrary fields.",
  paper = "Chen02.pdf"
}

@InCollection{Kalt11d,
  author = "Kaltofen, Erich and Storjohann, Arne",
  title = {{The Complexity of Computational Problems in Exact Linear Algebra}},
  booktitle = "Encyclopedia of Applied and Computational Mathematics",
  publisher = "Springer",
  year = "2011",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/11/KS11.pdf}",
  abstract = "
    Computational problems in exact linear algebra including computing an
    exact solution of a system of linear equations with exact scalars,
    which can be exact rational numbers, integers modulo a prime number,
    or algebraic extensions of those represented by their residues modulo
    a minimum polynomial. Classical linear algebra problems are computing
    for a matrix its rank, determinant, characteristic and minimal
    polynomial, and rational canonical form (= Frobenius normal form). For
    matrices with integer and polynomial entries one computes the Hermite
    and Smith normal forms. If a rational matrix is symmetric, one
    determines if the matrix is definite.",
  paper = "Kalt11d.pdf"
}

@Article{Come12,
  author = "Comer, Matthew T. and Kaltofen, Erich L.",
  title = {{On the {Berlekamp}/{Massey} Algorithm and Counting Singular 
           {Hankel} Matrices over a Finite Field}},
  year = "2012",
  month = "April",
  journal = "Journal of Symbolic Computation",
  volume  = "47",
  number = "4",
  pages = "480--491",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/10/CoKa10.pdf}",
  abstract = "
    We derive an explicit count for the number of singular $n\times n$
    Hankel (Toeplitz) matrices whose entries range over a finite field
    with $q$ elements by observing the execution of the Berlekamp / Massey
    algorithm on its elements. Our method yields explicit counts also when
    some entries above or on the anti-diagonal (diagonal) are fixed. For
    example, the number of singular $n\times n$ Toeplitz matrices with 0's
    on the diagonal is $q^{2n-3}+q^{n-1}-q^{n-2}$.

    We also derive the count for all $n\times n$ Hankel matrices of rank
    $r$ with generic rank profile, I.e., whose first $r$ leading principal
    submatrices are non-singular and the rest are singular, namely
    $q^r(q-1)^r$ in the case $r < n$ and $q^{r-1}(q-1)^r$ in the case
    $r=n$.  This result generalizes to block-Hankel matrices as well.",
  paper = "Come12.pdf"
}

@Article{Kalt13a,
  author = "Kaltofen, Erich and Yuhasz, George",
  title = {{A Fraction Free Matrix {Berlekamp}/{Massey} Algorithm}},
  journal = "Linear Algebra and Applications",
  year = "2013",
  volume = "439",
  number = "9",
  month = "November",
  pages = "2515--2526",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/08/KaYu08.pdf}",
  abstract = "
    We describe a fraction free version of the Matrix Berlekamp / Massey
    algorithm. The algorithm computes a minimal matrix generator of
    linearly generated square matrix sequences over an integral
    domain. The algorithm performs all operations in the integral domain,
    so all divisions performed are exact. For scalar sequences, the matrix
    algorithm specializes to a more efficient algorithm than the algorithm
    currently in the literature.  The proof of integrality of the matrix
    algorithm gives a new proof of integrality for the scalar
    specialization.",
  paper = "Kalt13a.pdf"
}

@Article{Kalt13,
  author = "Kaltofen, Erich and Yuhasz, George",
  title = {{On The Matrix {Berlekamp}-{Massey} Algorithm}},
  year = "2013",
  volume = "9",
  number = "4",
  month = "September",
  journal = "ACM Trans. Algorithms",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/06/KaYu06.pdf}",
  abstract = "
    We analyze the Matrix Berlekamp / Massey algorithm, which generalizes
    the Berlekamp / Massey algorithm [Massey 1969] for computing linear
    generators of scalar sequences. The Matrix Berlekamp / Massey
    algorithm computes a minimal matrix generator of a linearly generated
    matrix sequence and has been first introduced by Rissanen [1972a],
    Dickinson et al. [1974], and Coppersmith [1994]. Our version of the
    algorithm makes no restrictions on the rank and dimensions of the
    matrix sequence. We also give new proofs of correctness and complexity
    for the algorithm, which is based on self-contained loop invariants
    and includes an explicit termination criterion for a given
    determinantal degree bound of the minimal matrix generator",
  paper = "Kalt13.pdf"
}

@InProceedings{Kalt02a,
  author = "Kaltofen, Erich",
  title = {{An output-sensitive variant of the baby steps/\allowbreak 
           giant steps determinant algorithm}},
  booktitle = "Proc. 2002 Internat. Symp. Symbolic Algebraic Comput.",
  pages = "138--144",
  year = "2002",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/02/Ka02.pdf}",
  paper = "Kalt02a.pdf"
}

@InProceedings{Kalt01a,
  author = "Kaltofen, E. and Villard, G.",
  title = {{On the complexity of computing determinants}},
  booktitle = "Proc. Fifth Asian Symposium on Computer Mathematics 
              (ASCM 2001)",
  pages = "13--27",
  isbn = "981-02-4763-X",
  year = "2001",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/01/KaVi01.pdf}",
  abstract = "
    The computation of the determinant of an $n\times n$ matrix $A$ of
    numbers or polynomials is a challenge for both numerical and symbolic
    methods. Numerical methods, such as Clarkson's algorithm [10,7] for
    the sign of the determinant must deal with conditionedness that
    determines the number of mantissa bits necessary for obtaining a
    correct sign. Symbolic algorithms that are based on Chinese
    remaindering [6,17,Chapter 5.5] must deal with the fact that the
    length of the determinant in the worse case grows linearly in the
    dimension of the matrix. Hence the number of modular operations is $n$
    times the number of arithmetic operations in a given algorithm.
    Hensel lifting combined with rational number recovery [14,1] has cubic
    bit complexity in $n$, but the algorithm can only determine a factor
    of the determinant, namely the largest invariant factor. If the matrix
    is similar to a multiple of the identity matrix, the running time is
    again that of Chinese remaindering.",
  paper = "Kalt01a.pdf"
}

@Article{Kalt04a,
  author = "Kaltofen, Erich and Villard, Gilles",
  title = {{On the Complexity of Computing Determinants}},
  journal = "Computational Complexity",
  volume = "13",
  number = "3-4",
  year = "2004",
  pages = "91--130",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/04/KaVi04_2697263.pdf}",
  abstract = "
    We present new baby steps / giant steps algorithms of asymptotically
    fast running time for dense matrix problems. Our algorithms compute
    the determinant, characteristic polynomial, Frobenius normal form and
    Smith normal form of a dense $n\times n$ matrix $A$ with integer
    entries in $(x^{3.2} log ||A||)^{1+o(1)}$ and 
    $(x^{2.697263} log ||A||)^{1+o(1)}$ 
    bit operations; here $||A||$ denotes the largest
    entry in absolute value and the exponent adjustment by ``$+o(1)$''
    captures additional factors $C_1 (log n)^{C_2}(loglog ||A||)^{C_3}$
    for positive real constants $C_1$, $C_2$, $C_3$. The bit complexity
    $(n^{3.2} log ||A||)^{1+o(1)}$ results from using the classical cubic
    matrix multiplication algorithm.  Our algorithms are randomized, and
    we can certify that the output is the determinant of $A$ in a Las
    Vegas fashion. The second category of problems deals with the setting
    where the matrix $A$ has elements from an abstract commutative ring,
    that is, when no divisions in the domain of entries are possible. We
    present algorithms that deterministically compute the determinant,
    characteristic polynomial and adjoint of $A$ with $n^{3.2+o(1)}$ and
    $O(n^{2.697263})$ ring additions, substractions, and multiplications.",
  paper = "Kalt04a.pdf"
}

@InProceedings{Kalt97b,
  author = "Eberly, W. and Kaltofen, E.",
  title = {{On Randomized {Lanczos} Algorithms}},
  booktitle = "Proc. 1997 Internat. Symp. Symbolic Algebraic Comput.",
  year = "1997",
  pages = "176--183",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/97/EbKa97.pdf}",
  abstract = "
    Las Vegas algorithms that are based on Lanczo's method for solving
    symmetric linear systems are presented and analyzed. These are
    compared to a similar randomized Lanczos algorithm that has been used
    for integer factorization, and to the (provably reliable) algorithm of
    Wiedemann. The analysis suggests that our Lanczos algorithms are
    preferable to several versions of Wiedemann's method for computations
    over large fields, expecially for certain symmetric matrix
    computations.",
  paper = "Kalt97b.pdf"
}

@InProceedings{Kalt94c,
  author = "Kaltofen, E.",
  title = {{Asymptotically fast solution of {Toeplitz}-like singular 
           linear systems}},
  booktitle = "Proc. 1994 Internat. Symp. Symbolic Algebraic Comput.",
  pages = "297--304",
  year = "1994",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/94/Ka94_issac.pdf}",
  abstract = "
    The Toeplitz-likeness of a matrix (Kailath et al. 1979) is the
    generalization of the notion that a matrix is Toeplitz. Block matrices
    with Toeplitz blocks, such as the Sylvester matrix corresponding to
    the resultant of two univariate polynomials, are Toeplitz-like, as are
    products and inverses of Toeplitz-like matrices. The displacement rank
    of a matrix is a measure for the degree of being Toeplitz-like. For
    example, an $r\times s$ block matrix with Toeplitz blocks has
    displacement rank $r+s$ whereas a generic $N\times N$ matrix has
    displacement rank $N$.  A matrix of displacement rank $\alpha$ can be
    implicitly represented by a sum of $\alpha$ matrices, each of which is
    the product of a lower trainagular and an upper triangular Toeplitz
    matrices. Such a $\sigma LU$ representation can usually be obtained
    efficiently.",
  paper = "Kalt94c.pdf"
}

@Article{Kalt99,
  author = "Kaltofen, E. and Lobo, A.",
  title = {{Distributed matrix-free solution of large sparse linear systems 
           over finite fields}},
  journal = "Algorithmica",
  year = "1999",
  pages = "331--348",
  month = "July--Aug.",
  volume = "24",
  number = "3--4",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/99/KaLo99.pdf}",
  abstract = "
    We describe a coarse-grain parallel approach for the homogeneous
    solution of linear systems. Our solutions are symbolic, i.e., exact
    rather than numerical approximations. We have performed an outer loop
    parallelization that works well in conjunction with a black box
    abstraction for the coefficient matrix. Our implementation can be run
    on a network cluster of UNIX workstations as well as on an SP-2
    multiprocessor. Task distribution and management are effected through
    MPI and other packages. Fault tolerance, checkpointing, and recovery
    are incorporated. Detailed timings are presented for experiments with
    systems that arise in RSA challenge integer factoring efforts. For
    example, we can solve a 252,222$\times$252,222 system with about 11.04
    million nonzero entries over the Galois field with two elements using
    four processors of an SP-2 multiprocessor, in about 26.5 hours CPU time.",
  paper = "Kalt99.pdf"
}

@InProceedings{Kalt96a,
  author = "Kaltofen, E. and Lobo, A.",
  title = {{Distributed matrix-free solution of large sparse linear systems 
           over finite fields}},
  booktitle = "Proc. High Performance Computing '96",
  year = "1996",
  editor = "A. M. Tentner",
  pages = "244--247",
  organization = "Society for Computer Simulation",
  publisher = "Simulation Councils, Inc.",
  address = "San Diego, CA",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/96/KaLo96_hpc.pdf}",
  abstract = "
    We describe a coarse-grain parallel software system for the
    homogeneous solution of linear systems. Our solutions are symbolic,
    i.e., exact rather than numerical approximations. Our implementation
    can be run on a network cluster of SPARC-20 computers and on an SP-2
    multiprocessor.  Detailed timings are presented for experiments with
    systems that arise in RSA challenge integer factoring efforts. For
    example, we can solve a 252,222$\times$252,222 system with about 11.04
    million non-zero entries over the Galois field with 2 elements using 4
    processors of an SP-2 multiprocessor, in about 26.5 hours CPU time.",
  paper = "Kalt96a.pdf"
}

@InProceedings{Kalt94a,
  author = "Kaltofen, E. and Lobo, A.",
  title = {{Factoring high-degree polynomials by the black box 
           Berlekamp algorithm}},
  booktitle = "Proc. 1994 Internat. Symp. Symbolic Algebraic Comput.",
  pages = "90--98",
  year = "1994",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/94/KaLo94.ps.gz}",
  abstract = "
    Modern techniques for solving structured linear systems over finite
    fields, which use the coefficient matrix as a black box and require an
    efficient algorithm for multiplying this matrix by a vector, are
    applicable to the classical algorithm for factoring a univariate
    polynomial over a finite field by Berlekamp (1967 and 1970). We report
    aon a computer implementation of this idea that is based on the
    parallel block Wiedemann linear system solver (Coppersmith 1994 and
    Kaltofen 1993 and 1995). The program uses randomization and we also
    study the expected run time behavior of our method.",
  paper = "Kalt94a.ps"
}

@Article{Kalt95,
  author = "Kaltofen, E.",
  title = {{Analysis of {Coppersmith}'s block {Wiedemann} algorithm for the 
           parallel solution of sparse linear systems}},
  journal = "Math. Comput.",
  year = "1995",
  volume = "64",
  number = "210",
  pages = "777--806",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/95/Ka95_mathcomp.pdf}",
  abstract = "
    By using projections by a block of vectors in place of a single vector
    it is possible to parallelize the outer loop of iterative methods for
    solving sparse linear systems. We analyze such a scheme proposed by
    Coppersmith for Wiedemann's coordinate recurrence algorithm, which is
    based in part on the Krylov subspace approach. We prove that by use of
    certain randomizations on the input system the parallel speed up is
    roughly by the number of vectors in the blocks when using as many
    processors. Our analysis is valid for fields of entries that have
    sufficiently large cardinality. Our analysis also deals with an
    arising subproblem of solving a singular block Toeplitz system by use
    of the theory of Toeplitz-like matrices.",
  paper = "Kalt95.pdf"
}

@Article{Kalt90a,
  author = "Kaltofen, E. and Krishnamoorthy, M.S. and Saunders, B.D.",
  title = {{Parallel algorithms for matrix normal forms}},
  journal = "Linear Algebra and Applications",
  year = "1990",
  volume = "136",
  pages = "189--208",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/90/KKS90.pdf}",
  abstract = "
    Here we offer a new randomized parallel algorithm that determines the
    Smith normal form of a matrix with entries being univariate
    polynomials with coefficients in an arbitrary field. The algorithm has
    two important advantages over our previous one: the multipliers
    related the Smith form to the input matrix are computed, and the
    algorithm is probabilistic of Las Vegas type, i.e., always finds the
    correct answer. The Smith form algorithm is also a good sequential
    algorithm. Our algorithm reduces the problem of Smith form
    computations to two Hermite form computations.  Thus the Smith form
    problem has complexity asymptotically that of the Hermite form
    problem. We also construct fast parallel algorithms for Jordan normal
    form and testing similarity of matrices. Both the similarity and
    non-similarity problems are in the complexity class RNC for the usual
    coefficient fields, i.e., they can be probabilistically decided in
    poly-logarithmic time using polynomially many processors.",
  paper = "Kalt90a.pdf"
}

@Article{Kalt87,
  author = "Kaltofen, E. and Krishnamoorthy, M.S. and Saunders, B.D.",
  title = {{Fast parallel computation of Hermite and Smith forms of 
           polynomial matrices}},
  journal = "SIAM J. Alg. Discrete Math.",
  year = "1987",
  volume = "8",
  pages = "683--690",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/87/KKS87.pdf}",
  abstract = "
    Boolean circuits of polynomial size and poly-logarithmic depth are
    given for computing the Hermite and Smith normal forms of polynomial
    matrices over finite fields and the field of rational numbers. The
    circuits for the Smith normal form computation are probabilistic ones
    and also determine very efficient sequential algorithms. Furthermore,
    we give a polynomial-time deterministic sequential algorithm for the
    Smith normal form over the rationals. The Smith normal form algorithms
    are applied to the Rational canonical form of matrices over finite
    fields and the field of rational numbers.",
  paper = "Kalt87.pdf"
}

@InProceedings{Kalt92,
  author = "Kaltofen, E. and Pan, V.",
  title = {{Processor-efficient parallel solution of linear systems {II}: 
           the positive characteristic and singular cases}},
  booktitle = "Proc. 33rd Annual Symp. Foundations of Comp. Sci.",
  year = "1992",
  pages = "714--723",
  publisher = "IEEE Computer Society Press",
  address = "Los Alamitos, California",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/92/KaPa92.pdf}",
  abstract = "
    We show that over any field, the solution set to a system of $n$
    linear equations in $n$ unknowns can be computed in parallel with
    randomization simultaneously in poly-logarithmic time in $n$ and with
    only as many processors as are utilized to multiply two $n\times n$
    matrices. A time unit represents an arithmetic operation in the
    field. For singular systems our parallel timings are asymptotically as
    fast as those for non-singular systems, due to our avoidance of binary
    search in the matrix rank problem, except when the field has small
    positive characteristic; in that case, binary search is avoided to a
    somewhat higher processor count measure.",
  paper = "Kalt92.pdf"
}

@InProceedings{Kalt91c,
  author = "Kaltofen, E. and Pan, V.",
  title = {{Processor efficient parallel solution of linear systems over 
           an abstract field}},
  booktitle = "Proc. SPAA '91 3rd Ann. ACM Symp. Parallel Algor. Architecture",
  pages = "180--191",
  publisher = "ACM Press",
  year = "1991",
  address = "New York, N.Y.",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/91/KaPa91.pdf}",
  abstract = "
    Parallel randomized algorithms are presented that solve
    $n$-dimensional systems of linear equations and compute inverses of
    $n\times n$ non-singular matrices over a field in $O((log n)^2)$ time,
    where each time unit represents an arithmetic operation in the field
    generated by the matrix entries. The algorithms utilize with a $O(log n)$
    factor as many processors as are needed to multiply two $n\times n$
    matrices. The algorithms avoid zero divisions with controllably
    high probability provided the $O(n)$ random elements used are selected
    uniformly from a sufficiently large set. For fields of small positive
    characteristics, the processor count measures of our solutions are
    somewhat higher.",
  paper = "Kalt91c.pdf"
}

@InProceedings{Kalt91,
  author = "Kaltofen, E. and Saunders, B.D.",
  editor = "H. F. Mattson and T. Mora and T. R. N. Rao",
  title = {{On Wiedemann's method of solving sparse linear systems}},
  booktitle = "Proc. AAECC-9",
  series = "Lect. Notes Comput. Sci.",
  volume = "539",
  pages = "29--38",
  publisher = "Springer-Verlag",
  year = "1991",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/91/KaSa91.pdf}",
  abstract = "
    Douglas Wiedemann's (1986) landmark approach to solving sparse linear
    systems over finite fields provides the symbolic counterpart to
    non-combinatorial numerical methods for solving sparse linear systems,
    such as the Lanczos or conjugate gradient method (see Golub and van
    Loan (1983)). The problem is to solve a sparse linear system, when the
    individual entries lie in a generic field, and the only operations
    possible are field arithmethic; the solution is to be exact. Such is
    the situation, for instance, if one works in a finite field. Wiedemann
    bases his approach on Krylov subspaces, but projects further to a
    sequence of individual field elements. By making a link to the
    Berlekamp / Massey problem from coding theory -- the coordinate
    recurrences -- and by using randomization an algorithm is obtained
    with the following property. On input of an $n\times n$ coefficient
    matrix $A$ given by a so-called black box, which is a program that can
    multiply the matrix by a vector (see Figure 1), and of a vector $b$,
    the algorithm finds, with high probability in case the system is
    solvable, a random solution vector $x$ with $Ax=b$. It is assumed that
    the field has sufficiently many elements, say no less than $50n^2
    log(x)$, otherwise one goes to a finite algebraic extension. The
    complexity of the method is in the general singular case $O(n log
    (n))$ calls to the black box for $A$ and an additional $O(n^2
    log(n)^2)$ field arithmetic operations.",
  paper = "Kalt91.pdf"
}

@article{Wied86,
  author = "Wiedemann, Douglas H.",
  title = {{Solving sparse linear equations over finite fields}},
  journal = "IEEE Transactions on Information Theory",
  year = "1986",
  volume = "32",
  number = "1",
  pages = "54-62",
  link = "\url{http://www.csd.uwo.ca/~moreno/CS424/Ressources/WIEDEMANN-IEE-1986.pdf}",
  abstract = "
    A ``coordinate recurrence'' method for solving sparse systems of
    linear equations over finite fields is described. The algorithms
    discussed all require $O(n_1(\omega+n_1)log^k n_1)$ field operations,
    where $n_1$ is the maximum dimension of the coefficient matrix,
    $\omega$ is approximately the number of field operations required to
    apply the matrix to a test vector, and the value of $k$ depends on the
    algorithm. A probabilistic algorithm is shown to exist for finding the
    determinant of a square matrix. Also, probabilistic algorithms are
    shown to exist for finding the minimum polynomial and rank with some
    arbitrarily small possibility of error.",
  paper = "Wied86.pdf"
}

@article{Bord82,
  author = "Bordoni, L. and Colagrossi, A. and Miola, A.",
  title = {{Linear Algebraic Approach for Computing Polynomial Resultant}},
  journal = "Lecture Notes in Computer Science",
  volume = "144",
  pages = "231-236",
  year = "1982",
  abstract = 
    "This paper presents a linear algebraic method for computing the
    resultant of two polynomials. This method is based on the
    computation of a determinant of order equal to the minimum of the
    degrees of the two given polynomials. This method turns out to be
    preferable to other known linear algebraic methods both from a
    computational point of view and for a total generality respect to
    the class of the given polynomials. Some relationships of this
    method with the polynomial pseudo-remainder operation are discussed.",
  paper = "Bord82.pdf"
}

@article{Belo83,
  author = "Belovari, G.",
  title =
    {{Complex Analysis in Symbolic Computing of some Definite Integrals}},
  journal = "ACM SIGSAM",
  volume = "17",
  number = "2",
  year = "1983",
  pages = "6-11",
  paper = "Belo83.pdf"
}

@article{Brow78,
  author = "Brown, W. S.",
  title = {{The Subresultant PRS Algorithm}},
  journal = "ACM Transactions on Mathematical Software",
  volume = "4",
  number = "3",
  year = "1978",
  pages = "237-249",
  link = 
    "\url{https://people.eecs.berkeley.edu/~fateman/282/readings/brown.pdf}",
  abstract = 
    "Two earlier papers described the generalizations of Euclid's
    algorithm to deal with the problem of computing the greatest common
    divisor (GCD) or the resultant of a pair of polynomials. A sequal to
    those two papers is presented here.
    
    In attempting such a generalization one easily arrives at the concept
    of a polynomial remainder sequence (PRS) and then quickly discovers
    the phenomenon of explosive coefficient growth.  Fortunately, this
    explosive growth is not inherent in the problem, but is only an
    artifact of various naive solutions. If one removes the content (that
    is, the GCD of the coefficients) from each polynomial in a PRS, the
    coefficient growth in the resulting primitive PRS is relatively modest.
    However, the cost of computing the content (by applying Euclid's
    algorithm in the coefficient domain) may be unacceptably or even
    prohibitively high, especially if the coefficients are themselves
    polynomials in one or more addltional variables.
    
    The key to controlling coefficient growth without the costly
    computation of GCD's of coefficients is the fundamental theorem of
    subresuitants, which shows that every polynomial in a PRS is
    proportional to some subresultant of the first two.  By arranging for
    the constants of proportionahty to be unity, one obtains the
    subresultant PRS algorithm, in which the coefficient growth is
    essentially linear.  A corollary of the fundamental theorem is given
    here, which leads to a simple derivation and deeper understanding of
    the subresultant PRS algorithm and converts a conjecture mentioned in
    the earlier papers into an elementary remark.
    
    A possible alternative method of constructing a subresultant PRS is to
    evaluate all the subresultants directly from Sylvester's determinant
    via expansion by minors. A complexity analysis is given in conclusion,
    along lines pioneered by Gentleman and Johnson, showing that the
    subresultant PRS algorithm is superior to the determinant method
    whenever the given polynomials are sufficiently large and dense, but
    is inferior in the sparse extreme.",
  paper = "Brow78.pdf"
}

@article{Cann73,
  author = "Cannon, John J. and Dimino, Lucien A. and Havas, George and 
            Watson, Jane M.",
  title = {{Implementation and Analysis of the Todd-Coxeter Algorithm}},
  year = "1973",
  journal = "Mathematics of Computation",
  volume = "27",
  number = "123",
  pages = "463-490",
  abstract =
    "A recent form of the Todd-Coxeter algorithm, known as the lookahead
    algorithm, is described. The time and space requirements for this
    algorithm are shown experimentally to be usually either equivalent or
    superior to the Felsch and Haselgrove-Leech-Trotter algorithms. Some
    finding from an experimental study of the behaviour of Todd-Coxeter
    programs in a variety of situations is given.",
  paper = "Cann73.pdf"
}

@article{Coll87,
  author = "Collins, George E.",
  title = {{Subresultants and Reduced Polynomial Remainder Sequences}},
  journal = "J. ACM",
  volume = "14",
  number = "1",
  year = "1987",
  pages = "128-142",
  link =
   "\url{https://people.eecs.berkeley.edu/~fateman/282/readings/collins.pdf}",
  paper = "Coll87.pdf"
}

@article{Cool65,
  author = "Cooley, James W. and Tukey, John W.",
  title = {{An Algorithm for the Machine Calculation of Complex Fourier 
            Series}},
  journal = "Mathematics of Computation",
  volume = "19",
  number = "04",
  year = "1965",
  keywords = "printed"
}

@article{Dave17,
  author = "Davenport, James H.",
  title = {{A Generalized Successive Resultants Algorithm}},
  journal = "Lecture Notes in Computer Science",
  volume = "10064",
  year = "2017",
  abstract =
    "The Successive Resultants Algorithm (SRA) is a root-finding algorithm for
    polynomials over $\mathbb{F}_{p^n}$ and was introdoced at ANTS in 2014.
    THe algorithm is efficient when the characteristic $p$ is small and 
    $n > 1$. In this paper, we abstract the core SRA algorithm to arbitrary
    finite fields and present three instantiations of our general algorithm,
    one of which is novel and makes use of a series of isogenies derived
    from elliptic curves with sufficiently smooth order.",
  paper = "Dave17.pdf"
}

@InCollection{Diaz97,
  author = "Diaz, A. and Kaltofen, E. and Pan, V.",
  title = {{Algebraic Algorithms}},
  booktitle = "The Computer Science and Engineering Handbook",
  publisher = "CRC Press",
  year = "1997",
  editor = "A. B. Tucker",
  pages = "226--248",
  address = "Boca Raton, Florida",
  chapter = "10",
  keywords = "survey",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/97/DKP97.ps.gz}",
  ref = "00965",
  abstract = "
    The title's subject is the algorithmic approach to algebra: arithmetic
    with numbers, polynomials, matrices, differential polynomials, such as
    $y^{\prime\prime} + (1/2 + x^4/4)y$, truncated series, 
    and algebraic sets, i.e.,
    quantified expressions such as $\exists x \in {\bf R}: x^4+p\cdot x+q=0$, 
    which describes a subset of the two-dimensional space with
    coordinates $p$ and $q$ for which the given quartic equation has a
    real root.  Algorithms that manipulate such objects are the backbone
    of modern symbolic mathematics software such as the Maple and
    Mathematica systems, to name but two among many useful systems. This
    chapter restricts itself to algorithms in four areas: linear matrix
    algebra, root finding ov univariate polynomials, solution of systems
    of nonlinear algebraic equations, and polynomial factorization.",
  paper = "Diaz97.ps"
}

@InCollection{Diaz99,
  author = "Diaz, A. and Emiris, I. and Kaltofen, E. and Pan, V.",
  title = {{Algebraic Algorithms}},
  booktitle = "Algorithms \& Theory of Computation Handbook",
  publisher = "CRC Press",
  year = "1999",
  editor = "M. J. Atallah",
  address = "Boca Raton, Florida",
  pages	= "16.1--16.27",
  isbn = "0-8493-2649-4",
  keywords = "survey",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/99/DEKP99.ps.gz}",
  abstract = "
    The title's subject is the algorithmic approach to algebra: arithmetic
    with numbers, polynomials, matrices, differential polynomials, such as
    $y^n+(1/2+x^4/4)y$, truncated series, and algebraic sets, i.e.,
    quantified expressions such as $\exists x \in {\bf R}: x^4+p\cdot+q=0$, 
    which describes a subset of the two-dimensional space with
    coordinates $p$ and $q$ for which the given quartic equation has a
    real root.  Algorithms that manipulate such objects are the backbone
    of modern symbolic mathematics software such as the Maple and
    Mathematica systems, to name but two among many useful systems. This
    chapter restricts itself to algorithms in four areas: linear algebra,
    root finding for univariate polynomials, solution of systems of
    nonlinear algebraic equations, and polynomial factorization (see
    section 5 on some pointers to the vast further material on algebraic
    algorithms and section 2.2 and [Pan 1993] on further applications to
    the graph and combinatorial computations).",
  paper = "Diaz99.ps"
}

@misc{Doye93,
  author = "Doye, Nicolas James",
  title = {{The Implementation of Various Algorithms for Permuation Groups
           in the Computer Algebra System: AXIOM}},
  year = "1993",
  comment = "M.Sc thesis, University of Bath",
  link = "\url{https://static.worldofnic.org/cdn/ps/research/msc.ps}",
  paper = "Doye93.pdf",
  keywords = "axiomref"
}

@inproceedings{Fate92a,
  author = "Fateman, Richard",
  title = {{Honest Plotting, Global Extrema, and Interval Arithmetic}},
  booktitle = "ISSAC 92",
  year = "1992",
  pages = "216-223",
  isbn = "0-89791-489-9",
  abstract =
    "A computer program to honestly plot curves y = f(x) must locate
    maxima and minima in the domain of the graph.  To do so it may have to
    solve a classic problem in computation – global optimization.
    Reducing an easy problem to a hard one is usually not an advantage,
    but in fact there is a route to solving both problems if the function
    can be evaluated using interval arithmetic.  Since some computer
    algebra systems supply a version of interval arithmetic, it seems we
    have the ingredients for a solution.  

    In this paper we address a particular problem how to compute and
    display ``honest'' graphs of 2-D mathematical curves.  By
    ``honest'' we mean that no significant features (such as the
    location of poles, the values at maxima or minima, or the behavior
    of a curve at asymptotes) are misrepresented, By “mathematical” we
    mean curves like those generally needed in scientific disciplines
    where functions are represented by composition of common
    mathematical operations: rational operations ($+, –, *, /$),
    exponential and log, trigonometric functions as well as continuous
    and differentiable functions from applied mathematics.",
  paper = "Fate92a.pdf"
}

@misc{Fate00b,
  author = "Fateman, Richard",
  title = {{The (finite field) Fast Fourier Transform}},
  year = "2000",
  link = 
   "\url{https://people.eecs.berkeley.edu/~fateman/282/readings/fftnotes.pdf}",
  abstract =
    "There are numerous directions from which one can approach the subject
    of the fast Fourier Transform (FFT). It can be explained via numerous
    connections to convolution, signal processing, and various other
    properties and applications of the algorithm.  We (along with
    Geddes/Czapor/Labahn) take a rather simple view from the algebraic
    manipulation standpoint. As will be apparent shortly, we relate the
    FFT to the evaluation of a polynomial. We also consider it of interest
    primarily as an algorithm in a discrete (finite) computation structure
    rather than over the complex numbers.",
  paper = "Fate00b.pdf"
}

@misc{Fate00c,
  author = "Fateman, Richard",
  title = {{Additional Notes on Polynomial GCDs, Hensel construction}},
  year = "2000",
  link =
    "\url{https://people.eecs.berkeley.edu/~fateman/282/readings/hensel.pdf}",
  paper = "Fate00c.pdf"
}

@article{Gent76,
  author = "Gentleman, W. M. and Johnson, S. C.",
  title = {{Analysis of Algorithms, A Case Study: Determinants of Matrices
           With Polynomial Entries}},
  journal = "ACM Transactions on Mathematical Software",
  volume = "2",
  number = "3",
  year = "1976",
  pages = "232-241",
  link = 
  "\url{https://people.eecs.berkeley.edu/~fateman/282/readings/gentleman.pdf}",
  abstract =
    "The problem of computing the deternunant of a matrix of polynomials
    is considered; two algorithms (expansion by minors and expansion by
    Gaussian elimination) are compared; and each is examined under two models
    for polynomial computatmn (dense univariate and totally sparse).  The
    results, while interesting in themselves, also serve to display two
    points: (1) Asymptotic results are sometimes misleading for noninfinite
    (e.g.  practical) problems.  (2) Models of computation are by
    definition simplifications of reality: algorithmic analysis should be
    carried out under several distinct computatmnal models and should be
    supported by empirical data.",
  paper = "Gent76.pdf"
}

@book{Hard64,
  author = "Hardy, G. and Littlewood, J.E. and Polya, G.",
  title = {{Inequalities}},
  publisher = "Cambridge University Press",
  year = "1964"
}

@InCollection{Kalt87a,
  author = "Kaltofen, E.",
  editor = "J. F. Traub",
  title = {{Computer algebra algorithms}},
  booktitle = "Annual Review in Computer Science",
  pages = "91--118",
  publisher = "Annual Reviews Inc.",
  year = "1987",
  volume = "2",
  address = "Palo Alto, California",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/87/Ka87_annrev.pdf}",
  abstract = "
    The origins of the discipline of computer algebra can be found in
    Issac Newton's {\sl Universal Arithmetic} (1728) [130], where methods
    for methods for manipulating universal mathematical expressions (i.e.
    formulas containing symbolic indeterminates) and algorithms for
    solving equations built with these expressions are systematically
    discussed. One can interpret the misson of computer algebra as the
    construction of computer systems that enable scientific or engineering
    users, for instance, to carry out mathematical manipulation
    automatically. Indeed, systems with this goal already exist, among
    them {MACSYMA}, {MAPLE}, {muMATH}, {REDUCE}, {SAC/2}, {SCRATCHPAD/II},
    and {SMP}. These systems carry out scientific computing tasks, whose
    results are distinguished from numerical computing in two principle
    aspects.",
  paper = "Kalt87a.pdf",
  keywords = "survey,axiomref"
}

@article{Kemp81,
  author = "Kempelmann, Helmut",
  title = {{Recursive Algorithm for the Fast Calculation of the Limit of
           Derivatives at Points of Indeterminateness}},
  journal = "ACM SIGSAM",
  volume = "15",
  number = "4",
  year = "1981",
  pages = "10-11",
  abstract =
    "It is a common method in probability and queueing theory to gain the
    $n$-th moment $E[x^n]$ of a random variable $X$ 
    with density function $f_x(x)$
    by the $n$-th derivative of the corresponding Laplace transform $L(s)$ at
    the point $s = 0$
    \[E[x^n] = (-1)^n\cdot L^{n}(O)\]
    Quite often we encounter indetermined
    expressions of the form $0/0$ which normally are treated by the rule of
    L'Hospital. This is a time and memory consuming task requiring
    greatest common divisor cancellations. This paper presents an
    algorithm that calculates only those derivatives of numerator and
    denominator which do not equal zero when taking the limit /1/. The
    algorithm has been implemented in REDUCE /2/. It is simpler and more
    efficient than that one proposed by /3/.",
  paper = "Kemp81.pdf"
}

@article{Laza82,
  author = "Lazard, Daniel",
  title = {{Commutative Algebra and Computer Algebra}},
  journal = "Lecture Notes in Computer Science",
  volume = "144",
  pages = "40-48",
  year = "1982",
  abstract = 
    "It is well known that computer algebra deals with commutative rings
    such as the integers, the rationals, the modular integers and
    polynomials over such rings.
    
    On the other hand, commutative algebra is that part of mathematics
    which studies commutative rings.
    
    Our aim is to make this link more precise. It will appear that most of
    the constructions which appear in classical commutative algebra can be
    done explicitly in finite time. However, there are
    exceptions. Moreover, most of the known algorithms are intractable :
    The problems are generally exponential by themselves, but many
    algorithms are over-over-exponential. It needs a lot of work to find
    better methods, and to implement them, with the final hope to open a
    computation domain larger than this one which is possible by hand.
    
    To illustrate the limits and the possibilities of computerizing
    commutative algebra, we describe an algorithm which tests the
    primality of a polynomial ideal and we give an example of a single
    algebraic extension of fields $K\subset L$ wuch that there exists an
    algorithm of factorization for the polynomials over $k$, but not for
    the polynomials over $L$.",
  paper = "Laza82.pdf"
}

@article{Laza82a,
  author = "Lazard, Daniel",
  title = {{On Polynomial Factorization}},
  journal = "Lecture Notes in Computer Science",
  volume = "144",
  pages = "144-157",
  year = "1982",
  abstract = 
    "We present new algorithms for factoring univariate polynomials
    over finite fields. They are variants of the algorithms of Camion
    and Cantor-Zassenhaus and differ from them essentially by
    computing the primitive idempotents of the algebra $K[X]/f$ before
    factoring $f$.

    These algorithms are probabilistic in the following sense. The
    time of computation depends on random choices, but the validity of
    the result does not depend on them. So, worst case complexity,
    being infinite, is meaningless and we compute average
    complexity. It appears that our and Cantor-Zassenhaus algorithms
    have the same asymptotic complexity and they are the best
    algorithms actuall known; with elementary multiplication and GCD
    computation, Cantor-Zassenhaus algorithm is always bettern than
    ours; with fast multiplication and GCD, it seems that ours is
    better, but this fact is not yet proven.

    Finally, for factoring polynomials over the integers, it seems
    that the best strategy consists in choosing prime moduli as big as
    possible in the range where the time of the multiplication does
    not depend on the size of the factors (machine word size). An
    accurate computation of the involved constants would be needed for
    proving this estimation.",
  paper = "Laza82a.pdf"
}

@article{Loos72a,
  author = "Loos, Rudiger",
  title = {{Analytic Treatment of Three Similar Fredholm Integral Equations
           of the second kind with REDUCE 2}},
  journal = "ACM SIGSAM",
  volume = "21",
  year = "1972",
  pages = "32-40"
}

@article{Norf82,
  author = "Norfolk, Timothy S.",
  title = {{Symbolic Computation of Residues at Poles and Essential
           Singularities}},
  journal = "ACM SIGSAM",
  volume = "16",
  number = "1",
  year = "1982",
  pages = "17-23",
  abstract =
    "Although most books on the theory of complex variables include a
    classification of the types of isolated singularities, and the
    applications of residue theory, very few concern themselves with
    methods of computing residues. In this paper we derive some results on
    the calculation of residues at poles, and some special classes of
    essential singularities, with a view to implementing an algorithm in
    the VAXIMA computer algebra system.",
  paper = "Norf82.pdf"
}

@article{Padg82,
  author = "Padget, J.A.",
  title = {{Escaping from Intermediate Expression Swell: A Continuing Saga}},
  journal = "Lecture Notes in Computer Science",
  volume = "144",
  pages = "256-262",
  year = "1982",
  abstract = 
    "The notion of a closed continuation is introduced, is presented,
    coroutines using function call and return based on this concept, are
    applications and a functional dialect of LISP shown to be merely a
    more general form of for coroutines in algebraic simplification and
    are suggested, by extension function.  expression Potential evaluation
    and a specific example of their use is given in a novel attack on the
    phenomenon of intermediate expression swell in polynomial
    multiplication.",
  paper = "Padg82.pdf"
}

@article{Plat09,
  author = "Platzer, Andre and Quesel, Jan-David and Rummer, Philipp",
  title = {{Real World Verification}},
  journal = "LNCS",
  volume = "5663",
  year = "2009",
  pages = "495-501",
  link = "\url{http://www.cs.cmu.edu/~aplatzer/pub/rwv.pdf}",
  abstract =
    "Scalable handling of real arithmetic is a crucial part of the
    verification of hybrid systems, mathematical algorithms, and mixed
    analog/digital circuits.  Despite substantial advances in verification
    technology, complexity issues with classical decision procedures are
    still a major obstacle for formal verification of real-world
    applications, e.g., in automotive and avionic industries.  To identify
    strengths and weaknesses, we examine state of the art symbolic
    techniques and implementations for the universal fragment of
    real-closed fields: approaches based on quantifier elimination,
    Groebner Bases, and semidefinite programming for the
    Positivstellensatz. Within a uniform context of the verification tool
    KeYmaera, we compare these approaches qualitatively and quantitatively
    on verification benchmarks from hybrid systems, textbook algorithms,
    and on geometric problems. Finally, we introduce a new decision
    procedure combining Groebner Bases and semidefinite programming for
    the real Nullstellensatz that outperforms the individual approaches on
    an interesting set of problems.",
  paper = "Plat09,pdf"
}  

@misc{Schr17,
  author = "Wikipedia",
  title = {{Schreier-Sims algorithm}},
  year = "2017",
  link = 
    "\url{https://en.wikipedia.org/wiki/Schreier\%E2%80%93Sims\_algorithm}",
  abstract =
    "The Schreier-Sims algorithm is an algorithm in computational group
    theory named after mathematicians Otto Schreier and Charles Sims. Once
    performed, it allows a linear time computation of the order of a
    finite permuation group, group membership test (is a given permutation
    contained in a group?), and many other tasks. The algorith was
    introduced by Sims in 1970, based on Schreier's subgroup lemma. The
    timing was subsequently improved by Donald Knuth in 1991. Later, an
    even faster randomized version of the algorithm was developed.
    
    The algorithm is an efficient method of computing a base and strong
    generating set (BSGS) of a permutation group. In particular, an SGS
    determines the order of a group and makes it easy to test membership
    in the group. Since the SGS is critical for many algorithms in
    computational group theory, computer algebra systems typically rely on
    the Schreier-Sims algorithm for efficient calculations in groups"
}

@book{Somm01,
  author = "Sommer, Gerald",
  title = {{Geometric Computing with Clifford Algebras}},
  year = "2001",
  publisher = "Springer",
  isbn = "3-540-41198-4"
}

@misc{Wang90a,
  author = "Wang, Dongming",
  title = {{Some NOtes on Algebraic Methods for Geometry Theorem Proving}},
  link = "\url{http://www-polsys.lip6.fr/~wang/papers/GTPnote.ps.gz}",
  year = "1990",
  abstract = 
    "A new geometry theorem prover which provides the first complete
    implementation of Wu's method and includes several Groebner bases
    based methods is reported. This prover has been used to prove a number
    of non-trivial geometry theorems including several {\sl large} ones
    with less space and time cost than using the existing provers. The
    author presents a new technique by introducing the notion of {\sl
    normal ascending set}. This technique yields in some sense {\sl
    simpler} non-degenerate conditions for Wu's method and allows one to
    prove geometry theorems using characteristic sets but Groeber bases
    type reduction. Parallel variants of Wu's method are discussed; an
    implementation of the parallelized version of his algorithm utilizing
    workstation networks has also been included in our prover. Timing
    statistics for a set of typical examples is given.",
  paper = "Wang90a.pdf"
}

@inproceedings{Wang92,
  author = "Wang, Dongming",
  title = {{A Method for Factorizing Multivariate Polynomials over Successive
           Algebraic Extension Fields}},
  booktitle = "Mathematics and Mathematics-Mechanization (2001)",
  pages = "138-172",
  institution = "Johannes Kepler University",
  link = "\url{http://www-polsys.lip6.fr/~wang/papers/Factor.ps.gz}",
  year = "1992",
  abstract =
    "We present a method for factorizing multivariate polynomials over
    algebraic fields obtained from successive extensions of the rational
    number field. The basic idea underlying this method is the reduction
    of polynomial factorization over algebraic extension fields to the
    factorization over the rational number vield via linear transformation
    and the computation of characteristic sets with respect to a proper
    variable ordering. The factors over the algebraic extension fields are
    finally determined via GCD (greatest common divisor) computations. We
    have implemented this method in the Maple system. Preliminary
    experiments show that it is rather efficient. We give timing
    statistics in Maple 4.3 on 40 test examples which were partly taken
    from the literature and partly randomly generated. For all those
    examples to which Maple built-in algorithm is applicable, our
    algorithm is always faster.",
  paper = "Wang92.pdf"
}  

@InProceedings{Kalt96b,
  author = "Kaltofen, E.",
  title = {{Blocked iterative sparse linear system solvers for finite fields}},
  booktitle = "Proc. Symp. Parallel Comput. Solving Large Scale Irregular 
               Applic. (Stratagem '96)",
  editor = "C. Roucairol",
  publisher = "INRIA",
  address = "Sophia Antipolis, France",
  pages = "91--95",
  year = "1996",
  keywords = "survey",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/96/Ka96_stratagem.ps.gz}",
  abstract = "
    The problem of solving a large sparse or structured system of linear
    equations in the symbolic context, for instance when the coefficients
    lie in a finite field, has arisen in several applications. A famous
    example are the linear systems of ${\bf F}_2$, the field with 2
    elements, that arise in sieve based integer factoring algorithms. For
    example, for the factorization of the RSA-130 challenge number several
    column dependencies of a $3504823\times 3516502$ matrix with an
    average of $39.4.$ non-zero entries per column needed to be computed
    [10]. A second example is the Berlekamp polynomial factorization
    algorithm [6]. In that example, the matrix is not explicitly
    constructed, but instead a fast algorithm for performing the matrix
    times vector product is used. Further examples for such ``black box
    matrices'' arise in the power series solution of algebraic or
    differential equations by undetermined coefficients. The arising
    linear systems for the coefficients usually have a distinct structure
    that allows a fast coefficient matrix times vector product.",
  paper = "Kalt96b.ps"
}

@Article{Kalt04,
  author = "Kaltofen, E. and Villard, G.",
  title = {{Computing the sign or the value of the determinant of an integer 
           matrix, a complexity survey}},
  journal = "J. Computational Applied Math.",
  volume = "162",
  number = "1",
  month = "January",
  pages = "133--146",
  year = "2004",
  keywords = "survey",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/02/KaVi02.pdf}",
  abstract = "
    Computation of the sign of the determinant of a matrix and the
    determiant itself is a challenge for both numerical and exact
    methods. We survey the complexity of existing methods to solve these
    problems when the input is an $n\times n$ matrix $A$ with integer
    entries. We study the bit-complexities of the algorithms
    asymptotically in $n$ and the norm $A$.  Existing approaches rely on
    numerical approximate computations, on exact computations, or on both
    types of arithmetic in combination.",
  paper = "Kalt04.pdf"
}

@Article{Kalt00,
  author = "Kaltofen, E.",
  title = {{Challenges of Symbolic Computation My Favorite Open Problems}},
  journal = "Journal of Symbolic Computation",
  volume = "29",
  number = "6",
  pages = "891--919",
  year = "2000",
  keywords = "survey",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/2K/Ka2K.pdf}",
  abstract = "
    The success of the symbolic mathematical computation discipline is 
    striking. The theoretical advances have been continuous and significant:
    Gr{\"o}bner bases, the Risch integration algorithm, integer lattice 
    basis reduction, hypergeometric summation algorithms, etc. From the
    beginning in the early 60s, it has been the tradition of our discipline
    to create software that makes our ideas readily available to a 
    scientists, engineers, and education: {SAC-1}, {Reduce}, {Macsyma}, etc.
    The commercial viability of our system products is proven by Maple and
    Mathematica.

    Today's user communities of symbolic computation systems are diverse:
    educators, engineers, stock market anaysts, etc. The mathematics and
    computer science in the design and implementation of our algorithms are
    sophisticated. The research challenges in symbolic computation at the 
    close of the 20th century are formidable.

    I state my favorite eight open problems in symbolic computations. They
    range from problems in symbolic /numeric computing, symbolic algorithm
    synthesis, to system component construction. I have worked on seven of
    my problems and borrowed one from George Collins. I present background
    to each of my problems and a clear-cut test that evaluates whether a
    proposed attack has solved one of my problems. An additional ninth
    open problem by Rob Corless and David Jeffrey on complex function
    semantics is given in an appendix.",
  paper = "Kalt00.pdf"
}

@InCollection{Kalt93a,
  author = "Kaltofen, E.",
  editor = "J. Reif",
  title = {{Dynamic parallel evaluation of computation {DAG}s}},
  booktitle = "Synthesis of Parallel Algorithms",
  pages = "723--758",
  publisher = "Morgan Kaufmann Publ.",
  year = "1993",
  address = "San Mateo, California",
  keywords = "survey",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/93/Ka93_synthesis.ps.gz}",
  abstract = "
    One generic parallel evaluation scheme for algebraic objects, that of
    evaluating algebraic computation trees or formulas, is presented by
    Miller in a preceding chapter of this book. However, there are basic
    algebraic functions for which the tree model of computation seems not
    sufficient to allow an eficient -- even sequential -- decision-free
    algebraic computation. The formula model essentially restricts the use
    of an intermediate result to a single place, because the parse tree
    nodes have fan-out one. If an intermediate result participates in the
    computations of several further nodes, in the tree model it must be
    recomputed anew for each of these nodes. It is a small formal change
    to allow node values to propagate to more than one node deeper level
    of the computation. Thus we obtain the {\sl algebraic circuit model},
    which is equivalent to the {\sl straight-line program model}.",
  paper = "Kalt93a.ps"
}

@misc{Corl93,
  author = "Corless, R. M. and Gonnet, G. H. and Hare, D. E. G. and
            Jeffrey, D. J. and Knuth, D. E.",
  title = {{On the Lambert W Function}},
  year = "1993",
  link = "\url{http://cs.uwaterloo.ca/research/tr/1993/03/W.pdf}",
  abstract = 
   "The Lambert W function is defined to be the multivalued inverse of
    the function $w \rightarrow we^w$. It has many applications in pure
    and applied mathematics, some of which are briefly described here.  We
    present a new discussion of the complex branches of $W$, an asymptotic
    expansion valid for all branches, an efficient numerical procedure for
    evaluating the function to arbitrary precision, and a method for the
    symbolic integration of expressions containing $W$.",
  paper = "Corl93.pdf"
}

@InProceedings{Hutt10,
  author = "Hutton, Sharon E. and Kaltofen, Erich L. and Zhi, Lihong",
  title = {{Computing the radius of positive semidefiniteness of a 
           multivariate real polynomial via a dual of {Seidenberg}'s method}},
  year = "2010",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'10",
  pages = "227--234",
  month = "July",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/10/HKZ10.pdf}",
  abstract = "
    We give a stability criterion for real polynomial inequalities with
    floating point or inexact scalars by estimating from below or
    computing the radius of semdefiniteness. That radious is the maximum
    deformation of the polynomial coefficent vector measured in a weighted
    Euclidean vector norm within which the inequality remains true. A
    large radius means that the inequalities may be considered numerically
    valid.

    The radius of positive (or negative) semidefiniteness is the distance
    to the nearest polynomial with a real root, which has been thoroughly
    studied before. We solve this problem by parameterized Lagrangian
    multipliers and Karush-Kuhn-Tucker conditions. Our algorithms can
    compute the radius for several simultaneous inequalities including
    possibly additional linear coefficient constraints. Our distance
    measure is the weighted Euclidena coefficient norm, but we also
    discuss several formulas for the weighted infinity and 1-norms.

    The computation of the nearest polynomial with a real root can be
    interpreted as a dual of Seidenberg's method that decides if a real
    hypersurface contains a real point. Sums-of-squares rational lower
    bound certificates for the radius of semidefinitesness provide a new
    approach to solving Seidenberg's problem, especially when the
    coeffcients are numeric. They also offer a surprising alternative
    sum-of-squares proof for those polynomials that themselves cannot be
    represented by a polynomial sum-of-squares but that have a positive
    distance to the nearest indefinte polynomial.",
  paper = "Hutt10.pdf"
}

@InProceedings{Kalt06,
  author = "Kaltofen, Erich and Zhi, Lihong",
  title = {{Hybrid Symbolic-Numeric Computation}},
  year = "2006",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'06",
  pages = "7",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/06/KaZhi06.pdf}",
  abstract = "
    Several standard problems in symbolic computation, such as greatest
    common divisors and factorization of polynomials, sparse
    interpolation, or computing solutions to overdetermined systems of
    polynomial equations have non-trivial solutions only if the input
    coefficients satisfy certain algebraic constraints. Errors in the
    coefficients due to floating point round-off or through physical
    measurement thus render the exact symbolic algorithms unusable. By
    symbolic-numeric methods one computes minimal deformations of the
    coefficients that yield non-trivial results. We will present hybrid
    algorithms and benchmark computations based on Gauss-Newton
    optimazation, singular value decomposition (SVD) and
    structure-preserving total least squares (STLS) fitting for several of
    the above problems.

    A significant body of results to solve those ``approximate computer
    algebra'' problems has been discovered in the past 10 years. In the
    Computer Algebra Handbook the section on ``Hybrid Methods'' concludes
    as follows [2]: ``The challenge of hybrid symbolic-numeric algorithms
    is to explore the effects of imprecision, discontinuity, and
    algorithmic complexity by applying mathematical optimization,
    perturbation theory, and inexact arithmetic and other tools in order
    to solve mathematical problems that today are not solvable by
    numeriiical or symbolic methods alone.'' The focus of our tutorial is
    on how to formulate several approximate symbolic computation problems
    as numerical problems in linear algebra and optimization and on
    software that realizes their solutions.",
  paper = "Kalt06.pdf"
}

@misc{Hjor10,
  author = "Hjorth-Jensen, Morten",
  title = {{Computational Physics}},
  year = "2010",
  link = "\url{http://depts.washington.edu/ph506/Hjorth-JensenLectures2010.pdf}",
  paper = "Hjor10.pdf"
}

@InProceedings{Kalt09,
  author = "Kaltofen, Erich and Yang, Zhengfeng and Zhi, Lihong",
  title = {{A Proof of the {Monotone Column Permanent (MCP) Conjecture} for 
           Dimension 4 via Sums-Of-Squares of Rational Function}},
  year = "2009",
  booktitle = "Proc. 2009 Internat. Workshop on Symbolic-Numeric Comput.",
  pages = "65--69",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/09/KYZ09.pdf}",
  abstract = "
    For a proof of the monotone column permanent (MCP) conjecture for
    dimension 4 it is sufficient to show that 4 polynomials, which come
    rom the permanents of real marices, are nonnegative for all real
    values of the variables, where the degrees and the number of the
    variables of these polynomials are all 8. Here we apply a hybrid
    symbolic-numerical algorithm for certifying that these polynomials can
    be written as an exact fraction of two polynomial sums-of-squares
    (SOS) with rational coefficients.",
  paper = "Kalt09.pdf"
}

@Article{Kalt12,
  author = "Kaltofen, Erich L. and Li, Bin and Yang, Zhengfeng and 
            Zhi, Lihong",
  title = {{Exact Certification in Global Polynomial Optimization
           Via Sums-Of-Squares of Rational Functions
           with Rational Coefficients}},
  year = "2012",
  month = "January",
  journal = "Journal of Symbolic Computation",
  volume  = "47",
  number = "1",
  pages = "1--15",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/09/KLYZ09.pdf}",
  abstract = "
    We present a hybrid symbolic-numeric algorithm for certifying a
    polynomial or rational function with rational coefficients to be
    non-negative for all real values of the variables by computing a
    representation for it as a fraction of two polynomial sum-of-squares
    (SOS) with rational coeficients. Our new approach turns the earlier
    methods by Peyrl and Parrilo and SCN'07 and ours at ISSAC'08 both
    based on polynomial SOS, which do not always exist, into a universal
    algorithm for all inputs via Artin's theorem.

    Furthermore, we scrutinize the all-important process of converting the
    numerical SOS numerators and denomiators produced by block
    semidefinite programming into an exact rational identity. We improve
    on our own Newton iteration-based high precision refinement algorithm
    by compressing the initial Gram matrices and by deploying rational
    vector recovery aside from orthogonal projection. We successfully
    demenstrate our algorithm on 1. various exceptional SOS problems with
    necessary polynomial denominators from the literature and on 2. very
    large (thousands of variables) SOS lower bound certificates for Rump's
    model problem (up to $n=18$, factor degree $=17$).",
  paper = "Kalt12.pdf"
}

@InProceedings{Kalt08b,
  author = "Kaltofen, Erich and Li, Bin and Yang, Zhengfeng and Zhi, Lihong",
  title = {{Exact Certification of Global Optimality of Approximate
           Factorizations Via Rationalizing Sums-Of-Squares
           with Floating Point Scalars}},
  year = "2008",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'08",
  pages = "155--163",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/08/KLYZ08.pdf}",
  abstract = "
    We generalize the technique by Peyrl and Parillo [Proc. SNC 2007] to
    computing lower bound certificates for several well-known
    factorization problems in hybrid symbolic-numeric computation. The
    idea is to transform a numerical sum-of-squares (SOS) representation
    of a positive polynomial into an exact rational identity. Our
    algorithms successfully certify accurate rational lower bounds near
    the irrational global optima for benchmark approximate polynomial
    greatest common divisors and multivariate polynomial irreducibility
    radii from the literature, and factor coefficient bounds in the
    setting of a model problem by Rump (up to $n=14$, factor degree $=13$).

    The numeric SOSes produced by the current fixed precision
    semi-definite programming (SDP) packages (SeDuMi, SOSTOOLS, YALMIP)
    are usually too coarse to allow successful projection to exact SOSes
    via Maple 11's exact linear algebra. Therefore, before projection we
    refine the SOSes by rank-preserving Newton iteration. For smaller
    problems the starting SOSes for Newton can be guessed without SDP
    (``SDP-free SOS''), but for larger inputs we additionally appeal to
    sparsity techniques in our SDP formulation.",
  paper = "Kalt08b.pdf"
}

@InProceedings{Kalt06b,
  author = "Kaltofen, Erich and Yang, Zhengfeng and Zhi, Lihong",
  title = {{Approximate greatest common divisors of several polynomials
           with linearly constrained coefficients and singular polynomials}},
  year = "2006",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'06",
  pages = "169--176",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/06/KYZ06.pdf}",
  abstract = "
    We consider the problem of computing minimal real or complex
    deformations to the coefficients in a list of relatively prime real or
    complex multivariate polynomials such that the deformed polynomials
    have a greatest common divisor (GCD) of a least a given degree $k$. In
    addition, we restrict the deformed coefficients by a given set of
    linear constraints, thus introducing the {\sl linearly constrained
    aproximate GCD} problem.  We present an algorithm based on a version
    of the structured total least norm (STLN) method and demonstrate, on a
    diverse set of benchmark polynomials, that the algorithm in practice
    computes globally minimal approximations. As an application of the
    linearly constrained approximate GCD problem, we present an STLN-based
    method that computes for a real or complex polynomial the nearest real
    or complex polynomial the nearest real or complex polynomial that has
    a root of multiplicity at least $k$.  We demonstrate that the
    algorithm in practice computes, on the benchmark polynomials given in
    the literate, the known globally optimal nearest singular
    polynomials. Our algorithms can handle, via randomized
    preconditioning, the difficult case when the nearest solution to a
    list of real input polynomials actually has non-real complex
    coefficients.",
  paper = "Kalt06b.pdf"
}

@InCollection{Kalt05,
  author = "Kaltofen, Erich and Yang, Zhengfeng and Zhi, Lihong",
  title = {{Structured Low Rank Approximation of a Sylvester Matrix}},
  booktitle = "Symbolic-Numeric Computation",
  publisher = "Springer",
  pages = "69--83",
  year = "2005",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/05/KYZ05.pdf}",
  abstract = "
    The task of determining the approximate greatest common divisor (GCD)
    of univariate polynomials with inexact coefficients can be formulated
    as computing for a given Sylvester matrix a new Sylvester matrix of
    lower rank whose entries are near the corresponding entries of that
    input matrix.  We solve the approximate GCD problem by a new method
    based on structured total least norm (STLN) algorithms, in our case
    for matrices with Sylvester structure. We present iterative algorithms
    that compute an approximate GCD and that can certify an approximate
    $\epsilon$-GCD when a tolerence $\epsilon$ is given on input. Each
    single iteration is carried out with a number of floating point
    operations that is of cubic order in the input degrees. We also
    demonstrate the practical performance of our algorithms on a diverse
    set of univariate pairs of polynomials.",
  paper = "Kalt05.pdf"
}

@InProceedings{Kalt03a,
  author = "Kaltofen, Erich and May, John",
  title = 
    {{On Approximate Irreducibility of Polynomials in Several Variables}},
  year = "2003",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'03",
  pages = "161--168",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/03/KM03.pdf}",
  abstract = "
    We study the problem of bounding all factorizable polynomials away
    from a polynomial that is absolutely irreducible. Such separation
    bounds are useful for testing whether a numerical polynomial is
    absolutely irreducible, given a certain tolerance on its coefficients
    Using an absolute irreducibility criterion due to Ruppert, we are able
    to find useful separation bounds, in several norms, for bivariate
    polynomials.  We also use Ruppert's criterion to derive new, more
    effective Noether forms for polynomials of arbitrarily many
    variables. These forms lead to small separation bounds for polynomials
    of arbitrarily many variables.",
  paper = "Kalt03a.pdf"
}

@InProceedings{Gao04a,
  author = "Gao, Shuhong and Kaltofen, Erich and May, John P. and 
            Yang, Zhengfeng and Zhi, Lihong",
  title = {{Approximate factorization of multivariate polynomials via 
           differential equations}},
  year = "2004",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'04",
  pages = "167--174",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/04/GKMYZ04.pdf}",
  abstract = "
    The input to our algorithm is a multivariate polynomial, whose complex
    rational coefficient are considered imprecise with an unknown error
    that causes $f$ to be irreducible over the complex numbers {\bf C}.
    We seek to perturb the coefficients by a small quantity such that the
    resulting polynomial factors over {\bf C}. Ideally, one would like to
    minimize the perturbation in some selected distance measure, but no
    efficient algorithm for that is known. We give a numerical
    multivariate greatest common divisor algorithm and use it on a
    numerical variant of algorithms by W. M. Ruppert and S. Gao. Our
    numerical factorizer makes repeated use of singular value
    decompositions. We demonstrate on a significant body of experimental
    data that our algorithm is practical and can find factorizable
    polynomials within a distance that is about the same in relative
    magnitude as the input error, even when the relative error in the
    input is substantial ($10^{-3}$).",
  paper = "Gao04a.pdf"
}

@Article{Kalt08,
  author = "Kaltofen, Erich and May, John and Yang, Zhengfeng and Zhi, Lihong",
  title = {{Approximate Factorization of Multivariate Polynomials Using
           Singular Value Decomposition}},
  year = "2008",
  journal = "Journal of Symbolic Computation",
  volume = "43",
  number = "5",
  pages = "359--376",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/07/KMYZ07.pdf}",
  paper = "Kalt08.pdf"
}

@InProceedings{Hitz99,
  author = "Hitz, M.A. and Kaltofen, E. and Lakshman, Y.N.",
  title = {{Efficient Algorithms for Computing the Nearest Polynomial
           With A Real Root and Related Problems}},
  booktitle = "Proc. 1999 Internat. Symp. Symbolic Algebraic Comput.",
  pages = "205--212",
  year = "1999",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/99/HKL99.pdf}",
  paper = "Hitz99.pdf"
}

@InProceedings{Hitz98,
  author = "Hitz, M. A. and Kaltofen, E.",
  title = {{Efficient Algorithms for Computing the Nearest Polynomial
           with Constrained Roots}},
  booktitle = "Proc. 1998 Internat. Symp. Symbolic Algebraic Comput.",
  year = "1998",
  pages = "236--243",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/98/HiKa98.pdf}",
  paper = "Hitz98.pdf"
}

@InProceedings{Diaz91,
  author = "Diaz, A. and Kaltofen,E. and Schmitz, K. and Valente, T.",
  title = {{DSC A System for Distributed Symbolic Computation}},
  booktitle = "Proc. 1991 Internat. Symp. Symbolic Algebraic Comput.",
  pages = "323--332",
  year = "1991",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/91/DKSV91.pdf}",
  paper = "Diaz91.pdf"
}

@InProceedings{Chan94,
  author = "Chan, K.C. and Diaz, A. and Kaltofen, E.",
  editor = "R. J. Lopez",
  title = {{A distributed approach to problem solving in Maple}},
  booktitle = "Maple V: Mathematics and its Application",
  pages = "13--21",
  publisher = {Birkh\"auser},
  year = "1994",
  series = "Proceedings of the Maple Summer Workshop and Symposium (MSWS'94)",
  address = "Boston",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/94/CDK94.ps.gz}",
  paper = "Chan94.ps"
}

@InProceedings{Duma02,
  author = "Dumas, J.-G. and Gautier, T. and Giesbrecht, M. and Giorgi, P.
            and Hovinen, B. and Kaltofen, E. and Saunders, B.D. and
            Turner, W.J. and Villard, G.",
  title = {{LinBox: A Generic Library for Exact Linear Algebra}},
  booktitle =     "Proc. First Internat. Congress Math. Software ICMS 2002,
                   Beijing, China",
  pages =         "40--50",
  year = "2002",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/02/Detal02.pdf}",
  paper = "Duma02.pdf"
}

@inproceedings{Einw95,
  author = "Einwohner, T. and Fateman, Richard J.",
  title = {{Searching Techniques for Integral Tables}},
  booktitle = "ISSAC 95",
  year = "1995",
  pages = "133-139",
  abstract =
    "We describe the design of data structures and a computer program for
    storing a table of symbolic indefinite or definite integrals and
    retrieving user-requested integrals on demand.  Typical times are so
    short that a preliminary look-up attempt prior to any algorithmic
    integration approach seems justified.  In one such test for a table
    with around 700 entries, matches were found requiring an average of
    2.8 milliseconds per request, on a Hewlett Packard 9000/712 
    workstation.",
  paper = "Einw95.pdf"
}

@InProceedings{Kalt05a,
  author = "Kaltofen, Erich and Morozov, Dmitriy and Yuhasz, George",
  title = {{Generic Matrix Multiplication and Memory Management in LinBox}},
  year = "2005",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'05",
  pages = "216--223",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/05/KMY05.pdf}",
  paper = "Kalt05a.pdf"
}

@InProceedings{Diaz98,
  author = "Diaz, A. and Kaltofen, E.",
  title = {{FoxBox, a System for Manipulating Symbolic Objects in Black Box
           Representation}},
  booktitle = "Proc. 1998 Internat. Symp. Symbolic Algebraic Comput.",
  publisher = "ACM Press",
  year = "1998",
  pages = "30--37",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/98/DiKa98.pdf}",
  abstract =
    "The FOXBOX system puts in practice the black box representation of
    symbolic objects and provides algorithms for performing the symbolic
    calculus with such representations. Black box objects are stored as
    functions. For instance: a black box polynomial is a procedure that
    takes values for the variables as input and evaluates the polynomial
    at that given point. FOXBOX can compute the greatest common divisor
    and factorize polynomials in black box representation, producing as
    output new black boxes. It also can compute the standard sparse
    distributed representation of a black box polynomial, for example, one
    which was computed for an irreducible factor. We establish that the
    black box representation of objects can push the size of symbolic
    expressions far beyond what standard data structures could handle
    before.

    Furthermore, FOXBOX demonstrates the generic program design
    methodology. The FOXBOX system is written in C++. C++ template
    arguments provide for abstract domain types. Currently, FOXBOX can be
    compiled with SACLIB 1.1, Gnu-MP 1.0, and NTL 2.0 as its underlying
    field and polynomial arithmetic. Multiple arithmetic plugins can be
    used in the same computation. FOXBOX provides an MPI compliant
    distribution mechanism that allows for parallel and distributed
    execution of FOXBOX programs. Finally, FOXBOX plugs into a
    server/client-style Maple application interface.",
  paper = "Diaz98.pdf"
}

@InProceedings{Diaz93,
  author = "Diaz, A. and Kaltofen, E. and Lobo, A. and Valente, T.",
  editor = "A. Miola",
  title = {{Process scheduling in DSC and the large sparse linear 
           systems challenge}},
  booktitle = "Proc. DISCO '93",
  series = "Lect. Notes Comput. Sci.",
  pages = "66--80",
  year = "1993",
  volume = "722",
  publisher = "Springer-Verlag",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/93/DHKLV93.pdf}",
  paper = "Diaz93.pdf"
}

@Article{Diaz95a,
  author = "Diaz, A. and Hitz, M. and Kaltofen, E. and Lobo, A. and 
            Valente, T.",
  title = {{Process scheduling in DSC and the large sparse linear 
           systems challenge}},
  journal = "Journal of Symbolic Computing",
  year = "1995",
  volume = "19",
  number = "1--3",
  pages = "269--282",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/95/DHKLV95.pdf}",
  paper = "Diaz95a.pdf"
}

@Article{Free88,
  author = "Freeman, T.S. and Imirzian, G. and Kaltofen, E. and 
            Yagati, Lakshman",
  title = {{DAGWOOD: A system for manipulating polynomials given by 
           straight-line programs}},
  journal = "ACM Trans. Math. Software",
  year = "1988",
  volume = "14",
  number = "3",
  pages = "218--240",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/88/FIKY88.pdf}",
  paper = "Free88.pdf"
}

@article{Norm82,
  author = "Norman, Arthur",
  title = {{The Development of a Vector-Based Algebra System}},
  journal = "Lecture Notes in Computer Science",
  volume = "144",
  pages = "237-248",
  year = "1982",
  paper = "Norm82.pdf"
}

@InCollection{Kalt10a,
  author = "Kaltofen, Erich L.",
  title = {{The ``Seven Dwarfs'' of Symbolic Computation}},
  booktitle =    "Numeric and Symbolic Scientific Computing
                  Progress and Prospects",
  publisher = "Springer",
  pages = "95--104",
  year = "2010",
  keywords = "survey",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/10/Ka10_7dwarfs.pdf}",
  paper = "Kalt10a.pdf"
}

@inproceedings{Bro86,
  author = "Bronstein, Manuel",
  title = {{Gsolve: a faster algorithm for solving systems of algebraic 
           equations}},
  booktitle = "Proc of 5th ACM SYMSAC",
  year = "1986",
  pages = "247-249",
  isbn = "0-89791-199-7",
  abstract = "
    We apply the elimination property of Gr{\"o}bner bases with respect to
    pure lexicographic ordering to solve systems of algebraic equations.
    We suggest reasons for this approach to be faster than the resultant
    technique, and give examples and timings that show that it is indeed
    faster and more correct, than MACSYMA's solve."
}

@inproceedings{Corl95,
  author = "Corless, Robert M. and Gianni, Patrizia, M. and Trager, Barry M.
            and Watt, Stephen M.",
  title = {{The Singular Value Decomposition for Polynomial Systems}},
  booktitle = "ISSAC 95",
  year = "1995",
  pages = "195-207",
  publisher = "ACM",
  abstract =
    "This paper introduces singular value decomposition (SVD) algorithms
    for some standard polynomial computations, in the case where the
    coefficients are inexact or imperfectly known. We first give an
    algorithm for computing univariate GCD's which gives {\sl exact}
    results for interesting {\sl nearby} problems, and give efficient
    algorithms for computing precisely how nearby. We generalize this to
    multivariate GCD computations. Next, we adapt Lazard's $u$-resultant
    algorithm for the solution of overdetermined systems of polynomial
    equations to the inexact-coefficent case. We also briefly discuss an
    application of the modified Lazard's method to the location of
    singular points on approximately known projections of algebraic curves.",
  paper = "Corl95.pdf",
  keywords = "axiomref",
}  

@article{Lixx10,
  author = "Li, Xiaoliang and Mou, Chenqi and Wang, Dongming",
  title = {{Decomposing polynomial sets into simple sets over finite fields:
           The zero-dimensional case}},
  comment = "Provides clear polynomial algorithms",
  journal = "Computers and Mathematics with Applications",
  volume = "60",
  pages = "2983-2997",
  year = "2010",
  abstract = 
    "This paper presents algorithms for decomposing any zero-dimensional
    polynomial set into simple sets over an arbitrary finite field, with
    an associated ideal or zero decomposition. As a key ingredient of
    these algorithms, we generalize the squarefree decomposition approach
    for univariate polynomials over a finite field to that over the field
    product determined by a simple set. As a subprocedure of the
    generalized squarefree decomposition approach, a method is proposed to
    extract the $p$th root of any element in the field
    product. Experiments with a preliminary implementation show the
    effectiveness of our algorithms.",
  paper = "Lixx10.pdf"
}  

@book{Acto70,
  author = "Acton, F.S.",
  title = {{Numerical Methods that (Usually) Work}},
  year = "1970",
  publisher = "Harper and Row",
  address = "New York, USA"
}

@book{Acto96,
  author = "Acton, F.S.",
  title = {{Real Computing Made Real: Preventing Errors in Scientific
           and Engineering Calculations}},
  year = "1996",
  publisher = "Princeton University Press",
  address = "Princeton, N.J. USA",
  isbn = "0-691-03663-2"
}

@techreport{Ahre15,
  author = "Ahrens, Peter and Nguyen, Hong Diep and Demmel, James",
  title = {{Efficient Reproducible Floating Point Summation and BLAS}},
  institution = "University of California, Berkeley",
  year = "2015",
  month = "December",
  type = "technical report",
  number = "229",
  link = "\url{http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-229.pdf}",
  abstract =
    "We define reproducibility to mean getting bitwise identical results
    from multiple runs of the same program, perhaps with different
    hardware resources or other changes that should ideally not change the
    answer. Many users depend on reproducibility for debugging or
    correctness. However, dynamic scheduling of parallel computing
    resources, combined with nonassociativity of floating point addition,
    makes attaining reproducibility a challenge even for simple operations
    like summing a vector of numbers, or more complicated operations like
    Basic Linear Algebra Subprograms (BLAS). We describe an algorithm that
    computes a reproducible sum of floating point numbers independent of
    the order of summation. The algorithm depends only on a subset of the
    IEEE Floating Point Standard 754-2008. It is communication-optimal, in
    the sense that it does just one pass over the data in the sequential
    case, or one reduction operation in the parallel case, requiring an
    ``accumulator'' represented by just 6 floating point words (more can
    be used if higher precision is desired). Th arithmetic code with a
    6-word accumulator is $7n$ floating point additions to sum $n$ words,
    and (in IEEEE double precision) the final error bound can be up to
    $10^8$ times smaller than the error bound for conventional
    summation. We describe the basic summation algorithm, the software
    infrastructure used to build reproducible BLAS (ReproBLAS), and
    performance results. For example, when computing the dot product of
    4096 double precision floating point numbers, we get a $4x$ slowdown
    compared to Intel Math Kernel Library (MKL) running on an Intel Core
    i7-2600 CPU operating at 3.4 GHz and 256 KB L2 Cache.",
  paper = "Ahre15.pdf"
}

@article{Alef00,
  author = "Alefeld, G. and Mayer, G.",
  title = {{Interval analysis: Theory and applications}},
  journal = "J. Comput. Appl. Math.",
  volume = "121",
  pages = "421-464",
  year = "2000"
}

@article{Anda94,
  author = "Anda, A.A. and Park, H.",
  title = {{Fast plane rotations with dynamic scaling}},
  journal = "SIAM J. matrix Anal. Appl.",
  volume = "15",
  year = "1994",
  pages = "162-174"
}

@phdthesis{Anda95,
  author = "Anda, Andrew Allen",
  title = {{Self-Scaling Fast Plane Rotation Algorithms}},
  school = "University of Minnesota",
  year = "1995",
  month = "February",
  abstract = 
    "A suite of {\sl self-scaling} fast circular plane rotation algorithms
    is developed which obviates the monitoring and periodic rescaling
    necessitated by the standard set of fast plane rotation
    algorithms. Self-Scaling fast rotations dynamically preserve the
    normed scaling of the diagonal factor matrix whereas standard fast
    rotations exhibit divergent scaling. Variations on standard fast
    rotation matrices are developed and algorithms which implement them
    are offered. Self-Scaling fast rotations are shown to be essentially
    as accurate as slow rotations and at least as efficient as standard
    fast rotations. Computational experimental results utilizing the
    Cray-2 illustrate the effectively stable scaling exhibited by
    self-scaling fast rotations. Jacobi-class algorithms with one-sided
    alterations are developed for the algebraic eigenvalue decomposition
    using self-scaling fast plane rotations and one-sided
    modifications. The new algorithms are shown to be both accurate and
    efficient on both vector and parallel architectures. The utility is
    described of applying fast plane rotations towards the rank-one update
    and downdate of least squares factorizations. The equivalence is
    illuminated of LINPACK, hyperbolic rotation, and fast negatively
    weighted plane rotation downdating. Algorithms are presented which
    apply self-scaling fast plane rotations to the QR factorization for
    stiff least squares problems. Both fast and standard Givens
    rotation-based algorithms are shown to produce accurate results
    regardless of row sorting even with extremely heavily row weighted
    matrices. Such matrices emanate, e.g. from equality constrainted least
    squares problems solved via the weighting method. The necessity of
    column sorting is emphasized. Numerical tests expose the Householder
    QR factorization algorithm to be sensitive to row sorting and it
    yields less accurate results for greater weights. Additionally, the
    modified Gram-Schmidt algorithm is shown to be sensitive to row
    sorting to a notably significant but lesser degree. Self-Scaling fast
    plane rotation algorithms, having competitive computational
    complexities, must therefore be the method of choice for the QR
    factorization of stiff matrices. Timing reults on the Cray 2 [XY]M/P,
    and C90, of rotations both in and out of a matrix factorization
    context are presented. Architectural features that can best exploit
    the advantagous features of the new fast rotations are subsequently
    discussed.",
  paper = "Anda95.pdf"
}

@misc{LAPA99,
  author = "Anderson E. et al.",
  title = {{LAPACK User's Guide Third Addition}},
  year = "1999",
  month = "August",
  link = "\url{http://www.netlib.org/lapack/lug/}"
}

@book{Ande99,
  author = "Anderson, E. and Bai, Z. and Bischof, S. and Blackford, S. and
            Demmel, J. and Dongarra, J. J. and DuCroz, J. and Greenbaum, A.
            and Hammarling, S. and McKenney, A. Sorensen, D. C.",
  title = {{LAPACK Users' Guide}},
  publisher = "SIAM",
  year = "1999",
  isbn = "0-89871-447-8",
  link = "\url{http://www.netlib.org/lapack/lug/}"
}

@misc{Baud12,
  author = "Baudin, Michael and Smith, Robert L.",
  title = {{A Robust Complex Division in Scilab}},
  link = "\url{http://arxiv.org/pdf/1210.4539.pdf}",
  year = "2012",
  month = "October",
  abstract = 
    "The most widely used algorithm for floating point complex division,
    known as Smith's method, may fail more often than expected. This
    document presents two improved complex division algorithms. We present
    a proof of robustness of the first improved algorithm. Numerical
    simulations show that this algorithm performs well in practice and is
    significantly more robust than other known implementations. By
    combining additional scaling methods with this first algorithm, we
    were able to create a second algorithm, which rarely fails.",
  paper = "Baud12.pdf"
}

@article{Bind02,
  author = "Bindel, D. and Demmel, J. and Kahan, W. and Marques, O.",
  title = {{On computing Givens rotations reliably and efficiently}},
  journal = "ACM Trans. Math. Software",
  volume = "28",
  pages = "206-238",
  year = "2002"
}  

@article{Blac97,
  author = "Blackford, L. S. and Cleary, A. and Demmel, J. and Dhillon, I.
            and Dongarra, J. J. and Hammarling, S. and Petitet, A. and
            Ren, H. and Stanley, K. and Whaley, R. C.",
  title = {{Practical experience in the numerical dangers of heterogeneous
           computing}},
  journal = "ACM Trans. Math. Software",
  volume = "23",
  pages = "133-147",
  year = "1997"
}

@techreport{MMEF96,
  author = "Boisvert, Ronald F. and Pozo, Roldan and Remington, Karin A.",
  title = {{The Matrix Market Exchange Formats: Initial Design}},
  year = "1996",
  month = "December",
  institution = "National Institute of Standards and Technology",
  type = "Technical Report",
  link = "\url{http://math.nist.gov/MatrixMarket/reports/MMformat.ps}",
  abstract = 
    "We propose elementary ASCII exchange formats for matrices. Specific
    instances of the format are defined for dense and sparse matrices with
    real, complex, integer and pattern entries, with special cases for
    symmetric, skew-symmetric and Hermitian matrices. Sparse matrices are
    represented in a coordinate storage format. The overall file structure
    is designed to allow future definition of other specialized matrix
    formats, as well as for objects other than matrices.",
  paper = "MMEF96.pdf"
}

@article{Bran97,
  author = "Brankin, R. W. and Gladwell, I.",
  title = {{rksuite\_90: Fortran 90 software for ordinary differential
           equation initial-value problems}},
  journal = "ACM Trans. Math. Software",
  volume = "23",
  pages = "402-415",
  year = "1997"
}

@techreport{Bran92,
  author = "Brankin, R. W. and Gladwell, I. and Shampine, L. F.",
  title = {{RKSUITE: A suite of runge-kutta codes for the initial value
           problem for ODEs}},
  year = "1992",
  institution = "Southern Methodist University, Dept of Math.",
  number = "Softreport 92-S1",
  type = "Technical Report"
}  

@article{Bran93,
  author = "Brankin, R. W. and Gladwell, I. and Shampine, L. F.",
  title = {{RKSUITE: A suite of explicit runge-kutta codes}},
  journal = "Contributions ot Numerical Mathematics",
  pages = "41-53",
  publisher = "World Scientific",
  year = "1993"
}

@book{Brit92,
  author = "Britton, J. L.",
  title = {{Collected Works of A. M. Turing: Pure Mathematics}},
  publisher = "North-Holland",
  year = "1992",
  isbn = "0-444-88059-3"
}

@misc{Bron99,
  author = "Bronstein, Manuel",
  title = {{Fast Deterministic Computation of Determinants of Dense Matrices}},
  link = "\url{http://www-sop.inria.fr/cafe/Manuel.Bronstein/publications/mb_papers.html}",
  abstract = "
    In this paper we consider deterministic computation of the exact
    determinant of a dense matrix $M$ of integers. We present a new
    algorithm with worst case complexity
    \[O(n^4(log n+ log \verb?||M||?)+x^3 log^2 \verb?||M||?)\], 
    where $n$ is the dimension of the matrix
    and \verb?||M||? is a bound on the entries in $M$, but with 
    average expected complexity 
    \[O(n^4+m^3(log n + log \verb?||M||?)^2)\],
    assuming some plausible properties about the distribution of $M$.
    We will also describe a practical version of the algorithm and include
    timing data to compare this algorithm with existing ones. Our result
    does not depend on ``fast'' integer or matrix techniques.",
  paper = "Bron99.pdf"
}

@misc{Broo11,
  author = "Brookes, Mike",
  title = {{The Matrix Reference Manual}},
  year = "2011",
  link = "\url{http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/intro.html}"

}

@misc{Chel06,
  author = "Chellappa, Srinivas and Franchetti, Franz and Puschel, Markus",
  title = {{How To Write Fast Numerical Code: A Small Introduction}},
  year = "2006",
  institution = "Carnegie Mellon University",
  link = "\url{https://users.ece.cmu.edu/~franzf/papers/gttse07.pdf}",
  algebra = "\newline\refto{package BLAS1 BlasLevelOne}",
  abstract =
    "The complexity of modern computing platforms has made it extremely
    difficult to write numerical code that achieves the best possible
    performance. Straightforward implementations based on algorithms that
    minimize the operations count often fall short in performance by at
    least one order of magnitude. This tutorial introduces the reader to a
    set of general techniques to improve the performance of numerical
    code, focusing on optimizations for the computer's memory hierarchy. 
    Further, program generators are discussed as a way to reduce the 
    implementation and optimization effort. Two running examples are 
    used to demonstrate these techniques: matrix-matrix multiplication 
    and the discrete Fourier transform.",
  paper = "Chel06.pdf"
}

@book{Chai96,
  author = "Chaitin-Chatelin, F. and Fraysse, V.",
  title = {{Lectures on Finite Precision Computations}},
  publisher = "SIAM",
  year = "1996",
  isbn = "0-89871-358-7"
}

@article{Chan83,
  author = "Chan, T. F. and Golub, G. H. and LeVeque, R. J.",
  title = {{Algorithms for computing the sample variance: Analysis and
           recommendations}},
  journal = "The American Statistician",
  volume = "37",
  pages = "242-247",
  year = "1983"
}

@article{Cool03,
  author = "Cools, R. and Haegemans, A.",
  title = {{Algorithm 824: CUBPACK: A package for automatic cubature;
           framework description}},
  journal = "ACM Trans. Math. Software",
  volume = "29",
  pages = "287-296",
  year = "2003"
}

@techreport{Coxx00,
  author = "Cox, M. G. and Dainton, M. P. and Harris, P. M.",
  title = {{Testing spreadsheets and other packages used in metrology:
           Testing functions for the calculation of standard deviation}},
  year = "2000",
  institution = "National Physical Lab, Teddington, Middlesex UK",
  type = "Technical Report",
  number = "NPL Report CMSC07/00"
}

@article{Davi11,
  author = "Davis, Timothy A. and Hu, Yifan",
  title = {{The University of Florida Sparse Matrix Collection}},
  journal = "ACM Trans. on Math. Software",
  volume = "38",
  number = "1",
  year = "2011",
  month = "November",
  link = "\url{http://yifanhu.net/PUB/matrices.pdf}",
  abstract = 
    "We describe the Univerity of Florida Sparse Matrix Collection, a large
    and actively growing set of sparse matrices that arise in real
    applications. The Collection is widely used by the numerical linear
    algebra community for the development and performance evaluation of
    sparse matrix algorithms. It allows for robust and repeatable
    experiments: robust because performance results with artificially
    generated matrices can be misleading, and repeatable because matrices
    are curated and made publicly available in many formats. Its matrices
    cover a wide spectrum of domains, including those arising from
    problems with underlying 2D or 3D geometry (as structural engineering,
    computational fluid dynamics, model reduction, electromagnetics,
    semiconductor devices, thermodynamics, materials, acoustics, computer
    graphics/vision, robitics/kinematics, and other discretizations) and
    those that typically do not have such geometry (optimization, circuit
    simulation, economic and financial modeling, theoretical and quantum
    chemistry, chemical process simulation, mathematics and statistics,
    power networks, and other networks and graphs). We provide software
    for accessing and managing the Collection, from MATLAB, Mathematica,
    Fortran, and C, as well as an online search capability. Graph
    visualization of the matrices is provided, and a new multilevel
    coarsening scheme is proposed to facilitate this task.",
  paper = "Davi11.pdf"
}

@techreport{Davi16,
  author = "Davis, Timothy and Rajamanickam, Sivasankaran and
            Sid-Lakhdar, Wissam M.",
  title = {{A survey of direct methods for sparse linear systems}},
  year = "2016",
  month = "April",
  institution = "Texas A and M",
  type = "Technical Report",
  link = "\url{http://faculty.cse.tamu.edu/davis/publications_files/survey_tech_report.pdf}",
  abstract = 
    "Wilkinson defined a sparse matrix as one with enough zeros that it
    pays to take advantage of them. This informal yet practical definition
    captures the essence of the goal of direct methods for solving sparse
    matrix problems. They exploit the sparsity of a matrix to solve
    problems economically: much faster and using far less memory than if
    all the entries of a matrix were stored and took part in explicit
    computations. These methods form the backbone of a wide range of
    problems in computational science. A glimpse of the breadth of
    applications relying on sparse solvers can be seen in the origins of
    matrices in published matrix benchmark collections. The goal of this
    survey article is to impart a working knowledge of the underlying
    theory and practice of sparse direct methods for solving linear
    systems and least-squares problems, and to provide an overview of the
    algorithms, data structures, and software available to solve these
    problems, so that the reader can both understand the methods and know
    how best to use them.",
  paper = "Davi16.pdf"
}

@techreport{Demm88,
  author = "Demmel, James and Kahan, W.",
  title = {{Computing Small Singular Values of Bidiagonal Matrices with
           Guaranteed High Relative Accuracy}},
  year = "1988",
  institution = "New York University",
  type = "Technical Report",
  number = "326",
  abstract = 
    "Computing the singular values of a bidiagonal matrix is the final
    phase of the standard algorithm for the singular value decomposition
    of a general matrix. We present a new algorithm which computes all the
    singular values of a bidiagonal matrix to high relative accuracy
    independent of their magnitudes. In contrast, the standard algorithm
    for bidiagonal matrices may compute small singular values with no
    relative accuracy at all. Numerical experiments show that the new
    algorithm is comparable in speed to the standard algorithm, and
    frequently faster. We also show how to accurately compute tiny
    eigenvalues of some classes of symmetric trigiagonal matrices using
    the same technique.",
  paper = "Demm88.pdf"
}

@techreport{Demm05,
  author = "Demmel, James and Hida, Yozo and Kahan, W. and Li, Xiaoye S.
           and Mukherjee, Soni and Riedy, E. Jason",
  title = {{Error Bounds from Extra Precise Iterative Refinement}},
  year = "2005",
  institution = "Univerity of California, Berkeley",
  type = "Technical Report",
  number = "165",
  link = "\url{http://www.netlib.org/lapack/lawnspdf/lawn165.pdf}",
  abstract = 
    "We present the design and testing of an algorithm for iterative
    refinement of the solution of linear equations, were the residual is
    computed with extra precision. This algorithm was originally proposed
    in the 1960s as a means to compute very accurate solutions to all but
    the most ill-conditioned linear systems of equations. However two
    obstacles hae until now prevented its adoption in standard subroutine
    libraries like LAPACK: (1) There was no standard way to access the
    higher precision arithmetic needed to compute residuals, and (2) it
    was unclear how to compute a reliable error bound for the computed
    solution. The completion of the new BLAS Technical Forum Standard has
    recently removed the first obstacle. To overcome the second obstacle,
    we show how a single application of iterative refindment can be used
    to copute an error bound in any norm at small cost, and use this to
    compute both an error bound in the usual infinity norm, and a
    componentwise relative error bound.

    We report extensive test results on over 6.2 million matrices of
    dimension 5, 10, 100, and 1000. As long as a normwise
    (resp. componentwise) condition number computed by the algorithm is
    less than $1/max(10,\sqrt{n})\epsilon_w$, the computed normwise
    (resp. componentwise) error bound is at most
    $2max(10,\sqrt{n})\epsilon_w$, and indeed bounds the true error. Here,
    $n$ is the matrix dimension and $\epsilon_w$ is a single precision
    roundoff error. For worse conditioned problems, we get similarly small
    correct error bounds in over 89.4\% of cases.",
  paper = "Demm05.pdf"
}

@techreport{Demm08,
  author = "Demmel, James and Hoemmen, Mark and Hida, Yozo 
           and Riedy, E. Jason",
  title = {{Non-Negative Diagonals and High Performance on Low-Profile
           Matrices from Householder QR}},
  year = "2008",
  institution = "Univerity of California, Berkeley",
  type = "Technical Report",
  number = "203",
  link = "\url{http://www.netlib.org/lapack/lawnspdf/lawn203.pdf}",
  abstract = 
    "The Householder reflections used in LAPACK's QR factorization leave
    positive and negative real entries along R's diagonal. This is
    sufficient for most applications of QR factorizations, but a few
    require that R have a non-negative diagonal. This note provides a new
    Householder generation routine to produce a non-negative
    diagonal. Additionally, we find that scanning for trailing zeros in
    the generated reflections leads to large performance improvements when
    applying reflections with many trailing zeros. Factoring low-profile
    matrices, those with non-zero entries mostly near the diagonal (e.g
    band matrices), now requires far fewer operations. For example, QR
    factorization of matrices with profile width $b$ that are stored
    densely in an $n \cross n$ matrix improves form $O(n^3)$ 
    to $O(n^2+nb^2)$.",
  paper = "Demm08.pdf"
}

@article{Demm90,
  author = "Demmel, James and Kahan, W.",
  title = {{Accurate Singular Values of Bidiagonal Matrices}},
  journal = "SIAM J. Sci. Stat. Comput.",
  volume = "11",
  number = "5",
  pages = "873-912",
  year = "1990",
  link = "\url{http://www.netlib.org/lapack/lawnspdf/lawn03.pdf}",
  abstract = 
    "Computing the singular values of a bidiagonal matrix is the final
    phase of the standard algorithm for the singular value decomposition
    of a general matrix. We present a new algorithm which computes all the
    singular values of a bidiagonal matrix to high relative accuracy
    independent of their magnitudes. In contrast, the standard algorithm
    for bidiagonal matrices may compute small singular values with no
    relative accuracty at all. Numerical experiments show that the new
    algorithm is comparable in speed to the standard algorithm, and
    frequently faster.",
  paper = "Demm90.pdf"
}

@article{Demm92,
  author = "Demmel, James and Veselic, Kresimir",
  title = {{Jacobi's Method is More Accurate than QR}},
  journal = "SIAM J. Matrix Anal. and Appl",
  volume = "13",
  number = "4",
  year = "1992",
  pages = "1204-1245",
  link = "\url{http://www.netlib.org/lapack/lawnspdf/lawn15.pdf}",
  abstract =
    "We show that Jacobi's method (with a proper stopping criterion)
    computes small eigenvalues of symmetric positive definite matrices
    with a uniformly better relative accuracy bound than QR, divide and
    conquer, traditional bisection, or any algorithm which first involves
    tridiagonalizing the matrix. In fact, modulo an assumption based on
    extensive numerical tests, we show that Jacobi's method is optimally
    accurate in the following sense: if the matrix is such that small
    relative errors in its entries cause small relative errors in its
    eigenvalues, Jacobi will compute them with nearly this accuracy. In
    other words, as long as the initial matrix has small relative errors
    in each component, even using infinite precision will not improve on
    Jacobi (modulo factors of dimensionality). We also show the
    eigenvectors are computed more accurately by Jacobi than previously
    thought possible. We prove similar results for using one-sided Jacobi
    for the singular value decomposition of a general matrix.",
  paper = "Demm92.pdf"
}

@article{Rijk98,
  author = "de Rijk, P.P.",
  title = {{A One-sided Jacobi Algorithm for Computing the Singular Value 
           Decomposition on a vector computer}},
  journal = "SIAM J. Sci. Stat. Comput.",
  volume = "10",
  number = "2",
  month = "March",
  year = "1989",
  pages = "359-371",
  abstract =
    "An old algorithm for computing the singular value decomposition,
    which was first mentioned by Hestenes, has gained renewed interest
    because of its properties of parallelism and vectorizability. Some
    computational modifications are given and a comparison with the
    well-known Golub-Reinsch algorithm is made. Comparative experiments on
    a CYBER 205 are reported."
}

@phdthesis{Dhil97,
  author = "Dhillon, Inderjit Singh",
  title = {{A New $O(n^2)$ Algorithm for the Symmetric Tridiagonal 
           Eigenvalue/Eigenvector Problem}},
  school = "University of California, Berkeley",
  year = "1997",
  link = "\url{http://www.eecs.berkeley.edu/Pubs/TechRpts/1997/CSD-97-971.pdf}",
  abstract =
    "Computing the eigenvalues and orthogonal eigenvectors of an $n\times n$
    symmetric tridiagonal matrix is an important task that arises while
    solving any symmetric eigenproblem. All practical software requires
    $O(n^3)$ time to compute all the eigenvectors and ensure their
    orthogonality when eigenvalues are close. In the first part of this
    thesis we review earlier work and show how some existing
    implementations of inverse iteration can fail in surprising ways.
    
    The main contribution of this thesis is a new $O(n^2)$, easily
    parallelizable algorithm for solving the tridiagonal
    eigenproblem. Three main advances lead to our new algorithm. A
    tridiagonal matrix is traditionally represented by its diagonal and
    off-diagonal elements. Our most important advance is in recognizing
    that its bidiagonal factors are ``better'' for computational
    purposes. The use of bidiagonals enables us to invoke a relative
    criterion to judge when eigenvalues are ``close''. The second advance
    comes with using multiple bidiagonal factorizations in order to
    compute different eigenvectors independently of each other. Thirdly,
    we use carefully chosen dqds-like transformations as inner loops to
    compute eigenpairs that are highly accurate and ``faithful'' to the
    various bidiagonal representations. Orthogonality of the eigenvectors
    is a consequence of this accuracy. Only $O(n)$ work per eigenpair is
    neede by our new algorithm.
    
    Conventional wisdom is that there is usually a trade-off between speed
    and accuracy in numerical procedures, i.e., higher accuracy can be
    achieved only at the expense of greater computing time. An interesting
    aspect of our work is that increased accuracy in the eigenvalues and
    eigenvectors obviates the need for explicit orthogonalization and
    leads to greater speed.
    
    We present timing and accuracy results comparing a computer
    implementation of our new algorithm with four existing EISPACK and
    LAPACK software routines. Our test-bed contains a variety of
    tridiagonal matrices, some coming from quantum chemistry
    applications. The numerical results demonstrate the superiority of
    our new algorithm. For example, on a matrix of order 966 that occurs in
    the modeling of a biphenyl molecule our method is about 10 times
    faster than LAPACK's inverse iteration on a serial IBM RS/6000
    processor and nearly 100 times faster on a 128 processor IBM SP2
    parallel machine.",
  paper = "Dhil97.pdf"
}

@article{Dhil04a,
  author = "Dhillon, Inderjit S. and Parlett, Beresford N.",
  title = {{Multiple representations to compute orthogonal eigenvectors 
           of symmetric tridiagonal matrices}},
  journal = "Linear Algebra and its Applications",
  volume = "387",
  number ="1",
  pages = "1-28",
  year = "2004",
  month = "August",
  abstract = 
    "In this paper we present an $O(nk)$ procedure, Algorithm $MR^3$, for
    computing $k$ eigenvectors of an $n\times n$ symmetric tridiagonal
    matrix $T$. A salient feature of the algorithm is that a number of
    different $LDL^t$ products ($L$ unit lower triangular, $D$ diagonal)
    are computed. In exact arithmetic each $LDL^t$ is a factorization of a
    translate of $T$. We call the various $LDL^t$ productions 
    {\sl representations} (of $T$) and, roughly speaking, there is a
    representation for each cluster of close eigenvalues. The unfolding of
    the algorithm, for each matrix, is well described by a 
    {\sl representation tree}. We present the tree and use it to show that if
    each representation satisfies three prescribed conditions then the
    computed eigenvectors are orthogonal to working accuracy and have
    small residual norms with respect to the original matrix $T$.",
  paper = "Dhil04a.pdf"
}

@article{Dhil04,
  author = "Dhillon, Inderjit S. and Parlett, Beresford N.",
  title = {{Orthogonal Eigenvectors and Relative Gaps}},
  journal = "SIAM Journal on Matrix Analysis and Applications",
  volume = "25",
  year = "2004",
  abstract =
    "Let $LDL^t$ be the triangular factorization of a real symmetric
    $n\times n$ tridiagonal matrix so that $L$ is a unit lower bidiagonal
    matrix, $D$ is diagonal. Let $(\lambda,\nu)$ be an eigenpair, 
    $\lambda \ne 0$, with the property that both $\lambda$ and $\nu$ are 
    determined to high relative accuracy by the parameters in $L$ and $D$. 
    Suppose also that the relative gap between $\lambda$ and its nearest 
    neighbor $\mu$ in the spectrum exceeds $1/n; n|\lambda-\mu| > |\lambda|$.

    This paper presents a new $O(n)$ algorithm and a proof that, in the
    presence of round-off errors, the algorithm computes an approximate
    eigenvector $\hat{\nu}$ that is accurate to working precision 
    $|sin \angle(\nu,\hat{\nu})| = O(n\epsilon)$, where $\epsilon$ is the
    round-off unit. It follows that $\hat{\nu}$ is numerically orthogonal to
    all the other eigenvectors. This result forms part of a program to
    compute numerically orthogonal eigenvectors without resorting to the
    Gram-Schmidt process.
    
    The contents of this paper provide a high-level description and
    theoretical justification for LAPACK (version 3.0) subroutine DLAR1V.",
  paper = "Dhil04.pdf"
}

@article{Dods83,
  author = "Dodson, D. S.",
  title = {{Corrigendum: Remark on 'Algorithm 539: Basic Linear Algebra
           Subroutines for FORTRAN usage}},
  journal = "ACM Trans. Math. Software",
  volume = "9",
  pages = "140",
  year = "1983"
}

@article{Dods82,
  author = "Dodson, D. S. and Grimes, R. G.",
  title = {{Remark on algorithm 539: Basic Linear Algebra Subprograms for
           Fortran usage}},
  journal = "ACM Trans. Math. Software",
  volume = "8",
  pages = "403-404",
  year = "1982"
}

@article{Dong88,
  author = "Dongarra, J. J. and DuCroz, J. and Hammarling, S. and 
            Hanson, R. J.",
  title = {{An extended set of FORTRAN Basic Linear Algebra Subprograms}},
  journal = "ACM Trans. Math. Software",
  volume = "14",
  pages = "1-32",
  year = "1988"
}

@article{Dong88a,
  author = "Dongarra, J. J. and DuCroz, J. and Hammarling, S. and 
            Hanson, R. J.",
  title = {{Corrigenda: 'An extended set of FORTRAN Basic Linear Algebra
           Subprograms}},
  journal = "ACM Trans. Math. Software",
  volume = "14",
  pages = "399",
  year = "1988"
}

@article{Dong90,
  author = "Dongarra, J. and DuCroz, J. and Duff, I. S. and Hammarling, S.",
  title = {{A set of Level 3 Basic Linear Algebra Subprograms}},
  journal = "ACM Trans. Math. Software",
  volume = "16",
  pages = "1-28",
  year = "1990"
}

@article{Dong16,
  author = "Dongarra, Jack",
  title = {{With Extreme Scale Computing, the Rules Have Changed}},
  journal = "LNCS",
  volume = "9725",
  pages = "3-6",
  year = "2016",
  paper = "Dong16.pdf"
}

@article{Drma97,
  author = "Drmac, Zlatko",
  title = {{Implementation of Jacobi Rotations for Accurate Singular Value
           Computation in Floating Point Arithmetic}},
  journal = "SIAM Journal on Scientific Computing",
  volume = "18",
  number = "4",
  year = "1997",
  month = "July",
  pages = "1200-1222",
  abstract = 
    "In this paper we consider how to compute the singular value
    decomposition (SVD) $A = U\Sigma{}T^{\tau}$ of 
    $A=[a_1,a_2] \in R^{m\times 2}$ 
    accurately in floating point arithmetic. It is shown
    how to compute the Jacobi rotation $V$ (the right singular vector
    matrix) and how to compute $AV=U\Sigma$ even if the floating point
    representation of $V$ is the identity matrix. In the case
    underflow can produce the identity matrix as
    the floating point value of $V$ even for $a1$, $a2$ that are far from
    being mutually orthogonal. This can cause loss of accuracy and failure
    of convergence of the floating point implementation of the Jacobi
    method for computing the SVD. The modified Jacobi method recommended
    in this paper can be implemented as a reliable and highly accurate
    procedure for computing the SVD of general real matrices whenever the
    exact singular values do not exceed the underflow or overflow limits."
}

@article{Drma08a,
  author = "Drmac, Zlatko and Veselic, Kresimir",
  title = {{New fast and accurate Jacobi SVD algorithm I}},
  journal = "SIAM J. Matrix Anal. Appl.",
  volume = "35",
  number = "2",
  year = "2008",
  pages = "1322-1342",
  comment = "LAPACK Working note 169",
  link = "\url{http://www.netlib.org/lapack/lawnspdf/lawn169.pdf}",
  abstract = 
    "This paper is the result of contrived efforts to break the barrier
    between numerical accuracy and run time efficiency in computing the
    fundamental decomposition of numerical linear algebra - the singular
    value decomposition (SVD) of a general dense matrix. It is an
    unfortunate fact that the numerically most accurate one-sided Jacobi
    SVD algorithm is several times slower than generally less accurate
    bidiagonalization based methods such as the QR or the divide and
    conquer algorithm. Despite its sound numerical qualities, the Jacobi
    SVD is not included in the state of the art matrix computation
    libraries and it is even considered obsolete by some leading
    researchers. Our quest for a highly accurate and efficient SVD
    algorithm has led us to a new, superior variant of the Jacobi
    algorithm. The new algorithm has inherited all good high accuracy
    properties, and it outperforms not only the best implementations of
    the one-sided Jacobi algorithm but also the QR algorithm. Moreoever,
    it seems that the potential of the new approach is yet to be fully
    exploited.",
  paper = "Drma08a.pdf"
}

@article{Drma08b,
  author = "Drmac, Zlatko and Veselic, Kresimir",
  title = {{New fast and accurate Jacobi SVD algorithm II}},
  journal = "SIAM J. Matrix Anal. Appl.",
  volume = "35",
  number = "2",
  year = "2008",
  pages = "1343-1362",
  comment = "LAPACK Working note 170",
  link = "\url{http://www.netlib.org/lapack/lawnspdf/lawn170.pdf}",
  abstract =
    "This paper presents new implementation of one-sided Jacobi SVD for
    triangular matrices and its use as the core routine in a new
    preconditioned Jacobi SVD algorithm, recently proposed by the
    authors. New pivot strategy exploits the triangular form and uses the
    fact that the input triangular matrix is the result of rank revealing
    QR factorization. If used in the preconditioned Jacobi SVD algorithm,
    described in the first part of this report, it delivers superior
    performance leading to the currently fastest method for computing SVD
    decomposition with high relative accuracy. Furthermore, the efficiency
    of the new algorithm is comparable to the less accurate
    bidiagonalization based methods. The paper also discusses underflow
    issues in floating point implementation, and shows how to use
    perturbation theory to fix the imperfectness of machine arithmetic on
    some systems.",
  paper = "Drma08b.pdf"
}

@article{Drma08c,
  author = "Drmac, Zlatko and Bujanovic, Zvonimir",
  title = {{On the failure of rank-revealing QR factorization software - 
           a case study.}},
  journal = "ACM Trans. math. Softw.",
  volume = "35",
  number = "2",
  year = "2008",
  pages = "1-28",
  comment = "LAPACK Working note 176",
  link = "\url{http://www.netlib.org/lapack/lawnspdf/lawn176.pdf}",
  abstract = 
    "This note reports an unexpected and rather erratic behavior of the
    LAPACK software implementation of the QR factorization with
    Businger-Golub column pivoting. It is shown that, due to finite
    precision arithmetic, software implementation of the factorization can
    catastrophically fail to produce triangular factor with the structure
    characteristic to the Businger-Golub pivot strategy. The failure of
    current {\sl state of the art} software, and a proposed alternative
    implementations are analyzed in detail.",
  paper = "Drma08c.pdf"
}

@article{Dubr83,
  author = "Dubrulle, A. A.",
  title = {{A class of numerical methods for the computation of Pythagorean
           sums}},
  journal = "IBM J. Res. Develop.",
  volume = "27",
  number = "6",
  pages = "582-589",
  year = "1983"
}

@book{Eina05,
  author = "Einarsson, B.",
  title = {{Accuracy and Reliability in Scientific Computing}},
  publisher = "SIAM",
  year = "2005",
  isbn = "0-89871-584-9",
  link = "\url{http://www.nsc.liu.se/wg25/book/}"
}

@article{Elmr00,
  author = "Elmroth, E. and Gustavson, F. G.",
  title = {{Applying recursion to serial and parallel QR factorization leads
           to better performance}},
  journal = "IBM Journal of Research and Development",
  volume = "44",
  number = "4",
  month = "July",
  year = "2000",
  pages = "605--624",
  doi = "10.1.1.33.1820",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.1820}",
  abstract =
    "We present new recursive serial and parallel algorithms for QR
    factorization of an $m$ by $n$ matrix. They improve performance. The
    recursion leads to an automatic variable blocking, and it also
    replaces a Level 2 part in a standard aglorithm with Level 3
    operations. However, there are significant additional costs for
    creating and performing the updates, which prohibit the efficient use
    of the recursion for large $n$. We present a quantitative analysis of
    these extra costs. This analysis leads us to introduce a hybrid
    recursive algorithm that outperforms the LAPACK algorithm DGEQRF by
    about 20\% for large square matrices up to almost a factor of 3 for
    tall thin matrices. Uniprocessor performance results are presented for
    two IBM RS/6000 SP nodes -- a 120-Mhz IBM POWER2 node and one
    processor of a four-way 332-Mhz IBM PowerPC 604e SMP node. The hybrid
    recursive algorithm reaches more than 90\% of the theoretical peak
    performance of the POWER2 node. Compared to standard block algorithms,
    the recursive approach also shows a significant advantage in the
    automatic tuning obtained from its automatic variable blocking. A
    successful parallel implementation on a four-way 332-MHz IBM PPC604e
    SMP node based on dynamic load balancing is presented. For two, three,
    and four processors it shows speedups of up to 1.97, 299, and 3.97."
}

@misc{Fate13,
  author = "Fateman, Richard J.",
  title = {{Interval Arithmetic, Extended Numbers and Computer Algebra 
           Systems}},
  year = "2013",
  link = "\url{http://www.cs.berkeley.edu/~fateman/papers/interval.pdf}",
  abstract =
    "Many ambitious computer algebra systems were initially designed in a
    flush of enthusiasm, with the goal of automating any symbolic
    mathematical manipulation ``correctly''. Historically, this approach
    results in programs that implicitly used certain identities to
    simplify expressions. These identities, which very likely seemed
    universally true to the programmers in the heat of writing the CAS
    (and often were true in well-known abstract algebraic domains) later
    neede re-examination when such systems were extended for dealing with
    kinds of objects unanticipated in the original design. These new
    objects are generally introduced to the CAS by extending
    ``generically'' the arithmetic of other operations. For example,
    approximate floats do not have the mathematical properties of exact
    integers or rationals. Complex numbers may strain a system designed
    for real-valued variables. In the situation examined here, we consider
    two categories of ``extended'' numbers: $\infty$ and {\sl undefined},
    and real intervals. We comment on issues raised by these two
    troublesome notions, how their introduction into a computer algebra
    system may require a (sometimes painful) reconsideration and redesign
    of parts of the program, and how they are related. An alternative
    (followed most notably by the Axiom system is to essentially envision
    a ``meta'' CAS defined in terms of categories and inheritance with
    only the most fundamental built-in concepts; from these one can build
    many variants of specific CAS features. This approach is appealing but
    can fail to accommodate extensions that violate some mathematical
    tenets in the cause of practicality.",
  paper = "Fate13.pdf",
  keywords = "axiomref"
}

@techreport{Fern92,
  author = "Fernando, K. Vince and Parlett, Beresford N.",
  title = {{Accurate Singular Values and Differential qd Algorithms}},
  year = "1992",
  institution = "University of California, Berkeley",
  type = "Technical Report",
  number = "PAM-554",
  link = "\url{http://www.dtic.mil/dtic/tr/fulltext/u2/a256582.pdf}",
  abstract = 
    "We have discovered a new implementaiton of the qd algorithm that has
    a far wider domain of stability than Rutishauser's version. Our
    algorithm was developed from an examination of the LR-Cholesky
    transformation and can be adapted to parallel computation in stark
    contrast to traditional qd. Our algorithm also yields useful a
    posteriori upper and lower bounds on the smallest singular value of a
    bidiagonal matrix.

    The zero-shift bidiagonal QR of Demmel and Kahan computes the smallest
    singlar values to maximal relative accuracy and the others to maximal
    absolute accuracy with little or no degradation in efficiency when
    compared with the LINPACK code. Our algorithm obtains maximal relative
    accuracy for all the singluar values and runs at least four times
    faster than the LINPACK code.",
  paper = "Fern92.pdf"
}

@article{Fors70,
  author = "Forsythe, G. E.",
  title = {{Pitfalls in computations, or why a math book isn't enough}},
  journal = "Amer. Math. Monthly",
  volume = "9",
  pages = "931-995",
  year = "1970"
}

@incollection{Fors69,
  author = "Forsythe, G. E.",
  title = {{What is a satisfactory quadratic equation solver}},
  booktitle = "Constructive Aspects of the Fundamental Theorem of Algebra",
  pages = "53-61",
  publisher = "Wiley",
  year = "1969"
}
  
@article{Foxx71,
  author = "Fox, L.",
  title = {{How to get meaningless answers in scientific computations (and
           what to do about it)}},
  journal = "IMA Bulletin",
  volume = "7",
  pages = "296-302",
  year = "1971"
}

@article{Gent74,
  author = "Gentlman, W. Morven and Marovich, Scott B.",
  title = {{More on algorithms that reveal properties of floating point 
           arithmetic units.}},
  journal = "Comm. of the ACM",
  year = "1974",
  month = "April",
  volume = "17",
  number = "5",
  pages = "276-277",
  abstract =
    "In the interests of producing portable mathematical software, it is
    highly desirable for a program to be able directly to obtain
    fundamental properties of the environment in which it is to run. The
    installer would then not be obliged to change appropriate magic
    constants in the source code, and the user would not have to provide
    information he may very well not understand. Until the standard
    definitions of programming languages are changed to require builtin
    functions that provide this information, we will have to resort to
    writing routines that discover it."
}

@techreport{Give54,
  author = "Givens, W.",
  title = {{Numerical computation of the characteristic values of a real
           symmetric matrix}},
  year = "1954",
  institution = "Oak Ridge National Laboratory",
  type = "Technical Report",
  number = "ORNL-1574"
}

@article{Golu65,
  author = "Golub, G.H.",
  title = {{Numerical methods for solving linear least squares problems}},
  journal = "Numer. Math.",
  volume = "7",
  pages = "206-216",
  year = "1965"
}

@book{Golu89,
  author = "Golub, Gene H. and Van Loan, Charles F.",
  title = {{Matrix Computations}},
  publisher = "Johns Hopkins University Press",
  year = "1989",
  isbn = "0-8018-3772-3"
}

@book{Golu96,
  author = "Golub, Gene H. and Van Loan, Charles F.",
  title = {{Matrix Computations}},
  publisher = "Johns Hopkins University Press",
  isbn = "978-0-8018-5414-9",
  year = "1996"
}

@article{Hamm85,
  author = "Hammarling S.",
  title = {{The Singular Value Decomposition in Multivariate Statistics}},
  journal = "ACM Signum Newsletter",
  volume = "20",
  number = "3",
  pages = "2--25",
  year = "1985"
}

@book{Hamm05,
  author = "Hammarling, Sven",
  title = {{An Introduction to the Quality of Computed Solutions}},
  booktitle = "Accuracy and Reliability in Scientific Computing",
  year = "2005",
  publisher = "SIAM",
  pages = "43-76",
  link = "\url{http://eprints.ma.man.ac.uk/101/}",
  paper = "Hamm05.pdf"
}

@mastersthesis{Harg02,
  author = "Hargreaves, G.",
  title = {{Interval analysis in MATLAB}},
  school = "University of Manchester, Dept. of Mathematics",
  year = "2002"
}

@book{High05,
  author = "Higham, D. J. and Higham, N. J.",
  title = {{MATLAB Guide}},
  publisher = "SIAM",
  year = "2002",
  isbn = "0-89871-521-0"
}

@article{High88,
  author = "Higham, Nicholas J.",
  title = {{FORTRAN codes for estimating the one-norm of a real or complex 
           matrix, with applications to condition estimation}},
  journal = "ACM Trans. Math. Soft",
  volume = "14",
  number = "4",
  pages = "381-396",
  year = "1988"
}

@misc{High98,
  author = "Higham, Nicholas J.",
  title = {{Can you 'count' on your computer?}},
  link = "\url{http://www.maths.man.ac.uk/~higham/talks/}",
  year = "1998"
}

@book{High02,
  author = "Higham, Nicholas J.",
  title = {{Accuracy and stability of numerical algorithms}},
  publisher = "SIAM",
  isbn = "0-89871-521-0",
  year = "2002"
}

@article{High86,
  author = "Higham, Nicholas J.",
  title = {{Efficient Algorithms for Computing the Condition Number of a
           Tridiagonal Matrix}},
  journal = "SIAM J. Sci. Stat. Comput.",
  volume = "7",
  number = "1",
  year = "1986",
  month = "January",
  abstract = 
    "Let $A$ be a tridiagonal matrix of order $n$. We show that it is
    possible to compute $||A^{-1}||_{\infty}$ and hence 
    ${\rm cond}_{\infty}(A)$ in $O(n)$ operations. Several algorithms 
    which perform this task are given and their numerical properties are
    investigated.
    
    If $A$ is also positive definite then $||A^{-1}||_{\infty}$ can be
    computed as the norm of the solution to a positive definite
    tridiagonal linear system whose coefficient matrix is closely related
    to $A$. We show how this computation can be carried out in parallel
    with the solution of a linear system $Ax=b$. In particular we describe
    some simple modifications to the LINPACK routine SPTSL which enable
    this routine to compute ${\rm cond}_1(A)$, efficiently, in addtion to
    solving $Ax=b$.",
  paper = "High86.pdf"
}

@misc{IEEE85,
  author = "IEEE",
  title = {{ANSI/IEEE Standard for Binary Floating Point Arithmetic:
           Std 754-1985}},
  publisher = "IEEE Press",
  year = "1985"
}

@misc{IEEE87,
  author = "IEEE",
  title = {{ANSI/IEEE Standard for Radix Independent Floating Point Arithmetic:
           Std 854-1987}},
  publisher = "IEEE Press",
  year = "1987"
}

@book{Isaa94,
  author = "Isaacson, E. and Keller, H. B.",
  title = {{Analysis of Numerical Methods}},
  publisher = "Dover",
  year = "1994",
  isbn = "0-486-68029-0"
}

@article{Kags89,
  author = "Kagstrom, Bo and Westin, L.",
  title = {{Generalized Schur Methods with Condition Estimators for Solving 
           the Generalized Sylvester Equation}},
  journal = "IEEE Transactions on Automatic Control",
  volume = "34",
  number = "7",
  year = "1989",
  month = "July",
  pages = "745-751",
  abstract =
    "Stable algorithms are presented for solving the generalized Sylvester
    equation. They are based on orthogonal equivalence transformations of
    the original problem. Perturbation theory and rounding error analysis
    are included. Condition estimators (${\rm Dif}^{-1}$-estimators) are
    developed which when substituted into derived error bounds give
    accuracy estimates of a computed solution. Results from numerical
    experiments on well-conditioned and ill-conditioned problems are
    reported."
}
  
@book{Kags93a,
  author = "Kagstrom, B.",
  title = {{A Direct Method for Reordering Eigenvalues in the
           Generalized Real Schur Form of a Regular Matrix Pair (A, B)}},
  year = "1993",
  pages = "195-218",
  booktitle = "Linear Algebra for Large Scale and Real-Time Applications",
  volume = "232",
  journal = "NATO ASI Series",
  institution = "NATO ASI Series",
  isbn = "978-90-481-4246-0",  
  abstract =
    "A direct orthogonal equivalence transformation method for reordering
    the eigenvalues along the diagonal in the generalized real Schur form
    of a regular matrix pair $(A,B)$ is presented. Each swap of two
    adjacent eigenvalues (real, or complex conjugate pairs) involves
    solving a generalized Sylvester equation and the construction of two
    orthogonal transformation matrices from certain eigenspaces associated
    with the corresponding diagonal blocks. An error analysis of the
    direct reordering method is presented. Results from numerical
    experiments on well-conditionsed as well as ill-conditioned problems
    illustrate the stability and the accuracy of the method. Finally, a
    direct reordering algorithm with controlled backward error is described."
}

@techreport{Kags93,
  author = "Kagstrom, Bo and Poromaa, Peter",
  title = {{LAPACK-Style Algorithms and Software for Solving the Generalized 
           Sylvester Equation and Estimating the Separation between 
           Regular Matrix Pairs}},
  year = "1993",
  month = "December",
  link = "\url{http://www.netlib.org/lapack/lawnspdf/lawn75.pdf}",
  comment = "LAPACK Working Note 75",
  institution = "NETLIB",
  abstract =
    "Level 3 algorithms for solving the generalized Sylvester equation
    $(AR-LB,DR-LE)=(C,F)$ and the transposed analogue
    $(A^TU+D^TV,-UB^T-VE^T)=(C,F)$ are presented. These blocked algorithms
    permit reuse of data in complex memory hierarchies of current advanced
    computer architectures. The separation of two regular matrix pairs
    $(A,D)$ and $(B,E)$, Dif[$(A,D)$,$(B,E)$], is defined in terms of the
    generalized Sylvester operator $(AR-LB,DR-LE)$. Robust, efficient and
    reliable Dif-estimators are presented. The basic problem is to find a
    lower bound on Dif$^{-1}$, which can be done by solving generalized
    Sylvester equations in triangular form. Frobenius norm-based and one
    norm based Dif estimators are described and evaluated. These estimates
    lead to computable error bounds for the generalized Sylvester
    equation. The one-norm-based estimator makes the condition estimation
    uniform with LAPACK. Fortran 77 software that implements our
    algorithms for solving generalized Sylvester equations, and for
    computing error bounds and Dif-estimators are presented. Computational
    experiments that illustrate the accuracy, efficiency and reliability
    of our software are also described.",
  paper = "Kags93.pdf"
}

@misc{Kags94,
  author = "Kagstrom, Bo and Poromaa, Peter",
  title = {{Computing Eigenspaces with Specified Eigenvalues of a Regular 
           Matrix Pair (A, B) and Condition Estimation: Theory, 
           Algorithms and Software}},
  year = "1994",
  month = "April",
  link = "\url{http://www.netlib.org/lapack/lawns/lawn87.ps}",
  abstract =
    "Theory, algorithms and LAPACK style software for computing a pair of
    deflating subspaces with specified eigenvalues of a regular matrix
    pair $(A,B)$ and error bounds for computed quantities (eigenvalues and
    eigenspaces) are presented. The {\sl reordering} of specified
    eigenvalues is performed with a direct orthogonal transformation
    method with guaranteed numerical stability. Each swap of two adjacent
    diagonal blocks in the real generalized Schur form, where at least one
    of them corresponds to a complex conjugate pair of eigenvalues,
    involves solving a generalized Sylvester equation and the construction
    of two orthogonal transformation matrices from certain eigenspaces
    associated with the diagonal blocks. The swapping of two $1\cross 1$
    blocks is performed using orthogonal (unitary) Givens rotations. The
    {\sl error bounds} are based on estimates of condition numbers for
    eigenvalues and eigenspaces. The software computes reciprocal values
    of a condition number for an individual eigenvalue (or a cluster of
    eigenvalues), a condition number for an eigenvector (or eigenspace),
    and spectral projectors onto a selected cluster. By computing
    reciprocal values we avoid overflow. Changes in eigenvectors and
    eigenspaces are measured by their change in angle. The condition
    numbers yield both {\sl asymptotic} and {\sl global} error bounds. The
    asymptotic bounds are only accurate for small perturbations $(E,F)$ of
    $(A,B)$, while the global bounds work for all $||(E,F)||$ up to a
    certain bound, whose size is determined by the conditioning of the
    problem. It is also shown how these upper bounds can be
    estimated. Fortran 77 {\sl software} that implements our algorithms
    for reordering eigenvalues, computing (left and right) deflating
    subspaces with specified eigenvalues and condition number estimation
    are presented. Computational experiments that illustrate the accuracy,
    efficiency and reliability of our software are also described.",
  paper = "Kags94.pdf"
}

@article{Kags94b,
  author = "Kagstrom, Bo",
  title = {{A Perturbation Analysis of the Generalized Sylvester Equation 
           $(AR-LB,DR-LE)=(C,F)$}},
  journal = "SIAM J. Matrix Anal. and Appl.",
  volume = "15",
  number = "4",
  pages = "1045-1060",
  year = "1994",
  abstract =
    "Perturbation and error bounds for the generalized Sylvester equation
    $(AR-LB,DR-LE)=(C,F)$ are presented. An explicit expression for the
    normwise relative backward error associated with an approximate
    solution of the generalized Sylvester equation is derived and
    conditions when it can be much greater than the relative residual are
    given. This analysis is applicable to any method that solves the
    generalized Sylvester equation. A condition number that reflects the
    structure of the problem and a normwise forward error bound based on
    ${\rm Dif}^{-1}[(A,D),B,E)]$ and the residual are derived. The
    structure-preserving condition number can be arbitrarily smaller than
    a ${\rm Dif}^{-1}$-based condition number. The normwise error bound
    can be evaluated robustly and at moderate cost by using a reliable
    ${\rm Dif}^{-1}$ estimator. A componentwise LAPACK-style forward error
    bound that can be stronger than the normwise error bound is
    presented. A componentwise approximate error bound that can be
    evaluated to a much lower cost is also proposed. Finally, some
    computational experiments that validate and evaluate the perturbation
    and error bounds are presented."
}

@techreport{Kaha66,
  author = "Kahan, W.",
  title = {{Accurate Eigenvalues of a Symmetric Tri-Diagonal Matrix}},
  year = "1966",
  month = "July",
  institution = "Stanford University",
  type = "Technical Report",
  number = "CS41",
  abstract = 
    "Having established tight bounds for the quotient of two different
    lub-norms of the same tri-diagonal matrix J, the author observes that
    these bounds could be of use in an error-analysis provided a suitable
    algorithm were found. Such an algorithm is exhibited, and its errors
    are thoroughly accounted for, including the effects of scaling,
    over/underflow and roundoff. A typical result is that, on a computer
    using rounded floating point binary arithmetic, the biggest eigenvalue
    of J can be computed easily to within 2.5 units in its last place, and
    the smaller eigenvalues will suffer absolute errors which are no
    larger. These results are somewhat stronger than had been known before.",
  paper = "Kaha66.pdf"
}

@misc{Kels00,
  author = "Kelsey, Tom",
  title = {{Exact Numerical Computation via Symbolic Computation}},
  link = "\url{http://tom.host.cs.st-andrews.ac.uk/pub/ccapaper.pdf}",
  year = "2000",
  abstract = "
    We provide a method for converting any symbolic algebraic expression
    that can be converted into a floating point number into an exact
    numeric representation. We use this method to demonstrate a suite of
    procedures for the representation of, and arithmetic over, exact real
    numbers in the Maple computer algebra system. Exact reals are
    represented by potentially infinite lists of binary digits, and
    interpreted as sums of negative powers of the golden ratio.",
  paper = "Kels00.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Knus98,
  author = {Kn\"usel, L.},
  title = {{On the accuracy of statistical distributions in Microsoft
           Excel 97}},
  journal = "Comput. Statist. Data Anal.",
  volume = "26",
  pages = "375-377",
  year = "1998"
}

@misc{Kohl14,
  author = {K\"ohler, Martin and Saak, Jens},
  title = {{On BLAS Level-3 Implementations of Common Solvers for
           (Quasi-)Triangular Generalized Lyapunov Equations}},
  year = "2014",
  link = "\url{http://slicot.org/objects/software/reports/SLWN2014_1.pdf}",
  note = "SLICOT Working Note 2014-1",
  abstract =
    "The solutions of Lyapunov and generalized Lyapunov equations are a
    key player in many applications in systems and control theory. Their
    stable numerical computation, when the full solution is sought, is
    considered solved since the seminal work of Bartels and Stewart. A
    number of variants of their algorithm have been proposed, but none of
    them goes beyond BLAS level-2 style implementation. On modern
    computers, however, the formulation of BLAS level-3 type
    implementations is crucial to enable optimal usage of cache
    hierarchies and modern block scheduling methods based on directed
    acyclic graphs describing the interdependence of single block
    computations. Our contribution closes this gap by a transformation of
    the aforementioned level-2 variants to level-3 versions and a
    comparison on a standard multicore machine.",
  paper = "Kohl14.pdf"
}

@misc{Krei05,
  author = "Kreinovich, V.",
  title = {{Interval cmoputations}},
  year = "2005",
  link = "\url{http://www.cs.utep.edu/interval-comp/}"
}

@article{Kuki72a,
 author = "Kuki, Hirondo",
 title = {{Complex Gamma Function with Error Control}},
 year = "1972",
 publisher = "ACM",
 journal = "Communications of the ACM",
 volume = "15",
 number = "4",
 pages = "262-267",
 abstract = 
  "An algorithm to compute the gamma function and the loggamma function 
   of a complex variable is presented. The standard algorithm is modified
   in several respects to insure the continuity of the function value and
   to reduce accumulation of round-off errors. In addition to computation
   of function values, this algorithm includes an object-time estimation
   of round-off errors. Experimental data with regard to the effectiveness
   of this error control are presented. A Fortran program for the algorithm
   appears in the algorithms section of this issue."
}

@article{Kuki72b,
 author = "Kuki, Hirondo",
 title = {{Algorithm 421: Complex Gamma Function with Error Control}},
 year = "1972",
 publisher = "ACM",
 journal = "Communications of the ACM",
 volume = "15",
 number = "4",
 pages = "271-272",
 abstract = 
  "This Fortran program computes either the gamma function or the
   loggamma function of a complex variable in double precision. 
   In addition, it provides an error estimate of the computed answer.
   The calling sequences are:
   \verb|CALL CDLGAM (X, W, E, 0)|
   for the loggamma, and
   \verb|CALL CDLGAM (X, W, E, 1)|
   for the gamma, where Z is the double precision argument, W is the
   answer of the same type, and E is a single precision real variable.
   Before the call, the value of E is an estimate of the error in Z,
   and after the call, it is an estimate of the error in W."
}

@book{Laws74,
  author = "Lawson, C. L. and Hanson, R. J.",
  title = {{Solving Least Squares Problems}},
  publisher = "Prentice-Hall",
  year = "1974"
}

@book{Laws95,
  author = "Lawson, C. L. and Hanson, R. J.",
  title = {{Solving Least Squares Problems}},
  publisher = "SIAM",
  isbn = "0-89871-356-0",
  year = "1995"
}

@article{Livn04,
  author = "Livne, Oren E. and Golub, Gene H.",
  title = {{Scaling by Binormalization}},
  journal = "Numerical Algorithms",
  volume = "35",
  number = "1",
  pages = "97-120",
  year = "2004",
  month = "January",
  abstract =
    "We present an interative algorithm (BIN) for scaling all the rows and
    columns of a real symmetric matrix to unit 2-norm. We study the
    theoretical convergence properties and its relation to optimal
    conditioning. Numerical experiments show that BIN requires 2-4
    matrix-vector multiplications to obtain an adequate scaling, and in
    many cases significantly reduces the condition number, more than other
    scaling algorithms. We present generalizations to complex,
    non-symmetric and rectangular matrices.",
  paper = "Livn04.pdf"
}

@article{Malc72,
  author = "Malcolm, Michael A.",
  title = {{Algorithms to reveal properties of floating-point arithmetic}},
  journal = "Comms of the ACM",
  volume = "15",
  year = "1972",
  pages = "949-951",
  link = "\url{http://www.dtic.mil/dtic/tr/fulltext/u2/727104.pdf}",
  abstract =
    "Two algorithms are presented in the form of Fortran subroutines. Each
    subroutine computes the radix and number of digits of the floating
    point numbers and whether rounding or chopping is done by the machine
    on which it is run. The methods are shown to work on any ``reasonable''
    floating-point computer.",
  paper = "Malc72.pdf"
}

@article{Marq06,
  author = "Marques, Osni A. and Reidy, Jason and Vomel, Christof",
  title = {{Benefits of IEEE-754 Features in Modern Symmetric Tridiagonal 
           Eigensolvers}},
  journal = "SIAM Journal on Scientific Computing",
  volume = "28",
  number = "5",
  year = "2006",
  link = "\url{http://www.netlib.org/lapack/lawnspdf/lawn172.pdf}",
  abstract =
    "Bisection is one of the most common methods used to compute the
    eigenvalues of symmetric tridiagonal matrices. Bisection relies on the
    {\sl Sturm count}: for a given shift $\sigma$, the number of negative
    pivots in the factorization $T-\sigma{}I=LDL^T$ equals the number of
    eigenvalues of $T$ that are smaller than $\sigma$. In IEEE-754
    arithmetic, the value $\infty$ permits the computation to continue
    past a zero pivot, producing a correct Sturm count when $T$ is
    unreduced. Demmel and Li showed in the 90s that using $\infty$ rather
    than testing for zero pivots within the loop could improve performance
    significantly on certain architectures.
    
    When eigenvalues are to be computed to high relative accuracy, it is
    often preferable to work with $LDL^T$ factorizations instead of the
    original tridiagonal $T$, see for example the MRRR algorithm. In these
    cases, the Sturm count has to be computed from $LDL^T$. The
    differential stationary and progressive qds algorithms are the method
    of choice.
    
    While it seems trivial to replace $T$ by $LDL^T$, in reality these
    algorithms are more complicated: in IEEE-754 arithmetic, a zero pivot
    produces an overflow, followed by an invalid exception (NaN), that
    renders the Sturm count incorrect.
    
    We present alternative, safe formulations that are guaranteed to
    produce the correct result.
    
    Benchmarking these algorithms on a variety of platforms shows that the
    original formulation without tests is always faster provided no
    exception occurs. The transforms see speed-ups of up to 2.6x over the
    careful formulation.
    
    Tests on industrial matrices show that encountering exceptions in
    practice is rare. This leads to the following design: First, compute
    the Sturm count by the fast but unsafe algorithm. Then, if an
    exception occured, recompute the count by a safe, slower alternative.
    
    The new Sturm count algorithms improve the speed of bisection by up to
    2x on our test matrices. Furthermore, unlike the traditional
    tiny-pivot substitutions, proper use of IEEE-754 features provides a
    careful formulation that imposed no input range restrictions.",
  paper = "Marq06.pdf"
}

@article{Mart68,
  author = "Martin, R. S. and Wilkinson, J. H.",
  title = {{Similarity reduction ofa general matrix to Hessenberg form}},
  journal = "Numer. Math.",
  volume = "12",
  pages = "349-368",
  year = "1968"
}

@misc{Math05,
  author = "MathWorks",
  title = {{MATLAB}},
  publisher = "The Mathworks, Inc.",
  link = "\url{http://www.mathworks.com}"
}

@article{Mccu02,
  author = "McCullough, B. D. and Wilson, B.",
  title = {{On the accuracy of statistical procedures in Microsoft Excel
           2000 and Excel XP}},
  journal = "Comput. Statist. Data Anal.",
  volume = "40",
  pages = "713-721",
  year = "2002"
}

@article{Mccu99,
  author = "McCullough, B. D. and Wilson, B.",
  title = {{On the accuracy of statistical procedures in Microsoft Excel 97}},
  journal = "Comput. Statist. Data Anal.",
  volume = "31",
  pages = "27-37",
  year = "1999"
}

@book{Metc96,
  author = "Metcalf, M. and Reid, J. K.",
  title = {{Fortran 90/95 Explained}},
  publisher = "Oxford University Press",
  year = "1996"
}

@book{Metc04,
  author = "Metcalf, M. and Reid, J. K. and Cohen, M.",
  title = {{Fortran 95/2003 Explained}},
  publisher = "Oxford University Press",
  year = "2004",
  isbn = "0-19-852693-8"
}

@article{Mole83,
  author = "Moler, C. and Morrison, D.",
  title = {{Replacing square roots by Pythagorena sums}},
  journal = "IBM J. Res. Develop.",
  volume = "27",
  number = "6",
  pages = "577-581",
  year = "1983"
}

@article{Mole71,
  author = "Moler, C.B. and Stewart, G.W.",
  title = {{An Algorithm for Generalized Matrix Eigenvalue Problems}}, 
  journal = "SIAM J. Numer. Anal",
  volume = "10",
  year = "1973",
  pages = "241--256",
  abstract = 
    "A new method, called the QZ algorithm, is presented for the solution
    of the matrix eigenvalue problem $Ax=\lambda{}Bx$ with general square
    matrices $A$ and $B$. Particluar attention is paid to the degeracies
    which result when $B$ is singular. No inversions of $B$ or its
    submatrices are used. The algorithm is a generalization of the QR
    algorithm, and reduces to it when $B=I$. A Fortran program and some
    illustrative examples are included."
}

@book{Moon93,
  author = "Moonen, Marc S. and Golub, Gene H. and De Moor, Bart L.R.",
  title = {{Linear Algebra and Large Scale and Real-Time Applications}},
  year = "1993",
  publisher = "NATO ASI Series",
  isbn = "978-904814246-0"
}

@book{Moor79,
  author = "Moore, R. E.",
  title = {{methods and Applications of Interval Analysis}},
  publisher = "SIAM",
  year = "1979"
}

@misc{NAGa05,
  author = "Numerical Algorithms Group",
  title = {{The NAG Library}},
  link = "\url{http://www.nag.co.uk/numeric}",
  year = "2005"
}

@misc{NAGb05,
  author = "Numerical Algorithms Group",
  title = {{The NAG Fortran Library Manual}},
  link = "\url{http://www.nag.co.uk/numeric/fl/manual/html/FLlibrarymanual.asp}",
  year = "2005"
}

@book{Over01,
  author = "Overton, M. L.",
  title = {{Numerical Computing with IEEE Floating Point Arithmetic}},
  publisher = "SIAM",
  year = "2001",
  isbn = "0-89871-482-6"
}

@book{Pies83,
  author = {Piessens, R. and de Doncker-Kapenga, E. and \"Uberhuber, C. W.
            and Kahaner, D. K.},
  title = {{QUADPACK - A Subroutine Package for Automatic Integration}},
  publisher = "Springer-Verlag",
  year = "1983"
}

@misc{Pete12,
  author = "Petersen, Kaare Brandt and Pedersen, Michael Syskind",
  title = {{The Matrix Cookbook}},
  link = "\url{http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf}",
  year = "2012",
  month = "November"
} 

@article{Prie04,
  author = "Priest, D. M.",
  title = {{Efficient scaling for complex division}},
  journal = "ACM Trans. Math. Software",
  volume = "30",
  pages = "389-401",
  year = "2004"
}

@InProceedings{Rump99,
  author = "Rump, S. M.",
  title = {{INTLAB - INTerval LABoratory}},
  booktitle = "Developments in Reliable Computing",
  pages = "77-104",
  publisher = "Kluwer Academic",
  year = "1999"
}  

@InProceedings{Sham92,
  author = "Shampine, L. F. and Gladwell, I.",
  title = {{The next generation of runge-kutta codes}},
  booktitle = "Computational Ordinary Differential Equations",
  pages = "145-164",
  publisher = "Oxford University Press",
  year = "1992"
}

@article{Smit62,
  author = "Smith, R. L.",
  title = {{Algorithm 116: Complex division}},
  journal = "Communs. Ass. comput. Mach.",
  volume = "5",
  pages = "435",
  year = "1962"
}

@book{Stew98,
  author = "Stewart, G. W.",
  title = {{Matrix Algorithms: Basic Decompositions, volume I}},
  publisher = "SIAM",
  year = "1998",
  isbn = "0-89871-414-1"
}

@article{Stew85,
  author = "Stewart, G. W.",
  title = {{A note on complex division}},
  journal = "ACM Trans. Math. Software",
  volume = "11",
  pages = "238-241",
  year = "1985"
}

@book{Stew90,
  author = "Stewart, G. W. and Sun, J.",
  title = {{Matrix Perturbation Theory}},
  publisher = "Academic Press",
  year = "1990"
}

@article{Stou07,
  author = "Stoutemyer, David R.",
  title = {{Useful Computations Need Useful Numbers}},
  year = "2007",
  publisher = "ACM",
  journal = "Communications in Computer Algebra",
  volume = "41",
  number = "3",
  abstract =
    "Most of us have taken the exact rational and approximate numbers in
    our computer algebra systems for granted for a long time, not thinking
    to ask if they could be significantly better. With exact rational
    arithmetic and adjustable-precision floating-point arithmetic to
    precision limited only by the total computer memory or our patience,
    what more could we want for such numbers?  It turns out that there is
    much that can be done that permits us to obtain exact results more
    often, more intelligible results, approximate results guaranteed to
    have requested error bounds, and recovery of exact results from
    approximate ones."
}

@article{Sutt13,
  author = "Sutton, Brian D.",
  title = {{Computing the Complete CS Decomposition}},
  journal = "Numerical Algorithms",
  volume = "50",
  pages = "33-65",
  year = "2013",
  month = "February",
  link = "\url{http://arxiv.org/pdf/0707.1838v3.pdf}",
  abstract = 
    "An algorithm is developed to compute the complete CS decomposition
    (CSD) of a partitioned unitary matrix. Although the existence of the
    CSD has been recognized since 1977, prior algorithms compute only a
    reduced version (the 2-by-1 CSD) that is equivalent to two
    simultaneous singular value decompositions. The algorithm presented
    here computes the complete 2-by-2 CSD, which requires the simultaneous
    diagonalization of all four blocks of a unitary matrix partitioned
    into a 2-by-2 block structure. The algorithm appears to be the only
    fully specified algorithm available. The computation occurs in two
    phases. In the first phase, the unitary matrix is reduced to
    bidiagonal block form, as described by Sutton and Edelman. In the
    second phase, the blocks are simultaneously diagonalized using
    techniques from bidiagonal SVD algorithms of Golub, Kahan, and
    Demmel. The algorithm has a number of desirable numerical features.",
  paper = "Sutt13.pdf"
}

@article{Turi48,
  author = "Turing, A. M.",
  title = {{Rounding-off errors in matrix processes}},
  journal = "Q. J. Mech. Appl. Math.",
  volume = "1",
  pages = "287-308",
  year = "1948"
}

@article{Vign93,
  author = "Vignes, J.",
  title = {{A stochastic arithmetic for reliable scientific computation}},
  journal = "Math. and Comp. in Sim.",
  volume = "25",
  pages = "233-261",
  year = "1993"
}

@article{Ward81,
  author = "Ward, Robert C.",
  title = {{Balancing the generalized eigenvalue problem}},
  journal = "SIAM J. Sci. and Stat. Comput.",
  volume = "2",
  number = "2",
  year = "1981",
  pages = "141-152",
  abstract =
    "An algorithm is presented for balancing the $A$ and $B$ matirces
    prior to computing the eigensystem of the generalized eigenvalue
    problem $Ax=\lambda Bx$. The three-step algorithm is specifically
    designed to preceed the $QZ$-type algorithms, but improved performance
    is expected from most eigensystem solvers. Permutations and two-sided
    diagonal transformations are applied to $A$ and $B$ to produce
    matrices with certain desirable properties. Test cases are presented
    to illustrate the improved accuracy of the computed eigenvalues."
}

@book{Wilk63,
  author = "Wilkinson, J. H.",
  title = {{Rounding Erroors in Algebraic Processes}},
  publisher = "HMSO",
  series = "Notes on Applied Science, No. 32",
  year = "1963"
}

@book{Wilk65,
  author = "Wilkinson, J. H.",
  title = {{The Algebraic Eigenvalue Problem}},
  publisher = "Oxford University Press",
  year = "1965"
}

@InProceedings{Wilk84,
  author = "Wilkinson, J. H.",
  title = {{The perfidious polynomial}},
  booktitle = "Studies in Numerical Analysis",
  volume = "24",
  chapter = "1",
  pages = "1-28",
  year = "1984"
}

@article{Wilk86,
  author = "Wilkinson, J. H.",
  title = {{Error analysis revisited}},
  journal = "IMA Bulletin",
  volume = "22",
  pages = "192-200",
  year = "1986"
}

@article{Wilk61,
  author = "Wilkinson, J. H.",
  title = {{Error analysis of diret methods of matrix inversion}},
  journal = "J. ACM",
  volume = "8",
  pages = "281-330",
  year = "1961"
}

@article{Wilk85,
  author = "Wilkinson, J. H.",
  title = {{The state of the art in error analysis}},
  journal = "NAG Newsletter",
  volume = "2/85",
  pages = "5-28",
  year = "1985"
}

@article{Wilk60,
  author = "Wilkinson, J. H.",
  title = {{Error analysis of floating-point computation}},
  journal = "Numer. Math.",
  volume = "2",
  pages = "319-340",
  year = "1960"
}

@book{Wilk71,
  author = "Wilkinson, J. H.",
  title = {{Handbook for Automatic Computation, V2, Linear Algebra}},
  publisher = "Springer-Verlag",
  year = "1971"
}

@misc{Yang14,
  author ="Yang, Xiang and Mittal, Rajat",
  title = {{Acceleration of the Jacobi iterative method by factors exceeding 
           100 using scheduled relation}},
  link = "\url{http://engineering.jhu.edu/fsag/wp-content/uploads/sites/23/2013/10/JCP_revised_WebPost.pdf}",
  paper = "Yang14.pdf"
}

@inproceedings{Badd94,
  author = "Baddoura, Jamil",
  title = {{A Conjecture On Integration in Finite Terms with Elementary
           Functions and Polylogarithms}},
  booktitle = "ISSAC 94",
  year = "1994",
  pages = "158-162",
  isbn = "0-89791-638-7",
  abstract =
    "In this abstract, we report on a conjecture that gives the form of an
    integral if it can be expressed using elementary functions and
    polylogarithms. The conjecture is proved by the author in the cases of
    the dilogarithm and the trilogarithm [3] and consists of a
    generalization of Liouville's theorem on integration in finite terms
    with elementary functions. Those last structure theorems, for the
    dilogarithm and the trilogarithm, are the first case of structure
    theorems where logarithms can appear with non-constant
    coefficients. In order to prove the conjecture for higher
    polylogarithms we need to find the functional identities, for the
    polylogarithms that we are using, that characterize all the possible
    algebraic relations among the considered polylogarithms of functions
    that are built up from the rational functions by taking the considered
    polylogarithms, exponentials, logarithms and algebraics. The task of
    finding those functional identities seems to be a difficult one and is
    an unsolved problem for the most part to this date.",
  paper = "Badd94.pdf",
}

@article{Badd06,
  author = "Baddoura, Jamil",
  title = {{Integration in Finite Terms with Elementary Functions and
           Dilogarithms}},
  journal = "J. Symbolic Computation",
  volume = "41",
  number = "8",
  year = "2006",
  pages = "909-942",
  abstract =
    "In this paper, we report on a new theorem that generalizes
    Liouville’s theorem on integration in finite terms. The new theorem
    allows dilogarithms to occur in the integral in addition to
    transcendental elementary functions. The proof is based on two
    identities for the dilogarithm, that characterize all the possible
    algebraic relations among dilogarithms of functions that are built up
    from the rational functions by taking transcendental exponentials,
    dilogarithms, and logarithms. This means that we assume the integral
    lies in a transcendental tower.",
  paper = "Badd06.pdf"
}

@article{Barn89,
  author = "Barnett, Michael P.",
  title = {{Using Partial Fraction Formulas to Sum some slowly convergent
           series analytically for molecular integral calculations}},
  journal = "ACM SIGSAM",
  volume = "23",
  number = "3",
  year = "1989",
  abstract = 
    "Two sets of rational expressions, needed for quantum chemical
    calculations, have been constructed by mechanical application of
    partial fraction and polynomial operations on a CYBER 205. The
    algorithms were coded in FORTRAN, using simple array manipulation. The
    results suggest extensions that could be tackled with general
    algebraic manipulation programs.",
  paper = "Barn89.pdf"
}

@inproceedings{Bert94,
  author = "Bertrand, Laurent",
  title = {{On the Implementation of a new Algorithm for the Computation
           of Hyperelliptic Integrals}},
  booktitle = "ISSAC 94",
  isbn = "0-89791-638-7",
  pages = "211-215",
  year = "1994",
  abstract =
    "In this paper, we present an implementation in Maple of a new
    aJgorithm for the algebraic function integration problem in the
    particular case of hyperelliptic integrals.  This algorithm is based
    on the general algorithm of Trager [9] and on the arithmetic in the
    Jacobian of hyperelliptic curves of Cantor [2].",
  paper = "Bert94.pdf"
}  

@article{Brow69,
  author = "Brown, W.S.",
  title = {{Rational Exponential Expressions and a Conjecture Concerning 
           $\pi$ and $e$}},
  journal = "The American Mathematical Monthly",
  volume = "76",
  number = "1",
  year = "1969",
  pages = "28-34",
  abstract =
    "One of the most controversial and least well defined of mathematical
    problems is the problem of simplification.  The recent upsurge
    of interest in mechanized mathematics has lent new urgency to this
    problem, but so far very little has been accomplished.  This paper
    attempts to shed light on the situation by introducing the class of
    rational exponential expressions, defining simplification within
    this class, and showing constructively how to achieve it. It is shown
    that the only simplified rational exponential expression equivalent to
    0 is 0 itself, provided that an easily stated conjecture is true.
    However the conjecture, if true, will surely be difficult to prove,
    since it asserts as a special case that $\pi$ and $e$ are algebraically
    independent, and no one has yet been able to prove even the much
    weaker conjecture that $\pi+e$ is irrational.",
  paper = "Brow69.pdf"
}

@article{Clar89,
  author = "Clarkson, M.",
  title = {{MACSYMA's inverse Laplace transform}},
  journal = "ACM SIGSAM Bulletin",
  volume = "23",
  number = "1",
  year = "1989",
  pages = "33-38",
  abstract =
    "The inverse Laplace transform capability of MACSYMA has been improved
    and extended. It has been extended to evaluate certain limits, sums,
    derivatives and integrals of Laplace transforms. It also takes
    advantage of the inverse Laplace transform convolution theorem, and
    can deal with a wider range of symbolic parameters.",
  paper = "Clar89.pdf"
}

@misc{Corl05,
  author = "Corless, Robert M. and Jeffrey, David J. and Watt, Stephen M.
            and  Bradford, Russell and Davenport, James H.",
  title = {{Reasoning about the elementary functions of complex analysis}},
  link = "\url{http://www.csd.uwo.ca/~watt/pub/reprints/2002-amai-reasoning.pdf}",
  abstract = "
    There are many problems with the simplification of elementary
    functions, particularly over the complex plane. Systems tend to make
    ``howlers'' or not to simplify enough. In this paper we outline the
    ``unwinding number'' approach to such problems, and show how it can be
    used to prevent errors and to systematise such simplification, even
    though we have not yet reduced the simplification process to a
    complete algorithm.  The unsolved problems are probably more amenable
    to the techniques of artificial intelligence and theorem proving than
    the original problem of complex-variable analysis.",
  paper = "Corl05.pdf"
}

@book{Erde56,
  author = {Erd\'elyi, A.},
  title = {{Asymptotic Expansions}},
  year = "1956",
  isbn = "978-0-486-15505-0",
  publisher = "Dover Publications"
}

@misc{Ng68,
  author = "Ng, Edward W. and Geller, Murray",
  title = {{A Table of Integrals of the Error functions}},
  link =
    "\url{http://nvlpubs.nist.gov/nistpubs/jres/73B/jresv73Bn1p1_A1b.pdf}",
  abstract = "
    This is a compendium of indefinite and definite integrals of products
    of the Error functions with elementary and transcendental functions.",
  paper = "Ng68.pdf"
}

@article{Nort80,
  author = "Norton, Lewis M.",
  title = {{A Note about Laplace Transform Tables for Computer use}},
  journal = "ACM SIGSAM",
  volume = "14",
  number = "2",
  year = "1980",
  pages = "30-31",
  abstract =
    "The purpose of this note is to give another illustration of the fact
    that the best way for a human being to represent or process
    information is not necessarily the best way for a computer. The
    example concerns the use of a table of inverse Laplace transforms
    within a program, written in the REDUCE language [1] for symbolic
    algebraic manipulation, which solves linear ordinary differential
    equations with constant coefficients using Laplace transform
    methods. (See [2] for discussion of an earlier program which solved
    such equations.)",
  paper = "Nort80.pdf"
}

@article{Piqu89,
  author = "Piquette, J. C.",
  title = {{Special Function Integration}},
  journal = "ACM SIGSAM Bulletin",
  volume = "23",
  number = "2",
  year = "1989",
  pages = "11-21",
  abstract =
    "This article describes a method by which the integration capabilities
    of symbolic-mathematics computer programs can be extended to include
    integrals that contain special functions. A summary of the theory that
    forms the basis of the method is given in Appendix A. A few integrals
    that have been evaluated using the method are presented in Appendix
    B. A more thorough development and explanation of the method is given
    in Piquette, in review (b)."
}

@misc{Gell69,
  author = "Geller, Murray and Ng, Edward W.",
  title = {{A Table of Integrals of the Exponential Integral}},
  link =
    "\url{http://nvlpubs.nist.gov/nistpubs/jres/73B/jresv73Bn3p191_A1b.pdf}",
  abstract = "
    This is a compendium of indefinite and definite integrals of products
    of the Exponential Integral with elementary or transcendental functions.",
  paper = "Gell69.pdf"
}

@techreport{Segl98,
  author = "Segletes, S.B.",
  title = {{A compact analytical fit to the exponential integral $E_1(x)$}},
  year = "1998",
  institution = "U.S. Army Ballistic Research Laboratory, 
                 Aberdeen Proving Ground, MD",
  type = "Technical Report",
  number = "ARL-TR-1758",
  algebra = "\newline\refto{package DFSFUN DoubleFloatSpecialFunctions}",
  abstract = "
    A four-parameter fit is developed for the class of integrals known as
    the exponential integral (real branch). Unlike other fits that are
    piecewise in nature, the current fit to the exponential integral is
    valid over the complete domain of the function (compact) and is
    everywhere accurate to within $\pm 0.0052\%$ when evaluating the first
    exponential integral, $E_1$. To achieve this result, a methodology
    that makes use of analytically known limiting behaviors at either
    extreme of the domain is employed. Because the fit accurately captures
    limiting behaviors of the $E_1$ function, more accuracy is retained
    when the fit is used as part of the scheme to evaluate higher-order
    exponential integrals, $E_n$, as compared with the use of brute-force
    fits to $E_1$, which fail to accurately model limiting
    behaviors. Furthermore, because the fit is compact, no special
    accommodations are required (as in the case of spliced piecewise fits)
    to smooth the value, slope, and higher derivatives in the transition
    region between two piecewise domains. The general methodology employed
    to develop this fit is outlined, since it may be used for other
    problems as well.",
  paper = "Segl98.pdf"
}

@techreport{Se09,
  author = "Segletes, S.B.",
  title = {{Improved fits for $E_1(x)$ {\sl vis-\'a-vis} those presented 
           in ARL-TR-1758}},
  type = "Technical Report",
  number = "ARL-TR-1758",
  institution ="U.S. Army Ballistic Research Laboratory,
                Aberdeen Proving Ground, MD",
  year = "1998",
  month = "September",
  abstract = "
    This is a writeup detailing the more accurate fits to $E_1(x)$,
    relative to those presented in ARL-TR-1758.  My actual fits are to
    \[F1 =[x\ exp(x) E_1(x)]\] which spans a functional range from 0 to 1.
    The best accuracy I have been yet able to achieve, defined by limiting
    the value of \[[(F1)_{fit} - F1]/F1\] over the domain, is
    approximately 3.1E-07 with a 12-parameter fit, which unfortunately
    isn't quite to 32-bit floating-point accuracy.  Nonetheless, the fit
    is not a piecewise fit, but rather a single continuous function over
    the domain of nonnegative x, which avoids some of the problems
    associated with piecewise domain splicing.",
  paper = "Se09.pdf"
}

@InProceedings{Kalt99a,
  author = "Kaltofen, E. and Monagan, M.",
  title = {{On the Genericity of the Modular Polynomial {GCD} Algorithm}},
  booktitle = "Proc. 1999 Internat. Symp. Symbolic Algebraic Comput.",
  year = "1999",
  pages = "59--66",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/99/KaMo99.pdf}",
  paper = "Kalt99a.pdf"
}

@book{Knut71,
  author = "Knuth, Donald",
  title = 
    {{The Art of Computer Programming Vol. 2 (Seminumerical Algorithms)}},
  year = "1971",
  publisher = "Addison-Wesley"
}

@article{Ma90,
  author = "Ma, Keju and {von zur Gathen}, Joachim",
  title = 
    {{Analysis of Euclidean Algorithms for Polynomials over Finite Fields}},
  journal = "J. Symbolic Computation",
  year = "1990",
  volume = "9",
  pages = "429-455",
  link = "\url{http://www.researchgate.net/publication/220161718_Analysis_of_Euclidean_Algorithms_for_Polynomials_over_Finite_Fields/file/60b7d52b326a1058e4.pdf}",
  abstract = "
    This paper analyzes the Euclidean algorithm and some variants of it 
    for computing the greatest common divisor of two univariate polynomials
    over a finite field. The minimum, maximum, and average number of
    arithmetic operations both on polynomials and in the ground field
    are derived.",
  paper = "Ma90.pdf"
}

@phdthesis{Nayl00,
  author = "Naylor, Bill",
  title = {{Polynomial GCD Using Straight Line Program Representation}},
  school = "University of Bath",
  year = "2000",
  link = "\url{http://www.sci.csd.uwo.ca/~bill/thesis.ps}",
  abstract = "
    This thesis is concerned with calculating polynomial greatest common
    divisors using straight line program representation.

    In the Introduction chapter, we introduce the problem and describe
    some of the traditional representations for polynomials, we then talk
    about some of the general subjects central to the thesis, terminating
    with a synopsis of the category theory which is central to the Axiom
    computer algebra system used during this research.

    The second chapter is devoted to describing category theory. We follow
    with a chapter detailing the important sections of computer code
    written in order to investigate the straight line program subject.
    The following chapter on evalution strategies and algorithms which are
    dependant on these follows, the major algorith which is dependant on
    evaluation and which is central to our theis being that of equality
    checking. This is indeed central to many mathematical problems.
    Interpolation, that is the determination of coefficients of a
    polynomial is the subject of the next chapter. This is very important
    for many straight line program algorithms, as their non-canonical
    structure implies that it is relatively difficult to determine
    coefficients, these being the basic objects that many algorithms work
    on. We talk about three separate interpolation techniques and compare
    their advantages and disadvantages. The final two chapters describe
    some of the results we have obtained from this research and finally
    conclusions we have drawn as to the viability of the straight line
    program approach and possible extensions.

    Finally we terminate with a number of appendices discussing side
    subjects encountered during the thesis.",
  paper = "Nayl00.pdf"
}

@inproceedings{Shou93,
  author = "Shoup, Victor",
  title = {{Factoring Polynomials over Finite Fields: 
            Asymptotic Complexity vs Reality*}},
  booktitle = "Proc. IMACS Symposium, Lille, France",
  year = "1993",
  link = "\url{http://www.shoup.net/papers/lille.pdf}",
  abstract = 
    "This paper compares the algorithms by Berlekamp, Cantor and
    Zassenhaus, and Gathen and Shoup to conclude that (a) if large
    polynomials are factored the FFT should be used for polynomial
    multiplication and division, (b) Gathen and Shoup should be used if
    the number of irreducible factors of $f$ is small.  (c) if nothing is
    know about the degrees of the factors then Berlekamp's algorithm
    should be used.",
  paper = "Shou93.pdf"
}

@inproceedings{Hoei04,
  author = "van Hoeij, Mark and Monagan, Michael",
  title = {{Algorithms for Polynomial GCD Computation over Algebraic 
           Function Fields}},
  booktitle = "Proc. ISSAC 04",
  year = "2004",
  isbn = "1-58113-827-X",
  link = "\url{http://www.cecm.sfu.ca/personal/mmonagan/papers/AFGCD.pdf}",
  abstract = 
    "Let $L$ be an algebraic function field in $k \ge 0$ parameters
    $t_1,\ldots,t)k$. Let $f_1$, $f_2$ be non-zero polynomials in
    $L[x]$. We give two algorithms for computing their gcd. The first, a
    modular GCD algorithm, is an extension of the modular GCD algorithm
    for Brown for {\bf Z}$[x_1,\ldots,x_n]$ and Encarnacion for {\bf
    Q}$(\alpha[x])$ to function fields. The second, a fraction-free
    algorithm, is a modification of the Moreno Maza and Rioboo algorithm
    for computing gcds over triangular sets. The modification reduces
    coefficient grownth in $L$ to be linear.  We give an empirical
    comparison of the two algorithms using implementations in Maple.",
  paper = "Hoei04.pdf"
}

@article{Wang78,
  author = "Wang, Paul S.",
  title = {{An Improved Multivariate Polynomial Factoring Algorithm}},
  journal = "Mathematics of Computation",
  volume = "32",
  number = "144",
  year = "1978",
  pages = "1215-1231",
  link = "\url{http://www.ams.org/journals/mcom/1978-32-144/S0025-5718-1978-0568284-3/S0025-5718-1978-0568284-3.pdf}",
  abstract = "
    A new algorithm for factoring multivariate polynomials over the
    integers based on an algorithm by Wang and Rothschild is described.
    The new algorithm has improved strategies for dealing with the known
    problems of the original algorithm, namely, the leading coefficient
    problem, the bad-zero problem and the occurence of extraneous factors.
    It has an algorithm for correctly predetermining leading coefficients
    of the factors. A new and efficient p-adic algorith named EEZ is
    described. Basically it is a linearly convergent variable-by-variable
    parallel construction. The improved algorithm is generally faster and
    requires less store than the original algorithm. Machine examples with
    comparative timing are included.",
  paper = "Wang78.pdf"
}

@misc{Baez09,
  author = "Baez, John C.; Stay, Mike",
  title = {{Physics, Topology, Logic and Computation: A Rosetta Stone}},
  link = "\url{http://arxiv.org/pdf/0903.0340v3.pdf}",
  abstract = "
    In physics, Feynman diagrams are used to reason about quantum
    processes.  In the 1980s, it became clear that underlying these
    diagrams is a powerful analogy between quantum physics and
    topology. Namely, a linear operator behaves very much like a
    ``cobordism'': a manifold representing spacetime, going between two
    manifolds representing space.  But this was just the beginning: simiar
    diagrams can be used to reason about logic, where they represent
    proofs, and computation, where they represent programs. With the rise
    of interest in quantum cryptography and quantum computation, it became
    clear that there is an extensive network of analogies between physics,
    topology, logic and computation. In this expository paper, we make
    some of these analogies precise using the concept of ``closed
    symmetric monodial category''. We assume no prior knowledge of
    category theory, proof theory or computer science.",
  paper = "Baez09.pdf",
  keywords = "printed"
}

@misc{Meij91,
  author = "Meijer, Erik and Fokkinga, Maarten and Paterson, Ross",
  title = {{Functional Programming with Bananas, Lenses, Envelopes and 
           Barbed Wire}},
  link =
    "\url{http://eprints.eemcs.utwente.nl/7281/01/db-utwente-40501F46.pdf}",
  abstract = "
    We develop a calculus for lazy functional programming based on
    recursion operators associated with data type definitions. For these
    operators we derive various algebraic laws that are useful in deriving
    and manipulating programs. We shall show that all example functions in
    Bird and Wadler's ``Introduction to Functional Programming'' can be
    expressed using these operators.",
  paper = "Meij91.pdf"
}

@misc{Yous04,
  author = "Youssef, Saul",
  title = {{Prospects for Category Theory in Aldor}},
  year = "2004",
  link = "\url{http://axiom-wiki.newsynthesis.org/public/refs/Youssef-ProspectsForCategoryTheoryInAldor.pdf}",
  abstract = 
    "Ways of encorporating category theory constructions and results into
    the Aldor language are discussed. The main features of Aldor which
    make this possible are identified, examples of categorical
    constructions are provided and a suggestion is made for a foundation
    for rigorous results.",
  paper = "Yous04.pdf"
}

@book{Adax12,
  author = "ISO/IEC 8652:2012(E)",
  title = {{Ada Reference Manual}},
  publisher = "U.S. Government",
  year = "2012",
  link = "\url{http://www.ada-auth.org/standards/12rm/RM-Final.pdf}"
}

@inproceedings{Abad94,
  author = "Abadi, Martin and Cardelli, Luca",
  title = {{A Semantics of Object Types}},
  booktitle = "Symp. on Logic in Computer Science '94",
  publisher = "IEEE",
  year = "1994",
  abstract =
    "We give a semantics for a typed object calculus, an extension of
    System {\bf F} with object subsumption and method override.  We interpret
    the calculus in a per model, proving the soundness of both typing and
    equational rules.  This semantics suggests a syntactic translation
    from our calculus into a simpler calculus with neither subtyping nor
    objects",
  paper = "Abad94.pdf"
}

@inproceedings{Abad94a,
  author = "Abadi, Martin and Cardelli, Luca",
  title = {{A Theory of Primitive Objects: Untyped and First-Order Systems}},
  booktitle = "Proc. European Symposium on Programming",
  year = "1994",
  abstract =
    "We introduce simple object calculi that support method override and
    object subsumption.  We give an untyped calculus, typing rules, and
    equational rules. We illustrate the expressiveness of our calculi and
    the pitfalls that we avoid.",
  paper = "Abad94a.pdf"
}

@article{Amad93,
  author = "Amadio, Roberto M. and Cardelli, Luca",
  title = {{Subtyping Recursive Types}},
  journal = "TOPLAS '93",
  volume = "15",
  number = "4",
  year = "1993",
  pages = "575-631",
  abstract =
    "We investigate the interactions of subtyping and recursive types, in
    a simply typed $\lambda$-calculus. The two fundamental questions here are
    whether two (recursive)types are in the subtype relation and whether a
    term has a type. To address the first question, we relate various
    definitions of type equivalence and subtyping that are induced by a
    model, an ordering on infinite trees, an algorithm, and a set of type
    rules. We show soundness and completeness among the rules, the
    algorithm, and the tree semantics. We also prove soundness and a
    restricted form of completeness for the model. To address the second
    question, we show that to every pair of types in the subtype relation
    we can associate a term whose denotation is the uniquely determined
    coercion map between the two types. Moreover, we derive an algorithm
    that, when given a term with implicit coercions, can infer its least
    type whenever possible.",
  paper = "Amad93.pdf"
}

@article{Aspe04,
  author = "Asperti, Andrea and Guidi, Ferruccio and Coen, Claudio Sacerdoti
            and Tassi, Enrico and Zacchiroli, Stefano",
  title = {{A Content Based Mathematical Search Engine: Whelp}},
  journal = "LNCS",
  volume = "3839",
  year = "2004",
  pages = "17-32",
  isbn = "3-540-31428-8",
  abstract =
    "The prototype of a content based search engine for mathematical
    knowledge supporting a small set of queries requiring matching and/or
    typing operations is described. The prototype, called Whelp, exploits
    a metadata approach for indexing the information that looks far more
    flexible than traditional indexing techniques for structured
    expressions like substitution, discrimination, or context trees. The
    prototype has been instantiated to the standard library of the Coq
    proof assistant extended with many user contributions.",
  paper = "Aspe04.pdf"
}

@inproceedings{Aspe06,
  author = "Asperti, Andrea and Coen, Claudio Sacerdoti and 
            Tassi, Enrico and Zacchiroli, Stefano",
  title = {{Crafting a Proof Assistant}},
  booktitle = "Proc. Types 2006: Conf. of the Types Project",
  year = "2006",
  abstract =
    "Proof assistants are complex applications whose development has
    never been properly systematized or documented. This work is a
    contribution in this direction, based on our experience with the
    development of Matita: a new interactive theorem prover based—as
    Coq—on the Calculus of Inductive Constructions (CIC). In particular,
    we analyze its architecture focusing on the dependencies of its
    components, how they implement the main functionalities, and their
    degree of reusability. The work is a first attempt to provide a ground
    for a more direct comparison between different systems and to
    highlight the common functionalities, not only in view of
    reusability but also to encourage a more systematic comparison of
    different softwares and architectural solutions.",
  paper = "Aspe06.pdf"
}

@inproceedings{Aspe07,
  author = "Asperti, Andrea and Tassi, Enrico",
  title = {{Higher Order Proof Reconstruction from
            Paramodulation-based Refutations: The Unit Equality Case}},
  booktitle = "MKM 2007",
  year = "2007",
  abstract =
    "In this paper we address the problem of reconstructing a higher
    order, checkable proof object starting from a proof trace left by a
    first order automatic proof searching procedure, in a restricted
    equational framework. The automatic procedure is based on
    superposition rules for the unit equality case. Proof transformation
    techniques aimed to improve the readability of the final proof are
    discussed.",
  paper = "Aspe07.pdf"
}

@article{Aspe07a,
  author = "Asperti, Andrea and Coen, Claudio Sacerdoti and Tassi, Enrico
            and Zacchiroli, Stefano",
  title = {{User Interaction with the Matita Proof Assistant}},
  journal = "J. of Automated Reasoning",
  volume = "39",
  number = "2",
  pages = "109-139",
  year = "2007",
  abstract =
    "Matita is a new, document-centric, tactic-based interactive theorem
    prover. This paper focuses on some of the distinctive features of the
    user interaction with Matita, characterized mostly by the organization
    of the library as a searchable knowledge base, the emphasis on a
    high-quality notational rendering, and the complex interplay between
    syntax, presentation, and semantics.",
  paper = "Aspe07a.pdf"
}

@article{Aspe09,
  author = "Asperti, Andrea and Ricciotti, Wilmer and Coer, Claudio
            Sacerdoti and Tassi, Enrico",
  title = {{A Compact Kernel for the Calculus of Inductive Constructions}},
  journal = "Sadhana",
  volume = "34",
  number = "1",
  year = "2009",
  pages = "71-104",
  abstract =
    "The paper describes the new kernel for the Calculus of Inductive
    Constructions (CIC) implemented inside the Matita Interactive
    Theorem Prover.  The design of the new kernel has been completely
    revisited since the first release, resulting in a remarkably compact
    implementation of about 2300 lines of OCaml code. The work is meant
    for people interested in implementation aspects of Interactive
    Provers, and is not self contained . In particular, it requires good
    acquaintance with Type Theory and functional programming
    languages.",
  paper = "Aspe09.pdf",
  keywords = "printed"
}

@article{Aspe09a,
  author = "Asperti, Andrea and Ricciotti, Wilmer and Coer, Claudio
            Sacerdoti and Tassi, Enrico",
  title = {{Hints in Unification}},
  journal = "LNCS",
  volume = "5674",
  pages = "84-98",
  year = "2009",
  isbn = "978-3-642-03358-2",
  abstract =
    "Several mechanisms such as Canonical Structures, Type Classes, or
    Pullbacks have been recently introduced with the aim to improve the
    power and flexibility of the type inference algorithm for interactive
    theorem provers. We claim that all these mechanisms are particular
    instances of a simpler and more general technique, just consisting in
    providing suitable hints to the unification procedure underlying type
    inference. This allows a simple, modular and not intrusive
    implementation of all the above mentioned techniques, opening at the
    same time innovative and unexpected perspectives on its possible
    applications.",
  paper = "Aspe09a.pdf"
}

@inproceedings{Aspe09b,
  author = "Asperti, Andrea and Ricciotti, Wilmer and Coer, Claudio
            Sacerdoti and Tassi, Enrico",
  title = {{A New Type for Tactics}},
  booktitle = "SIGSAM PLMMS 2009",
  publisher = "ACM",
  year = "2009",
  isbn = "978-1-60558-735-6",
  abstract =
    "The type of tactics in all (procedural) proof assistants is (a
    variant of) the one introduced in LCF. We discuss why this is
    inconvenient and we propose a new type for tactics that 1) allows the
    implementation of more clever tactics; 2) improves the implementation
    of declarative languages on top of procedural ones; 3) allows for
    better proof structuring; 4) improves proof automation; 5) allows
    tactics to rearrange and delay the goals to be proved (e.g. in case of
    side conditions or PVS subtyping judgements).",
  paper = "Aspe09b.pdf"
}

@article{Aspe10,
  author = "Asperti, Andrea and Tassi, Enrico",
  title = {{Smart Matching}},
  journal = "LNCS",
  volume = "6167",
  pages = "263-277",
  year = "2010",
  isbn = "978-3-642-14128-7",
  abstract =
    "One of the most annoying aspects in the formalization of mathematics
    is the need of transforming notions to match a given, existing
    result. This kind of transformations, often based on a conspicuous
    background knowledge in the given scientific domain (mostly expressed
    in the form of equalities or isomorphisms), are usually implicit in
    the mathematical discourse, and it would be highly desirable to obtain
    a similar behavior in interactive provers. The paper describes the
    superposition-based implementation of this feature inside the Matita
    interactive theorem prover, focusing in particular on the so called
    smart application tactic, supporting smart matching between a goal and
    a given result.",
  paper = "Aspe10.pdf"
}

@inproceedings{Aspe11,
  author = "Asperti, Andrea and Ricciotti, Wilmer and Coer, Claudio
            Sacerdoti and Tassi, Enrico",
  title = {{The Matita Interactive Theorem Prover}},
  booktitle = "CADE-23 Automated Deduction",
  year = "2011",
  pages = "64-69",
  abstract =
    "Matita is an interactive theorem prover being developed by the Helm
    team at the University of Bologna. Its stable version 0.5.x may be
    downloaded at http://matita.cs.unibo.it . The tool originated in the
    European project MoWGLI as a set of XML-based tools aimed to provide a
    mathematician-friendly web-interface to repositories of formal
    mathematical knoweldge, supporting advanced content-based
    functionalities for querying, searching and browsing the library. It
    has since then evolved into a fully fledged ITP, specifically designed
    as a light-weight, but competitive system, particularly suited for the
    assessment of innovative ideas, both at foundational and logical
    level. In this paper, we give an account of the whole system, its
    peculiarities and its main applications.",
  paper = "Aspe11.pdf",
  keywords = "printed"
}

@article{Aspe12,
  author = "Asperti, Andrea and Ricciotti, Wilmer and Coer, Claudio
            Sacerdoti and Tassi, Enrico",
  title = {{A Bi-directional Refinement Algorithm for the Calculus
            of (Co)Inductive Constructions}},
  journal = "Logical Methods in Computer Science",
  year = "2012",
  volume = "8",
  pages = "1-49",
  abstract =
    "The paper describes the refinement algorithm for the Calculus of
    (Co)Inductive Constructions (CIC) implemented in the interactive
    theorem prover Matita. The refinement algorithm is in charge of giving
    a meaning to the terms, types and proof terms directly written by the
    user or generated by using tactics, decision procedures or general
    automation. The terms are written in an 'external syntax' meant to be
    user friendly that allows omission of information, untyped binders and
    a certain liberal use of user defined sub-typing. The refiner modifies
    the terms to obtain related well typed terms in the internal syntax
    understood by the kernel of the ITP. In particular, it acts as a type
    inference algorithm when all the binders are untyped. The proposed
    algorithm is bi-directional: given a term in external syntax and a
    type expected for the term, it propagates as much typing information
    as possible towards the leaves of the term. Traditional
    mono-directional algorithms, instead, proceed in a bottom-up way by
    inferring the type of a sub-term and comparing (unifying) it with the
    type expected by its context only at the end. We propose some novel
    bi-directional rules for CIC that are particularly effective. Among
    the benefits of bi-directionality we have better error message
    reporting and better inference of dependent types. Moreover, thanks to
    bi-directionality, the coercion system for sub-typing is more
    effective and type inference generates simpler unification problems
    that are more likely to be solved by the inherently incomplete higher
    order unification algorithms implemented. Finally we introduce in the
    external syntax the notion of vector of placeholders that enables to
    omit at once an arbitrary number of arguments. Vectors of placeholders
    allow a trivial implementation of implicit arguments and greatly
    simplify the implementation of primitive and simple tactics.",
  paper = "Aspe12.pdf"
}

@book{Appe17,
  author = "Appel, Andrew W.",
  title = {{Verified Functional Algorithms}},
  year = "2017",
  publisher = "University of Pennsylvania",
  link = 
    "\url{https://softwarefoundations.cis.upenn.edu/vfa-current/index.html}"
}

@misc{Atte15,
  author = "van Atten, Mark and Sundholm, Goran",
  title = {{L.E.J. Brouwer's 'Unrelability of the Logical Principles'
           translation}},
  year = "2015",
  link = "\url{https://arxiv.org/pdf/1511.01113.pdf}",
  abstract = 
    "We present a new English translation of L.E.J. Brouwer's paper 'De
    onbetrouwbaarheid der logische principes' (The unreliability of the
    logical principles) of 1908, together with a philosophical and
    historical introduction. In this paper Brouwer for the first time
    objected to the idea that the Principle of the Excluded Middle is
    valid. We discuss the circumstances under which the manuscript was
    submitted and accepted, Brouwer's ideas on the principle of the
    excluded middle, and its consistency and partial validity, and his
    argument against the possibility of absolutely undecidable
    propositions. We not that principled objections to the general
    exculded middle similar to Brouwer's had been advanced in print by
    Jules Molk two years before. Finally, we discuss the influence on
    George Griss' negationless mathematics",
  paper = "Atte15.pdf, printed"
}

@misc{Avig17b,
  author = "Avigad, Jeremy",
  title = {{Proof Theory}},
  year = "2017",
  abstract =
    "Proof theory began in the 1920's as part of Hilbert's program,
    which aimed to secrure the foundations of mathematics by modeling
    infinitary mathematics with formal axiomatic systems and proving
    those systems consistent using restricted, finitary means. The
    program thus viewed mathematics as a system of reasoning with
    precise linguistic norms, govenered by rules that can be described
    and studied in concrete terms. Today such a viewpoint has
    applications in mathematics, computer science, and the philosophy
    of mathematics.",
  paper = "Avig17b.pdf",
  keywords = "private communication"
}

@article{Bate85,
  author = "Bates, Joseph L. and Constable, Robert L.",
  title = {{Proofs as Programs}},
  journal = "ACM TOPLAS",
  volume = "7",
  number = "1",
  year = "1985",
  abstract =
    "The significant intellectual cost of programming is for problem
    solving and explaining, not for coding. Yet programming systems offer
    mechanical assistance for the coding process exclusively. We
    illustrate the use of an implemented program development system,
    called PRL ('pearl'), that provides automated assistance with the
    difficult part. The problem and its explained solution are seen as
    formal objects in a constructive logic of the data domains. These
    formal explanations can be executed at various stages of
    completion. The most incomplete explanations resemble applicative
    programs, the most complete are formal proofs.",
  paper = "Bate85.pdf",
  keywords = "printed"
}

@book{Bish12,
  author = "Bishop, Errett",
  title = {{Foundations of Constructive Analysis}},
  publisher = "ISHI Press",
  year = "2012",
  isbn = "978-4-87187-714-5"
}

@inproceedings{Blak96,
  author = "Blakley, Bob",
  title = {{The Emperor's Old Armor}},
  booktitle = "Proc. 1996 New Security Paradigms Workshop",
  publisher = "ACM",
  year = "1996",
  paper = "Blak96.pdf",
  keywords = "printed"
}

@inproceedings{Blan99,
  author = "Blanqui, Frederic and jouannaud, Jean-Pierre and Okada, Mitsuhiro",
  title = {{The Calculus of Algebraic Constructions}},
  booktitle = "Rewriting Techniques and Applications RTA-99",
  year = "1999",
  publisher = "LNCS 1631",
  link = "\url{https://hal.inria.fr/inria-00105545v1/document}",
  abstract = 
    "This paper is concerned with the foundations of the Calculus of
    Algebraic Constructions (CAC), an extension of the Calculus of
    Constructions by inductive data types. CAC generalizes inductive 
    types equipped with higher-order primitive recursion, by providing
    definition s of functions by pattern-matching which capture recursor
    definitions for arbitrary non-dependent and non-polymorphic inductive
    types satisfying a strictly positivity condition. CAC also
    generalizes the first-order framework of abstract data types by
    providing dependent types and higher-order rewrite rules.",
  paper = "Blan99.pdf",
  keywords = "printed"
}

@article{Blan05,
  author = "Blanqui, Frederic",
  title = {{Inductivev Types in the Calculus of Algebraic Constructions}},
  journal = "Fundamenta Informaticae",
  volume = "65",
  number = "1-2",
  pages = "61-86",
  year = "2005",
  abstract =
    "In a previous work, we proved that an important part of the Calculus
    of Inductive Constructions (CIC), the basis of the Coq proof
    assistant, can be seen as a Calculus of Algebraic Constructions
    (CAC), an extension of the Calculus of Constructions with functions
    and predicates defined by higher-order rewrite rules.  In this
    paper, we prove that almost all CIC can be seen as a CAC, and that it
    can be further extended with non-strictly positive types and
    inductive-recursive types together with non-free constructors and
    pattern-matching on defined symbols.",
  paper = "Blan05.pdf"
}

@article{Bowe95,
  author = "Bowen, Jonathan P. and Hinchey, Michael G.",
  title = {{Seven More Myths of Formal Methods}},
  journal = "IEEE Software",
  volume = "12",
  number = "4",
  pages = "34-41",
  year = "1995", 
  abstract =
    "New myths about formal methods are gaining tacit acceptance both
    outside and inside the system-development community. The authors
    address and dispel these myths based on their observations of
    industrial projects. The myths include: formal methods delay the
    development process; they lack tools; they replace traditional
    engineering design methods; they only apply to software; are
    unnecessary; not supported; and formal methods people always use
    formal methods.",
  paper = "Bowe95.pdf",
  keywords = "printed"
}

@article{Broo87,
  author = "Brooks, Frederick P.",
  title = {{No Silver Bullet: Essence and Accidents of Software Engineering}},
  journal = "IEEE Computer",
  volume = "20",
  number = "4",
  pages = "10-19",
  year = "1987",
  abstract =
    "Fashioning complex conceptual constructs is the essence; accidental
    tasks arise in representing the constructs in language. Past progress
    has so reduced the accidental tasks that future progress now depends
    upon addressing the essence.",
  paper = "Broo87.pdf",
  keywords = "printed"
}

@inproceedings{Bruc92,
  author = "Bruce, Kim and Mitchell, John C.",
  title = {{PER Models of Subtyping, Recursive Types and Higher-Order
            Polymorphism}},
  booktitle = "POPL '92",
  pages = "316-327",
  year = "1992",
  abstract =
    "We relate standard techniques for solving recursive domain equations
    to previous models with types interpreted as partial equivalence
    relations (per's) over a $D_\infty$ lambda model. This motivates a
    particular choice of type functions, which leads to an extension of
    such models to higher-order polymorphism. The resulting models provide
    natural interpretations for function spaces, records, recursively
    defined types, higher-order type functions, and bounded polymorphic
    types $\forall X <: Y.A$ where the bound may be of a higher kind. In
    particular, we may combine recursion and polymorphism in a way that
    allows the bound $Y$ in $\forall X <: Y.A$ to be recursively
    defined. The model may also be used to interpret so-called
    ``F-bounded polymorphism''. Together, these features allow us to
    represent several forms of type and type functions that seem to arise
    naturally in typed object-oriented programming.",
  paper = "Bruc92.pdf"
}

@techreport{Calu07,
  author = "Calude, C.S. and Calude, E. and Marcus, S.",
  title = {{Proving and Programming}},
  type = "technical report",
  institution = "Centre for Discrete Mathematics and Theoretical Computer
                 Science",
  number = "CDMTCS-309",
  year = "2007",
  abstract =
    "There is a strong analogy between proving theorems in mathematics and
    writing programs in computer science.  This paper is devoted to an
    analysis, from the perspective of this analogy, of proof in
    mathematics.  We will argue that while the Hilbertian notion of proof
    has few chances to change, future proofs will be of various types,
    will play different roles, and their truth will be checked
    differently.  Programming gives mathematics a new form of
    understanding.  The computer is the driving force behind these
    changes.",
  paper = "Calu07.pdf",
  keywords = "printed"
}

@book{Cann01,
  author = "Cannon, John J. and Playoust, Catherine",
  title = {{An Introduction to Algebraic Programming with Magma}},
  year = "2001",
  publisher = "University of Sydney",
  paper = "Cann01.pdf"
}

@inproceedings{Card88a,
  author = "Cardelli, Luca",
  title = {{Structural Subtyping and the Notion of Power Type}},
  booktitle = "POPL '88",
  year = "1988",
  publisher = "ACM",
  paper = "Card88a.pdf"
}

@misc{Chli17a,
  author = "Chlipala, Adam",
  title = {{Coming Soon: Machine-Checked Mathematical Proofs in Everyday
            Software and Hardware Development}},
  link = "\url{https://media.ccc.de/v/34c3-9105-coming_soon_machine-checked_mathematical_proofs_in_everyday_software_and_hardware_development}",
  year = "2017",
  abstract =
    "Most working engineers view machine-checked mathematical proofs as an
    academic curiosity, if they have ever heard of the concept at all. In
    contrast, activities like testing, debugging, and code review are
    accepted as essential. They are woven into the lives of nearly all
    developers. In this talk, I will explain how I see machine-checked
    proofs enabling new everyday activities for developers of computer
    software and hardware. These activities have the potential to lower
    development effort dramatically, at the same time as they increase our
    assurance that systems behave correctly and securely. I will give a
    cosmological overview of this field, answering the FAQs that seem to
    stand in the way of practicality; and I will illustrate the principles
    with examples from projects that you can clone from GitHub today,
    covering the computing stack from digital hardware design to
    cryptographic software and applications.
    
    Today's developers of computer software and hardware are tremendously
    effective, compared to their predecessors. We have found very
    effective ways of modularizing and validating our work. The talk is
    about ammunition for these activities from a perhaps-unexpected
    source.
    
    Modularity involves breaking a complex system into a hierarchy of
    simpler pieces, which may be written and understood
    separately. Structured programming (e.g., using loops and conditionals
    instead of gotos) helps us read and understand parts of a single
    function in isolation, and data abstraction lets us encapsulate
    important functionality in objects, with guarantees that other code
    can only access the private data by calling public methods. That way,
    we can convince ourselves that the encapsulated code upholds certain
    essential properties, regardless of which other code it is linked
    with. Systematic unit testing also helps enforce contracts for units
    of modularity. Each of these techniques can be rerun automatically, to
    catch regressions in evolving systems, and catch those regressions in
    a way that accurately points the finger of responsibility to
    particular modules.
    
    Validation is an important part of development that encompasses
    testing, debugging, code review, and anything else that we do to raise
    our confidence that the system behaves as intended. Experienced
    engineers know that validation tends to take up the majority of
    engineering effort. Often that effort involves mentally taxing
    activities that would not otherwise come up in coding. One example is
    thinking about test-case coverage, and another is including
    instrumentation that produces traces to consult during debugging.
    
    It is not hard for working developers to imagine great productivity
    gains from better ways to break systems into pieces or raise our
    confidence in those pieces. The claim I will make in this talk is that
    a key source of such insights has been neglected: machine-checked
    mathematical proofs. Here the basic functionality is an ASCII language
    for defining mathematical objects, stating theorems about them, and
    giving proofs of theorems. Crucially, an algorithm checks that
    purported proofs really do establish the theorems. By going about
    these activities in the style of programming, we inherit usual
    supporting tools like IDEs, version control, continuous integration,
    and automated build processes. But how could so esoteric a task as
    math proofs call for that kind of tooling, and what does it have to do
    with building real computer systems?
    
    I will explain a shared vision to that end, developed along with many
    other members of my research community. Let me try to convince you
    that all of the following goals are attainable in the next 10 years.
    
    \begin{itemize}
    \item We will have complete computer systems implementing moderately
    complex network servers for popular protocols, proved to implement
    those protocols correctly, from the level of digital circuits on
    up. We will remove all deployed code (hardware or software) from the
    trusted computing base, shifting our trust to much smaller
    specifications and proof checkers.
    \item Hobbyists will be able to design new embedded computing
    platforms by mixing and matching open-source hardware and software
    components, also mixing and matching the proofs of these components,
    guaranteeing no bugs at the digital-abstraction level or higher, with
    no need for debugging.
    \item New styles of library design will be enabled by the chance to
    attach a formal behavioral specification to each library. For
    instance, rank-and-file programmers will able to assemble their own
    code for cryptographic protocols, with code that looks like reference
    implementations in Python, but getting performance comparable to what
    experts handcraft in assembly today. Yet that benefit would come with
    no need to trust that library authors have avoided bugs or intentional
    backdoors, perhaps even including automatic proofs of cryptographic
    security properties.
    \end{itemize}
    
    Main technical topics to cover to explain my optimism:
    \begin{itemize}
    \item The basic functionality of proof assistants and why we should
    trust their conclusions
    \item How to think about system decomposition with specifications and
    proofs, including why, for most components, we do not need to worry
    about specification mistakes
    \item The different modes of applying proof technology to check or
    generate components
    \item The engineering techniques behind cost-effective proof authoring
    for realistic systems
    \item A hardware case study: Kami, supporting component-based digital
    hardware authoring with proofs
    \item A software case study: Fiat Cryptography, supporting
    correct-by-construction auto-generation of fast code for
    elliptic-curve cryptography
    \item Pointers to where to look next, if you would like to learn more
    about this technology
    \end{itemize}"
}

@book{Chli17,
  author = "Chlipala, Adam",
  title = {{Formal Reasoning About Programs}},
  year = "2017",
  publisher = "MIT",
  link = "\url{http://adam.chlipala.net/frap/frap_book.pdf}",
  abstract =
    "Briefly, this book is about an approach to bringing software
    engineering up to speed with more traditional engineering disciplines,
    providing a mathematical foundation for rigorous analysis of realistic
    computer systems. As civil engineers apply their mathematical canon to
    reach high certainty that bridges will not fall down, the software
    engineer should apply a different canon to argue that programs behave
    properly. As other engineering disciplines have their computer-aided
    design tools, computer science has proof assistants, IDEs for logical
    arguments. We will learn how to apply these tools to certify that
    programs behave as expected.
    
    More specifically: Introductions to two intertangled subjects: the Coq
    proof assistant, a tool for machine-checked mathematical theorem
    proving; and formal logical reasoning about the correctness of
    programs.",
  paper = "Chli17.pdf",
  keywords = "printed"
}  

@article{Coen04,
  author = "Coen, Claudio Sacerdoti and Zacchiroli, Stefano",
  title = {{Efficient Ambiguous Parsing of Mathematical Formulae}},
  journal = "LNCS",
  volume = "3119",
  pages = "347-362",
  year = "2004",
  isbn = "3-540-23029-7",
  abstract =
    "Mathematical notation has the characteristic of being ambiguous:
    operators can be overloaded and information that can be deduced is
    often omitted. Mathematicians are used to this ambiguity and can
    easily disambiguate a formula making use of the context and of their
    ability to find the right interpretation.
    
    Software applications that have to deal with formulae usually avoid
    these issues by fixing an unambiguous input notation. This solution is
    annoying for mathematicians because of the resulting tricky syntaxes
    and becomes a show stopper to the simultaneous adoption of tools
    characterized by different input languages.
    
    In this paper we present an efficient algorithm suitable for ambiguous
    parsing of mathematical formulae. The only requirement of the
    algorithm is the existence of a 'validity' predicate over abstract
    syntax trees of incomplete formulae with placeholders. This
    requirement can be easily fulfilled in the applicative area of
    interactive proof assistants, and in several other areas of
    Mathematical Knowledge Management.",
  paper = "Coen04.pdf"
}

@inproceedings{Coen06,
  author = "Coen, Claudio Sacerdoti and Tassi, Enrico and 
            Zacchiroli, Stefano",
  title = {{Tinycals: Step by Step Teacticals}},
  booktitle = "Proc. User Interfaces for Theorem Provers",
  year = "2006",
  pages = "125-142",
  abstract =
    "Most of the state-of-the-art proof assistants are based on procedural
    proof languages, scripts, and rely on LCF tacticals as the primary
    tool for tactics composition. In this paper we discuss how these
    ingredients do not interact well with user interfaces based on the
    same interaction paradigm of Proof General (the de facto standard in
    this field), identifying in the coarse-grainedness of tactical
    evaluation the key problem. We propose tinycals as an alternative to a
    subset of LCF tacticals, showing that the user does not experience the
    same problem if tacticals are evaluated in a more fine-grained
    manner. We present the formal operational semantics of tinycals as
    well as their implementation in the Matita proof assistant.",
  paper = "Coen06.pdf"
}

@inproceedings{Coen07a,
  author = "Coen, Claudio Sacerdoti and Zacchiroli, Stefano",
  title = {{Spurious Disambiguation Error Detection}},
  booktitle = "MKM 2007 Mathematical Knowledge Management",
  year = "2007",
  abstract =
    "The disambiguation approach to the input of formulae enables the user
    to type correct formulae in a terse syntax close to the usual
    ambiguous mathematical notation. When it comes to incorrect formulae
    we want to present only errors related to the interpretation meant by
    the user, hiding errors related to other interpretations (spurious
    errors). We propose a heuristic to recognize spurious errors, which
    has been integrated with our former efficient disambiguation
    algorithm.",
  paper = "Coen07a.pdf"
}

@article{Coen10,
  author = "Coen, Claudio Sacerdoti",
  title = {{Declarative Representation of Proof Terms}},
  journal = "J. Automated Reasoning",
  volume = "44",
  number = "1-2",
  pages = "25-52",
  year = "2010",
  abstract =
    "We present a declarative language inspired by the pseudo-natural
    language previously used in Matita for the explanation of proof
    terms. We show how to compile the language to proof terms and how to
    automatically generate declarative scripts from proof terms. Then we
    investigate the relationship between the two translations, identifying
    the amount of proof structure preserved by compilation and
    re-generation of declarative scripts.",
  paper = "Coen10.pdf",
  keywords = "printed"
}

@inproceedings{Coen11,
  author = "Coer, Claudio Sacerdoti and Tassi, Enrico",
  title = {{Nonuniform Coercions via Unification Hints}},
  booktitle = "Proc. Types for Proofs and Programs",
  volume = "53",
  pages = "19-26",
  year = "2011",
  abstract =
    "We introduce the notion of nonuniform coercion, which is the
    promotion of a value of one type to an enriched value of a different
    type via a nonuniform procedure. Nonuniform coercions are a
    generalization of the (uniform) coercions known in the literature and
    they arise naturally when formalizing mathematics in an higher order
    interactive theorem prover using convenient devices like canonical
    structures, type classes or unification hints. We also show how
    nonuniform coercions can be naturally implemented at the user level in
    an interactive theorem prover that allows unification hints.",
  paper = "Coen11.pdf"
}

@article{Cons85a,
  author = "Constable, R.L. and Knoblock, T.B. and Gates, J.L.",
  title = {{Writing Programs that Construct Proofs}},
  journal = "J. of Automated Reasoning",
  volume = "1",
  number = "3",
  pages = "285-326",
  year = "1985",
  abstract =
    "When we learn mathematics, we learn more than definitions and
    theorems. We learn techniques of proof. In this paper, we describe
    a particular way to express these techniques and incorporate them
    into formal theories and into computer systems used to build such
    theories. We illustrate the methods as they were applied in the
    $\lambda$-PRL system, essentially using the ML programming
    language from Edinburgh LCF [23] as the formalised
    metalanguage. We report our experience with such an approach
    emphasizing the ideas that go beyond the LCF work, such as
    transformation tactics and special purpose reasoners. We also show
    how the validity of tactics can be guaranteed. The introduction
    places the work in historical context and the conclusion briefly
    describes plans to carry the methods further. The majority of the
    paper presents the $\lambda$-PRL approach in detail.",
  paper = "Cons85a.pdf"
}

@article{Coqu85,
  author = "Coquand, Thierry and Huet, Gerard",
  title = {{Constructions: A Higher Order Proof System for Mechanizing
            Mathematics}},
  journal = "LNCS",
  volume = "203",
  pages = "151-184",
  year = "1985",
  abstract =
    "We present an extensive set of mathematical propositions and proofs
    in order to demonstrate the power of expression of the theory of
    constructions.",
  paper = "Coqu85.pdf, printed"
}

@inproceedings{Croc14,
  author = "Crocker, David",
  title = {{Can C++ Be Made as Safe as SPARK?}},
  booktitle = "Proc 2014 HILT",
  isbn = "978-1-4503-3217-0",
  year = "2014",
  abstract =
    "SPARK offers a way to develop formally-verified software in a
    language (Ada) that is designed with safety in mind and is further
    restricted by the SPARK language subset. However, much critical
    embedded software is developed in C or C++ We look at whether and how
    benefits similar to those offered by the SPARK language subset and
    associated tools can be brought to a C++ development environment.",
  paper = "Croc14.pdf",
  keywords = "printed"
}

@misc{Cyph17,
  author = "Cypherpunks",
  title = {{Chapter 4: Verification Techniques}},
  link = "\url{http://www.cypherpunks.to/~peter/04_verif_techniques.pdf}",
  year = "2017",
  abstract = "Wherein existing methods for building secure systems are
    examined and found wanting",
  paper = "Cyph17.pdf",
  keywords = "printed"
}

@article{Fass04,
  author = "Fass, Leona F.",
  title = {{Approximations, Anomalies and ``The Proof of Correctness Wars''}},
  journal = "ACM SIGSOFT Software Engineering Notes",
  volume = "29",
  number = "2",
  year = "2004",
  abstract =
    "We discuss approaches to establishing 'correctness' and describe the
    usefulness of logic-based model checkers for producing better
    practical system designs.  While we could develop techniques for
    'constructing correctness' in our theoretical behavioral-modeling
    research, when applied to Real World processes such as software
    development only approximate correctness might be established and
    anomalous behaviors subsequently found.  This we view as a positive
    outcome since resultant adaptation, or flaw detection and correction,
    may lead to improved development and designs.  We find researchers
    employing model checking as a formal methods tool to develop empirical
    techniques have reached similar conclusions. Thus we cite some
    applications of model checking to generate tests and detect defects in
    such Real World processes as aviation system development,
    fault-detection systems, and security.",
  paper = "Fass04.pdf"
}  

@article{Fent93,
  author = "Fenton, Norman",
  title = {{How Effective Are Software Engineering Methods}},
  journal = "J. Systems Software",
  volume = "22",
  pages = "141-148",
  year = "1993",
  abstract =
    "For 25 years, software engineers have sought methods which they hope
    can provide a technological fix for the “software crisis.” Proponents
    of specific methods claim that their use leads to significantly
    improved quality and productivity. Such claims are rarely, if ever,
    backed up by hard evidence. We show that often, where real empirical
    evidence does exist, the results are counter to the views of the
    so-called experts. We examine the impact on the software industry of
    continuing to place our trust in unproven, and often revolutionary,
    methods. The very poor state of the art of empirical assessment in
    software engineering can be explained in part by inappropriate or
    inadequate use of measurement. Numerous empirical studies are flawed
    because of their poor experimental design and lack of adherence to
    proper measurement principles.",
  paper = "Fent93.pdf",
  keywords = "printed"
}

@article{Fetz88,
  author = "Fetzer, James H.",
  title = {{Program Verification: The Very Idea}},
  journal = "Communications of the ACM",
  volume = "31",
  number = "9",
  pages = "1048-1063",
  year = "1988",
  abstract =
    "The notion of program verification appears to trade upon an
    equvocation. Algorithms, as logical structures, are appropriate
    subjects for deductive verification. Programs, as causal models of
    those structures, are not. The success of program verification as a
    generally applicable and completely reliable method for guaranteeing
    program performance is not even a theoretical possibility.",
  paper = "Fetz88.pdf",
  keywords = "printed"
}

@phdthesis{Fill99,
  author = "Filliatre, Jean-Christophe",
  title = {{Preuve de programmes imp\'eratifs en th\'eorie des types}},
  school = {{Universit\'e Paris Sud}},
  year = "1999",
  paper = "Fill99.pdf"
}

@article{Fill07,
  author = "Filliatre, Jean-Christophe",
  title = {{Formal Proof of a Program: FIND}},
  journal = "Science of Computer Programming",
  volume = "64",
  pages = "332-340",
  year = "2007",
  abstract =
    "In 1971, C.A.R. Hoare gave the proof of correctness and
    termination of a rather complex algorithm, in a paper entitled 
    {\sl Proof of a program: Find}. It is a handmade proof, where the
    program is given together with its formal specification and where
    each step is fully justified by mathematical reasoning. We present
    here a formal proof of the same program in the system Coq, using
    the recent tactic of the system developed to establish the total
    correctness of imperative programs. We follow Hoare's paper as
    closely as possible, keeping the same program and the same
    specification. We show that we get exactly the same proof
    obligations, which are proved in a straightforward way, following
    the original paper. We also explain how more informal aspects of
    Hoare's proof are formalized in the system Coq. This demonstrates
    the adequacy of the system Coq in the process of certifying
    imperative programs.",
  paper = "Fill07.pdf",
  keywords = "printed"
}

@book{Font16,
  author = "Font, Josep Marie",
  title = {{Abstract Algebraic Logic: An Introductory Textbook}},
  year = "2016",
  publisher = "College Publications, Kings College",
  isbn = "978-1-84890-207-7"
}

@misc{Garr14,
  author = "Garret, Ron",
  title = {{The Awesome Power of Theory}},
  link = "\url{http://www.flownet.com/ron/lambda-calculus.html}",
  year = "2014"
}

@article{Glas94,
  author = "Glass, Robert L.",
  title = {{The Software-Research Crisis}},
  journal = "IEEE Software",
  volume = "11",
  number = "6",
  year = "1994",
  abstract = 
    "With the advantage of more than 25 years' hindsight, this twenty-first
    century author looks askance at the 'crisis' in software practice and
    expresses deep concern for a crisis in software research.",
  paper = "Glas94.pdf",
  keywords = "printed"
}

@article{Glas02,
  author = "Glass, Robert L.",
  title = {{The Proof of Correctness Wars}},
  journal = "Communications of the ACM",
  volume = "45",
  number = "8",
  year = "2002",
  pages = "19-21",
  paper = "Glas02.pdf",
  keywords = "printed"
}

@misc{Gonz18,
  author = "Gonzalez, Gabriel",
  title = {{How to Prove Large Software Projects Correct}},
  year = "2018",
  link = "\url{https://www.youtube.com/watch?v=moAfgDFVLUs}"
}

@misc{Grab06a,
  author = "Grabmuller, Martin",
  title = {{Algorithm W Step by Step}},
  year = "2006",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7733&rep=rep1&type=pdf}",
  abstract =
    "In this paper we develop a complete implementation of the classic
    algoirhtm W for Hinley-Milner polymorphic type inference in Haskell",
  paper = "Grab06a.pdf",
  keywords = "printed"
}

@article{Hall90,
  author = "Hall, Anthony",
  title = {{7 Myths of Formal Methods}},
  journal = "IEEE Software",
  volume = "7",
  number = "5",
  pages = "11-19",
  year = "1990",
  abstract =
    "Formal methods are difficult, expensive, and not widely useful,
    detractors say. Using a case study and other real-world examples, this
    article challenges such common myths.",
  paper = "Hall80.pdf",
  keywords = "printed"
}

@phdthesis{Harp85,
  author = "Harper, Robert",
  title = {{Aspects of the Implementation of Type Theory}},
  school = "Cornell University",
  year = "1985",
  comment = "TR 85-675",
  abstract =
    "This thesis is about building an automated programming logic. For our
    purposes an automated programming logic consists of
    \begin{itemize}
    \item A formal system for reasoning about programs
    \item A proof development environment which includes, at least, an
    editor for the construction of proofs in the logic
    \item Mechanized decision methods to assist in the proof development
    process 
    \item A library mechanism for managing collections of theorems
    \end{itemize}",
  paper = "Harp85.pdf"
}

@inproceedings{Harp94,
  author = "Harper, Robert and Lillibridge, Mark",
  title = {{A Type-Theoretic Approach to Higher-Order Modules with Sharing}},
  booktitle = "POPL'21 Principles of Programming Languages",
  year = "1994",
  publisher = "ACM Press",
  abstract =
    "The design of a module system for constructing and maintaining
    large programs is a difficult task that raises a number of theoretical
    and practical issues.  A fundamental issue is the management of the
    flow of information between program units at compile time via the
    notion of an interface.  Experience has shown that fully opaque
    interfaces are awkward to use in practice since too much
    information is hidden, and that fully transparent interfaces lead to
    excessive interdependencies, creating problems for maintenance
    and separate compilation.  The ``sharing'' specications of Standard ML
    address this issue by allowing the programmer to specify equational
    relationships between types in separate modules, but are not
    expressive enough to allow the programmer complete control over
    the propagation of type information between modules.
    
    These problems are addressed from a type-theoretic viewpoint by
    considering a calculus based on Girard's system $F_\omega$.  The calculus
    differs from those considered in previous studies by relying exclusively
    on a new form of weak sum type to propagate information at
    compile-time, in contrast to approaches based on strong sums which
    rely on substitution.  The new form of sum type allows for the 
    specification of equational, as well as type and kind, information
    in interfaces.  This provides complete control over the propagation
    of compile-time information between program units and is
    sufficient to encode in a straightforward way most uses of type
    sharing specifications in Standard ML.  Modules are treated as 
    ``first-class'' citizens, and therefore the system supports 
    higher-order modules and some object-oriented programming idioms: the
    language may be easily restricted to ``second-class' modules found in
    ML-like languages.",
  paper = "Harp94.pdf"
}

@book{Herr73,
  author = "Herrlich, Horst and Strecker, G.E.",
  title = {{Category Theory: An Introduction}},
  year = "1973",
  publisher = "Allyn and Bacon"
}

@misc{Hick18,
  author = "Hickey, Rich",
  title = {{Effective Programs - 10 Years of Clojure}},
  link = "\url{https://www.youtube.com/watch?v=2V1FtfBDsLU}",
  year = "2018"
}

@article{Hoar71,
  author = "Hoare, Charles Antony Richard",
  title = {{Proof of a Program: FIND}},
  journal = "Communications of the ACM",
  volume = "14",
  number = "1",
  year = "1971",
  abstract =
    "A proof is given of the correctness of the algorithm 'Find.'
    First, an informal description is given of the purpose of the
    program and the method used. A systematic technique is described
    for constructing the program proof during the process of coding
    it, in such a way as to prevent the intrusion of logical
    errors. The prrof of termination is treated as a separate
    exercie. Finally, some conclusions related to general programming
    methodology are drawn.",
  paper = "Hoar71.pdf",
  keywords = "printed"
}

@article{Hoar87,
  author = "Hoare, Charles Antony Richard",
  title = {{An Overview of Some Formal Methods for Program Design}},
  journal = "Computer",
  year = "1987",
  volume = "20",
  number = "9",
  paper = "Hoar87.pdf",
  keywords = "printed"
}

@article{Hoar96,
  author = "Hoare, C.A.R",
  title = {{How did software get so reliable without proof?}},
  journal = "LNCS",
  volume = "1051",
  year = "1996",
  abstract =
    "By surveying current software engineering practice, this paper
    reveals that the techniques employed to achieve reliability are little
    different from those which have proved effective in all other branches
    of modern engineering: rigorous management of procedures for design
    inspection and review; quality assurance based on a wide range of
    targeted tests; continuous evolution by removal of errors from
    products already in widespread use; and defensive programming, among
    other forms of deliberate over-engineering. Formal methods and proof
    play a small direct role in large scale programming; but they do
    provide a conceptual framework and basic understanding to promote the
    best of current practice, and point directions for future
    improvement.",
  paper = "Hoar96.pdf",
  keywords = "printed"
}

@article{Kova16,
  author = "Kovacs, Laura",
  title = {{Symbolic Computation and Automated Reasoning for Program
            Analysis}},
  journal = "LNCS",
  volume = "9681",
  pages = "20-27",
  year = "2016",
  abstract =
    "This talk describes how a combination of symbolic computation
    techniques with first-order theorem proving can be used for solving
    some challenges of automating program analysis, in particular for
    generating and proving properties about the logically complex parts of
    software. The talk will first present how computer algebra methods,
    such as Gröbner basis computation, quantifier elimination and
    algebraic recurrence solving, help us in inferring properties of
    program loops with non-trivial arithmetic. Typical properties inferred
    by our work are loop invariants and expressions bounding the number of
    loop iterations. The talk will then describe our work to generate
    first-order properties of programs with unbounded data structures,
    such as arrays. For doing so, we use saturation-based first-order
    theorem proving and extend first-order provers with support for
    program analysis. Since program analysis requires reasoning in the
    combination of first-order theories of data structures, the talk
    also discusses new features in first-order theorem proving, such as
    inductive reasoning and built-in boolean sort. These extensions allow
    us to express program properties directly in first-order logic and
    hence use further first-order theorem provers to reason about program
    properties.",
  paper = "Kova16.pdf",
  keywords = "printed"
}

@book{Laka76,
  author = "Lakatos, Imre",
  title = {{Proofs and Refutations}},
  year = "1976",
  publisher = "Cambridge University Press",
  isbn = "978-1-107-53405-6"
}

@article{Lamp88,
  author = "Lampson, B. and Burstall, R.",
  title = {{Pebble, a Kernel Language for Modules and Abstract Data Types}},
  journal = "Information and Computation",
  volume = "76",
  pages = "278-346",
  year = "1988",
  abstract =
    "A small set of constructs can simulate a wide variety of apparently
    distinct features in modern programming languages.  Using a kernel
    language called Pebble based on the typed lambda calculus with
    bindings, declarations, dependent types, and types as compile time
    values, we show how to build modules, interfaces and implementations,
    abstract data types, generic types, recursive types, and unions.
    Pebble has a concise operational semantics given by inference rules.",
  paper = "Lamp88.pdf"
}

@incollection{Lams00,
  author = "van Lamsweerde, Axel",
  title = {{Formal Specification: A Roadmap}},
  booktitle = "The Future of Software Engineering",
  publisher = "ACM Press",
  year = "2000",
  abstract =
    "Formal specifications have been a focus of software engineering
    research for many years and have been applied in a wide variety of
    settings. Their industrial use is still limited but has been steadily
    growing. After recalling the essence, role, usage, and pitfalls of
    formal specification, the paper reviews the main specification
    paradigms to date and discuss their evaluation criteria. It then
    provides a brief assessment of the current strengths and weaknesses of
    today's formal specification technology. This provides a basis for
    formulating a number of requirements for formal specification to
    become a core software engineering activity in the future.",
  paper = "Lams00.pdf"
}

@phdthesis{Lepi16,
  author = "Lepigre, Rodolphe",
  title = {{Semantics and Implementation of an Extension of ML for
            Proving Programs}},
  year = "2016",
  school = "Universite Grenoble Alpes",
  link = "\url{http://lepigre.fr/these/manuscript_lepigre.pdf}",
  abstract =
    "In recent years, proof assistants have reached an impressive level
    of maturity. They have led to the certification of complex
    programs such as compilers and operating systems. Yet, using a
    proof assistant requires highly specialised skills and it remains
    very different from standard programming. To bridge this gap, we
    aim at designing an ML-style programming language with support for
    proofs of programs, combining in a single tool the flexibility of
    ML and the fine specification features of a proof assistant. In
    other words, the system should be suitable both for programming
    (in the strongly-typed, functional sense) and for gradually
    increasing the level of guarantees met by programs, on a by-need
    basis.

    We thus define and study a call-by-value language whose type
    system extends higher-order logic with an equality type over
    untyped programs, a dependent function type, classical logic and
    subtyping. The combination of call-by-value evaluation, dependent
    functions and classical logic is known to raise consistency
    issues. To ensure the correctness of the system (logical
    consistency and runtime safety), we design a theoretical framework
    based on Krivine's classical realisability. The construction of
    the model relies on an essential property linking the different
    levels of interpretation of types in a novel way.

    We finally demonstrate the expressive power of our system using
    our prototype implementaiton, by proving properties of standard
    programs like the map function on lists or the insertion sort.",
  paper = "Lepi16.pdf"
}

@inproceedings{Lero94,
  author = "Leroy, Xavier",
  title = {{Manifest Types, Modules, and Separate Compilation}},
  booktitle = "Principles of Programming Languages POPL'94",
  year = "1994",
  publisher = "ACM Press",
  pages = "109-122",
  abstract =
    "This paper presents a variant of the SML module system
    that introduces a strict distinction between abstract types
    and manifest types (types whose definitions are part of the
    module specification), while retaining most of the expressive
    power of the SML module system.  The resulting module
    system provides much better support for separate compilation.",
  paper = "Lero94.pdf"
}

@article{Lind88,
  author = "Lindsay, Peter A.",
  title = {{A Survery of Mechanical Support for Formal Reasoning}},
  journal = "Software Engineering Journal",
  volume = "3",
  number = "1",
  year = "1988",
  pages = "3-27",
  paper = "Lind88.pdf",
  keywords = "printed"
}

@inproceedings{Luox96,
  author = "Luo, Zhaohui",
  title = {{Coercive Subtyping in Type Theory}},
  booktitle = "CSL'96 Euro. Ass. for Comput. Sci. Logic",
  year = "1996",
  abstract =
    "We propose and study coercive subtyping, a formal extension with
    subtyping of dependent type theories such as Martin-Lof's type theory
    [NPS90] and the type theory UTT [Luo94]. In this approach, subtyping
    with specied implicit coercions is treated as a feature at the level
    of the logical framework; in particular, subsumption and coercion are
    combined in such a way that the meaning of an object being in a
    supertype is given by coercive definition rules for the definitional
    equality. It is shown that this provides a conceptually simple and
    uniform framework to understand subtyping and coercion relations in
    type theories with sophisticated type structures such as inductive
    types and universes. The use of coercive subtyping in formal
    development and in reasoning about subsets of objects is discussed in
    the context of computer-assisted formal reasoning.",
  paper = "Luox96.pdf"
}

@misc{Luox11,
  author = "Luo, Zhaohui",
  title = {{Type-Theoretical Semantics with Coercive Subtyping}},
  year = "2011",
  comment = "Lecture Notes 2011 ESSLLI Summer School, Ljubljana,
             Slovenia",
  paper = "Luox11.pdf"
}

@article{Luox13,
  author = "Luo, Zhaohui and Soloviev, Sergei and Xue, Tao",
  title = {{Coercive Subtyping: Theory and Implementation}},
  volume = "223",
  pages = "18-42",
  link = 
    "\url{https://hal-univ-tlse3.archives-ouvertes.fr/hal-01130574/document}",
  abstract =
    "Coercive subtyping is a useful and powerful framework of subtyping
    for type theories.  The key idea of coercive subtyping is subtyping
    as abbreviation. In this paper, we give a new and adequate formulation
    of T[C] , the system that extends a type theory T with coercive
    subtyping based on a set C of basic subtyping judgements, and show
    that coercive subtyping is a conservative extension and, in a more
    general sense, a definitional extension.  We introduce an intermediate
    system, the star-calculus T[C]∗ , in which the positions that 
    require coercion insertions are marked, and show that T[C]∗ is a
    conservative extension of T and that T[C]∗ is equivalent to T[C].
    This makes clear what we mean by coercive subtyping being a
    conservative extension, on the one hand, and amends a technical
    problem that has led to a gap in the earlier conservativity proof, on
    the other. We also compare coercive subtyping with the ‘ordinary’
    notion of subtyping – subsumptive subtyping, and show that the former
    is adequate for type theories with canonical objects while the latter
    is not. An improved implementation of coercive subtyping is done in
    the proof assistant Plastic.",
  paper = "Luox13.pdf",
  keywords = "printed"
}

@inproceedings{Hall94,
  author = "Hall, Condelia V.",
  title = 
    {{Using Hindley-Milner Type Inference to Optimize List Representation}},
  booktitle = "LFP'94 Lisp and Functional Programming",
  publisher = "ACM",
  year = "1994",
  pages = "162-172",
  abstract =
    "Lists are a pervasive data structure in functional programs.  The
    generality and simplicity of their structure makes them expensive.
    Hindley-Mllner type inference and partial evaluation are all that is
    needed to optimise this structure, yielding considerable improvements
    in space and time consumption for some interesting programs.  This
    framework is applicable to many data types and their optimised
    representations, such as lists and parallel implementations of bags,
    or arrays and quadtrees.",
  paper = "Hall94.pdf, printed"
}

@inproceedings{Harp89,
  author = "Harper, Robert and Mitchell, John C. and Moggi, Eugenio",
  title = {{Higher-Order Modules and the Phase Distinction}},
  booktitle = "Symp. on Principles of Programming Languages POPL'17",
  publisher = "ACM Press",
  year = "1989",
  comment = "CMU-CS-89-198",
  link = "\url{https://www.cs.cmu.edu/~rwh/papers/phase/tr.pdf}",
  pages = "341-354",
  abstract =
    "In earlier work, we used a typed function calculus, XML, with 
    dependent types to analyze several aspects of the Standard ML type
    system. In this paper , we introduce a refinement of XML with a clear
    compile-time/run-time phase distinction, and a direct compile-time
    type checking algorithm. The calculus uses a finer separation of 
    types into universes than XML and enforces the phase distinction using
    a nonstandard equational theory for module and signature
    expressions. While unusual from a type-theoretic point of view, the
    nonstandard equational theory arises naturally from the well-known 
    Grothendieck construction on an indexed category.",
  paper = "Harp89.pdf"
}

@article{Katz75,
  author = "Katz, Shmuel and Manna, Zohar",
  title = {{A Closer Look at Termination}},
  journal = "Acta Informatica",
  volume = "5",
  pages = "333-352",
  year = "1975",
  abstract =
    "Several methods for proving that computer programs terminate are
    presented and illustrated. The methods considered involve (a)
    using the 'no-infinitely-descending-chain' property of
    well-founded sets (Floyd's approach), (b) bounding a counter
    associated with each loop ({\sl loop} approach), (c) showing that
    some exit of each loop must be taken ({\sl exit} approach), or (d)
    inducting on the structure of the data domain (Burstall's
    approach). We indicate the relative merit of each method for
    proving termination or non-termination as an integral part of an
    automatic verification system.",
  paper = "Katz75.pdf",
  keywords = "printed"
}

@book{Lero17,
  author = "Leroy, Xavier and Doligez, Damien and Frish, Alain and
            Garrigue, Jacques and Remy, Didier and Vouillon, Jerome",
  title = {{The OCaml System (release 4.06)}},
  publisher = "INRIA",
  year = "2017",
  link = 
   "\url{https://caml.inria.fr/pub/distrib/ocaml-4.06/ocaml-4.06-refman.pdf}"
}

@inproceedings{Macq84,
  author = "MacQueen, David",
  title = {{Modules for Standard ML}},
  booktitle = "Conf. on Lisp and Functional Programming",
  publisher = "ACM Press",
  year = "1984",
  pages = "198-207",
  abstract =
    "The functional programming language ML has been undergoing a thorough
    redesign during the past year, and the module facility described here
    has been proposed as part of the revised language, now called Standard
    ML. The design has three main goals: (1) to facilitate the structuring
    of large ML programs; (2) to support separate compilation and generic
    library units; and (3) to employ new ideas in the semantics of data
    types to extend the power of ML's polymorphic type system. It is based
    on concepts inherent in the structure of ML, primarily the notions of
    a declaration, its type signature, and the environment that it
    denotes.",
  paper = "Macq84.pdf"
}

@inproceedings{Macq86,
  author = "MacQueen, David",
  title = {{Using Dependent Types to Express Modular Structure}},
  booktitle = "Principles of Programming Languages POPL'13",
  publisher = "ACM Press",
  pages = "277-286",
  year = "1986",
  link = 
   "\url{https://people.mpi-sws.org/~dreyer/courses/modules/macqueen86.pdf}",
  paper = "Macq86.pdf"
}

@inproceedings{Macq88,
  author = "MacQueen, David",
  title = {{An Implementation of Standard ML Modules}},
  booktitle = "Lisp and Functional Programming '88",
  year = "1988",
  pages = "212-223",
  publisher = "ACM Press",
  abstract =
    "Standard ML includes a set of module constructs that support
    programming in the large. These constructs extend ML's basic
    polymorphic type system by introducing the dependent types of Martin
    L/Sf's Intuitionistic Type Theory. This paper discusses the problems
    involved in implementing Standard ML's modules and describes a
    practical, efficient solution to these problems. The representations
    and algorithms of this implementation were inspired by a detailed
    formal semantics of Standard ML developed by Milner, Tofte, and
    Harper. The implementation is part of a new Standard ML compiler that
    is written ia Standard ML using the module system.",
  paper = "Macq88.pdf"
}

@article{Macq94,
  author = "MacQueen, David and Tofte, Mads",
  title = {{A Semantics for Higher-order Functors}},
  journal = "LNCS",
  volume = "788",
  pages = "409-423",
  year = "1994",
  abstract =
    "Standard ML has a module system that allows one to define parametric
    modules, called functors.  Functors are ``first-order,'' meaning
    that functors themselves cannot be passed as parameters or returned
    as results of functor applications.  This paper presents a semantics
    for a higher-order module system which generalizes the module system
    of Standard ML.  The higher-order functors described here are
    implemented in the current version of Standard ML of New Jersey and
    have proved useful in programming practice.",
  paper = "Macq94.pdf"
}

@article{Mann78,
  author = "Manna, Zohar and Waldinger, Richard",
  title = {{The Logic of Computer Programming}},
  journal = "IEEE Trans. on Software Engineering",
  volume = "4",
  number = "3",
  year = "1978",
  abstract = 
    "Techniques derived from mathematical logic promise to provide an
    alternative to the conventional methodology for constructing,
    debugging, and optimizing computer programs. Ultimately, these
    techniques are intended to lead to the automation of many of the
    facets of the programming process.
    
    This paper provides a unified tutorial exposition of the logical
    techniques, illustrating each with examples. The strengths and
    limitations of each technique as a practial programming aid are
    assessed and attempts to implement these methods in experimental
    systems are discussed.",
  paper = "Mann78.pdf, printed"
}

@article{Mann78a,
  author = "Manna, Zohar and Waldinger, Richard",
  title = {{Is 'sometime' sometimes better than 'always'?}},
  journal = "CACM",
  volume = "21",
  number = "2",
  year = "1978",
  abstract = 
    "This paper explores a technique for proving the correctness and
    termination of programs simultaneously. This approach, the 
    {\sl intermittent-assertion method}, involves documenting the
    program with assertions that must be true at some time when
    control passes through the corresponding point, but that need not
    be true every time. The method, introduced by Burstall, promises
    to provide a valuable complement to the more conventional methods.

    The intermittent-assertion method is presented with a number of
    examples of correctness and termination proofs. Some of these
    proofs are markedly simpler than their conventional
    counterparts. On the other hand, it is shown that a proof of
    correctness or termination by any of the conventional techniques
    can be rephrased directly as a proof using intermittent
    assertions. Finally it is shown how the intermittent assertion
    method can be applied to prove the validity of program
    transformations and the correctness of continuously operating
    systems.",
  paper = "Mann78a.pdf, printed"
}

@misc{Mann80,
  author = "Manna, Zohar",
  title = {{Lectures on the Logic of Computer Programming}},
  publisher = "Society for Industrial and Applied Mathematics",
  year = "1980",
  paper = "Mann80.tgz",
  keywords = "printed"
}

@misc{Mapl18,
  author = "Maplesoft",
  title = {{Maple Computer Algebra Software}},
  year = "2018",
  link = "\url{https://www.maplesoft.com}"
}

@inproceedings{Mart73,
  author = "Martin-Lof, P.",
  title = {{An Intuitionistic Theory of Types}},
  booktitle = "Logic Colloquium 1973",
  publisher = "H.E. Rose and J.G. Shepherdson",
  pages = "73-118",
  year = "1973",
  abstract =
    "The theory of types with which we shall be concerned is intended to
    be a full scale system for foralizing intuitionistic mathematics as
    developed, for example, in the book by Bishop 1967. The language of
    the theory is richer than the language of first order predicate
    logic. This makes it possible to strengthen the axioms for existence
    and disjunction. In the case of existence, the possibility of
    strengthening the usual elimination rule seems first to have been
    indicated by Howard 1969, whose proposed axioms are special cases of
    the existential elimination rule of the present theory. Furthermore,
    there is a reflection principle which links the generation of objects
    and types and plays somewhat the same role for the present theory as
    does the replacement axiom for Zermelo-Frankel set theory.
    
    An earlier, not yet conclusive, attempt at formulating a theory of
    this kind was made by Scott 1970. Also related, although less closely,
    are the type and logic free theories of consructions of Kreisel 1962
    and 1965 and Goodman 1970.
    
    In its first version, the present theory was based on the strongly
    impredicative axiom that there is a type of all types whatsoever,
    which is at the same time a type and an object of that type. This
    axiom had to be abandoned, however, after it was shown to lead to a
    contradiction by Jean Yves Girard. I am very grateful to him for
    showing me his paradox. The change that it necessitated is so drastic
    that my theory no longer contains intuitionistic simple type theory as
    it originally did. Instead, its proof theoretic strength should be
    close to that of predicative analysis.",
  paper = "Mart73.pdf",
  keywords = "printed"
}

@inproceedings{Mart79,
  author = "Martin-Lof, P.",
  title = {{Constructive Mathematics and Computer Programming}},
  booktitle = "Proc. 6th Int. Congress for Logic, Methodology and 
               Philosophy of Science",
  publisher = "North-Holland",
  year = "1979",
  pages = "153-179",
  abstract =
    "If programming is understood not as the writing of instructions for
    this or that computing machine but as the design of methods of
    computation that it is the computer's duty to execute (a difference
    that Dijkstra has referred to as the difference between computer
    science and computing science), then it no longer seems possible to
    distinguish the discipline of programming from constructive
    mathematics. This explains why the intuitionistic theory of types,
    which was originally developed as a symbolism for the precise
    codification of constructive mathematics, may equally well be viewed
    as a programming language. As such it provides a precise notation not
    only, like other programming languages, for the programs themselves
    but also for the tasks that the programs are supposed to
    perform. Moreover, the inference rules of the theory of types, which
    are again completely formal, appear as rules of correct program
    synthesis. Thus the correctness of a program written in the theory of
    types is proved formally at the same time as it is being
    synthesized.",
  paper = "Mart79.pdf",
  keywords = "printed"
}

@phdthesis{Mcbr99,
  author = "McBride, Conor",
  title = {{Dependently Typed Functional Programs and their Proofs}},
  school = "University of Edinburgh",
  year = "1999",
  link = "\url{http://strictlypositive.org/thesis.pdf}",
  abstract =
    "Research in dependent type theories has, in the past, concentrated on
    its use in the presentation of theorems and theorem-proving. This
    thesis is concerned mainly with the exploitation of the computational
    aspects of type theory for programming, in a context where the
    properties of programs may readily be specified and established. In
    particular, it develops technology for programming with dependent
    inductive families of datatypes and proving those programs correct. It
    demonstrates the considerable advantage to be gained by indexing data
    structures with pertinent characteristic information whose soundness
    is ensured by typechecking, rather than human effort.
    
    Type theory traditionally presents safe and terminating computation on
    inductive datatypes by means of elimination rules which serve as
    induction principles and, via their associated reduction behaviour,
    recursion operators. In the programming language arena, these appear
    somewhat cumbersome and give rise to unappealing code, complicated by
    the inevitable interaction between case analysis on dependent types
    and equational reasoning on their indices which must appear explicitly
    in the terms. Thierry Coquand's proposal to equip type theory directly
    with the kind of pattern matching notation to which functional
    programmers have become used to over the past three decades offers a
    remedy to many of these difficulties. However, the status of pattern
    matching relative to the traditional elimination rules has until now
    been in doubt. Pattern matching implies the uniqueness of identity
    proofs, which Martin Hofmann showed underivable from the conventiaonal
    definition of equality. This thesis shows that the adoption of this
    uniqueness as axiomatic is sufficient to make pattern matching
    admissible.
    
    A datatype's elimination rule allows abstraction only over the whole
    inductively defined family. In order to support pattern matching, the
    application of such rules to specific instances of dependent families
    has been systematised. The underlying analysis extends beyond
    datatypes to other rules of a similar second order character,
    suggesting they may have other roles to play in the specification,
    verification and, perhaps, derivation of programs. The technique
    developed shifts the specificity from the instantiation of the type's
    indices into equational constraints on indices freely chosen, allowing
    the elimination rule to be applied.
    
    Elimination by this means leaves equational hypotheses in the
    resulting subgoals, which must be solved if further progress is to be
    made. The first-order unification algorithm for constructor forms in
    simmple types presented in [McB96] has been extended to cover
    dependent datatypes as well, yielding completely automated solution of
    a class of problems which can be syntactically defined.
    
    The justification and operation of these techniques requires the
    machine to construct and exploit a standardised collection of
    auxiliary lemmas for each datatype. This is greatly facilitated by two
    technical developments of interest in their own right:
    \begin{itemize}
    \item a more convenient definition of equality, with a relaxed
    formulation rule allowing elements of different types to be compared,
    but nonetheless equivalent to the usual equality plus the axiom of
    uniqueness
    \item a type theory, OLEG, which incorporates incomplete objects,
    accounting for their 'holes' entirely within the typing judgments and,
    novelly, not requiring any notion of explicit substitution to manage
    their scopes.
    \end{itemize}
    
    A substantial prototype has been implemented, extending the proof
    assistant LEGO. A number of programs are developed by way of
    example. Chiefly, the increased expressivity of dependent datatypes is
    shown to capture a standard first-order unification algorithm within
    the class of structurally recursive programs, removing any need for a
    termination argument. Furthermore, the use of elimination rules in
    specifying the components of the program simplifies significantly its
    correctness proof.",
  paper = "Mcbr99.pdf",
  keywords = "printed"
}

@article{Mitc88a,
  author = "Mitchell, John C.",
  title = {{Polymorphic Type Inference and Containment}},
  journal = "Information and Computation",
  volume = "76",
  number = "2-3",
  year = "1988",
  pages = "211-249",
  abstract =
    "Type expressions may be used to describe the functional behavior of
    untyped lambda terms. We present a general semantics of polymorphic
    type expressions over models of untyped lambda calculus and give
    complete rules for inferring types for terms. Some simplified typing
    theories are studied in more detail, and containments between types
    are investigated.",
  paper = "Mitch88a.pdf"
}
    
@misc{Miet15,
  author = "Mietek, Bak",
  title = {{Build Your Own Proof Assistant}},
  year = "2015",
  link = "\url{https://www.youtube.com/watch?v=7u-jx1UyRmc}"
}

@inproceedings{Mitc91b,
  author = "Mitchell, John and Meidal, Sigurd and Madhav, Neel",
  title = {{An Extension of Standard ML Modules with Subtyping and
           Inheritance}},
  booktitle = "POPL'91",
  pages = "270-278",
  year = "1991",
  isbn = "0-89791-419-8",
  abstract =
    "We describe a general module language integrating abstract data
    types, specifications and object-oriented concepts.  The framework is
    based on the Standard ML module system, with three main extensions:
    subtyping, a form of object derived from ML structures, and inheritence
    primitives.  The language aims at supporting a range of
    programming styles, including mixtures of object-oriented programming
    and programs built around specified algebraic or higher-order abstract
    data types.  We separate specification from implementation,
    and provide independent inheritance mechanisms for each.  In order to
    support binary operations on objects within this framework, we
    introduce ``internal interfaces// which govern the way that function
    components of one structure may access components of another.  The
    language design has been tested by writing a number of program examples;
    an implementation is under development in the context of a
    larger project.",
  paper = "Mitc91b.pdf"
}

@article{Morr84,
  author = "Morris, F.L. and Jones, C.B.",
  title = {{An Early Program Proof by Alan Turing}},
  journal = "Annals of the History of Computing",
  volume = "6",
  number = "2",
  year = "1984",
  pages = "139-143",
  abstract =
    "The paper reproduces, with typographical corrections and
    comments, a 1949 paper by Alan Turing that foreshadoes much
    subsequent work in program proving.",
  paper = "Morr84.pdf",
  keywords = "printed"
}

@misc{Mull13,
  author = "Mulligan, Dominic P.",
  title = {{Mosquito: An Aimplementation for Higher-Order Logic}},
  school = "University of Cambridge",
  year = "2013",
  link = "\url{https://bitbucket.org/MosquitoProofAssistant/mosquito}",
  abstract =
    "We present Mosquito: an experimental stateless, pure, largely total
    LCF-style implementation of higher-order logic using Haskell as a
    metalanguage. We discuss details of the logic implemented, kernel
    design, and novel proof state and tactic representations.",
  paper = "Mull13.pdf",
  keywords = "printed"
}

@inproceedings{Nipk93,
  author = "Nipkow, Tobias and Prehofer, Christian",
  title = {{Type Checking Type Classes}},
  booktitle = "Principles of Programming Languages POPL'93",
  publisher = "ACM Press",
  year = "1993",
  pages = "409-418",
  abstract =
    "We study the type inference problem for a system with type classes as
    in the functional programming language Haskell. Type classes are an
    extension of ML-style polymorphism with overloading. We generalize
    Milner's work on polymorphism by introducing a separate context
    constraining the type variables in a typing judgement. This leads to
    simple type inference systems and algorithms which closely resemble
    those for ML. In particular we present a new unification algorithm
    which is an extension of syntactic unification with constraint
    solving. The existence of principal types follows from an analysis of
    this unification algorithm."
}

@article{Nipk95,
  author = "Nipkow, Tobias and Prehofer, Christian",
  title = {{Type Reconstruction for Type Classes}},
  journal = "J. of Functional Programming",
  year = "1995",
  pages = "201-224",
  abstract =
    "We study the type inference problem for a system with type classes as
    in the functional programming language Haskell. Type classes are an
    extension of ML-style polymorphism with overloading. We generalize
    Milner's work on polymorphism by introducing a separate context
    constraining the type variables in a typing judgement. This leads to
    simple type inference systems and algorithms which closely resemble
    those for ML. In particular we present a new unification algorithm
    which is an extension of syntactic unification with constraint
    solving. The existence of principal types follows from an analysis of
    this unification algorithm.",
  paper = "Nipk95.pdf"
}

@inproceedings{Pado06,
  author = "Padovani, Luca and Zacchiroli, Stefano",
  title = {{From Notation to Semantics: There and Back Again}},
  booktitle = "5th Conf. on Mathematical Knowledge Management",
  year = "2006",
  abstract =
    "Mathematical notation is a structured, open, and ambiguous
    language. In order to support mathematical notation in MKM
    applications one must necessarily take into account presentational as
    well as semantic aspects. The former are required to create a
    familiar, comfortable, and usable interface to interact with. The
    latter are necessary in order to process the information
    meaningfully. In this paper we investigate a framework for dealing
    with mathematical notation in a meaningful, extensible way, and we
    show an effective instantiation of its architecture to the field of
    interactive theorem proving. The framework builds upon well-known
    concepts and widely-used technologies and it can be easily adopted by
    other MKM applications.",
  paper = "Pado06.pdf"
}

@inproceedings{Pier93,
  author = "Pierce, Benjamin C. and Turner, David N.",
  title = {{Object-oriented Programming without Recursive Types}},
  booktitle = "POPL'93",
  pages = "299-312",
  year = "1993",
  abstract =
    "It is widely agreed that recursive types are inherent in the static
    typing of the essential mechanisms of object-oriented programming:
    encapsulation, message passing, subtyping, and inheritance. We
    demonstrate here that modeling object encapsulation in terms of
    existential types yields a substantially more straightforward
    explanation of these features in a simpler calculus without recursive
    types.",
  paper = "Pier93.pdf"
}

@techreport{Pier97,
  author = "Pierce, Benjamin C. and Turner, David N.",
  title = {{Local Type Inference}},
  institution = "Indiana University",
  year = "1997",
  type = "CSCI Technical Report",
  number = "493",
  abstract =
    "We study two partial type inference methods for a language combining
    subtyping and impredicative polymorphism. Both methods are {\sl local}
    in the sense that missing annotations are recovered using only
    information from adjacent nodes in the syntax tree, without long
    distance constraints such as unification variables. One method infers
    type arguments in polymorphic applications using a local constraint
    solver. The other infers annotations on bound variables in function
    abstractions by propagating type constraints downward from enclosing
    application nodes. We motivate our design choices by a statistical
    analysis of the uses of type inference in a sizable body of existing
    ML code.",
  paper = "Pier97.pdf, printed"
}

@misc{Reid17,
  author = "Reid, Alastair",
  title = {{How can you trust formally varified software}},
  link = "\url{https://media.ccc.de/v/34c3-8915-how_can_you_trust_formally_verified_software#t=12}",
  year = "2017",
  abstract =
    "Formal verification of software has finally started to become viable:
    we have examples of formally verified microkernels, realistic
    compilers, hypervisors, etc. These are huge achievements and we can
    expect to see even more impressive results in the future but the
    correctness proofs depend on a number of assumptions about the Trusted
    Computing Base that the software depends on. Two key questions to ask
    are: Are the specifications of the Trusted Computing Base correct? And
    do the implementations match the specifications? I will explore the
    philosophical challenges and practical steps you can taek in answering
    that question for one of the major dependencies: the hardware your
    software runs on. I will describe the combination of formal
    verification and testing that ARM uses to verify the processor
    specification and I will talk about our current challenge: getting the
    specification down to zero bugs while the architecture continues to
    evolve."
}

@book{Rest00,
  author = "Restall, Greg",
  title = {{An Introduction to Substructural Logics}},
  year = "2000",
  publisher = "Routledge",
  isbn = "0-415-21534-X"
}

@article{Robi65,
  author = "Robinson, J.A.",
  title = {{A Machine-Oriented Logic Based on the Resolution Principle}},
  journal = "ACM",
  volume = "12",
  pages = "23-41",
  year = "1965",
  abstract =
    "Theorem-proving on the computer, using procedures based on the 
    fundamental theorem of Herbrand concerning the first-order predicate
    calculus, is examined with a view towards improving the efficiency and
    widening the range of practical applicability of these procedures.  A
    elose analysis of the process of substitution (of terms for
    variables), and the process of truth-functional analysis of the
    results of such substitutions, reveals that both processes can be
    combined into a single new process (called resolution), iterating
    which is vastty more effieient than the older cyclic procedures
    consisting of substitution stages alternating with truth-functional
    analysis stages.

    The theory of the resolution process is presented in the form of a
    system of first-order logic with .just one inference principle (the
    resolution principle).  The completeness of the system is proved; the
    simplest proof-procedure based on the system is then the direct 
    implementation of the proof of completeness.  Howewer, this procedure is
    quite inefficient, and the paper concludes with a discussion of
    several principles (called search principles) which are applicable to
    the design of efficient proof-procedures employing resolution as the
    basic logical process.",
  paper = "Robi65.pdf",
  keywords = "printed"
}

@article{Ruzi89,
  author = "Ruzicka, Peter and Privara, Igor",
  title = {{An Almost Linear Robinson Unification Algorithm}},
  journal = "Acta Informatica",
  volume = "27",
  pages = "61-71",
  year = "1989",
  abstract =
    "Further asymptotical improvement of original Robinson's unification
    idea is presented.  By postponing the so-called occur-check in Corbin
    and Bidoit's quadratic rehabilitation of the Robinson algorithm at the
    end of unification an almost linear unification algorithm is obtained.
    In the worst case, the resulting algorithm has the time complexity
    O(p.A(p)), where p is the size of input terms and A is the inverse to
    the Ackermann function.  Moreover, the practical experiments are
    summarized comparing Corbin and Bidoit's quadratic algorithm with the
    resulting almost linear unification algorithm based on Robinson's
    principle. ",
  paper = "Ruzi89.pdf"
}

@article{Scot76,
  author = "Scott, Dana",
  title = {{Data Types as Lattices}},
  journal = "SIAM J. Comput.",
  volume = "5",
  year = "1976",
  pages = "522-587",
  abstract =
    "The meaning of many kinds of expressions in programming languages can
    be taken as elements of certain spaces of 'partial' objects. In this
    report these spaces are modeled in one univeral domain $P_\omega$, the
    set of all subsets of the integers. This domain renders the connection
    of this semantic theory with the ordinary theor of number theoretic
    (especailly general recursive) functions clear and straightforward.",
  paper = "Scot76.pdf"
}

@techreport{Site74,
  author = "Sites, Richard L.",
  title = {{Some Thoughts on Proving Clean Termination of Programs}},
  type = "technical report",
  institution = "Stanford University",
  number = "STAN-CS-74-417",
  year = "1974",
  abstract = 
    "Proof of clean termination is a useful sub-goal in the process of
    proving that a program is totally correct. Clean termination means
    that the program terminates (no infinite loops) and that it does
    so normally, without any execution-time semantic errors (integer
    overflow, use of undefined variables, subscript out of range,
    etc.). In contrast to proofs of correctness, proof of clean
    termination requires no extensive annotation of a program by a
    human user, but the proof says nothing about the results
    calculated by the program, just that whatever it does, it
    terminates cleanly. Two example proofs are given, of previously
    published programs: TREESORT3 by Robert Floyd, and SELECT by
    Ronald L. Revest and Robert Floyd.",
  paper = "Site74.pdf"
}

@article{Thie17,
  author = "Thiemann, Rene and Yamada, Akihisa",
  title = {{Polynomial Factorization: Proof Outline}},
  journal = "Archive of Formal Proofs",
  year = "2017",
  link = "\url{https://www.isa-afp.org/browser_info/current/AFP/Polynomial_Factorization/outline.pdf}",
  abstract =
    "Based on existing libraries for polynomial interpolation and
    matrices, we formalized several factorization algorihms for
    polynomials, including Kronecker's algorithm for intgeer polynomials,
    Yun's square-free factorization algorithm for field polynomials, and a
    factorization algorithm which delivers root-free polynomials.
    
    As side products, we developed division algorithms for polynomials
    over integral domains, as well as primality-testing and prime
    factorization algorithms for integers.",
  paper = "Thie17.pdf",
  keywords = "printed"
}  

@article{Thie17a,
  author = "Thiemann, Rene and Yamada, Akihisa",
  title = {{Polynomial Factorization: Proof Document}},
  journal = "Archive of Formal Proofs",
  year = "2017",
  link = "\url{https://www.isa-afp.org/browser_info/current/AFP/Polynomial_Factorization/document.pdf}",
  abstract =
    "Based on existing libraries for polynomial interpolation and
    matrices, we formalized several factorization algorihms for
    polynomials, including Kronecker's algorithm for intgeer polynomials,
    Yun's square-free factorization algorithm for field polynomials, and a
    factorization algorithm which delivers root-free polynomials.
    
    As side products, we developed division algorithms for polynomials
    over integral domains, as well as primality-testing and prime
    factorization algorithms for integers.",
  paper = "Thie17a.pdf"
}  

@article{Tryb92,
  author = "Trybulec, Zinaida and Swieczkowska, Halina",
  title = {{Some Remarks on The Language of Mathematical Texts}},
  journal = "Studies in Logic, Grammar and Rhetoric",
  volume = "10/11",
  pages = "103-124",
  year = "1992",
  link = "\url{http://mizar.org/trybulec65/5.pdf}",
  paper = "Tryb92.pdf"
}  

@book{Turn91,
  author = "Turner, Raymond",
  title = {{Constructive Foundations for Functional Languages}},
  publisher = "McGraw-Hill",
  year = "1991",
  isbn = "9780077074111"
}

@article{Unga91,
  author = "Ungar, David and Smith, Randall B.",
  title = {{SELF: The Power of Simplicity}},
  journal = "Lisp and Symbolic Computation",
  volume = "4",
  number = "3",
  year = "1991",
  publisher = "Kluwer",
  abstract =
    "SELF is an object-oriented language for exploratory programming
    based on a small number of simple and concrete ideas: prototypes,
    slots, and behavior. Prototypes combine inheritance and instantiation
    to provide a framework that is simpler and more flexible than most
    object-oriented languages. Slots unite variables and procedures into a
    single construct. This permits the inheritance hierarchy to take over
    the function of lexical scoping in conventional languages.  Finally,
    because SELF does not distinguish state from behavior, it narrows the
    gaps between ordinary objects, procedures, and closures.  SELF ’s
    simplicity and expressiveness offer new insights into object-oriented
    computation.",
  paper = "Unga91.pdf"
}

@book{Wilk85a,
  author = "Wilkes, Maurice",
  title = {{Memoirs of a Computer Pioneer}},
  publisher = "MIT Press",
  year = "1985"
}

@misc{Wirt80,
  author = "Wirth, N.",
  title = {{Modula-2}},
  year = "1980",
  link = "\url{http://www.ada-auth.org/standards/12rm/RM-Final.pdf}",
  abstract =
    "Modula-2 is a general purpose programming language primarily
    designed for system implementation. This report constitutes its
    definition in a concise, although informal style. It also
    describes the use of an implementation for the PDP-11 computer.",
  paper = "Wirt80.pdf"
}

@book{Wirt83,
  author = "Wirth, N.",
  title = {{Programming in Modula-2}},
  publisher = "Springer-Verlag",
  year = "1983",
  isbn = "978-3-642-96878-5"
}

@article{Wirt88,
  author = "Wirth, N.",
  title = {{Type Extensions}},
  journal = "TOPLAS",
  volume = "10",
  number = "2",
  year = "1988",
  pages = "203-214",
  abstract =
    "Software systems represent a hierarchy of modules. Client modules
    contain sets of procedures that extend the capabilities of imported
    modules. This concept of extension is here applied to data
    types. Extended types are related to their ancestor in terms of a
    hierarchy. Variables of an extended type are compatible with variables
    of the ancestor type. This scheme is expressed by three language
    constructs only: the declaration of extended record types, the type
    test, and the type guard. The facility of extended types, which
    closely resembles the class concept, is defined in rigorous and
    concise terms, and an efficient implementation is presented.",
  paper = "Wirt88.pdf"
}

@misc{Wolf18,
  author = "Wolfram, Stephen",
  title = {{Mathematica Website}},
  year = "2018",
  link = "\url{http://www.wolfram.com}"
}

@misc{Anon18,
  author = "Anonymous",
  title = {{A Compiler From Scratch}},
  link = "\url{https://www.destroyallsoftware.com/screencasts/catalog/a-compiler-from-scratch}",
  year = "2018"
}

@article{Appe87,
  author = "Appel, Andrew W. and MacQueen, David B.",
  title = {{A Standard ML Compiler}},
  journal = "LNCS",
  volume = "274",
  pages = "301-324",
  year = "1987",
  abstract =
    "Standard ML is a major revision of earlier dialects of the functional
    language ML. We describe the first compiler written for Standard ML in
    Standard ML.  The compiler incorporates a number of novel features and
    techniques, and is probably the largest system written to date in
    Standard ML.
    
    Great attention was paid to modularity in the construction of the
    compiler, leading to a successful large-scale test of the modular
    capabilities of Standard ML.  The front end is useful for purposes
    other than compilation, and the back end is easily retargetable (we
    have code generators for the VAX and MC68020). The module facilities
    of Standard ML were taken into account early in the design of the
    compiler, and they particularly influenced the environment management
    component of the front end. For example, the symbol table structure is
    designed for fast access to opened structures.
    
    The front end of the compiler is a single phase that integrates
    parsing, environment management, and type checking. The middle end
    uses a sophisticated decision tree scheme to produce efficient
    pattern matching code for functions and case expressions. The abstract
    syntax produced by the front end is translated into a simple
    lambda-calculus-based intermediate representation that lends itself to
    easy case analysis and optimization in the code generator. Special
    care was taken in designing the mntime data structures for fast
    allocation and garbage collection.
    
    We describe the overall organization of the compiler and present some
    of the data representations and algorithms used in its various
    phases. We conclude with some lessons learned about the ML language
    itself and about compilers for modem functional languages.",
  paper = "Appe87.pdf",
  keywords = "printed"
}

@techreport{Appe88,
  author = "Appel, Andrew W. and Jim, Trevor",
  title = {{Continuation-Passing, Closure-Passing Style}},
  institution = "Princston University",
  year = "1988",
  number = "CS-TR-183-88",
  abstract =
    "We implemented a continuation-passing style (CPS) code generator for
    ML. Our CPS language is represented as an ML datatype in which all
    functions are named and most kinds of ill-formed expressions are
    impossible. We separate the code generation into phases that rewrite
    this representation into ever-simpler forms. Closures are represented
    explicitly as records, so that closure strategies can be communicated
    from one phase to another. No stack is used. Our benchmark data shows
    that the new method is an improvement over our previous,
    abstract-machine based code generator.",
  paper = "Appe88.pdf",
  keywords = "printed"
}

@book{Appe92,
  author = "Appel, Andrew W.",
  title = {{Compiling with Continuations}},
  year = "1992",
  publisher = "Cambridge University Press",
  isbn = "0-521-03311-X"
}

@book{Appe98,
  author = "Appel, Andrew W.",
  title = {{Modern Compiler Implementation in ML}},
  year = "1998",
  publisher = "Cambridge University Press",
  isbn = "0-521-60764-7"
}

@article{Back81,
  author = "Back, R.J.R",
  title = {{On Correct Refinement of Programs}},
  journal = "J. Computer and System Sciences",
  volume = "23",
  number = "1",
  pages = "49-68",
  year = "1981",
  abstract =
    "The stepwise refinement technique is studied from a mathematical
    point of view. A relation of correct refinement between programs is
    defined, based on the principle that refinement steps should be
    correctness preserving. Refinement between programs will therefore
    depend on the criterion of program correctness used. The application
    of the refinement relation in showing the soundness of different
    techniques for refining programs is discussed. Special attention is
    given to the use of abstraction in program construction. Refinement
    with respect to partial and total correctness will be studied in more
    detail, both for deterministic and nondeterministic programs. The
    relationship between these refinement relations and the approximation
    relation of fixpoint semantics will be studied, as well as the
    connection with the predicate transformers used in program
    verification.",
  paper = "Back81.pdf",
  keywords = "printed"
}

@article{Bals91,
  author = "Balsters, Herman and Fokkinga, Maarten M.",
  title = {{Subtyping can have a simple semantics}},
  journal = "Theoretical Computer Science",
  volume = "87",
  pages = "81-96",
  year = "1991",
  abstract =
    "Consider a first order typed language, with semantics
     $\llbracket~\rrbracket$ for expressions and types. Adding
     subtyping means that a partial order $\le$ on types is defined
     and that the typing rules are extended to the effect that
     expression $e$ has type $\tau$ whenever $e$ has type $\sigma$ and 
     $\sigma \le \tau$. We show how to adapt the semantics
     $\llbracket~\rrbracket$ in a {\sl simple set theoretic way},
     obtaining a semantics $\llbracket~\rrbracket$ that satisfies, in
     addition to some obvious requirements, also the property
     $\llbracket\sigma\rrbracket \subseteq $\llbracket\tau\rrbracket$, 
     whenever $\sigma \le \tau$.",
  paper = "Bals91.pdf",
  keywords = "printed"
}

@techreport{Barr96a,
  author = "Barras, Bruno",
  title = {{Coq en Coq}},
  institution = "INRIA",
  year = "1996",
  type = "Research Report",
  number = "inria-00073667",
  comment = "French",
  abstract =
    "The essential step of the formal verification of a proof-checker such
    as Coq is the verification of its kernel: a type-checker for the 
    {\sl Calculus of Inductive Constructions} (CIC) whihc is its underlying
    formalism. The present work is a first small-scale attempt on a
    significative fragment of CIC: the Calculus of Constructions (CC). We
    formalize the definition and the metatheory of (CC) in Coq. In
    particular, we prove strong normalization and decidability of type
    inference. From the latter proof, we extract a certified {\sl Caml
    Light} program, which performs type inference (or type-checking) for
    an arbitrary typing judgement in CC. Integrating this program in a
    larger system, including a parser and pretty-printer, we obtain a
    stand-alone proof-checker, called {\sl CoC}, the {\sl Calculus of
    Constructions}. As an example, the formal proof of Newman's lemma,
    build with Coq, can be re-verified by {\sl CoC} with reasonable
    performance.",
  paper = "Barr96a.pdf"
}

@misc{Barr18,
  author = "Barras, Bruno and Werner, Benjamin",
  title = {{Coq in Coq}},
  link = 
"\url{http://www.lix.polytechnique.fr/Labo/Bruno.Barras/publi/coqincoq.pdf}",
  comment = "https://github.com/coq-contribs/coq-in-coq",
  abstract =
    "We formalize the definition and the metatheory of the Calculus of
    Constructions (CC) using the proof assistant Coq. In particular,
    we prove strong normalization and decidability of type
    inference. From the latter proof, we extract a certified Objective
    Caml program which performs type inference in CC and use this code
    to build a small-scale certified proof-checker.",
  paper = "Barr18.pdf",
  keywords = "printed"
}

@inproceedings{Bent07,
  author = "Benton, Nick and Zarfaty, Uri",
  title = {{Formalizing and Verifying Semantic Type Soundness of a 
            Simple Compiler}},
  booktitle = "Principles and Practice of Declarative Programming",
  publisher = "ACM",
  pages = "1-12",
  year = "2007",
  abstract =
    "We describe a semantic type soundness result, formalized in the Coq
    proof assistant, for a compiler from a simple imperative language with
    heap-allocated data into an idealized assembly language. Types in the
    high-level language are interpreted as binary relations, built using
    both second-order quantification and a form of separation structure,
    over stores and code pointers in the low-level machine.",
  paper = "Bent07.pdf",
  keywords = "printed"
}

@misc{Bonn18,
  author = "Bonnaire-Sergeant, Ambrose",
  title = {{Are unsound type systems wrong?}},
  link = 
"\url{http://frenchy64.github.io/2018/04/07/unsoundness-in-untyped-types.html}",
}

@article{Boye75,
  author = "Boyer, Robert S. and Moore, J Strother",
  title = {{Proving Theorems About LISP Functions}},
  journal = "J. ACM",
  volume = "22",
  number = "1",
  pages = "129-144",
  year = "1975",
  abstract =
    "Program verification is the idea that properties of programs can be
    precisely stated and proved in the mathematical sense. In this paper,
    some simple heuristics combining evaluation and mathematical induction
    are describe, which the authors have implemented in a program that
    automatically proves a wide variety of theorems about recursive LISP
    functions. The method the program uses a generate induction formulas
    is described at length. The theorems proved by the program include
    that REVERSE is its own inverse and that a particular SORT program is
    correct. A list of theorems proved by the program is given.",
  paper = "Boye75.pdf"
}

@book{Brod94,
  author = "Broda, Krysia and Eisenbach, Susan and Khoshnevisan, Hessam
            and Vickers, Steve",
  title = {{Reasoned Programming}},
  publisher = "Imperial College",
  year = "1994",
  paper = "Brod94.pdf"
}

@inproceedings{Cald97,
  author = "Caldwell, James L.",
  title = {{Moving proofs-as-programs into practice}},
  booktitle = "Automated Software Engineering",
  publisher = "IEEE",
  year = "1997",
  abstract =
    "Proofs in the Nuprl system, an implementation of a constructive
    type theory, yield “correct-by-construction” programs.  In this
    paper a new methodology is presented for extracting efficient and
    readable programs from inductive proofs. The resulting extracted
    programs are in a form suitable for use in hierarchical
    verifications in that they are amenable to clean partial evaluation
    via extensions to the Nuprl rewrite system. The method is based on two
    elements: specifications written with careful use of the Nuprl
    set-type to restrict the extracts to strictly computational content;
    and on proofs that use induction tactics that generate extracts
    using familiar fixed-point combinators of the untyped lambda
    calculus. In this paper the methodology is described and its
    application is illustrated by example.",
  paper = "Cald97.pdf",
  keywords = "printed"
}

@article{Card84,
  author = "Cardelli, Luca",
  title = {{A Semantics of Multiple Inheritance}},
  journal = "LNCS",
  volume = "173",
  year = "1984",
  paper = "Card84.pdf",
  keywords = "printed"
}

@inproceedings{Care07,
  author = "Carette, Jacques and Farmer, William M. and Sorge, Volker",
  title = {{A Rational Reconstruction of a System for Experimental
            Mathematics}},
  booktitle = "14th Workshop on Automated Reasoning",
  publisher = "unknown",
  year = "2007",
  abstract =
    "In previous papers we described the implementation of a system
    which combines mathematical object generation, transformation and
    filtering, conjecture generation, proving and disproving for
    mathematical discovery in non-associative algebra. While the system
    has generated novel, fully verified theorems, their construction
    involved a lot of ad hoc communication between disparate systems. In
    this paper we carefully reconstruct a specification of a sub-process
    of the original system in a framework for trustable communication
    between mathematics systems put forth by us. It employs the concept
    of biform theories that enables the combined formalisation of the
    axiomatic and algorithmic theories behind the generation
    process. This allows us to gain a much better understanding of the
    original system, and exposes clear generalisation opportunities.",
  paper = "Care07.pdf",
  keywords = "printed"
}

@phdthesis{Cart76,
  author = "Cartwright, Robert",
  title = {{A Practical Formal Semantic Definition and Verification
            System for Typed LISP}},
  school = "Stanford Artificial Intelligence Labs",
  year = "1976",
  abstract = 
    "Despite the fact that computer scientists have developed a variety of
    formal methods for proving computer programs correct, the formal
    verification of a non-trivial program is still a formidable
    task. Moreover, the notion of proof is so imprecise in most existing
    verification systems, that the validity of the proofs generated is
    open to question. With an aim toward rectifying these problems, the
    research discussed in this dissertation attempts to accomplish the
    following objectives:
    \begin{enumerate}
    \item To develop a programming language which is sufficiently powerful
    to express many interesting algorithms clearly and succintly, yet
    simple enough to have a tractable formal semantic definition
    \item To completely specify both proof theoretic and model theoretic
    formal semantics for this language using the simplest possible
    abstractions
    \item To develop an interactive program verification system for the
    language which automatically performs as many of the straightforward
    steps in a verification as possible
    \end{enumerate}",
  paper = "Cart76.pdf",
}

@article{Cart78,
  author = "Cartwright, Robert and McCarthy, John",
  title = {{Recursive Programs as Functions in a First Order Theory}},
  journal = "LNCS",
  volume = "75",
  pages = "576-629",
  year = "1978",
  abstract = 
    "Pure Lisp style recursive function programs are represented in a new
    way by sentences and schemata of first order logic. This permits easy
    and natural proofs of extensional properties of such programs by
    methods that generalize structural induction. It also systematizes
    known methods such as recursion induction, subgoal induction,
    inductive assertions by interpreting them as first order axiom
    schemata. We discuss the metatheorems justifying the representation
    and techniques for proving facts about specific programs. We also give
    a simpler version of the GiSdeI-Kleene way of representing computable
    functions by first order sentences.",
  paper = "Cart78.pdf"
}

@article{Chli07,
  author = "Chlipala, Adam J.",
  title = {{A Certified Type-Preserving Compiler from the Lambda Calculus
            to Assembly Language}},
  journal = "ACM SIGPLAN PLDI'07",
  volume = "42",
  number = "6",
  year = "2007",
  pages = "54-65",
  abstract =
    "We present a certified compiler from the simply-typed lambda calculus
    to assembly language. The compiler is certified in the sense that it
    comes with a machine-checked proof of semantics preservation,
    performed with the Coq proof assistant. The compiler and the terms of
    its several intermediate languages are given dependent types that
    guarantee that only well-typed programs are representable. Thus, type
    preservation for each compiler pass follows without any significant
    'proofs' of the usual kind. Semantics preservation is proved based on
    denotational semantics assigned to the intermediate languages. We
    demonstrate how working with a type-preserving compiler enables
    type-directed proof search to discharge large parts of our proof
    obligations automatically.",
  paper = "Chli07.pdf"
}

@article{Clin72,
  author = "Clint, M. and Hoare, C.A.R",
  title = {{Program Proving: Jumps and Functions}},
  journal = "Acta Informatica",
  volume = "1",
  pages = "214-224",
  year = "1972",
  abstract =
    "Proof methods adequate for a wide range of computer programs have
    been expounded in [1] and [2]. This paper develops a method suitable
    for programs containing functions, and a certain kind of jump. The
    method is illustrated by the proof of a useful and efficient program
    for table lookup by logarithmic search.",
  paper = "Clin72.pdf"
}

@inproceedings{Crar99,
  author = "Crary, Karl and Harper, Robert and Puri, Sidd",
  title = {{What is a Recursive Module}},
  booktitle = "Conf. on Programming Language Design and Implementation",
  year = "1999",
  link = "\url{https://www.cs.cmu.edu/~crary/papers/1999/recmod/recmod.dvi}",
  abstract = 
    "A hierarchical module system is an effective tool for structuring
    large programs. Strictly hierarchical module systems impose an
    acyclic ordering on import dependencies among program units. This
    can impede modular programming by forcing mutually-dependent
    components to be consolidated into a single module. Recently there
    have been several proposals for module systems that admit cyclic
    dependencies, but it is not clear how these proposals relate to
    one another, nor how one mught integrate them into an expressive
    module system such as that of ML.

    To address this question we provide a type-theoretic analysis of
    the notion of a recursive module in the context of a
    ``phase-distinction'' formalism for higher-order module
    systems. We extend this calculus with a recursive module mechanism
    and a new form of signature, called a {\sl recurslively dependent
    signature}, to support the definition of recursive modules. These
    extensions are justified by an interpretation in terms of more
    primitive language constructs. This interpretation may also serve
    as a guide for implementation.",
  paper = "Crar99.pdf",
  keywords = "printed"
}

@techreport{Crar02,
  author = "Crary, Karl",
  title = {{Toward a Foundational Typed Assembly Language}},
  institution = "Carnegie Mellon University",
  number = "CMU-CS-02-196",
  year = "2002",
  link = "\url{https://www.cs.cmu.edu/~crary/papers/2003/talt/talt-tr.pdf}",
  abstract =
    "We present the design of a typed assembly language called TALT that
    supports heterogeneous tuples, disjoint sums, arrays, and a general
    account of addressing modes. TALT also implements the von Neumann
    model in which programs are stored in memory, and supports relative
    addressing.  Type safety for execution and for garbage collection are
    shown by machine-checkable proofs. TALT is the first formalized typed
    assembly language to provide any of these features.",
  paper = "Crar02.pdf"
}

@misc{Crar08,
  author = "Crary, Karl and Sarkar, Susmit",
  title = {{Foundational Certified Code in a Metalogical Framework}},
  link = "\url{http://repository.cmu.edu/compsci/470/}",
  year = "2008",
  abstract =
    "Foundational certified code systems seek to prove untrusted programs
    to be safe relative to safety policies given in terms of actual
    machine architectures, thereby improving the systems' flexibility and
    extensibility. Using the Twelf metalogical framework, we have
    constructed a safety policy for the IA-32 architecture with a trusted
    runtime library. The safety policy is based on a formalized
    operational semantics. We have also developed a complete, foundational
    proof that a fully expressive typed assembly language satisfies that
    safety policy",
  paper = "Crar08.pdf"
}

@incollection{Dahl72,
  author = "Dahl, Ole-Johan and Hoare, C.A.R",
  title = {{Hierachical Program Structure}},
  booktitle = "Structured Programming",
  publisher = "Academic Press",
  year = "1972",
  pages = "175-220"
}

@article{Darg07,
  author = "Dargaye, Zaynah and Leroy, Xavier",
  title = {{Mechanized Verification of CPS Transformations}},
  journal = "LNCS",
  volume = "4790",
  pages = "211-225",
  year = "2007",
  abstract =
    "Transformation to continuation-passing style (CPS) is often performed
    by optimizing compilers for functional programming languages.  As part
    of the development and proof of correctness of a compiler for the
    mini-ML functional language, we have mechanically verified the
    correctness of two CPS transformations for a call-by-value λ
    $\lambda$-calculus with n-ary functions, recursive functions, data
    types and pattern-matching.  The transformations generalize Plotkin’s
    original call-by-value transformation and Danvy and Nielsen’s
    optimized transformation, respectively. We used the Coq proof
    assistant to formalize the transformations and conduct and check the
    proofs. Originalities of this work include the use of big-step
    operational semantics to avoid difficulties with administrative
    redexes, and of two-sorted de Bruijn indices to avoid difficulties
    with $\alpha$-conversion.",
  paper = "Darg07.pdf"
}

@phdthesis{Davi09,
  author = "Davis, Jared Curran",
  title = {{A Self-Verifying Theorem Prover}},
  school = "University of Texas at Austin",
  year = "2009",
  abstract =
    "Programs have precise semantics, so we can use mathematical proof to
    establish their properties. These proofs are often too large to
    validate with the usual “social process” of mathematics, so instead we
    create and check them with theorem-proving software. This software
    must be advanced enough to make the proof process tractable, but this
    very sophistication casts doubt upon the whole enterprise: who
    verifies the verifier?
    
    We begin with a simple proof checker, Level 1, that only accepts
    proofs composed of the most primitive steps, like Instantiation and
    Cut. This program is so straightforward the ordinary, social process
    can establish its soundness and the consistency of the logical
    theory it implements (so we know theorems are “always true”).
    
    Next, we develop a series of increasingly capable proof checkers,
    Level 2, Level 3, etc.  Each new proof checker accepts new kinds of
    proof steps which were not accepted in the previous levels. By taking
    advantage of these new proof steps, higher-level proofs can be
    written more concisely than lower-level proofs, and can take less time
    to construct and check. Our highest-level proof checker, Level 11, can
    be thought of as a simplified version of the ACL2 or NQTHM theorem
    provers. One contribution of this work is to show how such systems can
    be verified.
    
    To establish that the Level 11 proof checker can be trusted, we first
    use it, without trusting it, to prove the fidelity of every Level n to
    Level 1: whenever Level n accepts a proof of some $\phi$, there exists a
    Level 1 proof of $\phi$. We then mechanically translate the Level 11 proof
    for each Level n into a Level n − 1 proof—that is, we create a Level 1
    proof of Level 2’s fidelity, a Level 2 proof of Level 3’s fidelity,
    and so on. This layering shows that each level can be trusted, and
    allows us to manage the sizes of these proofs.
    
    In this way, our system proves its own fidelity, and trusting Level 11
    only requires us to trust Level 1.",
  paper = "Davi09.pdf"
}

@article{Davi15,
  author = "Davis, Jared and Myreen, Magnus O.",
  title = {{The Reflective Milawa Theorem Prover is Sound}},
  journal = "J. Automated Reasoning",
  volume = "55",
  number = "2",
  pages = "117-183",
  year = "2015",
  abstract =
    "This paper presents, we believe, the most comprehensive evidence of a
    theorem prover’s soundness to date. Our subject is the Milawa theorem
    prover. We present evidence of its soundness down to the machine
    code. Milawa is a theorem prover styled after NQTHM and ACL2. It is
    based on an idealised version of ACL2’s computational logic and
    provides the user with high-level tactics similar to ACL2’s. In
    contrast to NQTHM and ACL2, Milawa has a small kernel that is somewhat
    like an LCF-style system. We explain how the Milawa theorem prover is
    constructed as a sequence of reflective extensions from its
    kernel. The kernel establishes the soundness of these extensions
    during Milawa’s boot-strapping process. Going deeper, we explain how
    we have shown that the Milawa kernel is sound using the HOL4 theorem
    prover. In HOL4, we have formalized its logic, proved the logic sound,
    and proved that the source code for the Milawa kernel (1,700 lines of
    Lisp) faithfully implements this logic. Going even further, we have
    combined these results with the x86 machine-code level verification of
    the Lisp runtime Jitawa. Our top-level theorem states that Milawa can
    never claim to prove anything that is false when it is run on this
    Lisp runtime.",
  paper = "Davi15.pdf",
  keywords = "printed"
}

@misc{Dijk71,
  author = "Dijkstra, E.W.",
  title = {{A Short Introduction to the Art of Programming}},
  comment = "EWD316",
  year = "1971",
  paper = "Dijk71.pdf"
}

@incollection{Dijk72a,
  author = "Dijkstra, E.W.",
  title = {{Notes on Structured Programming}},
  booktitle = "Structured Programming",
  publisher = "Academic Press",
  year = "1972",
  pages = "1-82"
}

@phdthesis{Dola16,
  author = "Dolan, Stephen",
  title = {{Algebraic Subtyping}},
  school = "University of Cambridge",
  year = "2016",
  link = "\url{https://www.cl.cam.ac.uk/~sd601/thesis.pdf}",
  abstract =
    "Type inference gives programmers the benefit of static,
    compile-time type checking without the cost of manually specifying
    types, and has long been a standard feature of functional programming
    languages. However, it has proven difficult to integrate type
    inference with subtyping, since the unification engine at the core of
    classical type inference accepts only equations, not subtyping
    constraints.
    
    This thesis presents a type system combining ML-style parametric
    polymorphism and subtyping, with type inference, principal types,
    and decidable type subsumption. Type inference is based on {\sl
    biunification} , an analogue of unification that works with
    subtyping constraints.
    
    Making this possible are several contributions, beginning with the
    notion of an 'extensible' type system, in which an open world of
    types is assumed, so that no typeable program becomes untypeable
    by the addition of new types to the language. While previous
    formulations of subtyping fail to be extensible, this thesis
    shows that adopting a more algebraic approach can remedy this.
    Using such an approach, this thesis develops the theory of
    biunification, shows how it is used to infer types, and shows how
    it can be efficiently implemented, exploiting deep connections
    between the algebra of regular languages and polymorphic subtyping.",
  paper = "Dola16.pdf"
}

@article{Dola17,
  author = "Dolan, Stephen and Mycroft, Alan",
  title = {{Polymorphism, Subtyping, and Type Inference in MLsub}},
  journal = "ACM SIGPLAN Notices",
  volume = "52",
  number = "1",
  year = "2017",
  pages = "60-72",
  link = "\url{https://www.cl.cam.ac.uk/~sd601/papers/mlsub-preprint.pdf}",
  abstract =
    "We present a type system combining subtyping and ML-style parametric
    polymorphism. Unlike previous work, our system supports type inference
    and has compact principal types. We demonstrate this system in the
    minimal language MLsub, which types a strict superset of core ML
    programs.
    
    This is made possible by keeping a strict separation between the types
    used to describe inputs and those used to describe outputs, and
    extending the classical unification algorithm to handle subtyping
    constraints between these input and output types. Principal types are
    kept compact by type simplification, which exploits deep connections
    between subtyping and the algebra of regular languages. An
    implementation is available online.",
  paper = "Dola17.pdf"
}

@misc{Domi18,
  author = "Domipheus",
  title = {{Designing a CPU in VHDL}},
  link = "\url{http://labs.domipheus.com/blog/tpu-series-quick-links}",
  year = "2018",
  abstract = "A VHDL CPU"
}

@inproceedings{Down16,
  author = "Downen, Paul and Maurer, Luke and Ariola, Zena M.",
  title = {{Sequent Calculus as a Compiler Intermediate Language}},
  booktitle = "ICFP'16",
  year = "2016",
  pages = "74-88",
  publisher = "ACM",
  isbn = "978-1-4503-4219-3",
  abstract = 
    "The $\lambda$-calculus is popular as an intermediate language for
    practical compilers. But in the world of logic it has a
    lesser-known twin, born at the same time, called the {\sl sequent
    calculus}. Perhaps that would make for a good intermediate
    language, too? To explore this question we designed Sequent Core,
    a practically-oriented core calculus based on the sequent
    calculus, and used it to re-implement a substantial chunk of the
    Glasgow Haskell Compiler.",
  paper = "Down16.pdf"
}

@techreport{Drey02,
  author = "Dreyer, Derek and Crary, Karl and Harper, Robert",
  title = {{A Type System for Higher-Order Modules (Expanded Version)}},
  type = "technical report",
  institution = "Carnegie Mellon University",
  number = "CMU-CS-02-122R",
  year = "2002",
  link = "\url{https://www.cs.cmu.edu/~crary/papers/2003/thoms/thoms-tr.pdf}",
  abstract =
    "We present a type theory for higher-order modules that accounts for
    many central issues in module system design, including translucency, 
    applicativity , generativity , and modules as first-class values.
    Our type system harmonizes design elements from previous work,
    resulting in a simple, economical account of modular programming.  The
    main unifying principle is the treatment of abstraction mechanisms
    as computational effects.  Our language is the first to provide a
    complete and practical formalization of all of these critical issues
    in module system design.",
  paper = "Drey02.pdf"  
}

@article{Drey03,
  author = "Dreyer, Derek and Crary, Karl and Harper, Robert",
  title = {{A Type System for Higher-Order Modules}},
  journal = "ACM SIGPLAN Notices",
  volume = "38",
  number = "1",
  year = "2003",
  link = "\url{https://www.cs.cmu.edu/~crary/papers/2003/thoms/thoms.pdf}",
  abstract =
    "We present a type theory for higher-order modules that accounts for
    many central issues in module system design, including translucency, 
    applicativity , generativity , and modules as first-class values.
    Our type system harmonizes design elements from previous work,
    resulting in a simple, economical account of modular programming.  The
    main unifying principle is the treatment of abstraction mechanisms
    as computational effects.  Our language is the first to provide a
    complete and practical formalization of all of these critical issues
    in module system design.",
  paper = "Drey03.pdf",
  keywords = "printed"
}

@inproceedings{Drey07,
  author = "Dreyer, Derek and Harper, Robert and Chakravarty, Manuel M.T.
            and Keller, Gabriele",
  title = {{Modlular Type Classes}},
  booktitle = "Proc. POPL'07",
  pages = "63-70",
  year = "2007",
  abstract =
    "ML modules and Haskell type classes have proven to be highly 
    effective tools for program structuring.  Modules emphasize explicit
    configuration of program components and the use of data abstraction.
    Type classes emphasize implicit program construction and ad hoc
    polymorphism.  In this paper , we show how the implicitly-typed
    style of type class programming may be supported within the framework
    of an explicitly-typed module language by viewing type classes as a
    particular mode of use of modules.  This view offers a harmonious
    integration of modules and type classes, where type class features,
    such as class hierarchies and associated types, arise naturally as
    uses of existing module-language constructs, such as module
    hierarchies and type components.  In addition, programmers have
    explicit control over which type class instances are available for
    use by type inference in a given scope.  We formalize our approach as
    a Harper-Stone-style elaboration relation, and provide a sound type
    inference algorithm as a guide to implementation.",
  paper = "Drey07.pdf",
  keywords = "printed"
}

@article{Dura14,
  author = "Duran, Antonio J. and Perez, Mario and Varona, Juan L.",
  title = {{The Misfortunes of a Trio of Mathematicians Using Computer
            Algebra Systems. Can We Trust in Them?}},
  journal = "Notices of the AMS",
  volume = "61",
  number = "10",
  year = "2014",
  link = "\url{www.ams.org/notices/201410/rnoti-p1249.pdf}",
  pages = "1249-1252",
  paper = "Dura14.pdf",
  keywords = "printed"
}

@inproceedings{Farm00,
  author = "Farmer, William M.",
  title = {{A Proposal for the Development of an Interactive
            Mathematics Laboratory for Mathematics Eductions}},
  booktitle = "Workshop on Deductions Systems for Mathematics Eduation",
  pages = "20-25",
  year = "2000",
  paper = "Farm00.pdf",
  keywords = "axiomref, printed"
}

@inproceedings{Farm00a,
  author = "Farmer, William M. and Mohrenschildt, Martin v.",
  title = {{Transformers for Symbolic Computation and Formal Deduction}},
  booktitle = "CADE-17",
  pages = "36-45",
  year = "2000",
  abstract =
    "A transformer is a function that maps expressions to expressions.
    Many transformational operators -— such as expression evaluators and
    simplifiers, rewrite rules, rules of inference, and decision
    procedures -— can be represented by transformers. Computations and
    deductions can be formed by applying sound transformers in
    sequence. This paper introduces machinery for defining sound
    transformers in the context of an axiomatic theory in a formal
    logic. The paper is intended to be a first step in a development of an
    integrated framework for symbolic computation and formal deduction.",
  paper = "Farm00a.pdf",
  keywords = "printed"
}

@article{Farm04,
  author = "Farmer, William M.",
  title = {{MKM A New Interdisciplinary Field of Research}},
  journal = "SIGSAM",
  volume = "38",
  pages = "47-52",
  year = "2004",
  abstract =
    "Mathematical Knowledge Management (MKM) is a new interdisciplinary
    field of research in the intersection of mathematics, computer
    science, library science, and scientific publishing. Its objective is
    to develop new and better ways of managing mathematical knowledge
    using sophisticated software tools.  Its grand challenge is to create
    a universal digital mathematics library (UDML), accessible via the
    World-Wide Web, that contains essentially all mathematical knowledge
    (intended for the public). The challenges facing MKM are daunting,
    but a UDML, even just partially constructed, would transform how
    mathematics is learned and practiced.",
  paper = "Farm04.pdf",
  keywords = "printed, axiomref"
}

@article{Farm07,
  author = "Farmer, William M.",
  title = {{Biform Theories in Chiron}},
  journal = "LNCS",
  volume = "4573",
  pages = "66-79",
  year = "2007",
  abstract =
    "An axiomatic theory represents mathematical knowledge declaratively
    as a set of axioms. An algorithmic theory represents mathematical
    knowledge procedurally as a set of algorithms. A biform theory is
    simultaneously an axiomatic theory and an algorithmic theory. It
    represents mathematical knowledge both declaratively and procedurally.
    Since the algorithms of algorithmic theories manipulate th e syntax of
    expressions, biform theories —- as well as algorithmic theories -— are
    difficult to formalize in a traditional logic without the means to
    reason about syntax. Chiron is a derivative of
    von-Neumann-Bernays-G̈odel ( NBG ) set theory that is intended to be a
    practical, general-purpose logic for mechanizing mathematics. It
    includes elements of type theory, a scheme for handling undefinedness,
    and a facility for reasoning about the syntax of expressions. It is an
    exceptionally well-suited logic for formalizing biform theories. This
    paper defines the notion of a biform theory, gives an overview of
    Chiron, and illustrates how biform theories can be formalized in Chiron.",
  paper = "Farm07.pdf",
  keywords = "printed"
}

@misc{Farm14,
  author = "Farmer, William M. and Larjani, Pouya",
  title = {{Frameworks for Reasoning about Syntax that Utilize
            Quotation and Evaluation}},
  links = "\url{http://imps.mcmaster.ca/doc/syntax.pdf}",
  year = "2014",
  abstract =
    "It is often useful, if not necessary, to reason about the syntactic
    structure of an expression in an interpreted language (i.e., a
    language with a semantics).  This paper introduces a mathematical
    structure called a syntax framework that is intended to be an abstract
    model of a system for reasoning about the syntax of an interpreted
    language.  Like many concrete systems for reasoning about syntax, a
    syntax framework contains a mapping of expressions in the
    interpreted language to syntactic values that represent the syntactic
    structures of the expressions; a language for reasoning about the
    syntactic values; a mechanism called quotation to refer to the
    syntactic value of an expression; and a mechanism called evaluation to
    refer to the value of the expression represented by a syntactic value.
    A syntax framework provides a basis for integrating reasoning about
    the syntax of the expressions with reasoning about what the
    expressions mean.  The notion of a syntax framework is used to discuss
    how quotation and evaluation can be built into a language and to
    define what quasiquotation is.  Several examples of syntax frameworks
    are presented.",
  paper = "Farm14.pdf",
  keywords = "printed"
}

@article{Fill03,
  author = "Filliatre, Jean-Christophe",
  title = {{Verification of Non-Functional Programs using Interpretations
            in Type Theory}},
  journal = "J. Functional Programming",
  volume = "13",
  number = "4",
  pages = "709-745",
  year = "2003",
  abstract =
    "We study the problem of certifying programs combining imperative
    and functional features within the general framework of type
    theory. 

    Type theory is a powerful specification language, which is
    naturally suited for the proof of purely functional programs. To
    deal with imperative programs, we propose a logical interpretation
    of an annotated program as a partial proof of its
    specification. The construction of the corresponding partial proof
    term is based on a static analysis of the effects of the program
    which excludes aliases. The missing subterms in the partial proof
    term are seen as proof obligations, whose actual proofs are left
    to the user. We show that the validity of those proof obligations
    implies the total correctness of the program.
    
    This work has been implemented in the Coq proof assistant. It
    appears as a tactic taking an annotated program as argument and
    generating a set of proof obligations. Several nontrivial
    algorithms have been certified using this tactic.",
  paper = "Fill03.pdf",
  keywords = "printed"
}

@article{Fill13,
  author = "Filliatre, Jean-Christophe and Paskevich, Andrei",
  title = {{Why3 -- Where Programs Meet Provers}},
  journal = "LNCS",
  volume = "7792",
  year = "2013",
  link = "\url{https://hal.inria.fr/hal-00789533/document}",
  abstract =
    "We present Why3, a tool for deductive program verification, and
    WhyML, its programming and specification language. WhyML is a
    first-order language with polymorphic types, pattern matching, and
    inductive predicates. Programs can make use of record types with
    mutable fields, type invariants, and ghost code. Verification
    conditions are discharged by Why3 with the help of various existing
    automated and interactive theorem provers. To keep verification
    conditions tractable and comprehensible, WhyML imposes a static
    control of aliases that obviates the use of a memory model. A user
    can write WhyML programs directly and get correct-by-construction
    OCaml programs via an automated extraction mechanism. WhyML is also
    used as an intermediate language for the verification of C, Java, or
    Ada programs. We demonstrate the benefits of Why3 and WhyML on 
    non-trivial examples of program verification.",
  paper = "Fill13.pdf",
  keywords = "printed"
}

@misc{Flan03,
  author = "Flanagan, Cormac and Sabry, Amr and Duba, Bruce F. and
            Felleisen, Matthias",
  title = {{The Essence of Compiling with Continuations}},
  link =
  "\url{https://www.cs.rice.edu/~javaplt/411/17-spring/Readings/essence-retro.pdf}",
  paper = "Flan03.pdf",
  keywords = "printed"
}

@article{Floy64,
  author = "Floyd, Robert W.",
  title = {{Algorithm 245: Treesort}},
  journal = "CACM",
  volume = "7",
  number = "12",
  year = "1964",
  pages = "701",
  paper = "Floy64.pdf"
}

@inproceedings{Floy67,
  author = "Floyd, Robert W.",
  title = {{Assigning Meanings to Programs}},
  booktitle = "Proc. Symp. in Applied Mathematics",
  year = "1967",
  pages = "19-32",
  publisher = "American Mathematical Society",
  paper = "Floy67.pdf",
  keywords = "printed"
}

@inproceedings{Free91,
  author = "Freeman, Tim and Pfenning, Frank",
  title = {{Refinement Types for ML}},
  booktitle = "ACM SIGPLAN PLDI'91",
  year = "1991",
  link = "\url{http://www.cs.cmu.edu/~fp/papers/pldi91.pdf}",
  abstract =
    "We describe a refinement of ML's type system allowing the
    specification of recursively defined subtypes of user-defined
    datatypes. The resulting system of {\sl refinement types}
    preserves desirable properties of ML such as decidability of type
    inference, while at the same time allowing more errors to be
    detected at compile-time. The type system combines abstract
    interpretation with ideas from the intersection type discipline,
    but remains closely tied to ML in that refinement types are given
    only to programs which are already well-typed in ML.",
  paper = "Free91.pdf",
  keywords = "printed"
}

@article{Foxx03,
  author = "Fox, Anthony",
  title = {{Formal Specification and Verification of ARM6}},
  journal = "LNCS",
  volume = "2758",
  pages = "25-40",
  year = "2003",
  abstract =
    "This paper gives an overview of progress made on the formal
    specification and verification of the ARM6 micro-architecture using
    the HOL proof system. The ARM6 is a commercial processor design preva-
    lent in mobile and embedded systems – it features a 3-stage pipeline
    with a multi-cycle execute stage, six operating modes and a rich
    32-bit RISC instruction set. This paper describes some of the
    difficulties encountered when working with a full blown instruction
    set architecture that has not been designed with verification in mind.",
  paper = "Foxx03.pdf"
}

@book{Fran92,
  author = "Francez, Nissim",
  title = {{Program Verification}},
  year = "1992",
  publisher = "Addison-Wesley",
  isbn = "0-201-41608-5"
}

@techreport{Frie76,
  author = "Friedman, Daniel P. and Wise, David S.",
  title = {{CONS should not Evaluate its Arguments}},
  institution = "Indiana University",
  number = "TR44",
  year = "1976",
  abstract =
    "The constructor function which allocates and fills records in
    recursive, side-effect-free procedural languages is redefined to be a
    non-strict (Vuillemin 1974) elementary operation. Instead of
    evaluating its arguments, it builds suspensions of them which are not
    coerced until the suspensions is accessed by strict elementary
    function. The resulting evalutation procedures are strictly more
    powerful than existing schemes for languages such as LISP. The main
    results are that Landin's streams are subsumed into McCarthy's LISP
    merely by the redefinition of elementar functions, that invocations of
    LISP's evaluator can be minimized by redefining the elemntary
    functions without redefining the interpreter, and as a strong
    conjecture, that redefining the elementary functions yields the least
    fixed-point semantics for McCarthy's evalution scheme. This new
    insight into the role of construction functions will do much to ease
    the interface between recursive programmers and iterative programmers,
    as well as the interface between programmers and data structure
    designers.",
  paper = "Frie16.pdf",
  keywords = "printed"
}

@misc{Giar95,
  author = "Girard, Jean-Yves",
  title = {{Linear Logic: Its Syntax and Semantics}},
  year = "1995",
  paper = "Gira95.pdf",
  keywords = "printed"
}

@article{Glas78,
  author = "Glasner, Ingrid and Loeckx, Jacquest",
  title = {{A Calculus for Proving Properties of While-Programs}},
  journal = "LNCS",
  volume = "75",
  pages = "252-281",
  year = "1978",
  paper = "Glas78.pdf",
  keywords = "printed"
}

@article{Gord79,
  author = "Gordon, Michael J. and Milner, Arthur j. and 
            Wadsworth, Christopher P.",
  title = {{Edinburgh LCF, A Mechanised Logic of Computation:
            Introduction}},            
  publisher = "Springer-Verlag",
  journal = "LNCS",
  volume = "78",
  year = "1979",
  paper = "Gord79.pdf"
}

@article{Gord79a,
  author = "Gordon, Michael J. and Milner, Arthur j. and 
            Wadsworth, Christopher P.",
  title = {{Edinburgh LCF, A Mechanised Logic of Computation: ML}},
  journal = "LNCS",
  volume = "78",
  year = "1979",
  paper = "Gord79a.pdf"
}

@article{Gord79b,
  author = "Gordon, Michael J. and Milner, Arthur j. and 
            Wadsworth, Christopher P.",
  title = {{Edinburgh LCF, A Mechanised Logic of Computation: PPLAMBDA}},
  journal = "LNCS",
  volume = "78",
  year = "1979",
  paper = "Gord79b.pdf"
}

@inproceedings{Gord89,
  author = "Gordon, Michael J.C.",
  title = {{Mechanizing Programming Logics in Higher Order Logic}},
  booktitle = "Current Trends in Hardware Verification and Automated
               Theorem Proving",
  publisher = "Springer",
  year = "1989",
  abstract =
    "Formal reasoning about computer programs can be based directly on the
    seman tics of the programming language, or done in a special purpose
    logic like Hoare logic.  The advantage of the first approach is that
    it guarantees that the formal reasoning applies to the language being
    used (it is well known, for example, that Hoare's assignment axiom
    fails to hold for most programming languages).  The advantage of the
    second approach is that the proofs can be more direct and natural.
    
    In this paper, an attempt to get the advantages of both approaches
    is described.  The rules of Hoare logic are mechanically derived
    from the semantics of a simple imperative programming language
    (using the HOL system).  These rules form the basis for a simple
    program verifier in which verification conditions are generated by LCF
    -style tactics whose validations use the derived Hoare rules.
    Because Hoare logic is derived, rather than postulated, it is straight
    tforw ard to mix seman tic and axiomatic rea- soning.  It is also
    forward to combine the constructs of Hoare logic with other
    application-specific notations.  This is briefly illustrated for
    various logical constructs, including termination statements, VDM-style
    `relational' correctness specifications, weakest precondition
    statements and dynamic logic formulae.
    
    The theory underlying the work presented here is well known. Our
    contribution is to propose a way of mechanizing this theory in a
    way that makes certain practical details work out smoothly .",
  paper = "Gord89.pdf",
  keywords = "printed"
}

@article{Gord06,
  author = "Gordon, Mike and Iyoda, Juliano and Owens, Scott and 
            Slind, Konrad",
  title = {{Automatic Formal Synthesis of Hardware from Higher Order Logic}},
  journal = "Electronic Notes in Theoretical Computer Science",
  volume = "145",
  pages = "27-43",
  year = "2006",
  abstract =
    "A compiler that automatically translates recursive function
    definitions in higher order logic to clocked synchronous hardware is
    described. Compilation is by mechanised proof in the HOL4 system, and
    generates a correctness theorem for each function that is
    compiled. Logic formulas representing circuits are synthesised in a
    form suitable for direct translation to Verilog HDL for simulation and
    input to standard design automation tools. The compilation scripts are
    open and can be safely modified: synthesised circuits are
    correct-by-construction. The synthesisable subset of higher order
    logic can be extended using additional proof-based tools that
    transform definitions into the subset.",
  paper = "Gord06.pdf"
}

@misc{Gros15,
  author = "Gross, Jason and Chlipala, Adam",
  title = {{Parsing Parses}},
  link = "\url{https://people.csail.mit.edu/jgross/personal-website/papers/2015-parsing-parse-trees.pdf}",
  year = "2015",
  abstract = 
    "We present a functional parser for arbitrary context-free grammars,
    together with soundness and completeness proofs, all inside Coq. By
    exposing the parser in the right way with parametric polymorphism and
    dependent types, we are able to use the parser to prove its own
    soundness, and, with a little help from relational parametricity,
    prove its own completeness, too. Of particular interest is one strange
    instantiation of the type and value parameters: by parsing parse trees
    instead of strings, we convince the parser to generate its own
    completeness proof. We conclude with highlights of our experiences
    iterating through several versions of the Coq development, and some
    general lessons about dependently typed programming.",
  paper = "Gros15.pdf",
  keywords = "printed"
}

@article{Gutt95,
  author = "Guttman, Joshua D. and Ramsdell, John D. and Wand, Mitchell",
  title = {{VLISP: A Verified Implementation of Scheme}},
  journal = "Lisp and Symbolic Computation",
  volume = "8",
  pages = "5-32",
  year = "1995",
  abstract =
    "The VL!SP project showed how to produce a comprehensively verified
    implementation for a programming language, namely Scheme.  This paper
    introduces two more detailed studies on VLISP [13, 21).  It summarizes
    the basic techniques that were used repeatedly throughout the effort.
    It presents scientific conclusions about the applicability of the
    these techniques as well as engineering conclusions about the crucial
    choices that allowed the verification to succeed.",
  paper = "Gutt95.pdf"
}

@misc{Hamm95,
  author = "Hamming, Richard",
  title = {{Hamming, 'You and Your Research'}},
  link = "\url{https://www.youtube.com/watch?v=a1zDuOPkMSw}",
  year = "1995"
}

@inproceedings{Harp93b,
  author = "Harper, Robert and Lillibridge, Mark",
  title = {{Explicit Polymorphism and CPS Conversion}},
  booktitle = "Symp. of Principles of Programming Languages",
  publisher = "ACM Press",
  year = "1993",
  pages = "206=219",
  abstract =
    "We study the typing properties of CPS conversion for an extension
    of F$\omega$  with control operators.  Two classes of evaluation
    strategies are considered, each with call-by-name and call-by-value
    variants.  Under the 'standard' strategies, constructor abstractions
    are values, and constructor applications can lead to non-trivial 
    control effects.  In contrast, the 'ML-like' strategies evaluate 
    beneath constructor abstractions, reflecting the usual interpretation
    of programs in languages based on implicit polymorphism.  Three 
    continuation passing style sub-languages are considered, one on which
    the standard strategies coincide, one on which the ML-like
    strategies coincide, and one on which all the strategies coincide.
    Compositional, type-preserving CPS transformation algorithms are
    given for the standard strategies, resulting in terms on which all
    evaluation strategies coincide.  This has as a corollary the
    soundness and termination of well-typed programs under the standard
    evaluation strategies.  A similar result is obtained for the ML-like
    call-by-name strategy .  In contrast, such results are obtained for
    the call-by-value ML-like strategy only for a restricted
    sub-language in which constructor abstractions are limited to
    values.",
  paper = "Harp93b.pdf",
  keywords = "printed"
}  

@incollection{Hoar72,
  author = "Hoare, C.A.R",
  title = {{Notes on Data Structuring}},
  booktitle = "Structured Programming",
  publisher = "Academic Press",
  year = "1972",
  pages = "83-174"
}

@article{Huet75,
  author = "Huet, Gerard P.",
  title = {{A Unification Algorithm for typed $\lambda$-Calculus}},
  journal = "Theoretical Computer Science",
  volume = "1",
  number = "1",
  pages = "25-57",
  year = "1975",
  abstract = 
    "A semi-decision algorithm is presented, to search for unification of
    formulas in typed $\omega$-order $\lambda$-calculus, and its
    correctness is proved.
    
    It is shown that the search space is significantly smaller than the
    one for finding the most general unifiers. In particluar, our search
    is not redundant. This allows our algorithm to have good
    directionality and convergence properties.",
  paper = "Huet75.pdf"
}

@article{Huet78,
  author = "Huet, Gerard P. and Lang, Bernard",
  title = {{Proving and Applying Program Transformations Expressed
            with Second-Order Patterns}},
  journal = "Acta Informatica",
  volume = "11",
  number = "1",
  pages = "31-55",
  year = "1978",
  abstract =
    "We propose a program transformation method based on rewriting-rules
    composed of second-order schemas. A complete second-order matching
    algorithm is presented that allows effective use of these rules. We
    show how to formally prove the correctness of the rules using a
    denotational semantics for the programming language. We establish the
    correctness of the transformation method itself, and give techniques
    pertaining to its actual implementation. The paper is illustrated with
    recursion removal examples."
}

@inproceedings{Hugh90,
  author = "Hughes, John",
  title = {{Why Functional Programming Matters}},
  booktitle = "Research Topics in Functional Programming",
  publisher = "Addison-Wesley",
  year = "1990",
  pages = "17-42",
  abstract =
    "As software becomes more and more complex, it is more and more
    important to structure it well.  Well-structured software is easy to
    write and to debug, and provides a collection of modules that can be
    reused to reduce future programming costs.  In this paper we show that
    two fea- tures of functional languages in particular, higher-order
    functions and lazy evaluation, can contribute significantly to
    modularity.  As examples, we manipulate lists and trees, program
    several numerical algorithms, and implement the alpha-beta heuristic
    (an algorithm from Artificial Intelligence used in game-playing
    programs). We conclude that since modularity is the key to successful
    programming, functional programming offers important advantages for
    software development.",
  paper = "Hugh90.pdf",
  keywords = "printed"
}

@article{Kaes88,
  author = "Kaes, Stefan",
  title = {{Parametric Overloading in Polymorphic Programming Languages}},
  journal = "LNCS",
  volume = "300",
  pages = "131-144",
  year = "1988",
  abstract =
    "The introduction of unrestricted overloading in languagues with type
    systems based on implicit parametric potymorphism generally destroys
    the principal type property: namely that the type of every expression
    can uniformly be represented by a single type expression over some set
    of type variables.  As a consequence, type inference in the presence
    of unrestricted overloading can become a NP-complete problem.  In
    this paper we define the concept of parametric overloading as a
    restricted form of overloading which is easily combined with
    parametric polymorphism.  Parametric overloading preserves the
    principal type property, thereby allowing the design of efficient type
    inference algorithms.  We present sound type deduction systems, both
    for predefined and programmer defined overloading.  Finally we state
    that parametric overloading can be resolved either statically, at
    compile time, or dynamically, during program execution.",
  paper = "Kaes88.pdf",
  keywords = "printed"
}

@article{Kaes92,
  author = "Kaes, Stefan",
  title = {{Type Inference inthe Presence of Overloading, Subtyping and
            Recursive Types}},
  journal = "ACM Lisp Pointers",
  volume = "5",
  number = "1",
  year = "1992",
  pages = "193-204",
  abstract = 
    "We present a unified approach to type inference in the presence of
    overloading and coercions based on the concept of {\sl constrained
    types}. We define a generic inference system, show that subtyping and
    overloading can be treated as a special instance of this system and
    develop a simple algorithm to compute principal types. We prove the
    decidability of type inference for hte class of {\sl decomposable
    predicates} and deelop a canonical representation for principal types
    based on {\sl most accurate simplifications} of constraint
    sets. Finally, we investigate the extension of our techniques to 
    {\sl recursive types}.",
  paper = "Kaes92.pdf",
  keywords = "printed"
}

@article{Keim09,
  author = "Keimel, Klaus and Plotkin, Gordon D.",
  title = {{Predicate Transformers for Extended Probability and 
            Non-Determinism}},
  journal = "Math. Struct. in Comp. Science",
  volume = "19",
  pages = "501-539",
  year = "2009",
  abstract =
    "We investigate laws for predicate transformers for the combination of
    non-deterministic choice and (extended) probabilistic choice, where
    predicates are taken to be functions to the extended non-negative
    reals, or to closed intervals of such reals. These predicate
    transformers correspond to state transformers, which are functions to
    conical powerdomains, which are the appropriate powerdomains for the
    combined forms of non-determinism. As with standard powerdomains for
    non-deterministic choice, these come in three flavours – lower, upper
    and (order-)convex – so there are also three kinds of predicate
    transformers. In order to make the connection, the powerdomains are
    first characterised in terms of relevant classes of functionals.  
    
    Much of the development is carried out at an abstract level, a kind of
    domain-theoretic functional analysis: one considers d-cones, which are
    dcpos equipped with a module structure over the non-negative extended
    reals, in place of topological vector spaces. Such a development still
    needs to be carried out for probabilistic choice per se ; it would
    presumably be necessary to work with a notion of convex space rather
    than a cone.",
  paper = "Keim09.pdf"
}

@phdthesis{Kell13,
  author = "Keller, C.",
  title = {{A Matter of Trust: Skeptical Commuication Between Coq and
            External Provers}},
  school = "Ecole Polytechnique",
  year = "2013",
  link = 
"\url{https://www.lri.fr/~keller/Documents-recherche/Publications/thesis13.pdf}",
  abstract =
    "This thesis studies the cooperation between the Coq proof assistant
    and external provers through proof witnesses.  We concentrate on two
    different kinds of provers that can return certicates: first, answers
    coming from SAT and SMT solvers can be checked in Coq to increase both
    the confidence in these solvers and Coq 's automation; second,
    theorems established in interactive provers based on Higher-Order
    Logic can be exported to Coq and checked again, in order to offer the
    possibility to produce formal developments which mix these two
    different logical paradigms.  It ended up in two software : SMTCoq, a
    bi-directional cooperation between Coq and SAT/SMT solvers, and
    HOLLIGHTCOQ, a tool importing HOL Light theorems into Coq.
    
    For both tools, we took great care to define a modular and efficient
    architecture, based on three clearly separated ingredients: an
    embedding of the formalism of the external tool inside Coq which is
    carefully translated into Coq terms, a certified checker to establish
    the proofs using the certicates and a Ocaml preprocessor to transform
    proof witnesses coming from different provers into a generic
    certificate. This division allows that a change in the format of proof
    witnesses only affects the preprocessor, but no proved Coq code.
    Another fundamental component for efficiency and modularity is
    computational reflection, which exploits the computational power of
    Coq to establish generic and small proofs based on the certicates.",
  paper = "Kell13.pdf"
}

@inproceedings{Kran86,
  author = "Kranz, David and Kelsey, Richard and Rees, Jonathan and
            Hudak, Paul and Philbin, James and Adams, Norman",
  title = {{ORBIT: An Optimizing Compiler for Scheme}},
  booktitle = "SIGLAN '86",
  publisher = "ACM",
  pages = "219-233",
  year = "1986",
  abstract =
    "In this paper we describe an optimizing compiler for Scheme
    called {\sl Orbit} that incorporates our experience with an
    earlier Scheme compiler called TC, together with some ideas from
    Steele's Rabbit compiler. The three main design goals have been
    correctness, generating very efficient compiled code, and
    portability.",
  paper = "Kran86.pdf",
  keywords = "printed"
}

@article{Lamp81,
  author = "Lamport, Leslie and Owicki, Susan",
  title = {{Program Logics and Program Verification}},
  journal = "LNCS",
  volume = "131",
  pages = "197-199",
  year = "1981",
  paper = "Lamp81.pdf",
  keywords = "printed"
}

@article{Lero09,
  author = "Leroy, Xavier",
  title = {{A Formally Verified Compiler Back-end}},
  journal = "Logic in Computer Science",
  volume = "43",
  number = "4",
  pages = "363-446",
  year = "2009",
  abstract = 
    "This article describes the development and formal verification
    (proof of semantic preservation) of a compiler back-end from Cminor 
    (a simple imperative intermediate language) to PowerPC assembly code,
    using the Coq proof assistant both for programming the compiler and
    for proving its soundness. Such a verified compiler is useful in the
    context of formal methods applied to the certification of critical
    software: the verification of the compiler guarantees that the safety
    properties proved on the source code hold for the executable compiled
    code as well.",
  paper = "Lero09.pdf"
}

@article{Mano03,
  author = "Manolios, Panagiotis and Moore, J Strother",
  title = {{Partial Functions in ACL2}},
  journal = "J. of Automated Reasoning",
  volume = "31",
  pages = "107-127",
  year = "2003",
  abstract = 
    "We describe a method for introducing 'partial functions' into ACL2,
    that is, functions not defined everywhere. The function 'definitions'
    are actually admitted via the encapsulation principle: the new
    function symbol is constrained to satisfy the appropriate
    equation. This is permitted only when a witness function can be
    exhibited, establishing that the constraint is satisfiable. Of
    particular interest is the observation that every tail recursive
    definition can be witnessed in ACL2. We describe a macro that allows
    the convenient introduction of arbitrary tail recursive functions, and
    we discuss how such functions can be used to prove theorems about
    state machine models without reasoning about “clocks” or counting the
    number of steps until termination. Our macro for introducing “partial
    functions” also permits a variety of other recursive schemes, and we
    briefly illustrate some of them.",
  paper = "Mano03.pdf",
  keywords = "printed"
}

@incollection{Mcca63,
  author = "McCarthy, John",
  title = {{A Basis for a Mathematical Theory of Computation}},
  booktitle = "Computer Programming and Formal Systems",
  year = "1963",
  paper = "Mcca63.pdf"
}

@inproceedings{Mcdo97,
  author = "McDowell, Raymond and Miller, Dale",
  title = {{A Logic for Reasoning with Higher-Order Abstract Syntax}},
  booktitle = "LICS'97",
  publisher = "IEEE",
  year = "1997",
  link = "\url{http://www.lix.polytechnique.fr/~dale/papers/lics97.pdf}",
  abstract = 
    "Logical frameworks based on intuitionistic or linear logics with
    higher-type quantification have been successfully used to give
    high-level, modular, and formal specifications of many important
    judgments in the area of programming languages and inference
    systems. Given such specifications, it is natural to consider proving
    properties about the specified systems in the framework: for example,
    given the specification of evaluation for a functional programming
    language, prove that the language is deterministic or that the
    subject-reduction theorem holds.  One challenge in developing a
    framework for such reasoning is that higher-order abstract syntax
    (HOAS), an elegant and declarative treatment of object-level
    abstraction and substitution, is difficult to treat in proofs
    involving induction.  In this paper, we present a meta-logic that can
    be used to reason about judgments coded using HOAS; this meta-logic is
    an extension of a simple intuitionistic logic that admits higher-order
    quantification over simply typed $\lambda$-terms (key ingredients for
    HOAS) as well as induction and a notion of definition. The latter
    concept of a definition is a proof-theoretic device that allows
    certain theories to be treated as 'closed' or as defining fixed
    points. The resulting meta-logic can specify various logical
    frameworks and a large range of judgments regarding programming
    languages and inference systems.  We illustrate this point through
    examples, including the admissibility of cut for a simple logic and
    subject reduction, determinacy of evaluation, and the equivalence of
    SOS and natural semantics presentations of evaluation for a simple
    functional programming language.",
  paper = "Mcdo97.pdf"
}

@article{Mcdo02,
  author = "McDowell, Raymond and Miller, Dale",
  title = {{Reasoning with Higher-Order Abstract Syntax in a Logical 
            Framework}},
  journal = "ACM Trans. Computational Logic",
  volume = "3",
  year = "2002",
  pages = "80-136",
  link = "\url{http://www.lix.polytechnique.fr/~dale/papers/mcdowell01.pdf}",
  abstract =
    "Logical frameworks based on intuitionistic or linear logics with
    higher-type quantification have been successfully used to give
    high-level, modular, and formal specifications of many important
    judgments in the area of programming languages and inference systems.
    Given such specifications, it is natural to consider proving
    properties about the specified systems in the framework: for example,
    given the specification of evaluation for a functional programming
    language, prove that the language is deterministic or that evaluation
    preserves types.  One challenge in developing a framework for such
    reasoning is that higher-order abstract syntax (HOAS), an elegant and
    declarative treatment of object-level abstraction and substitution, is
    difficult to treat in proofs involving induction.  In this paper, we
    present a meta-logic that can be used to reason about judgments coded
    using HOAS; this meta-logic is an extension of a simple intuitionistic
    logic that admits higher-order quantification over simply typed
    $\lambda$-terms (key ingredients for HOAS) as well as induction and a
    notion of definition. The latter concept of definition is a
    proof-theoretic device that allows certain theories to be treated as
    'closed' or as defining fixed points.  We explore the difficulties of
    formal meta-theoretic analysis of HOAS encodings by considering
    encodings of intuitionistic and linear logics, and formally derive the
    admissibility of cut for important subsets of these logics.  We then
    propose an approach to avoid the apparent tradeoff between the
    benefits of higher-order abstract syntax and the ability to analyze
    the resulting encodings.  We illustrate this approach through examples
    involving the simple functional and imperative programming languages
    $PCF$ and $PCF_{:=}$. We formally derive such properties as unicity of
    typing, subject reduction, determinacy of evaluation, and the
    equivalence of transition semantics and natural semantics
    presentations of evaluation.",
  paper = "Mcdo02.pdf"
}

@book{Mich11,
  author = "Michaelson, Greg",
  title = {{Functional Programming Through Lambda Calculus}},
  year = "2011",
  publisher = "Dover",
  isbn = "978-0-486-47883-8"
}

@article{Mili09,
  author = "Mili, Ali and Aharon, Shir and Nadkarni, Chaitanya",
  title = {{Mathematics for Reasoning about Loop Functions}},
  journal = "Science of Computer Programming",
  volume = "79",
  year = "2009",
  pages = "989-1020",
  abstract =
    "The criticality of modern software applications, the pervasiveness of
    malicious code concerns, the emergence of third-party software
    development, and the preponderance of program inspection as a quality
    assurance method all place a great premium on the ability to analyze
    programs and derive their function in all circumstances of use and all
    its functional detail. For C-like programming languages, one of the
    most challenging tasks in this endeavor is the derivation of loop
    functions. In this paper, we outline the premises of our approach to
    this problem, present some mathematical results, and discuss how these
    results can be used as a basis for building an automated tool that
    derives the function of while loops under some conditions.",
  paper = "Mili09.pdf",
  keywords = "printed"
}

@inproceedings{Mell15,
  author = "Mellies, Paul-Andre and Zeilberger, Noam",
  title = {{Functors are Type Refinement Systems}},
  booktitle = "POPL'15",
  publisher = "ACM",
  year = "2015",
  abstract =
    "The standard reading of type theory through the lens of category
    theory is based on the idea of viewing a type system as a category
    of well-typed terms. We propose a basic revision of this reading;
    rather than interpreting type systems as categories, we describe
    them as {\sl functors} from a category of typing derivations to a
    category of underlying terms. Then, turning this around, we
    explain how in fact {\sl any} functor gives rise to a generalized
    type system, with an abstract notion of type judgment, typing
    derivations and typing rules. This leads to a purely categorical
    reformulation of various natural classes of type systems as
    natural classes of functors.

    The main purpose of this paper is to describe the general
    framework (which can also be seen as providing a categorical
    analysis of {\sl refinement types}, and to present a few
    applications. As a larger case study, we revisit Reynold's paper
    on ``The Meaning of Types'' (2000), showing how the paper's main
    results may be reconstructed along these lines.",
  paper = "Mell15.pdf",
  keywords = "printed"
}

@book{Morg98,
  author = "Morgan, Carroll",
  title = {{Programming from Specifcations, 2nd Ed.}},
  publisher = "Prentice Hall",
  year = "1998",
  link = 
"\url{http://www.cse.unsw.edu.au/~carrollm/ProgrammingFromSpecifications.pdf}",
  paper = "Morg98.pdf"
}

@inproceedings{Morr99,
  author = "Morrisett, Greg and Walker, David and Crary, Karl and Glew, Neil",
  title = {{From System F to Typed Assembly Language}},
  booktitle = "Trans. on Progamming Languages and Systems TOPLAS",
  volume = "21",
  number = "3",
  year = "1999",
  pages = "527-568",
  abstract =
    "We motivate the design of typed assembly language (TAL) and present a
    type-preserving ttranslation from Systemn F to TAL. The typed assembly
    language we pressent is based on a conventional RISC assembly
    language, but its static type sytem provides support for enforcing
    high-level language abstratctions, such as closures, tuples, and
    user-defined abstract data types. The type system ensures that
    well-typed programs cannot violatet these abstractionsl In addition,
    the typing constructs admit many low-level compiler optimiztaions. Our
    translation to TAL is specified as a sequence of type-preserving
    transformations, including CPS and closure conversion phases;
    type-correct source programs are mapped to type-correct assembly
    language. A key contribution is an approach to polymorphic closure
    conversion that is considerably simpler than previous work. The
    compiler and typed assembly lanugage provide a fully automatic way to
    produce certified code, suitable for use in systems where unstrusted
    and potentially malicious code must be checked for safety before
    execution.",
  paper = "Morr99.pdf",
  keywords = "printed"
}

@article{Moss84,
  author = "Mosses, Peter",
  title = {{A Basic Abstract Semantics Algebra}},
  journal = "LNCS",
  volume = "173",
  year = "1984",
  abstract =
    "It seems that there are some pragmatic advantages in using Abstract
    Semantic Algebras (ASAs) instead of X-notation in denotational
    semantics. The values of ASAs correspond to 'actions' (or
    'processes'), and the operators correspond to primitive ways of
    combining actions. There are simple ASAs for the various independent
    'facets' of actions: a functional ASA for data-flow, an imperative ASA
    for assignments, a declarative ASA for bindings, etc. The aim is to
    obtain general ASAs by systematic combination of these simple ASAs.
    
    Here we specify a basic ASA that captures the common features of the
    functional, imperative and declarative ASAs -- and highlights their
    differences. We discuss the correctness of ASA specifications, and
    sketch the proof of the consistency and (limiting) completeness of the
    functional ASA, relative to a simple model.
    
    Some familiarity with denotational semantics and algebraic
    specifications is assumed.",
  paper = "Moss84.pdf"
}

@article{Myre14,
  author = "Myreen, Magnus O. and Davis, Jared",
  title = {{The Reflective Milawa Theorem Prover is Sound}},
  journal = "LNAI",
  pages = "421-436",
  year = "2014",
  abstract =
    "Milawa is a theorem prover styled after ACL2 but with a small kernel
    and a powerful reflection mechanism. We have used the HOL4 theorem
    prover to formalize the logic of Milawa, prove the logic sound, and
    prove that the source code for the Milawa kernel (2,000 lines of Lisp)
    is faithful to the logic. Going further, we have combined these
    results with our previous verification of an x86 machine-code
    implementation of a Lisp runtime. Our top-level HOL4 theorem states
    that when Milawa is run on top of our verified Lisp, it will only
    print theorem statements that are semantically true. We believe that
    this top-level theorem is the most comprehensive formal evidence of a
    theorem prover’s soundness to date.",
  paper = "Myre14.pdf",
  keywords = "printed"
}

@article{Myre09,
  author = "Myreen, Magnus O. and Gordon, Michael J.C.",
  title = {{Verified LISP Implementations on ARM, x86 and PowerPC}},
  journal = "LNCS",
  volume = "5674",
  pages = "359-374",
  year = "2009",
  abstract =
    "This paper reports on a case study, which we believe is the first
    to produce a formally verified end-to-end implementation of a
    functional programming language running on commercial
    processors. Interpreters for the core of McCarthy’s LISP 1.5
    were implemented in ARM, x86 and PowerPC machine code, and proved
    to correctly parse, evaluate and print LISP s-expressions. The
    proof of evaluation required working on top of verified
    implementations of memory allocation and garbage collection. All
    proofs are mechanised in the HOL4 theorem prover.",
  paper = "Myre09.pdf",
  keywords = "printed"
}

@article{Myre09a,
  author = "Myreen, Magnus O. and Slind, Konrad and Gordon, Michael J.C.",
  title = {{Extensible Proof-Producing Compilation}},
  journal = "LNCS",
  volume = "5501",
  pages = "2-16",
  year = "2009",
  abstract =
    "This paper presents a compiler which produces machine code from
    functions defined in the logic of a theorem prover, and at the same
    time proves that the generated code executes the source functions.
    Unlike previously published work on proof-producing compilation from a
    theorem prover, our compiler provides broad support for user-defined
    extensions, targets multiple carefully modelled commercial machine
    languages, and does not require termination proofs for input
    functions. As a case study, the compiler is used to construct verified
    interpreters for a small LISP-like language. The compiler has been
    implemented in the HOL4 theorem prover.",
  paper = "Myre09a.pdf",
  keywords = "printed"
}

@article{Myre10,
  author = "Myreen, Magnus O.",
  title = {{Verified Just-In-Time Compiler on x86}},
  journal = "ACM SIGLAN Notices - POPL'10",
  volume = "45",
  number = "1",
  year = "2010",
  pages = "107-118",
  abstract =
    "This paper presents a method for creating formally correct just-in-
    time (JIT) compilers. The tractability of our approach is demonstrated
    through, what we believe is the first, verification of a JIT
    compiler with respect to a realistic semantics of self-modifying x86
    machine code. Our semantics includes a model of the instruction
    cache. Two versions of the verified JIT compiler are presented: one
    generates all of the machine code at once, the other one is incremental
    i.e. produces code on-demand. All proofs have been performed
    inside the HOL4 theorem prover.",
  paper = "Myre10.pdf",
  keywords = "printed"
}

@article{Myre11,
  author = "Myreen, Magnus O. and Davis, Jared",
  title = {{A Verified Runtime for a Verified Theorem Prover}},
  journal = "NCS",
  volume = "6898",
  pages = "265-280",
  year = "2011",
  abstract =
    "Theorem provers, such as ACL2, HOL, Isabelle and Coq, rely on the
    correctness of runtime systems for programming languages like ML,
    OCaml or Common Lisp. These runtime systems are complex and critical
    to the integrity of the theorem provers.
    
    In this paper, we present a new Lisp runtime which has been formally
    verified and can run the Milawa theorem prover. Our runtime consists
    of 7,500 lines of machine code and is able to complete a 4 gigabyte
    Milawa proof effort. When our runtime is used to carry out Milawa
    proofs, less unverified code must be trusted than with any other
    theorem prover.
    
    Our runtime includes a just-in-time compiler, a copying garbage collector,
    a parser and a printer, all of which are HOL4-verified down to
    the concrete x86 code. We make heavy use of our previously developed
    tools for machine-code verification. This work demonstrates that our
    approach to machine-code verification scales to non-trivial
    applications.",
  paper = "Myre11.pdf",
  keywords = "printed"
}

@article{Myre12,
  author = "Myreen, Magnus O.",
  title = {{Functional Programs: Conversions between Deep and Shallow
            Embeddings}},
  journal = "LNCS",
  volume = "7406",
  pages = "412-417",
  year = "2012",
  abstract =
    "This paper presents a method which simplifies verification of deeply
    embedded functional programs. We present a technique by which
    proof-certified equations describing the effect of functional 
    programs (shallow embeddings) can be automatically extracted from their
    operational semantics. Our method can be used in reverse, i.e. from
    shallow to deep embeddings, and thus for implementing certifying code
    synthesis: we have implemented a tool which maps HOL functions to
    equivalent Lisp functions, for which we have a verified Lisp runtime.
    A key benefit, in both directions, is that the verifier does not need
    to understand the operational semantics that gives meanings to the
    deep embeddings.",
  paper = "Myre12.pdf",
  keywords = "printed"
}

@article{Odon81,
  author = "O'Donnell, Michael J.",
  title = {{A Critique of the Foundations of Hoare-style Programming Logics}},
  journal = "LNCS",
  volume = "131",
  pages = "349-374",
  year = "1981",
  abstract = 
    "Much recent discussion in computing journals has been devoted to
    arguments about the feasibility and usefulness of formal
    verification methods for increasing confidence in computer
    programs. Too little attention has been given to precise criticism
    of specific proposed systems for reasoning about programs. Whether
    such systems are to be used for formal verification, by hand or
    automatically, or as a rigorous foundation for imformal reasoning,
    it is essential that they be logically sound. Several popular
    rules in the Hoare language are in fact not sound. These rules
    have been accepted because they have not been subjected to
    sufficiently strong standards of correctness. This paper attempts
    to clarify the different technical definitions of correctness of a
    logic, to show that only the strongest of these definitions is
    acceptable for Hoare logic, and to correct some of the unsound
    rules which have appeared in the literature. The corrected rules
    are given merely to show that it is possible to do so. Convenient
    and elegant rules for reasoning about certain programming
    constructs will probably require a more flexible notation than
    Hoare's.", 
  paper = "Odon81.pdf",
  keywords = "printed"
}

@book{Paul96,
  author = "Paulson, L.C.",
  title = {{ML for the WOrking Programmer 2nd Edition}},
  year = "1996",
  publisher = "Cambridge University Press",
  isbn = "0-521-56543-X"
}

@inproceedings{Peyt93,
  author = "Peyton-Jones, Simon and Wadler, Philip",
  title = {{Imperative Functional Programming}},
  booktitle = "Principles of Programming Languages POPL'93",
  publisher = "ACM",
  year = "1993",
  pages = "71-84",
  abstract =
    "We present a new model, based on monads, for performing
    input/output in a non-strict, purely functional language. It is
    composable, extensible, efficient, requires no extensions to the
    type system, and extends smoothly to incorporate mixed-language
    working and in-place array updates.",
  paper = "Peyt93.pdf"
}

@misc{Peyt17,
  author = "Peyton-Jones, Simon",
  title = {{Escape from the ivory tower: the Haskell journey}},
  link = "\url{https://www.youtube.com/watch?v=re96UgMk6GQ}",
  year = "2017"
}

@inproceedings{Pfen88a,
  author = "Pfenning, Frank and Elliott, Conal",
  title = {{Higher-Order Abstract Syntax}},
  booktitle = "Symp on Language Design and Implementation PLDI'88",
  publisher = "ACM",
  link = "\url{https://www.cs.cmu.edu/~fp/papers/pldi88.pdf}",
  year = "1988",
  abstract = 
    "We describe motivation, design, use, and implementation of
    higher-order abstract syntax as a central representation for
    programs, formulas, rules, and other syntactic objects in program
    manipulation and other formal systems where matching and substitution
    or unification are central operations.  Higher-order abstract syntax
    incorporates name binding information in a uniform and language
    generic way.  Thus it acts as a powerful link integrating diverse
    tools in such formal environments.  We have implemented higher-order
    abstract syntax, a supporting matching and unification algorithm, and
    some clients in Common Lisp in the framework of the Ergo project at
    Carnegie Mellon University.",
  paper = "Pfen88a.pdf",
  keywords = "printed"
}

@misc{Phel92,
  author = "Phelps, Tom",
  title = {{A Common Lisp CGOL}},
  year = "1992",
  link = "\url{https://people.eecs.berkeley.edu/~fateman/cgol/cgol.1/cgol.ps}",
  abstract =
    "CGOL is an Algol-like notation for Lisp. The original version,
    written by Vaughan Pratt at M.I.T. is the early 70s was written in
    Maclisp. This new version is based on Common Lisp. CGOL was translated
    to Common Lisp in the following four-stage process:
    
    (1) cgol.tok, the tokenizer has been almost completely rewritten; (2)
    cgoll.l, the main translation loop with library of translation schemas
    has been converted from Maclisp to Common Lisp; (3) the code that
    cgol1.l produces has been converted to Common Lisp; (4) selected
    examples of CGOL programs themselves were rewritten, since certain
    aspects of the semantics of Maclisp would otherwise not be modelled in
    the expected fashion. Maclisp differs from Common Lisp in a variety of
    respects, and some of them are apparent from CGOL including direct
    escapes to Lisp, variable scoping, function definitions and numerous
    other aspects.
    
    In contrast to the programming described above, the major contribution
    of this paper is annotation of selected code from the CGOL
    translator.",
  paper = "Phil92.pdf"
}

@misc{Piro08,
  author = "Piroi, Florina and Buchberger, Bruno and Rosenkranz, Camelia",
  title = {{Mathematical Journals as Reasoning Agents: Literature Review}},
  year = "2008",
  link = "\urlhttp://www.risc.jku.at/publications/download/risc_3442/Math-Agents-for-SFB-2008-03-10-12h00.pdf{}",
  abstract =
    "This report reviews the literature relevant for the research project
    'Math−Agents: Mathematical Journals as Reasoning Agents' proposed by
    Bruno Buchberger as a technology transfer project based on the results
    of the SFB Project 'Scientific Computing', in particular the project
    SFB 1302, 'Theorema'. The project aims at computer−supporting the
    refereeing process of mathematical journals by tools that are mainly
    based on automated reasoning and also at building up the knowledge
    archived in mathematical journals in such a way that they can act as
    interactive and active reasoning agents later on.  In this report,
    we review current mathematical software systems with a focus on the
    availability of tools that can contribute to the goals of the Math−
    Agents project.",
  paper = "Piro08.pdf",
  keywords = "printed"
}

@phdthesis{Poll94,
  author = "Pollack, Robert",
  title = {{The Theory of LEGO - A Proof Checker for the Extended Calculus
            of Constructions}},
  school = "University of Edinburgh",
  link =
     "\url{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.2610}",
  year = "1994",
  abstract = 
    "LEGO is a computer program for interactive typechecking in the
    Extended Calculus of Constructions and two of its subsystems. LEGO
    also supports the extension of these three systems with inductive
    types. These type systems can be viewed as logics, and as meta
    languages for expressing logics, and LEGO is intended to be used for
    interactively constructing proofs in mathematical theories presented
    in these logics. I have developed LEGO over six years, starting from
    an implementation of the Calculus of Constructions by Gerard
    Huet. LEGO has been used for problems at the limits of our abilities
    to do formal mathematics.  

    In this thesis I explain some aspects of the meta-theory of LEGO’s
    type systems leading to a machine-checked proof that typechecking is
    decidable for all three type theories supported by LEGO, and to a
    verified algorithm for deciding their typing judgements, assuming only
    that they are normalizing. In order to do this, the theory of Pure
    Type Systems (PTS) is extended and formalized in LEGO. This extended
    example of a formally developed body of mathematics is described, both
    for its main theorems, and as a case study in formal mathematics. In
    many examples, I compare formal definitions and theorems with their
    informal counterparts, and with various alternative approaches, to
    study the meaning and use of mathematical language, and suggest
    clarifications in the informal usage.

    Having outlined a formal development far too large to be surveyed in
    detail by a human reader, I close with some thoughts on how the human
    mathematician’s state of understanding and belief might be affected by
    posessing such a thing.",
  paper = "Poll94.pdf"
}

@article{Poll94a,
  author = "Pollack, Robert",
  title = {{On Extensibility of Proof Checkers}},
  journal = "LNCS",
  volume = "996",
  pages = "140-161",
  year = "1994",
  abstract = 
    "My suggestion is little different from LCF, just replacing one
    computational meta language (ML) with another (ECC, FS0,...). The
    philosophical point is that it is then possible to accept non-
    canonical proof notations as object level proofs, removing the need to
    actually normalize them. There are problems to be worked out in
    practice, such as extraction of programs from constructive proof, and
    efficient execution of pure, total programs. Although this approach
    doesn't address the difficulty of proving correctness of tactics in
    the meta level, it is immediatly useful for tactics with structural
    justification (e.g. weakening) which are not even representable in
    LCF, and are infeasible in the Nuprl variant of LCF. Since it can be
    used for any object system without adding new principles such as
    reflection, and is compatible with other approaches to extensibility
    (especially partial reflection), it should be considered as part of
    the answer to extensibility in proof checkers.",
  paper = "Poll94a.pdf"
}

@misc{Remy17,
  author = "Remy, Didier",
  title = {{Type Systems for Programming Languages}},
  ywar = "2017",
  link = "\url{http://gallium.inria.fr/~remy/mpri/cours1.pdf}",
  link = "\url{http://gallium.inria.fr/~remy/mpri/cours2.pdf}",
  link = "\url{http://gallium.inria.fr/~remy/mpri/cours3.pdf}",
  link = "\url{http://gallium.inria.fr/~remy/mpri/cours4.pdf}",
  link = "\url{http://gallium.inria.fr/~remy/mpri/cours5.pdf}",
  paper = "Remy17.tgz"
}

@article{Reyn85,
  author = "Reynolds, J.C.",
  title = {{Three Approaches to Type Structure}},
  journal = "LNCS",
  volume = "185",
  year = "1985",
  abstract =
    "We examine three disparate views of the type structure of
    programming languages: Milner's type deduction system and polymorphic
    let construct, the theory of subtypes and generic operators, and
    the polymorphic or second-order typed lambda calculus.  These
    approaches are illustrated with a functional language including
    product, sum and list constructors.  The syntactic behavior of types
    is formalized with type inference rules, bus their semantics is
    treated intuitively.",
  paper = "Reyn85.pdf",
  keywords = "printed"
}

@techreport{Sabr92,
  author = "Sabry, Amr and Felleisen, Matthias",
  title = {{Reasoning About Programs in Continuation-Passing Style}},
  type = "technical report",
  number = "TR 92-180",
  institution = "Rice University",
  abstract =
    "Plotkin's $\lambda$-value calculus is sound but incomplete for
    reasoning about $\beta\nu$-transformations on programs in continuation-
    passing style (CPS). To find a complete extension, we define a new,
    compactifying CPS transformation and an 'inverse' mapping, 
    $un$-CPS, both of which are interesting in their own right. Using the
    new CPS transformation, we can determine the precise language of CPS
    terms closed under $\beta\nu$ transformations. Using the $un$-CPS
    transformation, we can derive a set of axioms such that every equation
    between source programs is provable if and only if $\beta\nu$ can
    prove the corresponding equation between CPS programs. The extended
    calculus is equivalent to an untyped variant of Moggi's computational
    $\lambda$-calculus.",
  paper = "Sabr92.pdf",
  keywords = "printed"
}

@article{Sarm17,
  author = "Sarma, Gopal and Hay, Nick J.",
  title = {{Robust Computer Algebra, Theorem Proving, and Oracle AI}},
  journal = "Informatica",
  volume = "41",
  number = "3",
  link = "\url{https://arxiv.org/pdf/1708.02553.pdf}",
  year = "2017",
  abstract =
    "In the context of superintelligent AI systems, the term 'oracle' has
    two meanings.  One refers to modular systems queried for
    domain-specific tasks.  Another usage, referring to a class of systems
    which may be useful for addressing the value alignment and AI control
    problems, is a superintelligent AI system that only answers questions.
    The aim of this manuscript is to survey contemporary research problems
    related to oracles which align with long-term research goals of AI
    safety.  We examine existing question answering systems and argue that
    their high degree of architectural heterogeneity makes them poor
    candidates for rigorous analysis as oracles.  On the other hand, we
    identify computer algebra systems (CASs) as being primitive examples
    of domain-specific oracles for mathematics and argue that efforts to
    integrate computer algebra systems with theorem provers, systems which
    have largely been developed independent of one another, provide a
    concrete set of problems related to the notion of provable safety that
    has emerged in the AI safety community.  We review approaches to
    interfacing CASs with theorem provers, describe well-defined
    architectural deficiencies that have been identified with CASs, and
    suggest possible lines of research and practical software projects for
    scientists interested in AI safety.",
  paper = "Sarm17.pdf",
  keywords = "printed, axiomref"
}

@article{Scot71,
  author = "Scott, Dana S. and Strachey, C.",
  title = {{Towards a Mathematical Semantics for Computer Languages}},
  journal = "Proc. Symp. on Computers and Automata",
  volume = "21",
  year = "1971",
  abstract =
    "Compilers for high-level languages are generally constructed to
    give a complete translation of the programs into machine
    lanugage. As machines merely juggle bit patterns, the concepts of
    the original language may be lost or at least obscured during this
    passage. The purpose of a mathematical semantics is to give a
    correct and meaningful correspondence between programs and
    mathematical entities in a way that is entirely independent of an
    implementation. This plan is illustrated in a very elementary
    method with the usual idea of state transformations. The next
    section shows why the mathematics of functions has to be modified
    to accommodate recursive commands. Section 3 explains the
    modification. Section 4 introduces the environments for handling
    variables and identifiers and shows how the semantical equations
    define equivalence of programs. Section 5 gives an exposition of
    the new type of mathematical function spaces that are required for
    the semantics of procedures when these are allowed in assignment
    statements. The conclusion traces some of the background of the
    project and points the way to future work.",
  paper = "Scot71.pdf"
}

@inproceedings{Shie02,
  author = "Shields, Mark and Jones, Simon Peyton",
  title = {{First-Class Modules for Haskell}},
  booktitle = "9th Int. Conf. on Foundations of Object-Oriented Languages",
  pages = "28-40",
  year = "2002",
  link = "\url{http://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/first_class_modules.pdf}",
  abstract = 
    "Though Haskell's module language is quite weak, its core language
    is highly expressive. Indeed, it is tantalisingly close to being
    able to express much of the structure traditionally delegated to a
    separate module language. However, the encoding are awkward, and
    some situations can't be encoded at all.

    In this paper we refine Haskell's core language to support 
    {\sl first-class modules} with many of the features of ML-style
    modules. Our proposal cleanly encodes signatures, structures and
    functors with the appropriate type abstraction and type sharing,
    and supports recursive modules. All of these features work across
    compilation units, and interact harmoniously with Haskell's class
    system. Coupled with support for staged computation, we believe
    our proposal would be an elegant approach to run-time dynamic
    linking of structured code.

    Our work builds directly upon Jones' work on parameterised
    signatures, Odersky and Laufer's system of higher-ranked type
    annotations, Russo's semantics of ML modules using ordinary
    existential and universal quantifications, and Odersky and
    Zenger's work on nested types. We motivate the system by examples,
    and include a more formal presentation in the appendix.",
  paper = "Shie02.pdf",
  keywords = "printed"
}

@article{Soko87,
  author = "Sokolowski, Stefan",
  title = {{Soundness of Hoare's Logic: An Automated Proof Using LCF}},
  journal = "Trans. on Programming Languages and Systems",
  volume = "9",
  number = "1",
  pages = "100-120",
  year = "1987",
  abstract =
    "This paper presents a natural deduction proof of Hoare's logic
    carried out by the Edinburgh LCF theorem prover. The emphasis is
    on the way Hoare's theory is presented to the LCF, which looks
    very much like an exposition of syntax and semantics to human
    readers; and on the programmable heuristics (tactics). We also
    discuss some problems and possible improvements to the LCF.",
  paper = "Soko87.pdf",
  keywords = "printed"
}

@techreport{Stee78,
  author = "Steele, Guy Lewis",
  title = {{RABBIT: A Compiler for SCHEME}},
  institution = "MIT",
  type = "technical report",
  number = "AITR-474",
  year = "1978",
  abstract =
    "We have developed a compiler for the lexically-scoped dialect of LISP
    known as SCHEME. The compiler knows relatively little about specific
    data manipulation primitives such as arithmetic operators, but
    concentrates on general issues of environment and control. Rather than
    having specialized knowledge about a large variety of control and
    environment constructs, the compiler handles only a small basis set
    which reflects the semantics of lambda-calculus. All of the
    traditional imperative constructs, such as sequencing, assignment,
    looping, GOTO, as well as many standard LISP constructs such as AND,
    OR, and COND, are expressed in macros in terms of the applicative
    basis set. A small number of optimization techniques, coupled with the
    treatment of function calls as GOTO statements, serve to produce code
    as good as that produced by more traditional compilers. The macro
    approach enables speedy implementation of new constructs as desired
    without sacrificing efficiency in the generated code. 
    
    A fair amount of analysis is devoted to determining whether
    environments may be stack-allocated or must be heap-allocated.
    Heap-allocated environments are necessary in general because SCHEME
    (unlike Algol 60 and Algol 68, for example) allows procedures with
    free lexically scoped variables to be returned as the values of other
    procedures; the Algol stack-allocation environment strategy does not
    suffice. The methods used here indicate that a heap-allocating
    generalization of the 'display' technique leads to an efficient
    implementation of such 'upward funargs'. Moreover, compile-time
    optimization and analysis can eliminate many 'funargs' entirely, and
    so far fewer environment structures need be allocated at run time than
    might be expected.
    
    A subset of SCHEME (rather than triples, for example) serves as the
    representation intermediate between the optimized SCHEME code and the
    final output code; code is expressed in this subset in the so-called
    continuation-passing style. As a subset of SCHEME, it enjoys the same
    theoretical properties; one could even apply the same optimizer used
    on the input code to the intermediate code. However, the subset is so
    chosen that all temporary quantities are made manifest as variables,
    and no control stack is needed to evaluate it. As a result, this
    apparently applicative representation admits an imperative
    interpretation which permits easy transcription to final imperative
    machine code. These qualities suggest that an applicative language
    like SCHEME is a better candidate for an UNCOL than the more
    imperative candidates proposed to date.",
  paper = "Stee78.pdf"
}

@article{Stuc05,
  author = "Stuckey, Peter J. and Sulzmann, Martin",
  title = {{A Theory of Overloading}},
  journal = "ACM Trans. on Programming Languages and Systems",
  volume = "27",
  number = "6",
  pages = "1-54",
  year = "2005",
  abstract =
    "We present a novel approach to allow for overloading of
    identifiers in the spirit of type classes. Our approach relies on
    the combination of the HM(X) type system framework with Constraint
    Handling Rules (CHRs). CHRs are a declarative language for writing
    incremental constraint solvers, that provide our scheme with a
    form of programmable type language. CHRs allow us to precisely
    describe the relationships among overloaded identifiers. Under
    some sufficient conditions on the CHRs we achieve decidable type
    inference and the semantic meaning of programs is unambiguous. Our
    approach provides a common formal basis for many type class
    extensions such as multi-parameter type classes and functional
    dependencies.",
  paper = "Stuc05.pdf",
  keywords = "printed"
}

@inproceedings{Talp92,
  author = "Talpin, Jean-Pierre and Jouvelot, Pierre",
  title = {{The Type and Effect Discipline}},
  booktitle = "Conf. on Logic in Computer Science",
  publisher = "Computer Science Press",
  year = "1992",
  abstract =
    "The {\sl type and effect discipline} is a new framework for
    reconstructing the principal type and the minimal effect of
    expressions in implicitly typed polymorphic functional languages that
    support imperative constructs. The type and effect discipline
    outperforms other polymorphic type systems. Just as types abstract
    collections of concrete values, {\sl effects} denote imperative
    operations on regions. {\sl Regions} abstract sets of possibly aliased
    memory locations. 
    
    Effects are used to control type generalization in the presence of
    imperative constructs while regions delimit observable
    side-effects. The observable effects of an expression range over the
    regions that are free in its type environment and its type; effects
    related to local data structures can be discarded during type
    reconstruction. The type of an expression can be generalized with
    respect to the variables that are not free in the type environment or
    in the observable effect.",
  paper = "Talp92.pdf",
  keywords = "printed"
}

@techreport{Tard90,
  author = "Tarditi, David and Acharya, Anurag and Lee, Peter",
  title = {{No Assembly Required: Compiling Standard ML to C}},
  type = "technical report",
  institution = "Carnegie Mellon University",
  number = "CMU-CS-90-187",
  year = "1990",
  abstract = 
    "C has been proposed as a portable target language for
    implementing higher-order languages. Previous efforts at compiling
    such languages to C have produced efficient code, but have had to
    compromise on either the portability or the preservation of the
    tail-recursive properties of the languages. We assert that neither
    of these compromises is necessary for the generation of efficient
    code. We offer a Standard ML to C compiler, which does not make
    wither of these compromises, as an existence proof. The generated
    code achieves an execution speed that is just a factor of two
    slower than the best native code compiler. In this paper, we
    describe the design, implementation and the performance of this
    compiler.",
  paper = "Tard90.pdf",
  keywords = "printed"
}

@misc{Tuho18,
  author = "Tuhola, Henri",
  title = {{An another MLsub study}},
  link = "\url{http://boxbase.org/entries/2018/mar/12/mlsub-study/}",
  year = "2018",
  comment = "Subtyping does not solve the problem of coercion"
}

@misc{Tuho18a,
  author = "Tuhola, Henri",
  title = {{Boxbase - Index}},
  link = "\url{http://boxbase.org/}",
  year = "2018",
  keywords = "axiomref"
}

\subsection{U} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{V} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{W} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\index{Wadler, Philip}
\begin{chunk}{axiom.bib}
@article{Wadl95,
  author = "Wadler, Philip",
  title = {{Monads for Functional Programming}},
  journal = "LNCS",
  volume = "925",
  abstract =
    "The use of monads to structure functional programs is
    described. Monads provide a convenient framework for simulating
    effects found in other languages, such as global state, exception
    handling, output, or non-determinism. Three case studies are
    looked at in detail: how monads ease the modification of a simple
    evaluator; how monads act as the basis of a datatype of arrays
    subject to in-place update; and how monads can be used to build
    parsers.",
  paper = "Wadl95.pdf",
  keywords = "printed"
}

@article{Wadl92,
  author = "Wadler, Philip",
  title = {{Comprehending Monads}},
  journal = "Mathematical Structures in Computer Science",
  volume = "2",
  pages = "461-493",
  year = "1992",
  abstract =
    "Category theorists invented {\sl monads} in the 1960s to
    concisely express certain aspects of universal algebra. Functional
    programmers invented {\sl list comprehensions} in the 1970s ot
    concisely express certain programs involving lists. This paper
    shows how list comprehensions may be generalised to an arbitrary
    monad, and how the resulting programming feature can concisely
    express in a pure functional language some programs that
    manipulate state, handle exceptions, parse text, or invoke
    continuations. A new solution to the old problem of destructive
    array update is also presented. No knowledge of category theory is
    assumed.",
  paper = "Wadl92.pdf"
}

@article{Wadl07,
  author = "Wadler, Philip",
  title = {{The Girard-Reynolds isomorphism (second edition)}},
  journal = "Theoretical Computer Science",
  volume = "375",
  pages = "201-226",
  year = "2007",
  abstract =
    "ean-Yves Girard and John Reynolds independently discovered the
    second-order polymorphic lambda calculus, F2. Girard additionally
    proved a Representation Theorem : every function on natural numbers
    that can be proved total in second-order intuitionistic predicate
    logic, P2, can be represented in F2. Reynolds additionally proved an
    Abstraction Theorem : every term in F2 satisfies a suitable notion of
    logical relation; and formulated a notion of parametricity satisfied
    by well-behaved models.  
    
    We observe that the essence of Girard’s result is a projection from P2
    into F2, and that the essence of Reynolds’s result is an embedding of
    F2 into P2, and that the Reynolds embedding followed by the Girard
    projection is the identity. We show that the inductive naturals are
    exactly those values of type natural that satisfy Reynolds’s notion of
    parametricity, and as a consequence characterize situations in which
    the Girard projection followed by the Reynolds embedding is also the
    identity.
    
    An earlier version of this paper used a logic over untyped terms. This
    version uses a logic over typed term, similar to ones considered by
    Abadi and Plotkin and by Takeuti, which better clarifies the
    relationship between F2 and P2.",
  paper = "Wadl07.pdf"
}

@inproceedings{Wadl07a,
  author = "Wadler, Philip and Findler, Robert Bruce",
  title = {{Well-Typed Programs Can't Be Blamed}},
  booktitle = "Workshop on Scheme and Functional Programming",
  year = "2007",
  pages = "15-26",
  abstract = 
    "We show how {\sl contracts} with blame fit naturally with recent
    work on {\sl hybrid types} and {\sl gradual types}. Unlike hybrid
    types or gradual types, we require casts in the source code, in
    order to indicate where type errors may occur. Two (perhaps
    surprising) aspects of our approach are that refined types can
    provide useful static guarantees even in the absence of a theorem
    prover, and that type {\sl dynamic} should not be regarded as a
    supertype of all other types. We factor the well-known notion of
    subtyping into new notions of positive and negative subtyping, and
    use these to characterise where positive and negative blame may
    arise. Our approach sharpens and clarifies some recent results in
    the literature.",
  paper = "Wadl07a.pdf",
  keywords = "printed"
}

@phdthesis{Wehr05,
  author = "Wehr, Stefan",
  title = {{ML Modules and Haskell Type Classes: A Constructive 
            Comparison}},
  school = "Albert Ludwigs Universitat",
  year = "2005",
  abstract = 
    "Researchers repeatedly observed that the module system of ML and the
    type class mechanism of Haskell are related. So far, this relationship
    has received little formal investigation. The work at hand fills this
    gap: It introduces type-preserving translations from modules to type
    classes and vice versa, which enable a thorough comparison of the two
    concepts.

    The source language of the first translation is a subset of
    Standard ML. The target language is Haskell with common extensions
    and one new feature, which was deeloped as part of this work. The
    second translation maps a subset of Haskell 98 to ML, with
    well-established extensions. I prove that the translations
    preserve type correctness and provide implementations for both.

    Building on the insights obtained from the translations, I present
    a thorough comparison between ML modules and Haskell type
    classes. Moreover, I evaluate to what extent the techniques used
    in the translations can be exploited for modular programming in
    Haskell and for programming with ad-hoc polymorphism in ML.",
  paper = "Wehr05.pdf"
}

@article{Wehr08,
  author = "Wehr, Stefan and Chakravarty, Maneul M.T.",
  title = {{ML Modules and Haskell Type Classes: A Constructive 
            Comparison}},
  journal = "LNCS",
  volume = "5356",
  pages = "188-204",
  year = "2008",
  abstract =
    "Researchers repeatedly observed that the module system of ML and the
    type class mechanism of Haskell are related. So far, this relationship
    has received little formal investigation. The work at hand fills this
    gap: It introduces type-preserving translations from modules to type
    classes and vice versa, which enable a thorough comparison of the two
    concepts.",
  paper = "Wehr08.pdf",
  keywords = "printed"
}  

@phdthesis{Zeil09,
  author = "Zeilberger, Noam",
  title = {{The Logical Basis of Evaluation Order and Pattern-Matching}},
  school = "Carnegie Mellon University",
  year = "2009",
  link = "\url{https://www.cs.cmu.edu/~rwh/theses/zeilberger.pdf}",
  abstract =
    "An old and celebrated analogy says that writing programs is like
    proving theorems. This analogy has been productive in both
    directions, but in particular has demonstrated remarkable utility in
    driving progress in programming languages, for example leading towards
    a better understanding of concepts such as abstract data types and
    polymorphism.  One of the best known instances of the analogy actually
    rises to the level of an isomorphism: between Gentzen’s natural
    deduction and Church’s lambda calculus.  However, as has been
    recognized for a while, lambda calculus fails to capture some of the
    important features of modern programming languages. Notably, it does
    not have an inherent notion of evaluation order, needed to make sense
    of programs with side effects. Instead, the historical descendents of
    lambda calculus (languages like Lisp, ML, Haskell, etc.) impose
    evaluation order in an ad hoc way.

    This thesis aims to give a fresh take on the proofs-as-programs
    analogy—one which better accounts for features of modern programming
    languages—by starting from a different logical foundation.  Inspired
    by Andreoli’s focusing proofs for linear logic, we explain how to
    axiomatize certain canonical forms of logical reasoning through a
    notion of pattern.  Propositions come with an intrinsic polarity,
    based on whether they are defined by patterns of proof, or by patterns
    of refutation.  Applying the analogy, we then obtain a programming
    language with built-in support for pattern-matching, in which
    evaluation order is explicitly reflected at the level of types—and
    hence can be controlled locally, rather than being an ad hoc, global
    policy decision. As we show, different forms of continuation-passing
    style (one of the historical tools for analyzing evaluation order)
    can be described in terms of different polarizations.  This language
    provides an elegant, uniform account of both untyped and
    intrinsically-typed computation (incorporating ideas from infinitary
    proof theory), and additionally, can be provided an extrinsic type
    system to express and statically enforce more refined properties of
    programs. We conclude by using this framework to explore the theory of
    typing and subtyping for intersection and union types in the presence
    of effects, giving a simplified explanation of some of the unusual
    artifacts of existing systems.",
  paper = "Zeil09.pdf"
}

@misc{Zeil16,
  author = "Zeilberger, Noam",
  title = {{Towards a Mathematical Science of Programming}},
  year = "2016"
}

@inproceedings{Zeil16a,
  author = "Zeilberger, Noam",
  title = {{Principles of Type Refinement}},
  booktitle = "OPLSS 2106",
  link = "\url{http://noamz.org/oplss16/refinements-notes.pdf}",
  year = "2016",
  paper = "Zeil16a.pdf"
}

@incollection{Soze06,
  author = "Sozeau, Matthieu",
  title = {{Subset Coercions in Coq}},
  booktitle = "TYPES: Int. Workshop on Types for Proofs and Programs",
  publisher = "ACM Press",
  journal = "LNCS",
  volume = "4502",
  pages = "237-252",
  year = "2006",
  abstract =
    "We propose a new language for writing programs with dependent types
    on top of the Coq proof assistant. This language permits to establish
    a phase distinction between writing and proving algorithms in the Coq
    environment. Concretely, this means allowing to write algorithms as
    easily as in a practical functional programming language whilst giving
    them as rich a specification as desired and proving that the code
    meets the specification using the whole Coq proof apparatus.  This is
    achieved by extending conversion to an equivalence which relates
    types and subsets based on them, a technique originating from the
    ``Predicate subtyping'' featureof PVS and following mathematical 
    convention. The typing judgements can be translated to the Calculus of
    (Co-)Inductive Constructions (Cic) by means of an interpretation
    which inserts coercions at the appropriate places.These coercions
    can contain existential variables representing the propositional 
    parts of the final term, corresponding to proof obligations (or 
    PVS type-checking conditions). A prototype implementation of this 
    process is integrated with the Coq environment.",
  paper = "Soze06.pdf",
  keywords = "printed"
}

@article{Aban16,
  author = "Abanades, M. and Botana, F. and Kovacs, Z. and Recio, T. and
            Solyom-Gecse, C.",
  title = {{Development of automatic reasoning tools in GeoGebra}},
  journal = "ACM Comm. Computer Algebra",
  volume = "50",
  pages = "85-88",
  year = "2016",
  abstract =
    "Much effort has been put into the implementation of automatic proving
    in interactive geometric environments (e.g. Java Geometry Expert,
    GeoGebra). The closely related concept of automatic discovery, remains
    however almost unexplored.
    
    This software presentation will demonstrate our results towards the
    incorporation of automatic discovery capabilities into GeoGebra, an
    educational software with tens of millions of users worldwide. As main
    result, we report on a new command, currently available in the
    official version, that allows the automatic discovery of loci of
    points in diagrams defined by implicit conditions. This represents an
    extension of a previous command, similar in nature, but restricted to
    loci defined by the standard mover-tracer construction. Our proposal
    successfully automates the `dummy locus dragging' in dynamic
    geometry. This makes the cycle conjecturing-checking-proving
    accessible for general users in elementary geometry.",
  paper = "Aban16.pdf"
}

@inproceedings{Abra15,
  author = "Abraham, Erika",
  title = {{Building Bridges between Symbolic Computation and Satisfiability
           Checking}},
  booktitle = "ISSAC 15",
  year = "2015",
  pages = "1-6",
  publisher = "ACM",
  isbn = "978-1-4503-3435-8",
  abstract =
    "The satisfiability problem is the problem of deciding whether a
    logical formula is satisfiable. For first-order arithmetic theories,
    in the early 20th century some novel solutions in form of decision
    procedures were developed in the area of mathematical logic. With the
    advent of powerful computer architectures, a new research line started
    to develop practically feasible implementations of such decision
    procedures. Since then, symbolic computation has grown to an extremely
    successful scientific area, supporting all kinds of scientific
    computing by efficient computer algebra systems.
    
    Independently, around 1960 a new technology called SAT solving started
    its career. Restricted to propositional logic, SAT solvers showed to
    be very efficient when employed by formal methods for verification. It
    did not take long till the power of SAT solving for Boolean problems
    had been extended to cover also different theories. Nowadays, fast
    SAT-modulo-theories (SMT) solvers are available also for arithmetic
    problems.
    
    Due to their different roots, symbolic computation and SMT solving
    tackle the satisfiability problem differently. We discuss differences
    and similarities in their approaches, highlight potentials of
    combining their strengths, and discuss the challenges that come with
    this task.",
  paper = "Abra15.pdf",
  keywords = "printed"
}

@inproceedings{Abra16a,
  author = "Abraham, Erika",
  title = {{Symbolic Computation Techniques in Satisfiability Checking}},
  booktitle = "SYNASC 2016",
  publisher = "IEEE Press",
  year = "2016",
  isbn = "978-1-5090-5707-8",
  pages = "3-10"
}

@inproceedings{Abra17,
  author = "Abraham, Erika and Jebelean, Tudor",
  title = {{Adapting Cylindrical Algebraic Decomposition for Proof Specific
           Tasks}},
  booktitle = "IJCAI 2017",
  year = "2017",
  comment = "Extended Abstract",
  paper = "Abra17.pdf"
}

@misc{Abra16,
  author = "Abraham, Erika and Abbott, John and Becker, Bernd and
            Bigatti, Anna M. and Brain, Martin and Buchberger, Bruno
            and Cimatti, Alessandro and Davenport, James H. and
            England, Matthew and Fontaine, Pascal and Forrest, Stephen
            and Griggio, Alberto and Kroenig, Daniel and 
            Seiler, Werner M. and Sturm, Thomas",
  title = {{Satisfiability Checking meesg Symbolic Computation}},
  year = "2016",
  link = "\url{http://arxiv.org/abs/1607.08028}",
  abstract =
    "Symbolic Computation and Satisfiability Checking are two research
    areas, both having their individual scientific focus but sharing also
    common interests in the development, implementation and application of
    decision procedures for arithmetic theories. Despite their
    commonalities, the two communities are rather weakly connected. The
    aim of our newly accepted SC-square project (H2020-FETOPEN-CSA) is to
    strengthen the connection between these communities by creating common
    platforms, initiating interaction and exchange, identifying common
    challenges, and developing a common roadmap from theory along the way
    to tools and (industrial) applications. In this paper we report on the
    aims and on the first activities of this project, and formalise some
    relevant challenges for the unified SC-square community.",
  paper = "Abra16.pdf"
}

@misc{Abraxx,
  author = "Abraham, Erika and Abbott, John and Becker, Bernd and 
            Bigatti, Anna M. and Brain, Martin and Cimatti, Alessandro and
            Davenport, James H. and England, Matthew and Fontaine, Pascal
            and Forrest, Stephen and Ganesh, Vijay and Griggio, Alberto and
            Kroenig, Daniel and Seiler, Werner M.",
  title = {{SC2 challenges: when Satisfiability Checking and Symbolic
           Computation join forces}},
  year = "2017",
  abstract =
    "Symbolic Computation and Satisfiability Checking are two research
    areas, both having their individual scientific focus but with common
    interests, e.g., in the development, implementation and application
    of decision procedures for arithmetic theories.  Despite their
    commonalities, the two communities are rather weakly connected.  The
    aim of the SC 2 initiative is to strengthen the connection between
    these communities by creating common platforms, initiating interaction
    and exchange, identifying common challenges, and developing a common
    roadmap from theory along the way to tools and (industrial) applications.",
  paper = "Abraxx.pdf"
}

@book{Acze13,
  author = "Aczel, Peter et al.",
  title = {{Homotopy Type Theory: Univalent Foundations of Mathematics}},
  publisher = "Institute for Advanced Study",
  year = "2013",
  link = 
    "\url{https://hott.github.io/book/nightly/hott-letter-1075-g3c53219.pdf}",
  paper = "Acze13.pdf"
}

@inproceedings{Adam99,
  author = "Adams, Andrew A. and Gottlieben, Hanne and Linton, Steve A. and
            Martin, Ursula",
  title = {{Automated theorem proving in support of computer algebra: 
           symbolic definite integration as a case study}},
  booktitle = "ISSAC '99",
  pages = "253-260",
  year = "1999",
  abstract = "
    We assess the current state of research in the application of computer
    aided formal reasoning to computer algebra, and argue that embedded
    verification support allows users to enjoy its benefits without
    wrestling with technicalities. We illustrate this claim by considering
    symbolic definite integration, and present a verifiable symbolic
    definite integral table look up: a system which matches a query
    comprising a definite integral with parameters and side conditions,
    against an entry in a verifiable table and uses a call to a library of
    lemmas about the reals in the theorem prover PVS to aid in the
    transformation of the table entry into an answer. We present the full
    model of such a system as well as a description of our prototype
    implementation showing the efficacy of such a system: for example, the
    prototype is able to obtain correct answers in cases where computer
    algebra systems [CAS] do not. We extend upon Fateman's web-based table
    by including parametric limits of integration and queries with side
    conditions.",
  paper = "Adam99.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@InProceedings{Adam01,
  author = "Adams, Andrew A. and Dunstan, Martin and Gottlieben, Hanne and
            Kelsey, Tom and Martin, Ursula and Owre, Sam",
  title = {{Computer algebra meets automated theorem proving: Integrating 
           Maple and PVS}},
  booktitle = "Theorem proving in higher order logics",
  series = "TPHOLs 2001",
  year = "2001",
  location = "Edinburgh, Scotland",
  pages = "27-42",
  abstract =
    "We describe an interface between version 6 of the Maple computer
    algebra system with the PVS automated theorem prover. The interface is
    designed to allow Maple users access to the robust and checkable proof
    environment of PVS. We also extend this environment by the provision
    of a library of proof strategies for use in real analysis. We
    demonstrate examples using the interface and the real analysis
    library. These examples provide proofs which are both illustrative and
    applicable to genuine symbolic computation problems.",
  paper = "Adam01.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@misc{Ager88,
  author = "Ager, Tryg A. and Ravaglia, R.A. and Dooley, Sam",
  title = {{Representation of Inference in Computer Algebra Systems with
           Applications to Intelligent Tutoring}},
  year = "1988",
  abstract =
    "Presently computer algebra systems share with calculators the
    property that a sequence of computations is not a unified
    computational sequence, thereby allowing fallacies to occur. We argue
    that if computer algebra systems operate in a framework of strict
    mathematical proof, fallacies are eliminated. We show that this is
    possible in a working interactive system REQD. We explain why
    computational algebra, done under the strict constraints of proof, is
    relevant to uses of computer algebra systems in instruction."
}

@phdthesis{Aker93,
  author = "Akers, Robert Lawrence",
  title = {{Strong Static Type Checking for Functional Common Lisp}},
  school = "Univerity of Texas at Austin",
  year = "1995",
  link = "\url{ftp://www.cs.utexas.edu/pub/boyer/cli-reports/096.pdf}",
  abstract =
    "This thesis presents a type system which supports the strong static
    type checking of programs developed in an applicative subset of the
    Common Lisp language. The Lisp subset is augmented with a guard
    construct for function definitions, which allows type restrictions to
    be placed on the arguments. Guards support the analysis and safe use
    of partial functions, like CAR, which are well-defined only for
    arguments of a certain type.

    A language of type descriptors characterizes the type
    domain. Descriptors are composed into function signatures which
    characterize the guard and which map various combinations of actual
    parameter types to possible result types. From a base of signatures
    for a small collection of primitive functions, the system infers
    signatures for newly submitted functions.

    The system includes a type inference algorithm which handles
    constructs beyond the constraints of ML-style systems. Most notable
    are the free use of CONS to construct objects of undeclared type and
    the use of IF forms whose two result components have unrelated types,
    resulting in ad hoc polymorphism. Accordingly, the type descriptor
    language accommodates disjunction, unrestricted CONS, recursive type
    forms, and ad hoc polymorphic function signatures. As with traditional
    type inference systems, unification is a central algorithm, but the
    richness of our type language complicates many component algorithms,
    including unification. Special treatment is given to recognizer
    functions, which are predicates determining whether an object is of a
    particular type. Type inference in this setting is undecidable, so the
    algorithm is heuristic and is not complete.

    The semantics of the system are in terms of a function which
    determines whether values satisfy descriptors with respect to a
    binding of type variables. The soundness of each signature emitted by
    the system is validated by a signature checker, whose properties are
    formally specified with respect to the formal semantics and proven to
    hold. The checker algorithm is substantially simpler than the
    inference algorithm, as it need not perform operations such as
    discovering closed recursive forms. Thus, its proof is both more
    straightforward to construct and easier to validate than a direct
    proof of the inference algorithm would be.",
  paper = "Aker93.pdf, printed"
}

@inproceedings{Aran05,
  author = "Aransay, Jesus and Ballarin, Clemens and Rubio, Julio",
  title = {{Extracting Computer Algebra Programs from Statements}},
  booktitle = "Int. Conf. on Computer Aided System Theory",
  pages = "159-168",
  year = "2005",
  abstract =
    "In this paper, an approach to synthesize correct programs from
    specifications is presented. The idea is to extract code from
    definitions appearing in statements which have been mechanically
    proved with the help of a proof assistant. This approach has been
    found when proving the correctness of certain Computer Algebra
    programs (for Algebraic Topology) by using the Isabelle proof
    assistant. To ease the understanding of our techniques, they are
    illustrated by means of examples in elementary arithmetic.",
  paper = "Aran05.pdf",
  keywords = "printed"
}

@inproceedings{Arch93,
  author = "Archer, M. and Fink, G. and Yang, L.",
  title = {{Linking Other Theorem Provers to HOL Using PM: Proof Manager}},
  booktitle = "Int Workshop on the HOL Theorem Proving System and its
               Applications",
  publisher = "North-Holland",
  year = "1993"
}

@article{Aude02,
  author = "Audemard, Gilles and Bertoli, Piergiorgio and Cimatti,
            Alessandro and Kornilowicz, Artur and Sebastiani,
	    Roberto",
  title = {{Integrating Boolean and Mathematical Solving: Foundations,
           Basic Algorithms, and Requirements}},
  journal = "LNCS",
  volume = "2385",
  pages = "231-245",
  year = "2002",
  abstract =
    "In the last years we have witnessed an impressive advance in the
    efficiency of boolean solving techniques, which has brought large
    previously intractable problems at the reach of state-of-the-art
    solvers.  Unfortunately, simple boolean expressions are not expressive
    enough for representing many real-world problems, which require
    handling also integer or real values and operators. On the other
    hand, mathematical solvers, like computer-algebra systems or
    constraint solvers, cannot handle efficiently problems involving
    heavy boolean search, or do not handle them at all. In this paper we
    present the foundations and the basic algorithms for a new class of
    procedures for solving boolean combinations of mathematical
    propositions, which combine boolean and mathematical solvers, and we
    highlight the main requirements that boolean and mathematical solvers
    must fulfill in order to achieve the maximum benefits from their
    integration. Finally we show how existing systems are captured by our
    framework.",
  paper = "Aude02.pdf"
}

@inproceedings{Augu98,
  author = "Augustsson, Lennart",
  title = {{Cayenne -- a language with dependent types}},
  booktitle = "ICFP 98",
  year = "1998",
  pages = "239-250",
  isbn = "1-58113-024-4",
  link = "\url{http://fsl.cs.illinois.edu/images/5/5e/Cayenne.pdf}",
  abstract =
    "Cayenne is a Haskell-like language. The main difference between
    Haskell and Cayenne is that Cayenne has dependent types i.e.
    the result type of a function may depend on the argument value
    and types of record components which can be types or values
    may depend on other components. Cayenne also combines the 
    syntactic categories for value expressions and type expressions thus
    reducing the number of language concepts. 
    
    Having dependent types
    and combined type and value expressions makes the language very
    powerful. It is powerful enough that a special module concept
    is unnecessary; ordinary records suffice. It is also powerful enough to
    encode predicate logic at the type level allowing types to be
    used as specifications of programs. However this power comes at a
    cost: type checking of Cayenne is undecidable. While this may
    appear to be a steep price to pay it seems to work well in practice.",
  paper = "Augu98.pdf"
}

@article{Avig12a,
  author = "Avigad, Jeremy",
  title = {{Type Inference in Mathematics}},
  journal = "European Association of Theoretical Computer Science",
  volume = "106",
  pages = "78-98",
  year = "2012",
  abstract =
    "In the theory of programming languages, type inference is the process
    of inferring the type of an expression automatically, often making use
    of information from the context in which the expression appears. Such
    mechanisms turn out to be extremely useful in the practice of
    interactive theorem proving, whereby users interact with a
    computational proof assistant to construct formal axiomatic
    derivations of mathematical theorems. This article explains some of
    the mechanisms for type inference used by the Mathematical
    Components project, which is working towards a verification of the
    Feit-Thompson theorem.",
  paper = "Avig12a.pdf",
  keywords = "coercion"
}

@misc{Avig14,
  author = "Avigad, Jeremy",
  title = {{LEAN proof of GCD}},
  link =
 "\url{http://github.com/leanprover/lean2/blob/master/library/data/nat/gcd.lean}",
  year = "2014",
  keywords = "CAS-Proof, printed"
}

@misc{Avig16,
  author = "Avigad, Jeremy",
  title = {{LEAN github repository}},
  link = "\url{http://github.com/leanprover}",
  year = "2016"
}

@misc{Avig17,
  author = "Avigad, Jeremy and de Moura, Leonardo and Kong, Soonho",
  title = {{Theorem Proving in Lean}},
  year = "2017",
  link = "\url{https://leanprover.github.io/tutorial/tutorial.pdf}",
  paper = "Avig17.pdf"
}

@techreport{Back73,
  author = "Backus, John",
  title = {{Programming Language Semantics and Closed Applicative Language}},
  type = "technical report",
  institution = "IBM",
  number = "RJ 1245",
  year = "1973"
}

@inproceedings{Ball95,
  author = "Ballarin, Clemens and Homann, Karsten and Calmet, Jacques",
  title =
    {{Theorems and Algorithms: An Interface between Isabelle and Maple}},
  booktitle = "ISSAC 95",
  year = "1995",
  pages = "150-157",
  publisher = "ACM",
  link = "\url{https://pdfs.semanticscholar.org/077e/606f92b4095637e624a9efc942c5c63c4bc2.pdf}",
  abstract =
    "Solving sophisticated mathematical problems often requires algebraic
    algorithms {\sl and} theorems. However, there are no environments
    integrating theorem provers and computer algebra systems which
    consistently provide the inference capabilities of the first and the
    powerful arithmetic of the latter systems.
    
    As an example for such a mechanized mathematics environment we describe
    a prototype implementation of an interface between Isabelle and Maple.
    It is achieved by extending the simplifier of Isabelle through the
    introduction of a new class of simplification rules called evaluation
    rules in order to make selected operations of Maple available, and
    without any modification to the computer algebra system. Additionally,
    we specify syntax translations for the concrete syntax of Maple
    which enables the communication between both systems illustrated by
    some examples that can be solved by theorems and algorithms",
  paper = "Ball95.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@misc{Ball98,
  author = "Ballarin, Clemens and Paulson, Lawrence C.",
  title = {{Reasoning about Coding Theory: The Benefits We Get from
           Computer Algebra}},
  year = "1998",
  link = "\url{http://www21.in.tum.de/~ballarin/publications/aisc98.pdf}",
  abstract =
    "The use of computer algebra is usually considered beneficial for
    mechanised reasoning in mathematical domains. We present a case study,
    in the application domain of coding theory, that supports this claim:
    the mechanised proof depends on non-trivial algorithms from computer
    algebra and increase the reasoning power of the theorem prover. The
    unsoundness of computer algebra systems is a major problem in
    interfacing them to theorem provers. Our approach to obtaining a sound
    overall system is not blanket distrust but based on the distinction
    between algorithms we call sound and {\sl ad hoc} respectively. This
    distinction is blurred in most computer algebra systems OUr
    experimental interface therefore uses a computer algebra library. It
    is based on theorem templates, which provide formal specifications for
    the algorithms.",
  paper = "Ball98.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Ball99,
  author = "Ballarin, Clemens and Paulson, Lawrence C.",
  title = {{A Pragmatic Approach to Extending Provers by Computer Algebra -- 
           with Applications to Coding Theory}},
  journal = "Fundam. Inform.",
  volume = "39",
  number = "1-2",
  pages = "1-20",
  year = "1999",
  link = "\url{http://www.cl.cam.ac.uk/~lp15/papers/Isabelle/coding.pdf}",
  abstract = "
    The use of computer algebra is usually considered beneficial for
    mechanised reasoning in mathematical domains. We present a case study,
    in the application domain of coding theory, that supports this claim:
    the mechanised proofs depend on non-trivial algorithms from computer
    algebra and increase the reasoning power of the theorem prover.

    The unsoundness of computer algebra systems is a major problem in
    interfacing them to theorem provers. Our approach to obtaining a sound
    overall system is not blanket distrust but based on the distinction
    between algorithms we call sound and {\sl ad hoc} respectively. This
    distinction is blurred in most computer algebra systems. Our
    experimental interface therefore uses a computer algebra library. It
    is based on formal specifications for the algorithms, and links the
    computer algebra library Sumit to the prover Isabelle.

    We give details of the interface, the use of the computer algebra
    system on the tactic-level of Isabelle and its integration into proof
    procedures.",
  paper = "Ball99.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@article{Bare01,
  author = "Barendregt, Henk and Cohen, Arjeh M.",
  title = {{Electronic Communication of Mathematics and the Interaction
           of Computer Algebra Systems and Proof Assistants}},
  journal = "J. Symbolic Computation",
  volume = "32",
  pages = "3-22",
  year = "2001",
  abstract = 
    "Present day computer algebra systems (CASs) and proof assistants
    (PAs) are specialized programs that help humans with mathematical
    computations and deductions. Although several such systems are
    impressive, they all have certain limitations. In most CASs side
    conditions that are essential for the truth of an equality are not
    formulated; moreover there are bugs. The PAs have a limited power for
    computing and hence also for assistance with proofs.  Almost all
    examples of both categories are stand alone special purpose systems
    and therefore they cannot communicate with each other.  
    
    We will argue that the present state of the art in logic is such that
    there is a natural formal language, independent of the special purpose
    application in question, by which these systems can communicate
    mathematical statements. In this way their individual power will be
    enhanced.  
    
    Statements received at one particular location from other sites fall
    into two categories: with or without the qualification ``evidently
    impeccable'', a notion that is methodologically precise and
    sound. For statements having this quality assessment the evidence may
    come from the other site or from the local site itself, but in both
    cases it is verified locally.  In cases where there is no evidence of
    impeccability one has to rely on cross checking. There is a trade-off
    between these two kinds of statements: for impeccability one has to
    pay the price of obtaining less power.  
    
    Some examples of communication forms are given that show how the
    participants benefit",
  paper = "Bare01.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Baue16,
  author = "Bauer, Andrej",
  title = {{Five Stages of Accepting Constructive Mathematics}},
  year = "2016",
  journal = "Bull. of the American Mathematical Society",
  link = "\url{http://dx.doi.org/10.1090/bull/1556}",
  abstract =
    "On the odd day, a mathematician might wonder what constructive
    mathematics is all about. They may have heard arguments in favor of
    constructivism but are not at all convinced by them, and in any case
    they may care little about philosophy. A typical introductory text
    about constructivism spends a great deal of time explaining the
    principles and contains only trivial mathematics, while advanced
    constructive texts are impenetrable, like all unfamiliar mathematics. 
    How then can a mathematician find out what constructive mathematics 
    feels like? What new and relevant ideas does constructive mathematics 
    have to offer, if any? I shall attempt to answer these questions.",
  paper = "Baue16.pdf"
}

@techreport{Baum90,
  author = "Baumgartner, Gerald and Stansifer, Ryan",
  title = {{A Proposal to Study Type Systems for Computer Algebra}},
  institution = "RISC",
  year = "1990",
  type = "technical report",
  number = "90-07.0",
  abstract =
    "It is widely recognized that programming languages should offer
    features to help structure programs. To achieve this goal, languages
    like ADA, MODULA-2, object-oriented languages, and functional
    languages have been developed. The structuring techniques available so
    far (like modules, classes, parametric polymorphism) are still not
    enough or not appropriate for some application areas. In symbolic
    computation, in particular computer algebra, several problems occur
    that are difficult to handle with any existing programming language. 
    Indeed, nearly all available computer algebra systems suffer from the 
    fact that the underlying programming language imposes too many 
    restricitons.
    
    We propose to develop a language that combines the essential features
    from functional language, object-oriented languages, and computer
    algebra systems in a semantically clean manner. Although intended for
    use in symbolic computation, this language should prove interesting as
    a general purpose programming language. The main innovation will be
    the application of sophisticated type systems to the needs of computer
    algebra systems. We will demonstrate the capabilities of the language
    by using it to implement a small computer algebra library. This
    implementation will be compared against a straightforward Lisp
    implementation and against existing computer algebra systems. Our
    development should have an impact both on the programming languages
    world and on the computer algebra world.",
  paper = "Baum90.pdf",
  keywords = "axiomref"
}

@InCollection{Bees89,
  author = "Beeson, Michael J.",
  title = {{Logic and Computation in MATHPERT: An Expert System for
            Learning Mathematics}},
  booktitle = "Computers and Mathematics",
  publisher = "Springer-Verlag",
  year = "1989",
  pages = "299-307",
  isbn = "0-387-97019-3"
}

@article{Bees89a,
  author = "Beeson, Michael",
  title = {{Some Applications of Gentzen's Proof Theory in Automated
            Deduction}},
  journal = "LNCS",
  volume = "475",
  pages = "101-156",
  year = "1989",
  abstract =
    "We show that Prolog is intimately connected with Gentzen's cut-free
    sequent calculus G, analyzing Prolog computations as the construction
    of certain cut-free derivations. We introduce a theorem-proving
    program GENTZEN based on Gentzen's sequent calculus, which
    incorporates some features of Prolog's computation procedure. We show
    that GENTZEN has the following properties: (1) It is
    (non-deterministically) sound and complete for first-order
    intuitionistic predicate calculus; (2) Its successful computations
    coincide with those of Prolog on the Horn clause fragment (both
    deterministically and non-deterministically). The proofs of (1) and
    (2) contain a new proof of the completeness of (non-deterministic)
    Prolog for Horn clause logic, based on our analysis of Prolog in terms
    of Gentzen sequents instead of on resolution. GENTZEN has been
    implemented and tested on examples including some proofs by induction
    in number theory, an example constructed by J. McCarthy to show the
    limitations of Prolog, and ``Schubert's Steamroller.'' An extension of
    GENTZEN also provides a decision procedure for intuitionistic
    propositional calculus (but at some cost in efficiency).",
  paper = "Bees89a.pdf"
}

@article{Bees92,
  author = "Beeson, M.",
  title = {{Mathpert: Computer support for learning algebra, trig, and
            calculus}},
  journal = "LNCS",
  volume = "624",
  pages = "454-456",
  year = "1992",
  abstract =
    "Mathpert is a computerized environment for learning algebra, trig, 
    and one-variable calculus. It permits students to direct the 
    step-by-step solution of problems, and is capable of helping them
    by solving the problems itself if necessary.",
  paper = "Bees92.pdf"
}

@misc{Bees98,
  author = "Beeson, Michael",
  title = {{Automatic Generation of Epsilon-Delta Proofs of Continuity}},
  link = "\url{http://www.michaelbeeson.com/research/papers/aisc.pdf}",
  year = "1998",
  abstract =
    "As part of a project on automatic generation of proofs involving both
    logic and computation, we have automated the production of some proofs
    involving epsilon-delta arguments. These proofs involve two or three
    quantifiers on the logical side, and on the computational side, they
    involve algebra, trigonometry, and some calculus. At the border of
    logic and computation, they involve several types of arguments
    involving inequalities, including transitivity chaining and several
    types of bounding arguments, in which bounds are sought that do not
    depend on certain variables. Control mechanisms have been developed
    for intermixing logical deduction steps with computational steps and
    with inequality reasoning. Problems discussed here as examples involve
    the continuity and uniform continuity of various specific functions.",
  paper = "Bees98.pdf"
}

@article{Benk03,
  author = "Benke, Marcin and Dybjer, Peter and Jansson, Patrik",
  title = {{Universes for generic programs and proofs in dependent type
           theory}},
  journal = "Nordic Journal of Computing",
  volume = "10",
  year = "2003",
  pages = "265-269",
  link = "\url{http://www.cse.chalmers.se/~peterd/papers/generic.html}",
  abstract = 
    "We show how to write generic programs and proofs in {Martin L\"of}
    type theory. To this end we considier several extensions of
    {Martin-L\"of}'s logical framework for dependent types. Each extension
    has a universe of codes (signatures) for inductively defined sets with
    generic formation, introduction, elimination, and equality
    rules. These extensions are modeled on Dybjer and Setzer's finitely
    axiomatized theories of inductive-recursive definitions, which also
    have universese of codes for sets, and generic formation,
    introduction, elimination, and equality rules. Here we consider
    several smaller universes of interest for generic programming and
    universal algebra. We formalize one-sorted and many-sorted term
    algebras, as well as iterated, generalized, parameterized, and indexed
    inductive definitions. We also show how to extend the techniques of
    generic programming to these universes. Furthermore, we give generic
    proofs of reflexivity and substitutivity of a generic equality
    test. Most of the definitions in the paper have been implemented using
    the proof assistant Alfa for dependent type theory.",
  paper = "Benk03.pdf"
}

@inproceedings{Benz01,
  author = "Benzmuller, Christoph and Jamnik, Mateja and Kerber, Manfred
            and Sorge, Volker",
  title = {{An Agent-oriented Approach to Reasoning}},
  booktitle = "Proc. Calculemus Workshop 2001",
  pages = "48-63",
  link = "\url{http://christoph.benzmueller.di/papers/W11.pdf}",
  year = "2001",
  abstract =
    "This paper discusses experiments with an agent oriented approach to
    automated and interactive reasoning. The approach combines ideas from
    two subfields of AI (theorem proving/proof planning and multi-agent
    systems) and makes use of state of the art distribution techniques to
    decentralise and spread its reasoning agents over the internet. It
    particularly supports cooperative proofs between reasoning systems
    which are strong in different application areas, e.g., higher-order
    and first-order theorem provers and computer algebra systems.",
  paper = "Benz01.pdf"
}

@book{Bert04,
  author = {Bertot, Yves Cast\'eran, Pierre},
  title = {{Interactive Theorem Proving and Program Development}},
  publisher = "Springer",
  year = "2004",
  isbn = "3-540-20854-2",
  abstract = "
    Coq is an interactive proof assistant for the development of
    mathematical theories and formally certified software. It is based on
    a theory called the calculus of inductive constructions, a variant of
    type theory.

    This book provides a pragmatic introduction to the development of
    proofs and certified programs using Coq. With its large collection of
    examples and exercies it is an invaluable tool for researchers,
    students, and engineers interested in formal methods and the
    development of zero-fault software."
}

@book{Bido91,
  author = "Bidoit, M. and Kreowski, H.-J. and Lescanne, P. and
            Orejas, F. and Sannella, D.",
  title = {{Algebraic System Specification and Development}},
  publisher = "Springer-Verlag",
  comment = "LNCS 501",
  year = "1991",
  isbn = "3-540-54060-1",
  abstract =
    "A great deal of work has been devoted to methods of specification
    based on the simple idea that a functional program can be modelled as
    a {\sl many-sorted algebra}, i.e. as a number of sets of data values
    (one set of values for each data type) together with a number of total
    functions on those sets corresponding to the functions in the
    program. This abstracts away from the algorithms used to compute the
    functions and how those algorithms are expressed in a given
    programming language, focusing instead on the representation of data
    and the input/output behavior of functions. The pioneering work in
    this area is [Zil 74],[Gut 75], [GTW 76], of which the latter -- the
    so-called {\sl initial algebra approach} -- is the most formal This
    idea was soon taken up by other workers, see e.g. [GGM 76], [GHM 76], 
    [BG 77], [GHM 78]. Today the field of algebraic specification has
    grown into one of the major areas of research in theoretical computer
    science. More than fifteen years of research have led to an abundance
    of competing and complementary theories and approaches. The algebraic
    approach provides a conceptual basis, theoretical fundations, and
    prototype tools for the stepwise formal development of correct system
    components from their specifications, and thus covers the whole
    software development process from the specification of requirements to
    the finished system. These methos are potentially applicable to the
    development of correct hardware systems as well",
  keywords = "axiomref, CAS-Proof, printed"

}

@article{Bled74,
  author = "Bledsoe, W.W. and Bruell, Peter",
  title = {{A Man-Machine Theorem-Proving System}},
  journal = "Artificial Intelligence",
  volume = "5",
  number = "1",
  pages = "51-72",
  year = "1974",
  abstract =
    "This paper describes a man-machine theorem-proving system at the
    University of Texas at Austin which has been used to prove a few
    theorems in general topology. The theorem (or subgoal) being proved is
    presented on the scope in its natural form so that the user can easily
    comprehend it and, by a series of interactive commands, can help with
    the proof when he desires. A feature called DETAIL is employed which
    allows the human to interact only when needed and only to the extent
    necessary for the proof.
    
    The program is built around a modified form of IMPLY, a
    natural-deduction-like theorem proving technique which has been
    described earlier.",
  paper = "Bled74.pdf"
}

@techreport{Bled79,
  author = "Bledsoe, W.W. and Bruell, P. and Shostak, R.",
  title = {{A Prover for General Inequalities}},
  type = "technical report",
  institution = "Univ. of Texas at Austin",
  year = "1979",
  number = "ATP-40A"
}

@techreport{Bled83,
  author = "Bledsoe, W.W.",
  title = {{The UT Natural Deduction Prover}},
  type = "technical report",
  institution = "Univ. of Texas at Austin",
  year = "1983",
  number = "ATP-17B"
}  

@inproceedings{Bled84,
  author = "Bledsoe, W.W.",
  title = {{Some Automatic Proofs in Analysis}},
  booktitle = "Automated Theorem Proving: After 25 Years",
  publisher = "American Mathematical Society",
  year = "1984"
}

@misc{Bold07,
  author = "Boldo, Sylvie and Filliatre, Jean-Christophe",
  title = {{Formal Verification of Floating-Point programs}},
  link = "\url{http://www-lipn.univ-paris13.fr/CerPAN/files/ARITH.pdf}",
  paper = "Bold07.pdf"
}

@misc{Bold07a,
  author = "Boldo, Sylvie and Filliatre, Jean-Christophe",
  title = {{Formal Verification of Floating-Point programs}},
  link = "\url{http://www.lri.fr/~filliatr/ftp/publis/caduceus-floats.pdf}",
  abstract =
    "This paper introduces a methodology to perform formal verification of
    floating-point C programs.  It extends an existing tool for
    verification of C programs, Caduceus, with new annotations for
    specific floating-point arithmetic. The Caduceus first-order logic
    model for C programs is extended accordingly. Then verification
    conditions are obtained in the usual way and can be discharged
    interactively with the Coqa proof assistant, using an existing Coq
    formalization of floating-point arithmetic. This methodology is
    already implemented and has been successfully applied to several short
    floating-point programs, which are presented in this paper.",
  paper = "Bold07a.pdf"
}

@article{Bold11,
  author = "Boldo, Sylvie and Marche, Claude",
  title = {{Formal verification of numerical programs: from C annotated
           programs to mechanical proofs}},
  year = "2011",
  publisher = "Springer",
  journal = "Mathematics in Computer Science",
  volume = "5",
  pages = "377-393",
  link = "\url{https://hal.archives-ouvertes.fr/hal-00777605/document}",
  abstract = 
    "Numerical programs may require a high level of guarantee. This can be
    achieved by applying formal methods, such as machine-checked proofs.
    But these tools handle mathematical theorems while we are interested
    in C code, in which numerical computations are performed using
    floating-point arithmetic, whereas proof tools typically handle exact
    real arithmetic. To achieve this high level of confidence on C programs,
    we use a chain of tools: Frama-C, its Jessie plugin, Why and provers
    among Coq, Gappa, Alt-Ergo, CVC3 and Z3. This approach requires the C
    program to be annotated; each function must be precisely specified, and
    we prove the correctness of the program by proving both that it meets its
    specifications and that no runtime error may occur. The purpose of this
    paper is to illustrate, on various examples, the features of this 
    approach.",
  paper = "Bold11.pdf"
}

@misc{Bold16,
  author = "Boldo, Sylvie",
  title = {{Formal verification of numerical analysis programs}},
  year = "2016",
  link = "\url{https://www.youtube.com/watch?v=7MDwpwD6Ts4}"
}

@inproceedings{Boni07,
  author = "Bonichon, R. and Delahaye, D. and Doligez, D.",
  title = {{Zenon: An Extensible Automated Theorem Prover Producing 
           Checkable Proofs}},
  booktitle = "LPAR 2007",
  year = "2007",
  link = "\url{http://zenon.inria.fr/zenlpar07.pdf}",
  abstract =
    "We present Zenon, an automated theorem prover for first order
    classical logic (with equality), based on the tableau method. Zenon is
    intended to be the dedicated prover of the Focal environment, an
    object-oriented algebraic specification and proof system, which is
    able to produce OCaml code for execution and Coq code for
    certification. Zenon can directly generate Coq proofs (proof scripts
    or proof terms), which can be reinserted in the Coq specifications
    produced by Focal. Zenon can also be extended, which makes specific
    (and possibly local) automation possible in Focal.",
  paper = "Boni07,pdf"
}

@misc{Boul00,
  author = "Boulme, S. and Hardin, T. and Rioboo, R.",
  title = {{Polymorphic Data Types, Objects, Modules and Functors,: 
           is it too much?}},
  link = "\url{ftp://ftp.lip6.fr/lip6/reports/2000/lip6.2000.014.ps.gz}",
  abstract = "
    Abstraction is a powerful tool for developers and it is offered by
    numerous features such as polymorphism, classes, modules, and
    functors, $\ldots$ A working programmer may be confused by this
    abundance. We develop a computer algebra library which is being
    certificed. Reporting this experience made with a language (Ocaml)
    offering all these features, we argue that the are all needed
    together. We compare several ways of using classes to represent
    algebraic concepts, trying to follow as close as possible mathematical
    specification. Then we show how to combine classes and modules to
    produce code having very strong typing properties.  Currently, this
    library is made of one hundred units of functional code and behaves
    faster than analogous ones such as Axiom.",
  paper = "Boul00.pdf",
  keywords = "axiomref"
}

@techreport{Boul00a,
  author = "Boulme. Sylvain",
  title = {{Specifying in Coq inheritance used in Computer Algebra
            Libraries}},
  year = "2000",
  institution = "LIP6, Paris",
  number = "2000.013",
  abstract =
    "This paper is part of FOC[3], a project for developing Computer
    Algebra libraries, certified in Coq[2]. FOC has developed a
    methodology for programming Computer Algebra libraries, using modules
    and objects in Ocaml. In order to specify modularity features used by
    FOC in Ocaml, we are coding in Coq a theory for extensible records
    with dependent fields. This theory intends to express especially the
    kind of inheritance with method redefinition and late binding, that
    FOC uses in its Ocaml programs.
    
    The unit of FOC are coded as records. As we want to encode sematic
    information on units, the fields of our records may be proofs. Thus,
    our fields may depend on each others. We call the 
    {\sl Drecords}. Then, we introduce a new datatype, called
    {\sl mixDrec}, to represent FOC classes. Actually, mixDrecs are useful
    for describing a hierarchy of Drecords in an incremental way. In
    mixDrecs, fields can be only declared or they can be
    redefined. MixDrecs can be extended by inheritance.",
  paper = "Boul00a.pdf",
  keywords = "printed"
}

@InProceedings{Boul99,
  author = "Boulme, S. and Hardin, T. and Hirschkoff, D. and Rioboo, Renaud",
  title = {{On the way to certify Computer Algebra Systems}},
  booktitle = "Systems for integrated computation and deduction",
  series = "Calculemus 99",
  year = "1999",
  publisher = "Elsevier",
  location = "Trento, Italy",
  pages = "11-12",
  abstract = 
    "The FOC project aims at supporting, within a coherent software system,
    the entire process of mathematical computation, starting with proved
    theories, ending with certified implementations of algorithms. In this
    paper, we explain our design requirements for the implementation,
    using polynomials as a running example. Indeed, proving correctness of
    implementations depends heavily on the way this design allows
    mathematical properties to be truly handled at the programming level.

    The FOC project, started at the fall of 1997, is aimed to build a
    programming environment for the development of certified symbolic
    computation. The working languages are Coq and Ocaml. In this paper,
    we present first the motivations of the project. We then explain why
    and how our concern for proving properties of programs has led us to
    certain implementation choices in Ocaml. This way, the sources express
    exactly the mathematical dependencies between different structures.
    This may ease the achievement of proofs.",
  paper = "Boul99.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@techreport{Boul94,
  author = "Boulton, Richard John",
  title = {{Efficiency in a fully-expansive Theorem Prover}},
  year = "1994",
  type = "technical report",
  number = "UCAM-CL-TR-337",
  institution = "University of Cambridge",
  abstract =
    "The HOL system is a fully-expansive theorem prover: Proofs generated
    in the system are composed of applications of the primitive inference
    rules of the underlying logic. This has two main advantages. First,
    the soundness of the system depends only on the implementations of the
    primitive rules. Second, users can be given the freedom to write their
    own proof procedures without the risk of making the system unsound. A
    full functional programming language is provided for this purpose. The
    disadvantage with the approach is that performance is
    compromised. This is partly due to the inherent cost of fully
    expanding a proof but, as demonstrated in this thesis, much of the
    observed inefficiency is due to the way the derived proof procedures
    are written. This thesis seeks to identify sources of non-inherent
    inefficiency in the HOL system and proposes some general-purpose and
    some specialised techniques for eliminating it. One area that seems to
    be particularly amenable to optimisation is equational reasoning. This
    is significant because equational reasoning constitutes large portions
    of many proofs. A number of techniques are proposed that transparently
    optimise equational reasoning. Existing programs in the HOL system
    require little or no modification to work faster. The other major
    contribution of this thesis is a framework in which part of the
    computation involved in HOL proofs can be postponed. This enables
    users to make better use of their time. The technique exploits a form
    of lazy evaluation. The critical feature is the separation of the code
    that generates the structure of a theorem from the code that justifies
    it logically. Delaying the justification allows some non-local
    optimisations to be performed in equational reasoning. None of the
    techniques sacrifice the security of the fully-expansive approach. A
    decision procedure for a subset of the theory of linear arithmetic is
    used to illustrate many of the techniques. Decision procedures for
    this theory are commonplace in theorem provers due to the importance
    of arithmetic reasoning. The techniques described in the thesis have
    been implemented and execution times are given. The implementation of
    the arithmetic procedure is a major contribution in itself. For the
    first time, users of the HOL system are able to prove many arithmetic
    lemmas automatically in a practical amount of time (typically a second
    or two). The applicability of the techniques to other fully-expansive
    theorem provers and possile extensions of the ideas are considered."
}
    
@misc{Bove08,
  author = "Bove, Ana and Dybjer, Peter",
  title = {{Dependent Types at Work}},
  year = "2008",
  comment = "Lecture notes from LerNET Summer School, Piriapolis",
  link = 
    "\url{http://www.cse.chalmers.se/~peterd/papers/DependentTypesAtWork.pdf}",
  abstract =
    "In these lecture notes we give an introduction to functional
    programming with dependent types. We use the dependently typed
    programming language Agda which is an extension of {Martin-L\"of} type
    theory. First we show how to do simply typed functional programming in
    the style of Haskell and ML. Some differences between Agda's type
    system and the Hindley-Milner type system of Haskell and ML are also
    discussed. Then we show how to use dependent types for programming and
    we explain the basic ideas behind type-checking dependent types. We go
    on to explain the Curry-Howard identification of propositions and
    types. This is what makes Agda a programming logic and not only a
    programming language. According to Curry-Howard, we identify programs
    and proofs, something which is possible only by requiring that all
    programs terminate. However, at the end of these notes we present a
    method for encoding partial and general recursive functions as total
    functions using dependent types.",
  paper = "Bove08.pdf"
}

@misc{Boye81,
  author = "Boyer, Bob and Kaufmann, Matt and Moore, J",
  title = {{Metafunctions in Nqthm and Acl2}},
  link = "\url{https://www.cs.utexas.edu/users/boyer/meta-nqthm-acl2.text}",
  year = "1981"
}

@techreport{Boye85,
  author = "Boyer, Robert S. and Moore, J Strother",
  title = {{Integrating Decision Procedures into Heuristic Theorem Provers}},
  type = "technical report",
  institution = "Inst. for Comp. Sci. University of Texas at Austin",
  number = "ICSCA-CMP-44",
  year = "1985",
  abstract =
    "We discuss the problem of incorporating into a heuristic theorem
    prover a decision procedure for a fragment of the logic.  An obvious
    goal when incorporating such a procedure is to reduce the search space
    explored by the heuristic component of the system, as would be
    achieved by eliminating from the system’s data base some explicitly
    stated axioms.  For example, if a decision procedure for linear
    inequalities is added, one would hope to eliminate the explicit
    consideration of the transitivity axioms.  However, the decision
    procedure must then be used in all the ways the eliminated axioms
    might have been.  The difficulty of achieving this degree of
    integration is more dependent upon the complexity of the heuristic
    component than upon that of the decision procedure.  The view of the
    decision procedure as a ``black box'' is frequently destroyed by the
    need pass large amounts of search strategic information back and forth
    between the two components.  Finally, the efficiency of the decision
    procedure may be virtually irrelevant; the efficiency of the final
    system may depend most heavily on how easy it is to communicate
    between the two components.  This paper is a case study of how we
    integrated a linear arithmetic procedure into a heuristic theorem
    prover.  By linear arithmetic here we mean the decidable subset of
    number theory dealing with universally quantified formulas composed of
    the logical connectives, the identity relation, the Peano ``less than''
    relation, the Peano addition and subtraction functions, Peano
    constants, and variables taking on natural values.  We describe our
    system as it originally stood, and then describe chronologically the
    evolution of our linear arithmetic procedure and its interface to the
    heuristic theorem prover.  We also provide a detailed description of
    our final linear arithmetic procedure and the use we make of it.  This
    description graphically illustrates the difference between a
    stand-alone decision procedure and one that is of use to a more
    powerful theorem prover.",
  paper = "Boye85.pdf"
}

@book{Boye88,
  author = "Boyer, R.S. and Moore, J.S.",
  title = {{A Computational Logic Handbook}},
  publisher = "Academic Press",
  year = "1988"
}

@book{Brad07,
  author = "Bradley, Aaron R. and Manna, Zohar",
  title = {{The Calculus of Computation}},
  year = "2007",
  publisher = "Springer",
  isbn = "978-3-540-74112-1",
}

@article{Bres93,
  author = "Bressoud, David",
  title = {{Review of The problems of mathematics}},
  journal = "Math. Intell.",
  volume = "15",
  number = "4",
  year = "1993",
  pages = "71-73"
}

@article{Brow12,
  author = "Brown, Christopher W.",
  title = {{Fast simplifications for Tarski formulas based on monomial
           inequalities}},
  year = "2012",
  journal = "Journal of Symbolic Computation",
  volume = "47",
  pages = "859-882",
  abstract =
    "We define the ‘‘combinatorial part’’ of a Tarski formula in which
    equalities and inequalities are in factored or partially-factored
    form. The combinatorial part of a formula contains only
    ‘‘monomial inequalities’’,which are sign conditions on monomials.  We 
    give efficient algorithms for answering some basic questions about
    conjunctions of monomial inequalities and prove the
    NP-Completeness/Hardness of some others. By simplifying the
    combinatorial part of a Tarski formula, and mapping the simplified
    combinatorial part back to a Tarski formula, we obtain non-trivial
    simplifications without algebraic operations.",
  paper = "Brow12.pdf"
}

@article{Broy88,
  author = "Broy, Manfried",
  title = {{Equational Specification of Partial Higher-order Algebras}},
  journal = "Theoretical Computer Science",
  volume = "57",
  number = "1",
  year = "1988",
  pages = "3-45",
  abstract =
    "The theory of algebraic abstract types specified by positive
    conditional formulas formed of equations and a definedness predicate
    is outlined and extended to hierarchical types with ``noustrict''
    operations, partial and even infinite objects. Its model theory is
    based on the concept of partial interpretations. Deduction rules are
    given, too. Models of types are studied where all explicit equations
    have solutions. The inclusion of nigher-order types, i.e., types
    comprising higher-order functions leads to an algebraic (``equational'')
    specification of algebras including sorts with ``infinite'' objects and
    higher-order functions (``functionals'').",
  paper = "Broy88.pdf"
}

@article{Brui68,
  author = "de Bruijn, N.G.",
  title = {{The Mathematical Language Automath, its Usage, and Some of
           its Extensions}},
  journal = "Lecture Notes in Mathematics",
  publisher = "Springer",
  year = "1994",
  volume = "125",
  paper = "Brui68.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Buch97,
  author = "Buchberger, Bruno",
  title = {{Mathematica: doing mathematics by computer?}},
  journal = "Advances in the design of symbolic computation systems",
  year = "1997",
  publisher = "Springer-Verlag",
  pages = "2-20",
  isbn = "978-3-211-82844-1",
  paper = "Buch97.pdf",
  keywords = "printed"
}

@article{Buch01,
  author = "Buchberger, Bruno",
  title = {{Theorema: A Proving System Based on Mathematica}},
  journal = "The Mathematica Journal",
  volume = "8",
  number = "2",
  pages = "247-252",
  year = "2001",
  paper = "Buch01.pdf",
  keywords = "printed"
}

@misc{Buch00,
  author = "Buchberger, B. and Dupre, C. and Jebelean, T. and Kriftner, F.
            and Nakagawa, K. and Vasaru, D. and Windsteiger, W.",
  title = {{The Theorema Project: A Progress Report}},
  year = "2000",
  abstract =
    "The THEOREMA project aims at supporting, within one consistent logic
    and one coherent software system, the entire mathematical exploration
    cycle including the phase of proving. In this paper we report on some
    of the new features of THEOREMA that have been designed and
    implemented since the first expository version of THEOREMA in
    1997. These features are: - the THEOREMA formal text language - the
    THEOREMA computational sessions - the Prove-Compute-Solve (PCS) prover
    of THEOREMA - the THEOREMA set theory prover - special provers within
    THEOREMA - the cascade-meta-strategy for THEOREMA provers - proof
    simplification in THEOREMA. In the conclusion, we formulate design
    goals for the next version of THEOREMA",
  paper = "Buch00.pdf"
}

@article{Buch06,
  author = "Buchberger,B and Craciun, A. and Jebelean, T. and 
            Kovacs, L. and Kutsia, T. and Nakagawa, K. and Piroi, F.
            and Popov, N. and Robu, J. and Rosenkranz, M. and
            Windsteiger, W.",
  title = {{Theorema: Towards Computer-Aided Mathematical Theory
            Exploration}}, 
  journal = "J. of Applied Logic",
  volume = "4",
  number = "4",
  pages = "470-504",
  year = "2006",
  abstract =
    "Theorema is a project that aims at supporting the entire process of
    mathematical theory exploration within one coherent logic and software
    system. This survey paper illustrates the style of Theorema-supported
    mathematical theory exploration by a case study (the automated
    synthesis of an algorithm for the construction of Groebner Bases) and
    gives an overview on some reasoners and organizational tools for
    theory exploration developed in the Theorema project.",
  paper = "Buch06.pdf",
  keywords = "printed"
}

@misc{Buch03,
  author = "Buchberger, Bruno and Aigner, K. and Dupre, C. and
            Jebelean, T. and Kriftner, F. and Marin, M. and
            Nakagawa, K. and Podisor, O. and Tomuta, E. and
            Usenko, Y. and Vasaru, D. and Windsteiger, W.",
  title = {{Theorema: An Integrated System for Computation and
            Deduction in Natural Style}},
  abstract =
    "The Theorema project aims at integrating computation and deduction in
    a system that can be used by the working scientist for building and
    checking mathematical models, including the design and verication of
    new algorithms. Currently, the system uses the rewrite engine of the
    computer algebra system Mathematica for building and combining a
    number of automatic/interactive provers (high-order predicate-logic,
    induction for lists/tuples and natural numbers, etc.) in natural
    deduction style and in natural language presentation. These provers
    can be used for dening and proving properties of mathematical models
    and algorithms, while a specially provided ``computing engine'' can
    execute directly the logical description of these algorithms.",
  paper = "Buch03.pdf",
  keywords = "printed"
}

@article{Buch16,
  author = "Buchberger, Bruno and Jebelean, Tudor and Kutsia, Temur
           and Maletzky, Alexander and Windsteiger, Wolfgang",
  title = {{Theorema 2.0: Computer-Assisted Natural-Style Mathematics}},
  journal = "J. of Formalized Reasoning",
  volume = "9",
  number = "1",
  pages = "149-155",
  year = "2016",
  abstract =
    "The Theorema project aims at the development of a computer assistant
    for the working mathematician. Support should be given throughout all
    phases of mathematical activity, from introducing new mathematical
    concepts by definitions or axioms, through first (computational)
    experiments, the formulation of theorems, their justification by an
    exact proof, the application of a theorem as an algorithm, until to
    the dissemination of the results in form of a mathematical
    publication, the build up of bigger libraries of certified
    mathematical content and the like. This ambitious project is exactly
    along the lines of the QED manifesto issued in 1994 and it was
    initiated in the mid-1990s by Bruno Buchberger. The Theorema system is
    a computer implementation of the ideas behind the Theorema
    project. One focus lies on the natural style of system input (in form
    of definitions, theorems, algorithms, etc.), system output (mainly in
    form of mathematical proofs) and user interaction. Another focus is
    theory exploration, i.e. the development of large consistent
    mathematical theories in a formal frame, in contrast to just proving
    single isolated theorems. When using the Theorema system, a user
    should not have to follow a certain style of mathematics enforced by
    the system (e.g. basing all of mathematics on set theory or certain
    variants of type theory), rather should the system support the user in
    her preferred flavour of doing math. The new implementation of the
    system, which we refer to as Theorema 2.0, is open-source and
    available through GitHub.",
  paper = "Buch16.pdf",
  keywords = "printed"
}

@article{Bulo04,
  author = {Medina-Bulo, I. and Palomo-Lozano, F. and Alonso-Jim\'enez, J.A.
           and Ruiz-Reina, J.L.},
  title = {{Verified Computer Algebra in ACL2}},
  journal = "ASIC 2004, LNAI 3249",
  year = "2004",
  pages = "171-184",
  abstract = "In this paper, we present the formal verification of a
    Common Lisp implementation of Buchberger's algorithm for computing
    Groebner bases of polynomial ideals. This work is carried out in the
    ACL2 system and shows how verified Computer Algebra can be achieved
    in an executable logic.",
  paper = "Bulo04.pdf"
}

@article{Bulo10,
  author = "Medina-Bulo, I. and Palomo-Lozano, F. and Alonso-Jim\'enez, J.A.
           and Ruiz-Reina, J.L.",
  title = {{A verified Common Lisp implementation of Buchberger's algorithm
           in ACL2}},
  journal = "Journal of Symbolic Computation",
  year = "2010",
  pages = "96-123",
  abstract = "In this article, we present the formal verification of a
    Common Lisp implementation of Buchberger's algorithm or computing
    Groebner bases of polynomial ideals. This work is carried out in ACL2,
    a system which provices an integrated environment where programming
    (in a pure functional subset of Commmon Lisp) and formal verification
    of programs, with the assistance of a theorem prover, are possible. Our
    imlementation is written in a real programming language and it is
    directly executable within the ACL2 system or any compliant Common Lisp
    system. We provide here snippets o real verified code, discuss the
    formalization details in depth, and present quantitative data about
    the proof effort.",
  paper = "Bulo10.pdf"
}

@inproceedings{Bund75,
  author = "Bundy, Alan and Wallen, Lincoln",
  title = {{The UT Theorem Prover}},
  booktitle = "Catalogue of Artificial Intelligence Tools",
  pages = "132-133",
  year = "1975",
  abstract =
    "The UT theorem prover is probably the best known natural deduction
    <153> theorem prover. It was written in LISP <34> by woody Bledsoe and
    his co-workers at the University of Texas, and is best described in
    (Bledsoe and Tyson 75]. The theorem prover embodies a Gentzen-like
    deduction system for first-order predicate calculus, and many special
    purpose techniques, including: subgoaling, rewrite rules, controlled
    lorward chaining, controlled definition instantiation, conditional
    procedures, and induction. The prover, though powerful in its own
    right, is essentially interactive and thus allows the user of the
    prover to control the search for the proof in radical ways. The user
    can for example : add hypotheses, instruct the prover to instantiate
    certain variables with values, or instruct the prover as to which
    deduction rule to use next."
}

@article{Bund88,
  author = "Bundy, Alan",
  title = {{The Use of Explicit Plans to Guide Inductive Proofs}},
  journal = "LNCS 310",
  volume = "310",
  pages = "111-120",
  year = "1998",
  abstract =
    "We propose the use of explicit proof plans to guide the search for a
    proof in automatic theorem proving. By representing proof plans as the
    specifications of LCF-like tactics and by recording these
    specifications in a sorted meta-logic, we are able to reason about the
    conjectures to be proved and the methods available to prove them. In
    this way we can build proof plans of wide generality, formally account
    for and predict their successes and failures, apply them flexibly,
    recover from their failures, and learn them from example proofs.
    
    We illustrate this technique by building a proof plan based on a
    simple subset of the implicit proof plan embedded in the Boyer-Moore
    theorem prover.",
  paper = "Bund88.pdf"
}

@article{Bund90,
  author = "Bundy, Alan and van Harmelen, Frank and Horn, Christian and
            Smaill, Alan",
  title = {{The Oyster-Clam System}},
  journal = "Lecture Notes in Artificial Intelligence",
  volume = "449",
  year = "1990",
  pages = "647-648",
  paper = "Bund90.pdf"
}

@article{Bund93b,
  author = "Bundy, Alan and Stevens, Andrew and van Hemelen, Frank and
            Ireland, Andrew and Smaill, Alan",
  title = {{Rippling: A heuristic for guiding inductive proofs}},
  journal = "Artifical Intelligence",
  volume = "62",
  number = "2",
  year = "1993",
  pages = "185-253",
  abstract =
    "We describe rippling: a tactic for the heuristic control of the key
    part of proofs by mathematical induction. This tactic significantly
    reduces the search for a proof of a wide variety of inductive
    theorems. We first present a basic version of rippling, followed by
    various extensions which are necessary to capture larger classes of
    inductive proofs. Finally, we present a generalised form of rippling
    which embodies these extensions as special cases. We prove that
    generalised rippling always terminates, and we discuss the
    implementation of the tactic and its relation with other inductive
    proof search heuristics.",
  paper = "Bund93b.pdf"
}

@misc{Byrd17,
  author = "Byrd, William",
  title = {{The Most Beautiful Program Ever Written}},
  link = "\url{https://www.youtube.com/watch?v=OyfBQmvr2Hc}",
  comment = "See miniKanren and Barliman (program synthesis with proof)"
}

@inproceedings{Calm95,
  author = "Calmet, Jacques and Homann, Karsten",
  title = {{Distributed Mathematical Problem Solving}},
  booktitle = "4th Bar-Han SYmp. on Foundations of Artificial Intelligence",
  pages = "220-230",
  year = "1995",
  abstract =
    "Coupling computer algebra systems and theorem provers enables to
    extend the capabilities they have when standing alone. We report on an
    ongoing research project whose long term goal is to provide an open
    environment for doing mathematics including reasoners and symbolic
    calculators. It is extensible by users which can construct complex
    systems by combination and insertion of existing packages. These
    systems may be based on different logics, formalisms, data structures,
    interfaces. A result of this work is illustrated by a prototype
    implementation of an interface between Isabelle and Maple.",
  paper = "Calm95.pdf",
  keywords = "printed"
}

@inproceedings{Calm96a,
  author = "Calmet, Jacques and Homann, Karsten",
  title = {{Proofs in Computational Algebra: An Interface between DTP
           and Magma}},
  booktitle = "Proc. 2nd Magma Conf. on Computational Algebra",
  year = "1996",
  comment = "Extended Abstract",
  link = "\url{https://pdfs.semanticscholar.org/c709/338dfde245e638690bc7414d8a191eae3a82.pdf}",
  paper = "Calm96a.pdf"
}

@article{Calm97a,
  author = "Calmet, Jacques and Homann, Karsten",
  title = {{Towards the Mathematics Software Bus}},
  journal = "Theoretical Computer Science",
  volume = "197",
  number = "1-2",
  year = "1997",
  pages = "221-230",
  abstract =
    "The Mathematics Software Bus is a software environment for combining
    heterogeneous systems performing any kind of mathematical
    computation. Such an environment will provide combinations of
    graphics, editing and computation tools through interfaces to already
    existing powerful software by flexible and powerful semantically
    integration.
    
    Communication and cooperation mechanisms for logical and symbolic
    computation systems enable to study and solve new classes of problems
    and to perform efficient computation in mathematics through
    cooperating specialized packages.
    
    We give an overview on the need for cooperation in solving
    mathematical problems and illustrate the advantages by several
    well-known examples. The needs and requirements for the Mathematics
    Software Bus and its architecture are demonstrated through some
    implementations of powerful interfaces between mathematical services.",
  paper = "Calm97a.pdf",
  keywords = "printed"
}

@book{Cann05,
  author = "Cannon, John and Bosma, Wieb",
  title = {{Handbook of Magma Functions}},
  year = "2005",
  publisher = "University of Sydney, School of Math and Statistics"
}

@article{Capr01,
  author = "Caprotti, Olga and Oostdijk, Martijn",
  title = {{Formal and Efficient Primality Proofs by Use of Computer
           Algebra Oracles}},
  journal = "J. Symbolic Computation",
  volume = "32",
  pages = "55-70",
  year = "2001",
  abstract =
    "This paper focuses on how to use Pocklington's criterion to
    produce efficient formal proof-objects for showing primality of
    large positive numbers. First, we describe a formal development of
    Pocklington's criterion, done using the proof assistant Coq. Then
    we present an algorithm in which computer algebra software is
    employed as oracle to the proof assistant to generate the
    necessary witnesses for applying the criterion. Finally, we
    discuss the implementation of this approach and tackle the proof
    of primality for some of the largest numbers expressible in Coq.",
  paper = "Capr01.pdf"
}

@misc{Cast16,
  author = "Casteran, Pierre and Sozeau, Mattieu",
  title = {{A Gentle Introduction to Type Classses and Relations in Coq}},
  year = "2016",
  link = "\url{http://www.labri.fr/perso/casteran/CoqArt/TypeClassesTut/typeclassestut.pdf}",
  paper = "Cast16.pdf"
}

@article{Care08,
  author = "Carette, Jacques and Farmer, William M.",
  title = {{High-Level Theories}},
  journal = "LNCS",
  volume = "5144",
  pages = "232-245",
  year = "2008",
  abstract =
    "We introduce high-level theories in analogy with high-level
    programming languages. The basic point is that even though one can
    define many theories via simple, low-level axiomatizations , that is
    neither an effective nor a comfortable way to work with such theories.
    We present an approach which is closer to what users of mathematics
    employ, while still being based on formal structures.",
  paper = "Care08.pdf",
  keywords = "printed"
}

@article{Care09,
  author = "Carette, Jacques and Farmer, William M.",
  title = {{A Review of Mathematical Knowledge Management}},
  journal = "LNCS",
  volume = "5625",
  pages = "233-246",
  year = "2009",
  abstract = 
    "Mathematical Knowledge Management (MKM), as a field, has seen
    tremendous growth in the last few years. This period was one where
    many research threads were started and the field was defining
    itself. We believe that we are now in a position to use the MKM body
    of knowledge as a means to define what MKM is, what it worries about,
    etc. In this paper, we review the literature of MKM and gather various
    metadata from these papers. After offering some definitions
    surrounding MKM, we analyze the metadata we have gathered from these
    papers, in an effort to cast more light on the field of MKM and its
    evolution",
  paper = "Care09.pdf",
  keywords = "printed"
}

@misc{Care11a,
  author = "Carette, Jacques and Farmer, William M. and Jeremic, Filip and
            Maccio, Vincent and O'Connor, Russell and Tran, Quang M.",
  title = {{The MathScheme Library: Some Preliminary Experiments}},
  year = "2011",
  link = "\url{https://arxiv.org/pdf/1106.1862.pdf}",
  abstract =
    "We present some of the experiments we have performed to best test our
    design for a library for MathScheme, the mechanized mathematics
    software system we are building. We wish for our library design to use
    and reflect, as much as possible, the mathematical structure present
    in the objects which populate the library.",
  paper = "Care11a.pdf",
  keywords = "axiomref, printed"
}

@misc{Care11b,
  author = "Carette, Jacques and Farmer, William M. and Wajs, Jeremie",
  title = {{Trustable Communication Between Mathematics Systems}},
  year = "2011",
  link = "\url{https://pdfs.semanticscholar.org/0d0b/206edf7ef1c01d7bfa1284c85b469b2fbd29.pdf}",
  abstract =
    "This paper presents a rigorous, unified framework for facilitating
    communication between mathematics systems. A mathematics system is
    given one or more interfaces which offer deductive and computational
    services to other mathematics systems. To achieve communication 
    between systems, a client interface is linked to a server interface by
    an asymmetric connection consisting of a pair of translations.
    Answers to requests are trustable in the sense that they are correct
    provided a small set of prescribed conditions are satisfied. The
    frame work is robust with respect to interface extension and can
    process requests for abstract services, where the server interface is
    not fully specified.",
  paper = "Care11b.pdf",
  keywords = "printed"
}

@misc{Care11c,
  author = "Carette, Jacques and Farmer, William M. and O'Connor, Russell",
  title = {{MathScheme: Project Description}},
  year = "2011",
  link = "\url{http://imps.mcmaster.ca/doc/cicm-2011-proj-desc.pdf}",
  paper = "Care11c.pdf",
  keywords = "printed"
}

@article{Care12,
  author = "Carette, Jacques and O'Connor, Russell",
  title = {{Theory Presentation Combinators}},
  journal = "LNCS",
  volume = "7362",
  year = "2012",
  abstract = 
    "We motivate and give semantics to theory presentation combinators
    as the foundational building blocks for a scalable library of
    theories. The key observation is that the category of contexts and
    fibered categories are the ideal theoretical tools for this
    purpose.",
  paper = "Care12.pdf",
  keywords = "printed"
}

@article{Chli10,
  author = "Chlipala, Adam",
  title = {{An Introduction to Programming and Proving with Dependent Types
           in Coq}},
  journal = "Journal of Formalized Reasoning",
  volume = "3",
  number = "2",
  pages = "1-93",
  year = "2010",
  paper = "Chli10.pdf"
}

@misc{Chli14,
  author = "Chlipala, Adam and Braibant, Thomas and Cuellar, Santiago and 
            Delaware, Benjamin and Gross, Jason and Malecha, Gregory and 
            Clement,-Claudel, Pit and Wang, Peng",
  title =
    {{Bedrock: A Software Development Ecosystem Inside a Proof Assistant}},
  year = "2014",
  link = "\url{https://www.youtube.com/watch?v=BSyrp-iYBMo}",
  abstract =
    "The benefits of formal correctness proofs for software are clear
    intuitively, but the high human costs of proof construction have
    generally been viewed as prohibitive. To support that integration, we
    need to rethink the familiar programming toolchains. The new world
    needn't be all about doing prodigious extra work to achieve the virtue
    of correct programs; formal methods also suggest new programming
    approaches that better support abstraction and modularity than do
    coarser-grained specification styles like normal static types. This
    talk overviews Bedrock, a framework for certified programming inside
    of the Coq proof assistant. Bedrock programs are implemented,
    specified, verified, and compiled inside of Coq. A single program may
    be divided into modules with formal interfaces, written in different
    programming languages and verified with different proof styles. The
    common foundation is an assembly language with an operational
    semantics (serving as the trusted code base) and a semantic module
    system (orchestrating linking of code and proofs across source
    languages). A few different programming styles have been connects to
    the shared foundation, including a C-like language with an ``array of
    bytes'' memory model, higher-level more C++-like languages with ``array
    of abstract data types'' memory models, a domain-specific language for
    XML processing, standard Coq functional programs, and even declarative
    specifications that are refined automatically into assembly code with
    correctness proofs. The talk will present Bedrock's shared foundation
    and sketch the pieces that go into refining declarative specifications
    into closed assembly programs, covering joint work with Thomas
    Braibant, Santiago Cuellar, Benjamin Delaware, Jason Gross, Gregory
    Malecha, Clement Pit-Claudel, and Peng Wang."

}

@book{Chli15,
  author = "Chlipala, Adam",
  title = {{Certified Programming with Dependent Types}},
  year = "2015",
  link = "\url{http://adam.chlipala.net/cpdt/cpdt.pdf}",
  publisher = "MIT Press",
  isbn = "9780262026659",
  paper = "Chli15.pdf"
}

@misc{Clar91,
  author = "Clarke, Edmund and Zhao, Xudong",
  title = {{Analytica -- A Theorem Prover in Mathematica}},
  year = "1991",
  link = "\url{http://www.cs.cmu.edu/~emc/papers/Conference%20Papers/Analytica%20A%20Theorem%20Prover%20in%20Mathematica.pdf}",
  paper = "Clar91.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Clar93,
  author = "Clarke, Edmund and Zhao, Xudong",
  title = {{Analytica -- A Theorem Prover for Mathematica}},
  journal = "The Mathematica Journal",
  volume = "3",
  number = "1",
  year = "1993",
  pages = "761-765",
  abstract =
    "Analytica is an automatic theorem prover for theorems in
    elementary analysis. The prover is written in Mathematica language
    and runs in the Mathematica environment. The goal of the project
    is to use a powerful symbolic computation system to prove theorems
    that are beyond the scope of previous automatic theorem
    provers. The theorem prover is also able to guarantee the
    correctness of certain steps that are made by the symbolic
    computation system and therefore prevent common errors like
    division by a symbolic expression that could be zero. In this
    paper we describe the structure of Analytica and explain the main
    techniques that it uses to construct proofs. We have tried to make
    the paper as self-contained as possible so that it will be
    accessible to a wide audience of potential users. We illustrate
    the power of our theorem prover by several non-trivial examples
    including the basic properties of the stereographic projection and
    a series of three lemmas that lead to a proof of Weierstrass's
    example of a continuous nowhere differentiable function. Each of
    the lemmas in the latter example is proved completely
    automatically.",
  paper = "Clar93.pdf",
  keywords = "printed"
}

@article{Clem91,
  author = "Clement, Dominique and Prunet, Vincent and Montagnac, Francis",
  title =
    {{Integrated software components: A Paradigm for Control Integration}},
  journal = "LNCS",
  volume = "509",
  pages = "167-177",
  year = "1991",
  abstract =
    "This report describes how control integration between software
    components may be organised using an encapsulation technique combined
    with broadcast message passing : each software component, which is
    encapsulated within an integrated software component (IC),
    communicates by sending and receiving events. Events are emitted
    without the emitter knowing whether there are any receivers. The
    proposed mechanism can be used for intertool communication as well as
    for communication within a single tool.
    
    This programming architecture frees the code from dependencies upon
    the effective software components environments, and simplifies its
    extension.",
  paper = "Clem91.pdf"
}

@misc{Cohe93,
  author = "Cohen, Arjeh M. and Davenport, James H. and Heck, J.P.",
  title = {{An overview of computer algebra}},
  year = "1993",
  paper = "Cohe93.pdf"
}

@article{Como88,
  author = "Comon, H. and Lugiez, D. and Schnoebelen, P.H.",
  title = {{A Rewrite-based Type Discipline for a Subset of Computer Algebra}},
  journal = "J. Symbolic Computation",
  year = "1991",
  volume = "11",
  pages = "349-368",
  abstract =
    "This paper is concerned with the type structure of a system including
    polymorphism, type properties, and subtypes. This type system
    originates from computer algebra but it is not intended to be the
    solution of all type problems in this area.
    
    Types (or sets of types) are denoted by terms in some order-sorted
    algebra. We consider a rewrite relation in this algebra, which is
    intended to express subtyping. The relations between the semantics and
    the axiomatization are investigated. It is shown that the problem of
    type inference is undecidable but that a narrowing strategy for
    semi-decision procedures is described and studied.",
  paper = "Como88.pdf",
  keywords = "axiomref"
}

@book{Cons85,
  author = "Constable, R.L. and Allen, S.F. and Bromley, H.M. and Cremer, J.F.
            and Harper, R.W. and Howe, D.J. and Knoblock, T.B. and 
            Mendler, N.P. and Panagaden, P. and Tsaaki, J.T. and Smith, S.F.",
  title = {{Implementing Mathematics with The Nuprl Proof Development System}},
  publisher = "Prentice-Hall",
  year = "1985"
}

@techreport{Coqu86,
  author = {Coquand, Thierry and Huet, G\'erard},
  title = {{The Calculus of Constructions}},
  year = "1986",
  institution = "INRIA Centre de Rocquencourt",
  number = "530",
  link = "\url{https://hal.inria.fr/inria-00076024/document}",
  abstract = 
    "The Calculus of Constructions is a higher-order formalism for
    constructive proofs in natural deduction style. Every proof is a
    $\lambda$-expression, typed with propositions of the underlying
    logic. By removing types we get a pure $\lambda$-expression,
    expressing its associated algorithm. Computing this
    $\lambda$-expression corresponds roughly to cut-elimination. It is our
    thesis that (as already advocated by Martin-Lof) the Curry-Howard
    correspondence between propositions and types is a powerful paradigm
    for Computer Science. In the case of Constructions, we obtain the
    notion of a very high-level functional programming language, with
    complex polymorphism well-suited for modules specification. The notion
    of type encompasses the usual notioin of data type, but allows as well
    arbitrarily complex algorithmic specifications. We develop the basic
    theory of a Calculus of Constructions, and prove a strong
    normalization theorem showing that all computations terminate. 
    Finally, we suggest various extensions to stronger calculi.",
  paper = "Coqu86.pdf"
}

@article{Coqu93,
  author = "Coquand, Thierry",
  title = {{Infinite Objects in Type Theory}},
  journal = "LNCS",
  volume = "806",
  year = "1993",
  pages = "62-78",
  abstract =
    "We show that infinite objects can be constructively understood
    without the consideration of partial elements, or greatest fixed-
    points, through the explicit consideration of proof objects.  We
    present then a proof system based on these explanations.  According to
    this analysis, the proof expressions should have the same structure
    as the program expressions of a pure functional lazy language:
    variable, constructor, application, abstraction, case expressions,
    and local let expressions.",
  paper = "Coqu93.pdf",
  keywords = "printed"
}

@article{Coqu96,
  author = "Coquand, Thierry and Dybjer, Peter",
  title = {{Intuitionistic Model Constructions and Normalization Proofs}},
  journal = "Mathematical Structures in Computer Science",
  volume = "7",
  pages = "75-94",
  year = "1996",
  link = "\url{http://www.cse.chalmers.se/~peterd/papers/Glueing.ps}",
  abstract =
    "The traditional notions of {\sl strong} and {\sl weak normalization}
    refer to properties of a binary {\sl reduction relation}. In this
    paper we explore an alternative approach to normalization, where we
    bypass the reduction relation and instead focus on the 
    {\sl normalization function}, that is, the function which maps a term into
    its normal form.
    
    Such a normalization function can be constructed by building an
    appropriate model and a function ``quote'' which inverts the
    interpretation function. The normalization function is then obtained
    by composing the quote function with the interpretation function. We
    also discuss how to gt a simple proof of the property that
    constructors are one-to-one, which usually is obtained as a corollary
    of Church-Rosser and normalization in the traditional sense.
    
    We illustrate this approach by showing how a glueing model (closely
    related to the glueing construction used in category theory) gives
    rise to a normalization algorithm for a combinatory formlation of
    Godel System T. We then show how the method extends in a
    straightforward way when we add cartesian products and disjoint unions
    (full intuitionistic propositional logic under a Curry-Howard
    interpretation) and transfinite inductive types such as the Brouwer
    ordinals.",
  paper = "Coqu96.pdf"
}

@misc{Coqu16,
  author = {Coquand, Thierry and Huet, G\'erard and Paulin, Christine},
  title = {{The COQ Proof Assistant}},
  year = "2016",
  link = "\url{https://coq.inria.fr}"
}

@misc{COQR16,
  author = {Coquand, Thierry and Huet, G\'erard and Paulin, Christine},
  title = {{The COQ Proof Assistant Reference Manual}},
  year = "2016",
  link="\url{https://coq.inria.fr/distrib/current/files/Reference-Manual.pdf}",
  paper = "COQR16.pdf"
}

@misc{Coqu16a,
  author = {Coquand, Thierry and Huet, G\'erard and Paulin, Christine},
  title = {{COQ Proof Assistant Library Coq.ZArith.Znumtheory}},
  year = "2016",
  link = "\url{https://coq.inria.fr/library/Coq.ZArith.Znumtheory.html}"
}

@misc{COQnat,
  author = "COQ Proof Assistant",
  title = {{Library Coq.Init.Nat}},
  link = "\url{https://coq.inria.fr/library/Coq.Init.Nat.html}",
  abstract = "Peano natural numbers, defintions of operations",
  year = "2017"
}

@misc{Cons98,
  author = "Constable, Robert L. and Jackson, Paul B.",
  title = {{Towards Integrated Systems for Symbolic Algebra and Formal
           Constructive Mathematics}},
  link = "\url{http://www.nuprl.org/documents/Constable/towardsintegrated.pdf}",
  year = "1998",
  abstract = 
    "The purpose of this paper is to report on our efforts to give a
    formal account of some of the algebra used in Computer Algebra Systems
    (CAS). In particular, we look at the concepts used in the so called
    3rd generation algebra systems, such as Axiom[4] and Weyl[9].  It is
    our claim that the Nuprl proof development system is especially well
    suited to support this kind of mathematics.",
  paper = "Cons98.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@book{Crol93,
  author = "Crole, R.L.",
  title = {{Categories for Types}},
  publisher = "Cambridge University Press",
  year = "1993",
  isbn = "978-0521457019"
}

@misc{Daly10,
  author = "Daly, Timothy",
  title = {{Intel Instruction Semantics Generator}},
  link = "\url{http://daly.axiom-developer.org/TimothyDaly_files/publications/sei/intel/intel.pdf}",
  abstract = 
    "Given an Intel x86 binary, extract the semantics of the instruction
    stream as Conditional Concurrent Assignments (CCAs). These CCAs 
    represent the semantics of each individual instruction. They can be
    composed to represent higher level semantics.",
  paper = "Daly10.pdf"
}

@InProceedings{Dani06,
  author = "Danielsson, Nils Anders and Hughes, John and Jansson, Patrik and
            Gibbons, Jeremy",
  title = {{Fast and Loose Reasoning is Morally Correct}},
  booktitle = "Proc. of ACM POPL '06",
  series = "POPL '06",
  year = "2006",
  location = "Charleston, South Carolina",
  abstract = 
    "Functional programmers often reason about programs as if they were
    written in a total language, expecting the results to carry over to
    non-toal (partial) languages. We justify such reasoning.

    Two languages are defined, one total and one partial, with identical
    syntax. The semantics of the partial language includes partial and
    infinite values, and all types are lifted, including the function
    spaces. A partial equivalence relation (PER) is then defined, the
    domain of which is the total subset of the partial language. For types
    not containing function spaces the PER relates equal values, and
    functions are related if they map related values to related values.

    It is proved that if two closed terms have the same semantics in the
    total language, then they have related semantics in the partial
    language. It is also shown that the PER gives rise to a bicartesian
    closed category which can be used to reason about values in the domain
    of the relation.",
  paper = "Dani06.pdf",
  keywords = "axiomref"
}

@misc{Dave93a,
  author = "Davenport, James H.",
  title = {{C9: Universal Algebra}},
  year = "1993",
  comment = "Lecture Notes for 2nd year undergrad and mather's course
             in Universal Algebra",
  school = "University of Bath"
}

@misc{Dave98,
  author = "Davenport, James H.",
  title = {{Is Computer Algebra the same as Computer Mathematics?}},
  year = "1998",
  comment = "Talk for British Colloquium for Theoretical Computer Science"
}

@article{Dave08,
  author = "Davenport, James H.",
  title = {{Effective Set Membership in Computer Algebra and Beyond}},
  journal = "LNAI",
  volume = "5144",
  pages = "266-269",
  year = "2008",
  abstract = 
    "In previous work, we showed the importance of distinguishing ``I know
    that $X \ne Y$'' from ``I don't know that $X = Y$''.  In this paper we
    look at effective set membership, starting with Groebner bases, where
    the issues are well-expressed in algebra systems, and going on to
    integration and other questions of `computer calculus'.
    
    In particular, we claim that a better recognition of the role of set
    membership would clarify some features of computer algebra systems,
    such as `what does an integral mean as output'.",
  paper = "Dave08.pdf"
}

@misc{Dave17a,
  author = "Davenport, James",
  title = {{Computer Algebra and Formal Proof}},
  year = "2017",
  comment = "BPR presentation, Cambridge, England",
  video = "Dave17a.mp4",
  paper = "Dave17a.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Dela05,
  author = "Delahaye, David and Mayero, Micaela",
  title = {{Dealing with algebraic expressions over a field in Coq
           using Maple}},
  journal = "J. Symbolic Computation",
  volume = "39",
  pages = "569-592",
  year = "2005",
  abstract =
    "We describe an interface between the Coq proof assistant and the
    Maple symbolic computations system, which mainly consists in
    importing, in Coq. Maple computations regarding algebraic
    expressions over fields. These can either be pure computations,
    which do not require any validation, or computations used during
    proofs, which must be proved (to be correct) within Coq. These
    correctneess proofs are completed automatically thanks to the
    tactic Field, which deals with equalities over fields. This
    tactic, which may generate side conditions (regarding the
    denominators) that must be proved by the user, has been
    implemented in a reflxive way, which ensures both efficiency and
    certification. The implementation of this interface is quite light
    and can be very easily extended to get other Maple functions (in
    addition to the four functions we have imported and used in the
    examples given here).",
  paper = "Dela05.pdf",
  keywords = "printed"
}

@article{Demi79,
  author = "DeMilo, Richard A. and Lipton, Richard J. and Perlis, Alan J.",
  title = {{Social Processes and Proofs of Theorems and Programs}},
  journal = "Communications of the ACM",
  volume = "22",
  number = "5",
  year = "1979",
  pages = "271-280",
  abstract = 
    "It is argued that formal verifications of programs, no matter how
    obtained, will not play the same key role in the development of 
    computer science and software engineering as proofs do in mathematics.
    Furthermore the absence of continuity, the inevitability of change, and
    the complexity of specification of significantly many real programs
    make the formal verification process difficult to justify and manage.
    It is felt that ease of formal verification should not dominate program
    language design.",
  paper = "Demi79.pdf",
  keywords = "printed"
}

@article{Denn00,
  author = "Dennis, Louise A. and Collins, Graham and Norrish, Michael
            and Boulton, Richard and Slind, Konrad and 
            Robinson, Graham and Gordon, Mike and Melham, Tom",
  title = {{The PROSPER Toolkit}},
  journal = "LNCS",
  volume = "1785",
  publisher = "Springer-Verlag",
  pages = "78-92",
  year = "2000",
  link =
     "\url{https://link.springer.com/content/pdf/10.1007/3-540-46419-0_7.pdf}",
  abstract =
    "The PROSPER (Proof and Specification Assisted Design Environments)
    project advocates the use of toolkits which allow existing
    verification tools to be adapted to a more flexible format so that
    they may be treated as components. A system incorporating such tools
    becomes another component that can be embedded in an application. This
    paper describes the PROSPER Toolkit which enables this. The nature of
    communication between components is specified in a
    language-independent way. It is implemented in several common
    programming languages to allow a wide variety of tools to have access
    to the toolkit.",
  paper = "Denn00.pdf"
}

@book{Devl79,
  author = "Devlin, Keith J.",
  title = {{Fundamentals of Contemporary Set Theory}},
  publisher = "Springer-Verlag",
  year = "1979",
  isbn = "978-0387904412"
}

@misc{Dijk72,
  author = "Dijkstra, Edsger",
  title = {{The Humble Programmer}},
  year = "1972",
  number = "EWD340",
  comment = "ACM Turing Lecture 1972",
  paper = "Dijk72.txt"
}

@book{Dijk76,
  author = "Dijkstra, Edsger",
  title = {{A Discipline of Programming}},
  publisher = "Prentice-Hall",
  year = "1976",
  isbn = "0-13-215871-X"
}

@misc{Dijk83,
  author = "Dijkstra, Edsger",
  title = {{Fruits of Misunderstanding}},
  year = "1983",
  link = "\url{https://www.cs.utexas.edu/users/EWD/transcriptions/EWD08xx/EWD854.html}",
  paper = "Dijk83.txt"
}

@misc{Dolz04a,
  author = "Dolzmann, A. and Seidl, A. and Sturm, T.",
  title = {{Redlog User Manual}},
  year = "2004",
  comment = "Edition 3.0",
  link = "\url{http://www.reduce-algebra.com/reduce38-docs/redlog.pdf}",
  paper = "Dolz04a.pdf"
}

@inproceedings{Dybj90,
  author = "Dybjer, Peter",
  title = {{Inductive Sets and Families in Marin-L\"of's Type Theory and
           Their Set-Theoretic Semantics}},
  booktitle = "Proc. First Workshop on Logical Frameworks",
  year = "1990",
  link = 
    "\url{http://www.cse.chalmers.se/~peterd/papers/Setsem\_Inductive.pdf}",
  abstract =
    "{Martin-L\"of}'s type theory is presented in several steps. The kernel
    is a dependently typed $\lambda$-calculs. Then there are schemata for
    inductive sets and families of sets and for primitive recursive functions
    and families of functions. Finally, there are set formers (generic
    polymorphism) and universes. At each step syntax, inference rules, and
    set-theoretic sematics are given",
  paper = "Dybj90.pdf"
}

@article{Dybj03,
  author = "Dybjer, Peter and Setzer, Anton",
  title = {{Induction-recursion and initial algebras}},
  journal = "Annals of Pure and Applied Logic",
  volume = "124",
  year = "2003",
  pages = "1-47",
  abstract = 
    "Induction-recursion is a powerful definition method in intuitionistic
    type theory. It extends (generalized) inductive definitions and allows us
    to define all standard sets of Martin-{L\"of} type theory as well as a
    large collection of commonly occuring inductive data structures. It also
    includes a variety of universes which are constructive analogues of 
    inaccessibles and other large cardinals below the first Mahlo cardinal.
    In this article we give a new compact formalization of inductive-recursive
    definnitions by modeling them as initial algebras in slice categories. We
    give generic formation, introduction, elimination, and equality rules
    generalizing the usual rules of type theory. Moreover, we prove that the
    elimination and equality rules are equivalent to the principle of the 
    existence of initial algebras for certain endofunctors. We also show the
    equivalence of the current formulation with the formulation of 
    induction-recursion as a reflection principle given in Dybjer and
    Setzer (Lecture Notes in Comput. Sci. 2183 (2001) 93). Finally we discuss
    two type-theoretic analogues of Mahlo cardinals in set theory: an external
    Mahlo universe which is defined by induction-recursion and captured by our
    formalization, and an internal Mahlo universe, which goes beyond induction-
    recursion. We show that the external Mahlo universe, and therefore also
    the theory of inductive-recursive definitions, have proof-theoretical 
    strength of at least Rathjen's theory KPM.",
  paper = "Dybj03.pdf"
}

@misc{Dolz97,
  author = "Dolzmann, Andreas and Sturm, Thomas",
  title = {{Guarded Expressions in Practice}},
  link = "\url{http://redlog.dolzmann.de/papers/pdf/MIP-9702.pdf}",
  year = "1997",
  abstract = 
    "Computer algebra systems typically drop some degenerate cases when
    evaluating expressions, e.g. $x/x$ becomes 1 dropping the case
    $x=0$. We claim that it is feasible in practice to compute also the
    degenerate cases yielding {\sl guarded expressions}. We work over real
    closed fields but our ideas about handling guarded expressions can be
    easily transferred to other situations. Using formulas as guards
    provides a powerful tool for heuristically reducing the combinatorial
    explosion of cases: equivalent, redundant, tautological, and
    contradictive cases can be detected by simplification and quantifier
    elimination. Our approach allows to simplify the expressions on the
    basis of simplification knowledge on the logical side. The method
    described in this paper is implemented in the REDUCE package GUARDIAN,
    which is freely available on the WWW.",
  paper = "Dolz97.pdf"
}

@inbook{Dosr11,
  author = "Dos Reis, Gabriel and Matthews, David and Li, Yue",
  title = {{Retargeting OpenAxiom to Poly/ML: Towards an Integrated Proof 
           Assistants and Computer Algebra System Framework}},
  booktitle = "Calculemus",
  pages = "15-29",
  year = "2011",
  publisher = "Springer",
  isbn = "978-3-642-22673-1",
  link = "\url{http://paradise.caltech.edu/~yli/paper/oa-polyml.pdf}",
  abstract = "
    This paper presents an ongoing effort to integrate the Axiom family of
    computer algebra systems with Poly/ML-based proof assistants in the
    same framework. A long term goal is to make a large set of efficient
    implementations of algebraic algorithms available to popular proof
    assistants, and also to bring the power of mechanized formal
    verification to a family of strongly typed computer algebra systems at
    a modest cost. Our approach is based on retargeting the code generator
    of the OpenAxiom compiler to the Poly/ML abstract machine.",
  paper = "Dosr11.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@misc{Duns00,
  author = "Dunstan, Martin N.",
  title = {{Adding Larch/Aldor Specifications to Aldor}},
  abstract = 
    "We describe a proposal to add Larch-style annotations to the Aldor
    programming language, based on our PhD research. The annotations
    are intended to be machine-checkable and may be used for a variety
    of purposes ranging from compiler optimizations to verification
    condition (VC) generation. In this report we highlight the options
    available and describe the changes which would need to be made to
    the compiler to make use of this technology.",
  paper = "Duns00.pdf",
  keywords = "axiomref, printed"
}

@InProceedings{Duns98,
  author = "Dunstan, Martin and Kelsey, Tom and Linton, Steve and 
            Martin, Ursula",
  title = {{Lightweight Formal Methods For Computer Algebra Systems}},
  publisher = "ACM Press",
  booktitle = "Proc. ISSAC 1998",
  year = "1998",
  location = "Rostock, Germany",
  pages = "80-87",
  link = "\url{http://www.cs.st-andrews.ac.uk/~tom/pub/issac98.pdf}",
  abstract = 
    "Demonstrates the use of formal methods tools to provide a semantics
    for the type hierarchy of the Axiom computer algebra system, and a
    methodology for Aldor program analysis and verification. There are
    examples of abstract specifications of Axiom primitives.",
  paper = "Duns98.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@phdthesis{Duns99a,
  author = "Dunstan, Martin N.",
  title = {{Larch/Aldor - A Larch BISL for AXIOM and Aldor}},
  school = "University of St. Andrews",
  year = "1999",
  abstract = "
    In this thesis we investigate the use of lightweight formal methods
    and verification conditions (VCs) to help improve the reliability of
    components constructed within a computer algebra system. We follow the
    Larch approach to formal methods and have designed a new behavioural
    interface specification language (BISL) for use with Aldor: the
    compiled extension language of Axiom and a fully-featured programming
    language in its own right. We describe our idea of lightweight formal
    methods, present a design for a lightweight verification condition
    generator and review our implementation of a prototype verification
    condition generator for Larch/Aldor.",
  paper = "Duns99a.pdf",
  keywords = "axiomref, printed"
}

@InProceedings{Duns99,
  author = "Dunstan, Martin and Kelsey, Tom and Martin, Ursula and 
            Linton, Steve A.",
  title = {{Formal Methods for Extensions to CAS}},
  booktitle = "Proc. of FME'99",
  series = "FME'99",
  location = "Toulouse, France",
  year = "1999",
  pages = "1758-1777",
  link = "\url{http://tom.host.cs.st-andrews.ac.uk/pub/fm99.ps}",
  abstract = 
    "We demonstrate the use of formal methods tools to provide a semantics
    for the type hierarchy of the AXIOM computer algebra system, and a
    methodology for Aldor program analysis and verification. We give a
    case study of abstract specifications of AXIOM primitives, and provide
    an interface between these abstractions and Aldor code.",
  paper = "Duns99.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@article{Farm92a,
  author = "Farmer, William H.",
  title = {{IMPS: System Description}},
  journal = "Lecture Notes in Computer Science",
  volume = "607",
  pages = "701-705",
  year = "1992",
  paper = "Farm92a.pdf"
}

@article{Farm96,
  author = "Farmer, William M. and Guttman, Joshua D. and
            Fabrega, F. Javier Thayer",
  title = {{IMPS: An Updated System Description}},
  journal = "LNCS",
  volume = "1104",
  pages = "298-302",
  year = "1996",
  paper = "Farm96.pdf"
}

@article{Farm92,
  author = "Farmer, William H.",
  title = {{Little Theories}},
  journal = "LNCS",
  volume = "607",
  year = "1992",
  pages = "567-581",
  abstract =
    "In the ``little theories'' version of the axiomatic method, different
    portions of mathematics are developed in various different formal
    axiomatic theories. Axiomatic theories may be related by inclusion or
    by theory interpretation. We argue that the little theories approach
    is a desirable way to formalize mathematics, and we describe how IMPS,
    an Interactive Mathematical Proof System, supports it.",
  paper = "Farm92.pdf"
}    

@article{Farm93b,
  author = "Farmer, William M.",
  title = {{A simple type theory with partial functions and subtypes}},
  journal = "Annals of Pure and Applied Logic",
  volume = "64",
  pages = "211-240",
  year = "1993",
  abstract =
    "Simple type theory is a higher-order predicate logic for reasoning
    about truth values, individuals, and simply typed total functions.  We
    present in this paper a version of simple type theory, called PF*, in
    which functions may be partial and types may have subtypes.  We define
    both a Henkin-style general models semantics and an axiomatic system
    for PF*, and we prove that the axiomatic system is complete with
    respect to the general models semantics.  We also define a notion of
    an interpretation of one PF* theory in another.  PF* is intended as a
    foundation for mechanized mathematics.  It is the basis for the logic
    of IMPS, an Interactive Mathematical Proof System developed at The
    MITRE Corporation.",
  paper = "Farm93b.pdf",
  keywords = "printed"
}

@article{Farm93a,
  author = "Farmer, William M. and Guttman, Joshua D. and Thayer, Javier",
  title = {{IMPS: An Interactive Mathematical Proof Systems}},
  journal = "J. of Automated Reasoning",
  volume = "11",
  pages = "213-248",
  year = "1993",
  abstract =
    "IMPS is an interactive mathematical proof system intended as a
    general-purpose too! formulating and applying mathematics in a
    familiar fashion.  The logic of IMPS is based on a version of simple
    type theory with partial functions and subtypes.  Mathematical
    specification and inference are performed relative to axiomatic
    theories, which can be related to one another via inclusion and theory
    interpretation.  IMPS provides relatively large primitive inference
    steps to facilitate human control of the deductive process and human
    comprehension of the resulting proofs.  An initial theory library 
    containing over a thousand repeatable proofs covers significant portions
    of logic, algebra, and analysis and provides some support for modeling
    applications in computer science.",
  paper = "Farm93a.pdf"
}

@article{Farm93,
  author = "Farmer, William M. and Guttman, Joshua D. and Thayer, Javier",
  title = {{Reasoning with contexts}},
  journal = "LNCS",
  volume = "722",
  year = "1993",
  abstract =
    "Contexts are sets of formulas used to manage the assumptions that
    arise in the course of a mathematical deduction or calculation. This
    paper describes some techniques for symbolic computation that are
    dependent on using contexts, and are implemented in IMPS, an
    Interactive Mathematical Proof System.",
  paper = "Farm93.pdf"
}

@article{Farm94,
  author = "Farmer, William M. and Guttman, Joshua D. and Nadel, Mark E.
            and Fabrega, F. Javier Thayer",
  title = {{Proof Script Pragmatics in IMPS}},
  journal = "LNCS",
  volume = "814",
  pages = "356-370",
  year = "1994",
  abstract =
    "This paper instroduces the IMPS proof script mechanism and some
    practical methods for exploiting it.",
  paper = "Farm94.pdf"
}

@misc{Fate02a,
  author = "Fateman, Richard J.",
  title = {{Symbolic Execution Merges Construction, Debugging and Proving}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/symex.pdf}",
  year = "2002",
  abstract =
    "There is naturally an interest in any technology which promises to
    assist us in producing correct programs. Some efforts attempt to
    insure correct programs by making their construction simpler. Some
    efforts are oriented toward increasing the effectiveness of testing to
    make the programs appear to perform as required. Other efforts are
    directed to prove the correctness of the resulting program. Symbolic
    execution, in which symbols instead of numbers are used in what
    appears to be a numerical program, is an old but to-date still not
    widely-used technique. It has been available in various forms for
    decades from the computer algebra community. Symbolic execution has
    the potential to assist in all these phases: construction, debugging,
    and proof. We describe how this might work specifically with regard to
    our own recent experience in the construction of correct linear
    algebra programs for structured matrices and LU factorization. We show
    how developing these programs with a computer algebra system, and then
    converting incrementally to use more efficient forms. Frequent symbolic 
    execution of the algorithms, equivalent to testing over infinite test 
    sets, aids in debugging, while strengthening beliefs that the correctness 
    of results is an algebraic truth rather than an accident.",
  paper = "Fate02a.pdf, CAS-Proof, printed"
}

@inproceedings{Fate03a,
  author = "Fateman, Richard J.",
  title = {{High-level proofs of mathematical programs using automatic
           differentiation, simplification, and some common sense}},
  booktitle = "Proc. ISSAC 2003",
  pages = "88-94",
  year = "2003",
  isbn = "1-58113-641-2",
  abstract =
    "One problem in applying elementary methods to prove correctness of
    interesting scientific programs is the large discrepancy in level of
    discourse between low-level proof methods and the logic of scientific
    calculation, especially that used in a complex numerical program. The
    justification of an algorithm typically relies on algebra or analysis,
    but the correctness of the program requires that the arithmetic
    expressions are written correctly and that iterations converge to
    correct values in spite of truncation of infinite processes or series
    and the commission of numerical roundoff errors. We hope to help
    bridge this gap by showing how we can, in some cases, state a
    high-level requirement and by using a computer algebra system (CAS)
    demonstrate that a program satisfies that requirement. A CAS can
    contribute program manipulation, partial evaluation, simplification or
    other algorithmic methods. A novelty here is that we add to the usual
    list of techniques automatic differentiation, a method already widely
    used in optimization contexts where algorithms are differentiated. We
    sketch a proof of a numerical program to compute sine, and display a
    related approach to a version of a Bessel function algorithm for J0(x)
    based on a recurrence.",
  paper = "Fate03a.pdf",
  keywords = "CAS-Proof, printed"
}

@book{Fitt90,
  author = "Fitting, M.",
  title = {{First-order Logic and Automated Theorem Proving}},
  publisher = "Springer-Verlag",
  year = "1990",
  isbn = "978-1461275152"
}

@incollection{Floy86,
  author = "Floyd, W.",
  title = {{Toward Interactive Design of Correct Programs}},
  booktitle = "Reading in Artificial Intelligence and Software Engeering",
  publisher = "Elsevier",
  pages = "331-334",
  year = "1986",
  isbn = "0-934613-12-5"
}

@article{Frad08,
  author = "Frade, Maria Joao",
  title = {{Calculus of Inductive Construction. Software Formal Verification}},
  year = "2008",
  link = "\url{http://www4.di.uminho.pt/~jno/mfes/0809/SFV-CIC.pdf}",
  journal = "MFES",
  paper = "Frad08.pdf"
}

@misc{Freg1891,
  author = "Frege, Gottlob",
  title = {{Function and Concept}},
  year = "1891",
  link = "\url{http://fitelson.org/proseminar/frege_fac.pdf}",
  paper = "Frege.pdf",
  keywords = "printed"
}

@book{Frey90,
  author = "Freyd, Peter and Scedrov, Andre",
  title = {{Categories, Allegories}},
  year = "1990",
  publisher = "North-Holland",
  comment = "Mathematical Library Vol 39",
  isbn = "978-0-444-703368-2",
  abstract =
    "On the Categories side, the book centers on that part of categorical
    algebra that studies exactness properties, or other properties enjoyed
    by nice or convenient categories such as toposes, and their
    relationship to logic (for example, geometric logic). A major theme
    throughout is the possibility of representation theorems (aka
    completeness theorems or embedding theorems) for various categorical
    structures, spanning back now about five decades (as of this writing)
    to the original embedding theorems for abelian categories, such as the
    Freyd-Mitchell embedding theorem.
    
    On the Allegories side: it may be said they were first widely
    publicized in this book. They comprise many aspects of relational
    algebra corresponding to the categorical algebra studied in the first
    part of the book"
}

@book{Frie08,
  author = "Friedman, Daniel P. and Wand, MItchell",
  title = {{Essentials of Programming Languages}},
  publisher = "MIT Press",
  year = "2008",
  isbn = "978-0-262-06279-4"
}

@book{Gall86,
  author = "Gallier, Jean H.",
  title = {{Logic for Computer Science: Foundations of Automatic
            Theorem Proving}},
  publisher = "Harper and Row",
  year = "1986",
  isbn = "978-0486780825"
}

@book{Gedd94,
  author = "Geddes, D.",
  title = {{The DTP Manual}},
  publisher = "Stanford University",
  year = "1994",
  comment = "Don's Theorem Prover in Common Lisp",
  paper = "Gedd94.pdf"
}

@misc{Gent35,
  author = "Gentzen, Gerhard",
  title = {{Investigations into Logical Deduction}},
  year = "1935",
  pages = "68-131",
  paper = "Gent35.pdf"
}

@article{Gent64,
  author = "Gentzen, Gerhard",
  title = {{Investigations into Logical Deduction}},
  journal = "American Philosophical Quarterly",
  volume = "1",
  number = "4",
  year = "1964",
  pages = "288-306",
  paper = "Gent64.pdf",
  keywords = "printed"
}

@article{Gent65,
  author = "Gentzen, Gerhard",
  title = {{Investigations into Logical Deduction: II}},
  journal = "American Philosophical Quarterly",
  volume = "2",
  number = "3",
  year = "1965",
  pages = "204-218",
  paper = "Gent65.pdf"
}

@misc{Geuv92,
  author = "Geuvers, Herman",
  title = {{The Calculus of Constructions and Higher Order Logic}},
  year = "1992",
  link = "\url{http://www.cs.ru.nl/~herman/PUBS/CC\_CHiso.pdf}",
  abstract =
    "The Calculus of Constructions (CC) is a typed lambda calculus for
    higher order intuitionistic logic: proofs of the higher order logic
    are interpreted as lambda terms and formulas as types. It is also the
    union of Girard's system $F_omega$, a higher order typed lambda
    calculus, and a first order dependent typed lambda calculus in the
    style of de Bruijn's Automath or Martin-Lof's intuitionistic theory
    of types. Using the impredicative coding of data types in $F_omega$,
    the Calculus of Constructions thus becomes a higher order language for
    the typing of functional programs. We shall introduce and try to
    explain CC by exploiting especially the first point of view, by
    introducing a typed lambda calculus that faithfully represent higher
    order predicate logic (so for this system the Curry-Howard
    'formuals-as-types isomorphism' is really an isomorphism.) Then we
    discuss some propositions that are provable in CC but not in the
    higher order logic, showing that the formulas-as-types embedding of
    higher order predicate logic into CC is not an isomorphism. It is our
    intention that this chapter can be read without any specialist
    knowledge of higher order logic or higher order typed lambda calculi.",
  paper = "Geuv92.pdf"
}

@article{Geuv02,
  author = "Geuvers, Herman and Pollack, Randy and Wiedijk, Freek and 
            Zwanenburg, Jan",
  title = {{A Constructive Algebraic Hierarchy in Coq}},
  year = "2002",
  journal = "Journal of Symbolic Computation",
  abstract =
    "We describe a framework of algebraic structures in the proof assistant
    Coq. We have developed this framework as part of the FTA project in
    Nijmegen, in which a constructive proof of the Fundamental Theorem of
    Algebra has been formalized in Coq.

    The algebraic hierarchy that is described here is both abstract and
    structured. Structures like groups and rings are port of it in an
    abstract way, defining e.g. a ring as a tuple consisting of a group, a
    binary operation and a constant that together satisfy the properties
    of a ring. In this way, a ring automatically inherits the group
    properties of the additive subgroup. The algebraic hierarchy is
    formalized in Coq by applying a combination of labeled record types
    and coercions. In the labeled record types of Coq, one can use 
    {\sl dependent types}: the type of one label may depend on another
    label. This allows to give a type to a dependent-typed tuple like
    $\langle A, f, a \rangle$, where $A$ is a set, $f$ an operation on $A$
    and $a$ an element of $A$. Coercions are functions that are used
    implicitly (they are inferred by the type checker) and allow, for
    example, to use the structure $\mathcal{A} := \langle A, f, a \rangle$
    as a synonym or the carrier set $A$, as is often done in mathematical
    practice. Apart rom the inheritance and reuse of properties, the
    algebraic hierarchy has proven very useful for reusing notations.",
  paper = "Geuv02.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@misc{Gime16,
  author = "Gimenez, Eduardo and Casteran, Pierre",
  title = {{A Tutorial on [Co-]Inductive Types in Coq}},
  year = "2016",
  link = "\url{https://coq.inria.fr/distrib/current/files/RecTutorial.pdf}",
  abstract =
    "This document is an introduction to the definition and use of
    inductive and co-inductive types in the {\sl Coq} proof environment. 
    It explains how types like natural numbers and infinite streams are 
    defined in {\sl Coq}, and the kind of proof techniques that can be 
    used to reason about them (case analysis, induction, inversion of 
    predicates, co-induction, etc.) Each technique is illustrated 
    through an executable and self-contained {\sl Coq} script.",
  paper = "Gime16.pdf"
}

@techreport{Giun94,
  author = "Giunchiglia, F. and Pecchiari, P. and Talcott, C.",
  title = {{Reasoning Theories: Towards an Architecture for Open Mechanized 
           Reasoning Systems}},
  type = "technical report",
  number = "9409-15",
  institution = "IRST Trento Italy",
  year = "1994",
  paper = "Giun94.pdf"
}

@article{Gogu82,
  author = "Goguen, J.A. and Meseguer, J.",
  title = {{Completeness of Many-sorted Equational Logic}},
  journal = "ACM SIGPLAN Notices",
  volume = "17",
  number = "1",
  year = "1982",
  pages = "9-17",
  abstract =
    "The rules of deduction which are usually used for many-sorted
    equational logic in computer science, for example in the study of
    abstract data types, are not sound. Correcting these rules by
    introducing explicit quantifiers yields a system which, although it is
    sound, is not complete; some new rules are needed for the addition and
    deletion of quantifiers. This note is intended as an informal, but
    precise, introduction to the main issues and results. It gives an
    example showing the unsoundness of the usual rules; it also gives a
    completeness theorem for our new rules, and gives necessary and
    sufficient conditions for the old rules to agree with the new.",
  paper = "Gogu82.pdf",
}

@article{Gogu06,
  author = "Goguen, Healfdene and McBride, Conor and McKinna, James",
  title = {{Eliminating Dependent Pattern Matching}},
  year = "2006",
  journal = "Lecture Notes in Computer Science",
  volume = "4060",
  pages = "521-540",
  link = "\url{http://cs.ru.nl/~james/RESEARCH/goguen2006.pdf}",
  abstract =
    "This paper gives a reduction-preserving translation from Coquand's
    {\sl dependent pattern matching} into a traditional type theory
    with universes, inductive types and relations and the axiom K. This
    translation serves as a proof of termination for structurally
    recursive pattern matching programs, provides an implementable
    compilation technique in the style of functional programming languages,
    and demonstrates the equivelence with a more easily understood type
    theory.",
  paper = "Gogu06.pdf"
}

@article{Good75,
  author = "Good, Donald I. and London, Ralph L. and Bledsoe, W.W.",
  title = {{An Interactive Program Verification System}},
  journal = "SIGPLAN Notices",
  volume = "10",
  number = "6",
  pages = "482-492",
  year = "1975",
  abstract =
    "This paper is an initial progress report on the development of an
    interactive system for verifying that computer programs meet given
    formal specifications. The system is based on the conventional
    inductive assertion method: given a program and its specifications,
    the object is to generate the verification conditions, simplify them,
    and prove what remains. The important feature of the system is that
    the human user has the opportunity and obligation to help actively in
    the simplifying and proving. The user, for example, is the primary
    source of problem domain facts and properties needed in the proofs. A
    general description is given of the overall design philosophy,
    structure, and functional components of the system, and a simple
    sorting program is used to illustrate both the behavior of major
    system components and the type of user interaction the system provides.",
  paper = "Good75.pdf"
}

@misc{Gord96,
  author = "Gordon, Mike",
  title = {{From LCF to HOL: a short history}},
  year = "1996",
  link = "\url{http://www.cl.cam.ac.uk/~mjcg/papers/HolHistory.pdf}",
  paper = "Gord96.pdf"
}

@book{Gord93,
  author = "Gordon, Mike J.C. and Melham, T.F.",
  title = {{Introduction to HOL: A Theorem Proving Environment for Higher
           Order Logic}},
  link = "\url{http://www.cs.ox.ac.uk/tom.melham/pub/Gordon-1993-ITH.html}",
  publisher = "Cambridge University Press",
  year = "1993",
  isbn = "0-521-44189-7"
}

@article{Gott05,
  author = "Gottliebsen, Hanne and Kelsey, Tom and Martin, Ursula",
  title = {{Hidden verification for computational mathematics}},
  journal = "Journal of Symbolic Computation",
  volume = "39",
  number = "5",
  pages = "539-567",
  year = "2005",
  link = 
   "\url{http://www.sciencedirect.com/science/article/pii/S0747717105000295}",
  abstract = 
    "We present hidden verification as a means to make the power of
    computational logic available to users of computer algebra systems
    while shielding them from its complexity. We have implemented in PVS a
    library of facts about elementary and transcendental function, and
    automatic procedures to attempt proofs of continuity, convergence and
    differentiability for functions in this class. These are called
    directly from Maple by a simple pipe-lined interface. Hence we are
    able to support the analysis of differential equations in Maple by
    direct calls to PVS for: result refinement and verification, discharge
    of verification conditions, harnesses to ensure more reliable
    differential equation solvers, and verifiable look-up tables.",
  paper = "Gott05.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@misc{Grab17,
  author = {Grabm\"uller, Martin},
  title = {{Algorithm W}},
  year = "2017",
  link = "\url{https://github.com/mgrabmueller/AlgorithmW}",
  abstract = 
    "In this paper we develop a complete implementation of Algorithm W for 
     Hindley-Milner polyhmorphic type inference in Haskell",
  paper = "Grab17.pdf"
}

@misc{Gran11,
  author = "Grant, Ian",
  title = {{The Hindley-Milner Type Inference Algorithm}},
  year = "2011",
  link = "\url{http://steshaw.org/hm/hindley-milner.pdf}",
  abstract =
    "The Hindley-Milner algorithm is described and an implementation in
    Standard ML is presented.",
  paper = "Gran11.pdf, printed"
}

@book{Grie81,
  author = "Gries, David",
  title = {{The Science of Programming}},
  publisher = "Springer-Verlag",
  year = "1981",
  isbn = "0-387-90641-X"
}

@phdthesis{Guil98,
  author = "Guillaume, Alexandre",
  title = {{De Aldor A Zermelo}},
  year = "1998",
  school = "Universite Pierre et Marie Curie (Paris)"
}

@book{Gutt93,
  author = "Guttag, John V. and Horning, James J.",
  title = {{LARCH: Languages and Tools for Formal Specifications}},
  publisher = "Springer-Verlag",
  year = "1993",
  isbn = "3-540-94006-5"
}

@misc{Hame89,
  author = "Van Hamelen, Frank",
  title = {{The CLAM Proof Planner}},
  year = "1989",
  publisher = "Dept. of AI, Univ. of Edinburgh"
}

@misc{Hard13,
  author = "Hardin, David S. and McClurg, Jedidiah R. and Davis, Jennifer A.",
  title = {{Creating Formally Verified Components for Layered Assurance 
           with an LLVM to ACL2 Translator}},
  link = "\url{http://www.jrmcclurg.com/papers/law\_2013\_paper.pdf}",
  abstract = 
    "This paper describes an effort to create a library of formally
    verified software component models from code that have been compiled
    using the Low-Level Virtual Machine (LLVM) intermediate form. The idea
    is to build a translator from LLVM to the applicative subset of Common
    Lisp accepted by the ACL2 theorem prover. They perform verification of
    the component model using ACL2's automated reasoning capabilities.",
  paper = "Hard13.pdf"
}

@misc{Hard14,
  author = "Hardin, David S. and Davis, Jennifer A. and Greve, David A. and 
            McClurg, Jedidiah R.",
  title = {{Development of a Translator from LLVM to ACL2}},
  link = "\url{http://arxiv.org/pdf/1406.1566}",
  abstract = "
    In our current work a library of formally verified software components
    is to be created, and assembled, using the Low-Level Virtual Machine
    (LLVM) intermediate form, into subsystems whose top-level assurance
    relies on the assurance of the individual components. We have thus
    undertaken a project to build a translator from LLVM to the
    applicative subset of Common Lisp accepted by the ACL2 theorem
    prover. Our translator produces executable ACL2 formal models,
    allowing us to both prove theorems about the translated models as well
    as validate those models by testing.  The resulting models can be
    translated and certified without user intervention, even for code with
    loops, thanks to the use of the def::ung macro which allows us to
    defer the question of termination. Initial measurements of concrete
    execution for translated LLVM functions indicate that performance is
    nearly 2.4 million LLVM instructions per second on a typical laptop
    computer. In this paper we overview the translation process and
    illustrate the translator's capabilities by way of a concrete example,
    including both a functional correctness theorem as well as a
    validation test for that example.",
  paper = "Hard14.pdf"
}

@book{Harp11,
  author = "Harper, Robert",
  title = {{Programming in Standard ML}},
  year = "2011",
  publisher = "CMU",
  keywords = "printed"
}

@misc{Harp13,
  author = "Harper, Robert",
  title = {{15.819 Homotopy Type Theory Course}},
  link = "\url{http://www.cs.cmu.edu/~rwh/courses/hott}",
  year = "2013"
}

@incollection{Harr93,
  author = "Harrison, John and Thery, Laurent",
  title = {{Reasoning about the Reals: The Marriage of HOL and
            Maple}},
  booktitle = "Logic Programming and Automated Reasoning",
  publisher = "Springer-Verlag",
  year = "1993",
  pages = "351-353",
  link = 
    "\url{https://link.springer.com/content/pdf/10.1007/3-540-56944-8_68.pdf}",
  abstract =
    "Computer algebra systems are extremely powerful and flexible, but
    often give results which require careful interpretation or are
    downright incorrect. By contrast, theorem provers are very reliable
    but lack the powerful specialized decision procedures and heuristics
    of computer algebra systems. In this paper we try to get the best of
    both worlds by careful exploitation of a link between a theorem prover
    and a computer algebra system.",
  paper = "Harr93.pdf"
}

@article{Harr94a,
  author = "Harrison, John",
  title = {{Constructing the Real Numbers in HOL}},
  journal = "Formal Methods in Systems Design",
  volume = "5",
  pages = "35-39",
  year = "1994",
  abstract =
    "This paper describes a construction of the real numbers in the HOL
    theorem-prover by strictly definitional means using a version of
    Dedekind's method. It also outlines the theory of mathematical
    analysis that has been built on top of it and discusses current and
    potential applications in verification and computer algebra.",
  paper = "Harr94a.pdf"
}

@inproceedings{Harr94,
  author = "Harrison, John and Thery, Laurent",
  title = {{Extending the HOL Thoerem Prover with a Computer Algebra System
           to Reason about the Reals}},
  booktitle = "Proc. Higher Order Logic Theorem Proving",
  year = "1994",
  publisher = "Springer",
  pages = "174-184",
  isbn = "978-3-540-48346-5",
  abstract =
    "In this paper we describe an environment for reasoning about the
    reals which combines the rigour of a theorem prover with the power of
    a computer algebra system.",
  paper = "Harr94.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@techreport{Harr95,
  author = "Harrison, John",
  title = {{Metatheory and Reflection in Theorem Proving: A Survey
           and Critique}},
  institution = "SRI Cambridge",
  year = "1995",
  type = "technical report",
  number = "CRC-053",
  abstract =
    "One way to ensure correctness of the inference performed by computer
    theorem provers is to force all proofs to be done step by step in a
    simple, more or less traditional, deductive system. Using techniques
    pioneered in Edinburgh LCF, this can be made palatable. However, some
    believe such an approach will never be efficient enough for large,
    complex proofs. One alternative, commonly called reflection, is to
    analyze proofs using a second layer of logic, a metalogic, and so
    justify abbreviating or simplifying proofs, making the kinds of
    shortcuts humans often do or appealing to specialized decision
    algorithms. In this paper we contrast the fully-expansive LCF approach
    with the use of reflection. We put forward arguments to suggest that
    the inadequacy of the LCF approach has not been adequately
    demonstrated, and neither has the practical utility of reflection
    (notwithstanding its undoubted intellectual interest). The LCF system
    with which we are most concerned is the HOL proof assistant.
    
    The plan of the paper is as follows. We examine ways of providing user
    extensibility for theorem provers, which naturally places the LCF and
    reflective approaches in opposition. A detailed introduction to LCF is
    provided, emphasizing ways in which it can be made efficient. Next, we
    present a short introduction to metatheory and its usefulness, and,
    starting from Gödel's proofs and Feferman's transfinite progressions
    of theories, look at logical `reflection principles'. We show how to
    introduce computational `reflection principles' which do not extend
    the power of the logic, but may make deductions in it more efficient,
    and speculate about their practical usefulness. Applications or
    proposed applications of computational reflection in theorem proving
    are surveyed, following which we draw some conclusions. In an
    appendix, we attempt to clarify a couple of other notions of
    `reflection' often encountered in the literature.
    
    The paper questions the too-easy acceptance of reflection principles
    as a practical necessity. However I hope it also serves as an adequate
    introduction to the concepts involved in reflection and a survey of
    relevant work. To this end, a rather extensive bibliography is
    provided.",
  paper = "Harr95.pdf"
}

@phdthesis{Harr96,
  author = "Harrison, John Robert",
  title = {{Theorem Proving with the Real Numbers}},
  school = "Churchhill College",
  comment = "U. Cambrige Computer Lab Tech Report 408,
            also Springer-Verlag 1988 ISBN 3-548-762566-6",
  year = "1996",
  abstract =
    "This thesis discusses the use of the real numbers in theorem
    proving. Typically, theorem provers only support a few `discrete'
    datatypes such as the natural numbers. However the availability of the
    real numbers opens up many interesting and important application
    areas, such as the verification of floating point hardware and hybrid
    systems. It also allows the formalization of many more branches of
    classical mathematics, which is particularly relevant for attempts to
    inject more rigour into computer algebra systems.
    
    Our work is conducted in a version of the HOL theorem prover. We
    describe the rigorous definitional construction of the real numbers,
    using a new version of Cantor's method, and the formalization of a
    significant portion of real analysis. We also describe an advanced
    derived decision procedure for the `Tarski subset' of real algebra as
    well as some more modest but practically useful tools for automating
    explicit calculations and routine linear arithmetic reasoning.
    
    Finally, we consider in more detail two interesting application
    areas. We discuss the desirability of combining the rigour of theorem
    provers with the power and convenience of computer algebra systems,
    and explain a method we have used in practice to achieve this. We then
    move on to the verification of floating point hardware. After a
    careful discussion of possible correctness specifications, we report
    on two case studies, one involving a transcendental function.
    
    We aim to show that a theory of real numbers is useful in practice and
    interesting in theory, and that the `LCF style' of theorem proving is
    well suited to the kind of work we describe. We hope also to convince
    the reader that the kind of mathematics needed for applications is
    well within the abilities of current theorem proving technology.",
  paper = "Harr96.pdf",
  keywords = "printed"
}

@misc{Harr06a,
  author = "Harrison, John",
  title = {{Formal Verification of Floating-point Arithmetic at Intel}},
  year = "2006",
  link = "\url{http://www.cl.cam.ac.uk/~jrh13/slides/jnao-02jun06/slides.pdf}",
  paper = "Harr06a.pdf"
}

@misc{Hear12,
  author = "Hearn, Peter W.O.",
  title = {{A Primer on Separation Logic (and Automatic Program 
           Verification and Analysis}},
  year = "2012",
  link =
"\url{www0.cs.ucl.ac.uk/staff/p.ohearn/papers/Marktoberdorf11LectureNotes.pdf}",
  abstract =
    "These are the notes to accompany a course at the Marktoberdorf PhD
    summer school in 2011. The course consists of an introduction to
    separation logic, with a slant towards its use in automatic program
    verification and analysis.",
  paper = "Hear12.pdf",
  keywords = "printed"
}

@misc{Heer02,
  author = "Heeren, Bastiaan and Hage, Jurriaan and Swierstra, Doaitse",
  title = {{Generalizing Hindley-Milner Type Inference Algorithms}},
  year = "2002",
  link = "\url{https://pdfs.semanticscholar.org/8983/233b3dff2c5b94efb31235f62bddc22dc899.pdf}",
  abstract =
    "Type inferencing according to the standard algorithms $W$ and $M$
    often yields uninformative error messages. Many times, this is a
    consequence of a bias inherent in the algorithms. The method
    developed here is to first collect constraints from the program, and
    to solve these afterwards, possibly under the influence of a
    heuristic.  We show the soundness and completeness of our algorithm.
    The algorithms $W$ and $M$ turn out to be deterministic instances of our
    method, giving the correctness for $W$ and $M$ with respect to the
    Hindley-Milner typing rules for free.  We also show that our algorithm
    is more flexible, because it naturally allows the generation of
    multiple messages",
  paper = "Heer02.pdf"
}

@article{Hoar69,
  author = "Hoare, C. A. R.",
  title = {{An Axiomatic Basis for Computer Programming}},
  journal = "CACM",
  volume = "12",
  number = "10",
  pages = "576-580",
  year = "1969",
  link = "\url{https://www.cs.cmu.edu/~crary/819-f09/Hoare69.pdf}",
  abstract = 
    "In this paper an attempt is made to explore the logical foundations
    of computer programming by use of techniques which were first applied
    in the study of geometry and have later been extended to other branches
    of mathematics. This involves the elucidation of sets of axioms and
    rules of inference which can be used in proofs of the properties of
    computer programs. Examples are given of such axioms and rules, and
    a formal proof of a simple theorem is displayed. Finally, it is argued
    that important advantages, both theoretical and practical, may follow
    from a pursuance of these topics",
  paper = "Hoar69.pdf",
  keywords = "printed"
}

@inproceedings{Homa94a,
  author = "Homann, Karsten",
  title = {{Integrating Explanation-Based Learning in Symbolic
           Computing}},
  booktitle = "Advances in Artificial Intelligence -- Theory and
               Application II, Volume II",
  pages = "130-135",
  year = "1994"
}

@phdthesis{Homa96a,
  author = "Homann, Karsten",
  title = {{Symbolisches L\"osen mathematischer Probleme durch
           Kooperation algorithmischer und logischer Systeme}},
  comment = {{Symboic Mathematical Problem Solving by Cooperation of Algorithmic and Logical Services (in German)}},
  year = "1996",
  school = {Universit\"at Karlsruhe},
  paper = "Homa96a.pdf"
} 

@misc{Howa80,
  author = "Howard, W. A.",
  title = {{The Formulae-as-Types Notion of Construction}},
  link = "\url{http://lecomte.al.free.fr/ressources/PARIS8_LSL/Howard80.pdf}",
  year = "1980",
  abstract =
    "The following consists of notes which were privately circulated in
    1969. Since they have been referred to a few times in the literature,
    it seems worth while to publish them. They have been rearranged for
    easier reading, and some inessential corrections have been made.
    
    The ultimate goal was to develop a notion of construction suitable for
    the interpretation of intuitionistic mathematics. The notion of
    construction developed in the notes is certainly too crude for that,
    so the use of the word {\sl construction} is not very appropriate.
    However, the terminology has been kept in order to preserve the
    original title and also to preserve the character of the notes. The
    title has a second defect; namely, the {\sl type} should be regarded
    as a abstract object whereas a {\sl formula} is the name of a type.",
  paper = "Howa80.pdf"
}

@article{Howe88,
  author = "Howe, Douglas J.",
  title = {{Computational Metatheory in Nuprl}},
  journal = "LNCS",
  volume = "310",
  pages = "238-257",
  year = "1988",
  publisher = "Springer-Verlag",
  abstract =
    "This paper describes an implementation within Nuprl of mechanisms
    that support the use of Nuprl's type theory as a language for
    constructing theorem-proving procedures. The main component of the
    implementation is a large library of definitions, theorems and
    proofs. This library may be regarded as the beginning of a book of
    formal mathematics; it contains the formal development and explanation
    of a useful subset of Nuprl's metatheory, and of a mechanism for
    translating results established about this embedded metatheory to the
    object level. Nuprl's rich type theory, besides permitting the
    internal development of this partial reflection mechanism, allows us
    to make abstractions that drastically reduce the burden of
    establishing the correctness of new theorem-proving procedures. Our
    library includes a formally verified term-rewriting system"
}  

@inproceedings{Huet87,
  author = {Huet, G\'erard},
  title = {{Induction Principles Formalized in the Calculus of Constructions}},
  booktitle = "TAPSOFT 87",
  publisher = "Springer-Verlag",
  series ="LNCS 249",
  year = "1987",
  pages = "276-286",
  abstract =
    "The Calculus of Constructions is a higher-order formalism for writing
    constructive proofs in a natural deduction style, inspired from work
    by de Bruijn, Girard, Martin-Lof, and Scott. THe calculus and its
    syntactic theory were presented in Coquand's thesis, and an
    implementation by the author was used to mechanically verify a
    substantial number of proofs demonstrating the power of expression of
    the formalism. The Calculus of Constructions is proposed as a
    foundation for the design of programming environments where programs
    are developed consistently with formal specifications. The current
    paper shows how to define inductive concepts in the calculus.
    
    A very general induction schema is obtained by postulating all
    elements of the type of interest to belong to the standard
    interpretation associated with a predicate map. This is similar to the
    treatment of D. Park, but the power of expression of the formallism
    permits a very direct treatment, in a language that is formalized
    enough to be actually implemented on a computer. Special instances of
    the induction schema specialize to Noetherian induction and Structural
    induction over any algebraic type.  Computational Induction is treated
    in an axiomatization of Domain Theory in Constructions. It is argued
    that the resulting principle is more powerful than LCF's, since the
    restriction on admissibility is expressible in the object language.",
  paper = "Huet87.pdf"
}

@misc{Heut16,
  author = {Huet, G\'erard  and Kahn, Gilles and Paulin-Mohring, Christine},
  title = {{The COQ Proof Assistant. A Tutorial}},
  year = "2016",
  link = "\url{https://coq.inria.fr/distrib/current/files/Tutorial.pdf}",
  paper = "Heut16.pdf"
}

@misc{Jack94,
  author = "Jackson, Paul",
  title = {{Exploring Abstract Algebra in Constructive Type Theory}},
  year = "1994",
  abstract = 
    "I describe my implementation of computational abstract algebra in
    the Nuprl system. I focus on my development of multivariate
    polynomials. I show how I use Nuprl's expressive type theory to define
    classes of free abelian monoids and free monoid algebras. These
    classes are combined to create a class of all implementations of
    polynomials.  I discuss the issues of subtyping and computational
    content that came up in designing the class definitions. I give
    examples of relevant theory developments, tactics and proofs. I
    consider how Nuprl could act as an algebraic 'oracle' for a computer
    algebra system and the relevance of this work for abstract functional
    programming.",
  paper = "Jack94.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@phdthesis{Jack95,
  author = "Jackson, Paul Bernard",
  title = {{Enhancing the NUPRL Proof Development System and Applying it to 
           Computational Abstract Algebra}},
  school = "Cornell University",
  year = "1995",
  month = "1",
  abstract = "
    This thesis describes substantial enhancements that were made to the
    software tools in the Nuprl system that are used to interactively
    guide the production of formal proofs. Over 20,000 lines of code were
    written for these tools. Also, a corpus of formal mathematics was
    created that consists of roughly 500 definitions and 1300
    theorems. Much of this material is of a foundational nature and
    supports all current work in Nuprl. This thesis concentrates on
    describing the half of this corpus that is concerned with abstract
    algebra and that covers topics central to the mathematics of the
    computations carried out by computer algebra systems.
    
    The new proof tools include those that solve linear arithmetic
    problems, those that apply the properties of order relations, those
    that carry out inductive proof to support recursive definitions, and
    those that do sophisticated rewriting. The rewrite tools allow
    rewriting with relations of differing strengths and take care of
    selecting and applying appropriate congruence lemmas automatically.
    The rewrite relations can be order relations as well as equivalence
    relations. If they are order relations, appropriate monotonicity
    lemmas are selected.
    
    These proof tools were heavily used throughout the work on
    computational algebra. Many examples are given that illustrate their
    operation and demonstrate their effectiveness.
    
    The foundation for algebra introduced classes of monoids, groups, ring
    and modules, and included theories of order relations and
    permutations.  Work on finite sets and multisets illustrates how a
    quotienting operation hides details of datatypes when reasoning about
    functional programs. Theories of summation operators were developed
    that drew indices from integer ranges, lists and multisets, and that
    summed over all the classes mentioned above. Elementary factorization
    theory was developed that characterized when cancellation monoids are
    factorial. An abstract data type for the operations of multivariate
    polynomial arithmetic was defined and the correctness of an
    implementation of these operations was verified. The implementation is
    similar to those found in current computer algebra systems.
    
    This work was all done in Nuprl's constructive type theory. The thesis
    discusses the appropriateness of this foundation, and the extent to
    which the work relied on it.",
  paper = "Jack95.pdf",
  keyword = "axiomref, CAS-Proof, printed"
}

@book{Jone93,
  author = "Jones, C.",
  title = {{Completing the Rationals and Metric Spaces in LEGO}},
  booktitle = "Logical Frameworks",
  pages = "209-222",
  year = "1993",
  publisher = "Cambridge University Press"
}  

@misc{JARx96,
  author = "various",
  title = {{Journal of Automated Reasoning}},
  publisher = "Springer",
  year = "1996",
  volume = "16",
  number = "1/2",
  comment = "Special Issue on Inductive Proof"
}

@misc{Juds16,
  author = "Judson, Thomas W.",
  title = {{Abstract Algebra: Theory and Applications}},
  link = "\url{http://abstract.ups.edu/download/aata-20160809-sage-7.3.pdf}",
  year = "2016"
}

@inproceedings{Kali07,
  author = "Kaliszyk, Cezary and Wiedijk, Freek",
  title = {{Certified Computer Algebra on Top of an Interactive Theorem
           Prover}},
  booktitle = "Toward Mecanized Mathematical Assistants",
  pages = "94-105",
  year = "2007",
  abstract =
    "We present a prototype of a computer algebra system that is built on
    top of a proof assistant, HOL Light. This architecture guarantees that
    one can be certain that the system will make no mistakes. All
    expressions in the system will have precise semantics, and the proof
    assistant will check the correctness of all simplifications according
    to this semantics. The system actually proves each simplification
    performed by the computer algebra system.
    
    Although our system is built on top of a proof assistant, we designed
    the user interface to be very close in spirit to the interface of
    systems like Maple and Mathematica. The system, therefore, allows the
    user to easily probe the underlying automation of the proof assistant
    for strengths and weaknesses with respect to the automation of
    mainstream computer algebra systems. The system that we present is a
    prototype, but can be straightforwardly scaled up to a practical
    computer algebra system.",
  paper = "Kali07.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@phdthesis{Kali09,
  author = "Kaliszyk, Cezary",
  title = {{Correctness and Availability: Building Computer Algebra on top
           of Proof Assistants and making Proof Assistants available over
           the Web}},
  year = "2009",
  school = "Radboud University, Nijmegen",
  link = "\url{http://cl-informatik.uibk.ac.at/users/cek/docs/09/kaliszyk\_thesis\_webdoc.pdf}",
  abstract =
    "In this thesis we present an approach to extending the usability
     of proof assistants in mathematics and computer science. We do it
     in two ways: by combining proof assistants with computer algebra
     systems and by providing interactive access to such systems on
     the web.",
  paper = "Kali09.pdf"
}

@misc{Kauf98,
  author = "Kaufmann, Matt and Moore, J Strother",
  title = {{A Precise Description of the ACL2 Logic}},
  year = "1998",
  link = "\url{www.cs.utexas.edu/users/moore/publications/km97a.pdf}",
  abstract = "The ACL2 logic is a first-order, essentially quantifier-free
   logic of total recursive functions providing mathematical induction
   and several extension principles, including symbol package definition
   and recursive function definition. In this document we describe the
   logic more precisely.",
  paper = "km97a.pdf"
}

@misc{Kenn13,
  author = "Kennedy, Andrew and Benton, Nick and Jensen, Jonas B. and
           Dagand, Pierre-Evariste",
  title = {{Coq: The world's best macro assembler?}},
  year = "2013",
  link = "\url{http://research.microsoft.com/en-us/um/people/nick/coqasm.pdf}",
  abstract = 
    "We describe a Coq formalization of a subset of the x86
     architecture. One emphasis of the model is brevity: using dependent
     types, type classes and notation we give the x86 semantics a makeover
     that counters its reputation for baroqueness. We model bits, bytes,
     and memory concretely using functions that can be computed inside Coq
     itself; concrete representations are mapped across to mathematical
     objects in the SSREFLECT library (naturals, and integers modulo 2n)
     to prove theorems. Finally, we use notation to support conventional
     assembly code syntax inside Coq, including lexically-scoped
     labels. Ordinary Coq definitions serve as a powerful ``macro'' feature
     for everything from simple conditionals and loops to stack-allocated
     local variables and procedures with parameters. Assembly code can be
     assembled within Coq, producing a sequence of hex bytes. The assembler
     enjoys a correctness theorem relating machine code in memory to a
     separation-logic formula suitable for program verification.",
  paper = "Kenn13.pdf"
}

@article{Kerb96,
  author = "Kerber, Manfred and Kohlhase, Michael and Sorge, Volker",
  title = {{Integrating Computer Algebra with Proof Planning}},
  journal = "Lecture Notes in Computer Science",
  volume = "1128",
  pages = "204-215",
  year = "1996",
  abstract = 
    "Mechanised reasoning systems and computer algebra systems have
    apparently different objectives.  Their integration is, however,
    highly desirable, since in many formal proofs both of the two
    different tasks, proving and calculating, have to be performed.  In
    the context of producing reliable proofs, the question how to ensure
    correctness when integrating a computer algebra system into a
    mechanised reasoning system is crucial.  In this contribution, we
    discuss the correctness problems that arise from such an integration
    and advocate an approach in which the calculations of the computer
    algebra system are checked at the calculus level of the mechanised
    reasoning system.  We present an implementation which achieves this
    by adding a verbose mode to the computer algebra system which produces
    high-level protocol information that can be processed by an interface
    to derive proof plans.  Such a proof plan in turn can be expanded to
    proofs at different levels of abstraction, so the approach is
    well-suited for producing a high-level verbalised explication as well
    as for a low-level (machine checkable) calculus-level proof.",
  paper = "Kerb96.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@article{Kerb07,
  author = "Kerber, Manfred and Pollet, Martin",
  title = {{Informal and Formal Representations in Mathewmatics}},
  journal = "Studies in Logic, Grammar and Rhetoric 10",
  volume = "23",
  year = "2007",
  isbn = "978-83-7431-128-1",
  abstract =
    "In this paper we discuss the importance of good representations in
    mathematics and relate them to general design issues. Good design
    makes life easy, bad design difficult. For this reason experienced 
    mathematicians spend a significant amount of their time on the design
    of their concepts.  While many formal systems try to support this by
    providing a high-level language, we argue that more should be learned
    from the mathematical practice in order to improve the applicability
    of formal systems.",
  paper = "Kerb07.pdf",
  keywords = "printed"
}

@article{Kome11,
  author = "Komendantsky, Vladimir and Konovalov, Alexander and Linton, Steve",
  title = {{View of Computer Algebra Data from Coq}},
  journal = "Lecture Notes in Computer Science",
  volume = "6824",
  pages = "74-80",
  year = "2011",
  publisher = "Springer-Verlag",
  abstract = 
    "Data representation is an important aspect of software composition. 
    It is often the case that different software components are
    programmed to represent data in the ways which are the most
    appropriate for their problem domains. Sometimes, converting data from
    one representation to another is a non-trivial task. This is the
    case with computer algebra systems and type-theory based interactive
    theorem provers such as Coq. We provide some custom instrumentation
    inside Coq to support a computer algebra system (CAS) communication
    protocol known as SC-SCP. We describe general aspects of viewing
    OpenMath terms produced by a CAS in the calculus of Coq, as well as
    viewing pure Coq terms in a simpler type system that is behind OpenMath.",
  paper = "Kome11.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Kowa79,
  author = "Kowalski, Robert",
  title = {{Algorithm = Logic + Control}},
  journal = "CACM",
  volume = "22",
  number = "7",
  year = "1979",
  paper = "Kowa79.pdf",
  keywords = "printed"
}

@inproceedings{Koun90,
  author = "Kounalis, Emmanuel and Rusinowitch, Michael",
  title = {{A Proof System for Conditional Algebraic Specifications}},
  booktitle = "Conference Conditional and Typed Rewriting Systems",
  year = "1990",
  abstract =
    "Algebraic specifications provide a formal basis for designing
    data-structures and reasoning about their properties. 
    Sufficient-completeness and consistency are fundamental
    notions for building algebraic specifications in a modular way. We
    give in this paper effective methods for testing these properties for
    specifications with conditional axioms."
}

@phdthesis{Kreb15,
  author = "Krebbers, Robbert Jan",
  title = {{The C standard formalized in Coq}},
  school = "Radboud University",
  year = "2015",
  file = "Kreb15.pdf",
  link = "\url{http://robbertkrebbers.nl/thesis.html}"
}

@misc{Kreb15a,
  author = "Krebbers, Robbert and Wiedijk, Freek",
  title = {{A Typed C11 Semantics for Interactive Theorem Proving}},
  year = "2015",
  link = "\url{http://robbertkrebbers.nl/research/articles/interpreter.pdf}",
  abstract =
    "We present a semantics of a significant fragment of the C programming
    language as described by the C11 standard. It consists of a small step
    semantics of a core language, which uses a structured memory model to
    capture subtleties of C11, such as strict-aliasing restrictions
    related to unions, that have not yet been addressed by others. The
    semantics of actual C programs is defined by translation into this
    core language. We have an explicit type system for the core language,
    and prove type preservation and progress, as well as type correctness
    of the translation.
    
    Due to unspecified order of evaluation, our operational semantics is
    non-deterministic. To explore all defined and undefined behaviors, we
    present an executable semantics that computes a stream of finite sets
    of reachable states. It is proved sound and complete with respect to
    the operational semantics.
    
    Both the translation into the core language and the executable
    semantics are defined as Coq programs.  Extraction to OCaml is used to
    obtain a C interpreter to run and test the semantics on actual C
    programs. All proofs are fully formalized in Coq.",
  paper = "Kreb15a.pdf"
}

@misc{Kreb17,
  author = "Krebbers, Robbert Jan",
  title = {{The CH$_2$O formalization of ISO C11}},
  year = "2017",
  link = "\url{http://robbertkrebbers.nl/research/ch2o/}"
}

@article{Kupe14,
  author = "Kuper, Jan and Wester, Rinse",
  title = {{N Queens on an FPGA: Mathematics, Programming, or Both?}},
  year = "2014",
  link = "\url{http://doc.utwente.nl/94663/1/NQueensOnFPGA.pdf}",
  publisher = "Open Channel Publishing Ltd",
  journal = "Communicating Process Architectures 2014",
  abstract = 
    "This paper presents a design methodology for deriving an FPGA
    implementation directly from a mathematical specification, thus
    avoiding the switch in semantic perspective as is present in widely
    applied methods which include an imperative implementation as an
    intermediate step.

    The first step in the method presented in this paper is to transform a
    mathematical specification into a Haskell program. The next step is to
    make repetition structures explicit by higher order functions, and
    after that rewrite the specification in the form of a Mealy
    Machine. Finally, adaptations have to be made in order to comply to
    the fixed nature of hardware. The result is then given to
    C$\lambda$aSH, a compiler which generates synthesizable VHDL from the
    resulting Haskell code. An advantage of the approach presented here is
    that in all phases of the process the design can be directly simulated
    by executing the defining code in a standard Haskell environment.

    To illustrate the design process, the N queens problem is chosen as a
    running example."
}

@article{Kell12,
  author = "Keller, Chantal and Lasson, Marc",
  title = {{The Refined Calculus of Inductive Construction: Parametricity and
           Abstraction}},
  journal = "arXiv",
  year = "2012",
  link = "\url{http://arxiv.org/pdf/1211.6341v1.pdf}",
  abstract = 
    "We present a refinement of the Calculus of Inductive Constructions in
    which one can easily define a notion of relational parametricity. It
    provides a new way to automate proofs in an interactive theorem prover
    like Coq.",
  paper = "Kell12.pdf"
}

@inproceedings{Kuma91,
  author = "Kumar, Ramayya and Kropf, Thomas and Schneider, Klaus",
  title = 
    {{Integrating a First-Order Automatic Prover in the HOL Environment}},
  booktitle = "Int. Workshop on the HOL Theorem Prover System and its 
               Applications",
  publisher = "IEEE Computer Society Press",
  year = "1991",
  abstract =
    "The HOL system is a powerful tool for proving higher-order formulae.
    However, proofs have to be performed interactively and only little
    automation using tactics is possible.  Even though interaction is
    desirable to guide major and creative backward proof steps of complex
    proofs, a deluge of simple sub-goals may evolve which all have to be
    proven manually in order to accomplish the proof.  Although these
    sub-goals are often simple formulae, their proof has not yet been
    automated in HOL.  
    
    In this paper it is shown how it is possible to
    automate these tasks by integrating a first-order automated theorem
    proving tool, called FAUST, into HOL.  It is based on an efficient
    variant of the well-known sequent calculus.  In order to maintain the
    high confdence in HOL-generated proofs, FAUST is able to generate HOL
    tactics which may be used to post-justifr the theorem derived by FAUST
    in HOL.  The underlying calculus of FAUST, the tactic generation, as
    well as experimental results are presented.",
  paper = "Kuma91.pdf"
}

@article{Kutz86,
  author = "Kutzler, B. and Stifter, S.",
  title = {{On the application of Buchberger's algorithm to automated
           geometry theorem proving}},
  journal = "J. Symbolic Computation",
  volume = "2",
  number = "4",
  year = "1986",
  pages = "389-397",
  abstract =
    "In this paper we present a new approach to automated geometry theorem
    proving that is based on Buchberger's Gröbner bases method. The goal
    is to automatically prove geometry theorems whose hypotheses and
    conjecture can be expressed algebraically, i.e. by polynomial
    equations. After shortly reviewing the problem considered and
    discussing some new aspects of confirming theorems, we present two
    different methods for applying Buehberger's algorithm to geometry
    theorem proving, each of them being more efficient than the other on a
    certain class of problems. The second method requires a new notion of
    reduction, which we call pseudoreduction. This pseudoreduction yields
    results on polynomials over some rational function field by
    computations that are done merely over the rationals and, therefore,
    is of general interest also. Finally, computing time statistics on 70
    non-trivial examples are given, based on an implementation of the
    methods in the computer algebra system SAC-2 on an IBM 4341.",
  paper = "Kutz86.pdf"
}

@book{Lamp02,
  author = "Lamport, Leslie",
  title = {{Specifying Systems}},
  year = "2002",
  link = "\url{http://research.microsoft.com/en-us/um/people/lamport/tla/book-02-08-08.pdf}",
  publisher = "Addison-Wesley",
  isbn = "0-321-14306-X",
  paper = "Lamp02.pdf"
}

@misc{Lamp13,
  author = "Lamport, Leslie",
  title = {{Errata to Specifying Systems}},
  year = "2013",
  link = "\url{http://research.microsoft.com/en-us/um/people/lamport/tla/errata-1.pdf}",
  publisher = "Microsoft",
  abstract = "
    These are all the errors and omissions to the first printing (July
    2002) of the book {\sl Specifying Systems} reported as of 29 October
    2013.  Positions in the book are indicated by page and line number,
    where the top line of a page is number 1 and the bottom line is number
    $-1$. A running head and a page number are not considered to be lines,
    but all other lines are. Please report any additional errors to the
    author, whose email address is posted on {\tt http://lamport.org}. The
    first person to report an error will be acknowledged in any revised
    edition.",
  paper = "Lamp13.pdf"
}

@misc{Lamp14,
  author = "Lamport, Leslie",
  title = {{How to Write a $21^{st}$ Century Proof}},
  year = "2014",
  link = "\url{http://research.microsoft.com/en-us/um/people/lamport/pubs/paper.pdf}",
  publisher = "Microsoft",
  abstract = "
   A method of writing proofs is described that makes it harder to prove
   things that are not true. The method, based on hierarchical
   structuring, is simple and practical. The author's twenty years of
   experience writing such proofs is discussed.",
  paper = "Lamp14.pdf"
}

@misc{Lamp14a,
  author = "Lamport, Leslie",
  title = {{Talk: How to Write a $21^{st}$ Century Proof}},
  year = "2014",
  link = "\url{http://hits.mediasite.com/mediasite/Play/29d825439b3c49f088d35555426fbdf81d}",
  comment = "2nd Heidelberg Laureate Forum Lecture Tuesday Sep 23, 2014"
}

@misc{Lamp16,
  author = "Lamport, Leslie",
  title = {{TLA+ Proof System}},
  year = "2016",
  link = "\url{https://tla.msr-inria.inria.fr/tlaps/content/Documentation/Tutorial/The_example.html}",
  abstract = "Demonstration of Euclid Algorithm Proof in TLA+"
}

@article{Wony18,
  author = "Lee, Wonyeol and Sharma, Rahul and Aiken, Alex",
  title = {{On Automatically Proving the Correctness of math.h 
            Implementation}},
  journal = "Proc. ACM Programming Languages",
  volume = "2",
  number = "42",
  year = "2018",
  pages = "1-32",
  abstract = 
    "Industry standard implementations of math.h claim (often without
    formal proof) tight bounds on floating-point errors. We demonstrate a
    novel static analysis that proves these bounds and verifies the
    correctness of these implementations. Our key insight is a reduction
    of this verification task to a set of mathematical optimization
    problems that can be solved by off-the-shelf computer algebra
    systems. We use this analysis to prove the correctness of
    implementations in Intel’s math library automatically. Prior to this
    work, these implementations could only be verified with significant
    manual effort.",
  paper = "Wony18.pdf",
  keywords = "printed"
}

@misc{Lond74,
  author = "London, Ralph L. and Musser, David R.",
  title = {{The Application of a Symbolic Mathematical System to Program
           Verification}},
  publisher = "USC Information Sciences Institute",
  year = "1974",
  abstract =
    "Program verification is a relatively new application area for
    symbolic mathematical systems.  We report on an interactive
    program verification system, based on the inductive assertion method,
    which system is implemented using an existing symbolic mathematical
    language and supporting system, Reduce.  Reduce has been augmented
    with a number of capabilities which are important to program 
    verification, particularly transformations on relational and Boolean
    expressions.  We believe these capabilities would be valuable in other
    contexts and should be incorporated more widely into symbolic
    mathematical systems for general use.  The program verification
    application can serve as a guide to an appropriate definition of
    such capabilities, particularly with regard to the need to distinguish
    between undefined program variables and polynomial indeterminates.
    Additional capabilities which would benefit the program verification
    application include representation of user-defined functions by
    internal forms which directly incorporate properties such as
    commutativity and associativity (as is commonly done with plus and
    times), and a comprehensive facility for defining conditionally
    applicable transformations.",
  paper = "Lond74.pdf",
  keywords = "axiomref, printed"
}

@article{Mahb06,
  author = "Mahboubi, Assia",
  title = {{Proving Formally the Implementation of an Efficient gcd 
           Algorithm for Polynomials}},
  journal = "Lecture Notes in Computer Science",
  volume = "4130",
  year = "2006",
  pages = "438-452",
  abstract = "
    We describe here a formal proof in the Coq system of the structure
    theorem for subresultants which allows to prove formally the
    correctness of our implementation of the subresultants algorithm.
    Up to our knowledge it is the first mechanized proof of this result.",
  paper = "Mahb06.pdf"
}

@book{Mahb16,
  author = "Mahboubi, Assia and Tassi, Enrico and Bertot, Yves and 
            Gonthier, Georges",
  title = {{Mathematical Components}},
  year = "2016",
  publisher = "math-comp.github.io/mcb",
  link = "\url{https://math-comp.github.io/mcb/book.pdf}",
  abstract =
    "{\sl Mathematical Components} is the name of a library of formalized
    mathematic for the COQ system. It covers a veriety of topics, from the
    theory of basic data structures (e.g. numbers, lists, finite sets) to
    advanced results in various flavors of algebra. This library
    constitutes the infrastructure for the machine-checked proofs of the
    Four Color Theorem and the Odd Order Theorem.

    The reason of existence of this books is to break down the barriers to
    entry. While there are several books around covering the usage of the
    COQ system and the theory it is based on, the Mathematical Components
    library is build in an unconventional way. As a consequence, this book
    provides a non-standard presentation of COQ, putting upfront the
    formalization choices and the proof style that are the pillars of the
    library.

    This book targets two classes of public. On one hand, newcomers, even
    the more mathematically inclined ones, find a soft introduction to the
    programming language of COQ, Gallina, and the Ssreflect proof
    language.  On the other hand accustomed COQ users find a substantial
    accound of the formalization style that made the Mathematical
    Components library possible.

    By no means does this book pretend to be a complete description of COQ
    or Ssreflect: both tools already come with a comprehensive user
    manual.  In the course of the book, the reader is nevertheless invited
    to experiment with a large library of formalized concepts and she is
    given as soon as possible sufficient tools to prove non-trivial
    mathematical results by reusing parts of the library. By the end of
    the first part, the reader has learnt how to prove formally the
    infinitude of prime numbers, or the correctnes of the Euclidean's
    division algorithm, in a few lines of proof text.",
  paper = "Mahb16.pdf"
}

@techreport{Male17,
  author = "Maletzky, Alexander",
  title = {{A New Reasoning Framework for Theorema 2.0}},
  year = "2017",
  institution = "RISC Linz",
  abstract =
    "We present a new add-on for the Theorema 2.0 proof assistant,
    consisting of a reasoning framework in the spirit of (though not
    exactly as) the well-known LCF approach to theorem proving: a small,
    trusted kernel of basic inferences complemented by an extensive
    collection of automatic and interactive proof methods that construct
    proofs solely in terms of the basic inferences. We explain why such an
    approach is desirable in the first place in Theorema (at least as a
    possible alternative to the existing paradigm), how it fits together
    with the current default set-up of the system, and how proof-checking
    with the inference kernel of the new framework proceeds. Since all
    this is heavily inspired by the Isabelle proof assistant, we in
    particular also highlight the differences between Isabelle and our
    approach.", 
  paper = "Male17.pdf",
  keywords = "printed"
}

@article{Male16,
  author = "Maletzky, Alexander",
  title = {{Interactive Proving, Higher-Order Rewriting, and Theory Analysis
           in Theorema 2.0}},
  journal = "LNCS",
  volume = "9725",
  pages = "59-66",
  year = "2016",
  abstract =
    "In this talk we will report on three useful tools recently
    implemented in the frame of the Theorema project: a graphical user
    interface for interactive proof development, a higher-order
    rewriting mechanism, and a tool for automatically analyzing the
    logical structure of Theorema-theories. Each of these three tools
    already proved extremely useful in the extensive formal exploration of
    a non-trivial mathematical theory, namely the theory of Groeobner
    bases and reduction rings, in Theorema 2.0.",
  paper = "Male16.pdf"
}

@misc{Mart80,
  author = {Martin-L\"of, Per},
  title = {{Intuitionistic Type Theory}},
  link = "\url{http://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf}",
  year = "1980",
  paper = "Mart80.pdf"
}

@inproceedings{Mart85,
  author = {Martin-L\"of, Per},
  title = {{Costructive Mathematics and Computer Programming}},
  booktitle = "Proc Royal Soc. of London on Math. Logic and Programming Lang.",
  link = "\url{http://www.cs.tufts.edu/~nr/cs257/archive/per-martin-lof/constructive-math.pdf}",
  year = "1985",
  isbn = "0-13-561465-1",
  pages = "168-184",
  publisher = "Prentice-Hall",
  paper = "Mart85.pdf"
}

@article{Mart96,
  author = {Martin-L\"of, Per},
  title = {{On the Meaning of the Logical Constants and the Justifications
           of the Logical Laws}},
  year = "1996",
  journal = "Nordic Journal of Philosophical Logic",
  volume = "1",
  number = "1",
  pages = "11-60",
  abstract =
    "The following three lectures were given in the form of a short course
    at the meeting Teoria della Dimonstrazione e Filosofia della Logica,
    organized in Siena, 6-9 April 1983, by the Scuola di Specializzazione
    in Logica Matematica of the Universit\`a degli Studi di Siena. I am
    very grateful to Giovanni Sambin and Aldo Ursini of that school, not
    only for recording the lectures on tape, but, above all, for
    transcribing the tapes produced by the recorder: no machine could have
    done that work. This written version of the lectures is based on their
    transcription. The changes that I have been forced to make have mostly
    been of a stylistic nature, except at one point. In the second
    lecture, as I actually gave it, the order of conceptual priority
    between the notions of proof and immediate inference was wrong. Since
    I discovered my mistake later the same month as the meeting was held,
    I thought it better to let the written text diverge from the oral
    presentation rather than possibly confusing others by letting the
    mistake remain. The oral origin of these lectures is the source of the
    many redundancies of the written text. It is also my sole excuse for
    the lack of detailed references.",
  paper = "Mart96.pdf"
}

@misc{Mart97,
  author = "Martin, Ursula and  Shand, D",
  title = {{Investigating some Embedded Verification Techniques for 
           Computer Algebra Systems}},
  link = 
    "\url{http://www.risc.jku.at/conferences/Theorema/papers/shand.ps.gz}",
  abstract = "
    This paper reports some preliminary ideas on a collaborative project
    between St. Andrews University in the UK and NAG Ltd. The project aims
    to use embedded verification techniques to improve the reliability and
    mathematical soundness of computer algebra systems. We give some
    history of attempts to integrate computer algebra systems and
    automated theorem provers and discuss possible advantages and
    disadvantages of these approaches. We also discuss some possible case
    studies.",
  paper = "Mart97.ps",
  keywords = "axiomref, CAS-Proof, printed"
}

@book{Maso86,
  author = "Mason, Ian A.",
  title = {{The Semantics of Destructive Lisp}},
  publisher = "Center for the Study of Language and Information",
  year = "1986",
  isbn = "0-937073-06-7",
  abstract = "
    Our basic premise is that the ability to construct and modify programs
    will not improve without a new and comprehensive look at the entire
    programming process. Past theoretical research, say, in the logic of
    programs, has tended to focus on methods for reasoning about
    individual programs; little has been done, it seems to us, to develop
    a sound understanding of the process of programming -- the process by
    which programs evolve in concept and in practice. At present, we lack
    the means to describe the techniques of program construction and
    improvement in ways that properly link verification, documentation and
    adaptability."
}

@article{Maur73,
  author = "Maurer, D.",
  title = {{Applications of Symbolic Mathematical Manipulation to 
            Algorithmic Verification (Abstract)}},
  journal = "SIGSAM Bulletin",
  volume = "26",
  year = "1973",
  pages = "4"
}

@inproceedings{Mcal96,
  author = "McAllester, D. and Arkondas, K.",
  title = {{Walther Recursion}},
  booktitle = "CADE 13",
  publisher = "Springer-Verlag",
  year = "1996",
  abstract =
    "Primitive recursion is a well known syntactic restriction on
    recursion definitions which guarantees termination. Unfortunately
    many natural definitions, such as the most common definition of
    Euclid's GCD algorithm, are not primitive recursive. Walther has
    recently given a proof system for verifying termination of a
    broader class of definitions. Although Walther's system is highly
    automatible, the class of acceptable definitions remains only
    semi-decidable. Here we simplify Walther's calculus and give a
    syntactic criteria generalizes primitive recursion and handles
    most of the examples given by Walthar. We call the corresponding
    class of acceptable defintions ``Walther recursive''.",
  paper = Mcal96.pdf
}

@article{Mcbr06,
  author = "McBride, Conor and Goguen, Healfdene and McKinna, James",
  title = {{A Few Constructions on Constructors}},
  journal = "Lecture Notes in Computer Science",
  volume = "3839",
  pages = "186-200",
  year = "2006",
  link = "\url{http://www.strictlypositive.org/concon.ps.gz}",
  abstract =
    "We present four constructions for standard equipment which can be
    generated for every inductive datatype: case analysis, structural
    recursion, no confusion, acyclicity. Our constructions follow a
    two-level approach -- they require less work than the standard
    techniques which inspired them. Moreover, given a suitably
    heterogeneous notion of equality, they extend without difficulty to
    inductive families of datatypes. These constructions are vital
    components of the translation from dependently typed programs in
    pattern matching style to the equivalent programs expressed in terms
    of induction principles and as such play a crucial behind-the-scenes
    role in Epigram.",
  paper = "Mcbr06.pdf"
}  

@article{Mcca78,
  author = "McCarthy, John",
  title = {{A Micro-Manual for Lisp -- Not The Whole Truth}},
  journal = "ACM SIGPLAN Notices",
  volume = "13",
  number = "8",
  year = "1978",
  paper = "Mcca78.pdf"
}

@article{Mccu93,
  author = "McCune, William W.",
  title = {{Single Axioms for Groups and Abelian Groups with Various 
           Operations}},
  journal = "J. Automated Reasoning",
  volume = "10",
  number = "1",
  year = "1993",
  abstract = 
    "This paper summarizes the results of an investigation into single
    axioms for groups, both ordinary and Abelian, with each of following
    six sets of operations: \{product, inverse\}, \{division\}, \{double
    division, identity\}, \{double division, inverse\}, \{division,
    identity\} , and \{division, inverse\}. In all but two of the twelve
    corresponding theories, we present either the rst single axioms known
    to us or single axioms shorter than those previously known to us. The
    automated theorem-proving program Otter was used extensively to
    construct sets of candidate axioms and to search for and nd proofs
    that given candidate axioms are in fact single axioms.",
  paper = "Mccu93.pdf",
  keywords = "printed"
}

@article{Medi04,
  author = "Medina-Bulo, Inmaculada and Lozano-Palomo, F. and
            Alonso-Jimenez, J.A. and Ruiz-Reina, J.L.",
  title = {{Verified Computer Algebra in ACL2}},
  journal = "LNAI",
  volume = "3249",
  year = "2004",
  pages = "171-184",
  abstract =
    "In this paper, we present the formal verification of a Common Lisp
    implementation of Buchberger’s algorithm for computing Gröbner bases
    of polynomial ideals. This work is carried out in the Acl2 system and
    shows how verified Computer Algebra can be achieved in an executable
    logic.",
  paper = "Medi04.pdf"
}

@mastersthesis{Mein13,
  author = "Meindl, Diana",
  title = {{Implementation of an Algorithm Computing the Greatest
            Common Divisor for Multivariate Polynomials}},
  year = "2013",
  school = "RISC Linz"
}    

@article{Melq12,
  author = "Melquiond, Guillaume",
  title = {{Floating-point arithmetic in the Coq system}},
  journal = "Information and Computation",
  volume = "216",
  pages = "14-23",
  year = "2012",
  link = "\url{https://www.lri.fr/~melquion/doc/08-mc8-article.pdf}",
  abstract = 
    "The process of proving some mathematical theorems can be greatly
    reduced by relying on numerically-intensive computations with a
    certified arithmetic. This article presents a formalization of
    floating-point arithmetic that makes it possible to efficiently
    compute inside the proofs of the Coq system. This certified library is
    a multi-radix and multi-precision implementation free from underflow
    and overflow. It provides the basic arithmetic operators and a few
    elementary functions.",
  paper = "Melq12.pdf",
  keywords = "printed"
}

@book{Mend87,
  author = "Mendelson, Elliot",
  title = {{Introduction to Mathematical Logic}},
  publisher = "Wadsworth and Brooks/Cole",
  year = "1987",
  isbn = "978-1482237726"
}

@inproceedings{Mesh01,
  author = "Meshveliani, Sergei D.",
  title = {{Computer Algebra with Haskell: Applying 
           Functional-Categorical-`Lazy' Programming}},
  booktitle = "Computer Algebra and its Application to Physics",
  year = "2001",
  pages = "203-211",
  link = 
    "\url{compalg.jinr.ru/Confs/CAAP\_2001/Final/proceedings/proceed.pdf}",
  abstract = 
    "We give an outline of a computer algebra program writting in a
     functional language Haskell and implementing certain piece of
     commutative algebra",
  paper = "Mesh01.pdf",
  keywords = "axiomref, printed"

}

@misc{Mesh05,
  author = "Meshveliani, Sergei D.",
  title = {{Term rewriting, Equationional Reasoning, Automatic proofs}},
  link = "\url{ftp://ftp.botik.ru/pub/local/Mechveliani/dumatel/1.02/}",
  year = "2005",
  paper = "Mesh05.pdf",
  keywords = "printed"
}

@misc{Mesh13,
  author = "Meshveliani, Sergei D.",
  title = {{Dependent Types for an Adequate Programming of Algebra}},
  link = "\url{http://ceur-ws.org/Vol-1010/paper-05.pdf}",
  year = "2013",
  abstract =
    "This research compares the author’s experience in programming
    algebra in Haskell and in Agda (currently the former experience is
    large, and the latter is small). There are discussed certain hopes and
    doubts related to the dependently typed and verified programming of
    symbolic computation. This concerns the 1) author’s experience
    history, 2) algebraic class hierarchy design, 3) proof cost overhead
    in evaluation and in coding, 4) other subjects. Various examples are
    considered.",
  paper = "Mesh13.pdf"
}

@misc{Mesh10,
  author = "Meshveliani, Sergei D.",
  title = {{Haskell and computer algebra}},
  link = "\url{http://www.botik.ru/pub/local/Mechveliani/basAlgPropos/haskellinCA2.pdf.zip}",
  year = "2010",
  abstract =
    "We consider the ways to program mathematics in the Haskell language.
    To start a discussion, we pretend to propose certain basic algebra
    library BAL for Haskell. We also mention several desirable language
    features. Algebraic additions in BAL are divided into the 'ordinary'
    and 'advanced'. Standard algebraic classes are reorganized to make
    them mathematically meaningful. For the 'advanced' part a sample
    argument approach is introduced -- as certain alternative for the
    dependent type language extension. The library is implemented in the
    existing Haskell, by 'hiding' a certain part of the existing Prelude.",
  paper = "Mesh10.pdf",
  keywords = "axiomref"
}

@article{Mesh14,
  author = "Meshveliani, Sergei D.",
  title = {{On dependent types and intuitionism in programming mathematics}},
  journal = "Program systems: theory and applications",
  year = "2014",
  volume = "5",
  numbrer = "3(21)",
  pages = "27-50",
  comment = "(In Russian)",
  link = "\url{http://psta.psiras.ru/read/psta2014_3_27-50.pdf}",
  abstract =
    "It is discussed a practical possibility of a provable programming
    of mathematics basing of the approach of intuitionism, a language
    with dependent types, proof-carrying code. This approach is 
    illustrated with examples. The discourse bases on the experience
    of implementing in the {\tt Agda} language of a certain small
    algebraic library including the arithmetic of a residue domain
    $R/(b)$ for an arbitrary Euclidean ring R. (In Russian)",
  paper = "Mesh14.pdf",
  keywords = "axiomref"
}

@article{Mesh15,
  author = "Meshveliani, Sergei D.",
  title = {{Programming basic computer algebra in a language with 
           dependent types}},
  journal = "Program systems: theory and applications",
  year = "2015",
  volume = "6",
  numbrer = "4(27)",
  pages = "313-340",
  comment = "(In Russian)",
  link = "\url{http://psta.psiras.ru/read/psta2015_4_313-340.pdf}",
  abstract =
    "It is described the experience in provable programming of certain
    classical categories of computational algebra (``group'', ``ring'',
    and so on) basing on the approach of intuitionism, a language with
    dependent types, forming of machine-checked proofs. There are detected
    the related problems, and are described certain additional possibilities
    given by the approach. The {\tt Agda} functional language is used as an
    instrument. This paper is a continuation for the introductory paper
    published in this journal in 2014. (In Russian)",
  paper = "Mesh15.pdf",
  keywords = "CAS-Proof"
}

@book{Mesh16,
  author = "Meshveliani, Sergei D.",
  title = {{DoCon -- A Provable Algebraic Domain Constructor}},
  link = 
    "\url{http://www.botik.ru/pub/local/Mechveliani/docon-A/0.04/manual.pdf}",
  publisher = "User Manual, Version 0.04",
  year = "2016",
  abstract =
    "This book is about 1) a manual on the DoCon-A program library, 2) a book
    explaining how to program algebra in a purely functional language with
    {\sl dependent types} (specifially, in {\tt Agda}), with providing
    machine-checked proofs, and following constructive mathematics.
    
    The above point of proofs means that a program not only implements an
    algorithm, but explains to the compiler the needed mathematical notions
    and provides the needed proofs in the form of type expressions and
    functions. And the compiler (more precisely, type checker) is able to
    verify these proofs statically (before running), and to prepare the
    algorithm for running.",
  paper = "Mesh16,pdf",
  keywords = "axiomref"
}

@misc{Mesh16a,
  author = "Meshveliani, Sergei D.",
  title = {{Provable programming of algebra: particular points, examples}},
  link = "\url{http://www.botik.ru/pub/local/Mechveliani/provProgExam.zip}",
  year = "2016",
  abstract =
    "It is discussed an experiance in provable programming of a computer 
    algebra library with using a purely functional language with dependent
    tyhpes ({\tt Agda}). There are given several examples illustrating
    particular points of implementing the approach of constructive 
    mathematics.",
  paper = "Mesh16a.pdf",
  keywords = "printed"
}

@article{Mitc88,
  author = "Mitchell, John C. and Plotkin, Gordon D.",
  title = {{Abstract types have existential type}},
  journal = "ACM TOPLAS",
  volume = "10",
  number = "3",
  year = "1988",
  pages = "470-502",
  abstract =
    "Abstract data type declarations appear in typed programming languages
    like Ada, Alphard, CLU and ML. This form of declaration binds a list
    of identifiers to a type with associated operations, a composite
    “value” we call a data algebra. We use a second-order typed lambda
    calculus SOL to show how data algebras may be given types, passed as
    parameters, and returned as results of function calls. In the process,
    we discuss the semantics of abstract data type declarations and review
    a connection between typed programming languages and constructive
    logic.",
  paper = "Mitc88.pdf"
}

@techreport{Miln72,
  author = "Milner, Robert",
  title = {{Logic for Computable Functions: Description of a Machine
           Implementation}},
  year = "1972",
  institution = "Stanford Artificial Intelligence Project",
  number = "STAN-CS-72-288",
  link =
 "\url{http://i.standford.edu/pub/cstr/reports/cs/tr/72/288/CS-TR-72-288.pdf}",
  abstract =
    "LCF is based on a logic by Dana Scott, proposed by him at Oxford in the
    fall of 1969, for reasoning about computable functions.",
  paper = "Miln72.pdf"
}

@article{Miln78,
  author = "Milner, Robin",
  title = {{A Theory of Type Polymorphism in Programming}},
  year = "1978",
  journal = "Journal of Computer and System Sciences",
  volume = "17",
  pages = "348-375",
  link = "\url{https://courses.engr.illinois.edu/cs421/sp2013/project/milner-polymorphism.pdf}",
  abstract =
    "The aim of this work is largely a practical one.  A widely employed
    style of programming, particularly in structure-processing languages
    which impose no discipline of types, entails defining procedures which
    work well on objects of a wide variety.  We present a formal type
    discipline for such polymorphic procedures in the context of a simple
    programming language, and a compile time type-checking algorithm $W$
    which enforces the discipline.  A Semantic Soundness Theorem (based on
    a formal semantics for the language) states that well-type programs
    cannot “go wrong” and a Syntactic Soundness Theorem states that if $W$
    accepts a program then it is well typed.  We also discuss extending
    these results to richer languages; a type-checking algorithm based on
    $W$ is in fact already implemented and working, for the metalanguage ML
    in the Edinburgh LCF system,",
  paper = "Miln78.pdf",
  keywords = "printed"
}

@article{Miln84,
  author = "Milner, R.",
  title = {{The Use of Machines to Assist in Rigorous Proof}},
  journal = "Philosophical Transactions of the Royal Society",
  volume = "312",
  pages = "411-422",
  number = "1522",
  year = "1984",
  abstract = 
    "A methodology for computer assisted proof is presented with an
    example. A central ingredient in the method is the presentation of
    tactics (or strategies) in an algorithmic metalanguage. Further, the
    same language is also used to express combinators, by which simple
    elementary tactics - which often correspond to the inference rules of
    the logic employed - are combined into more complex tactics, which may
    even be strategies complete for a class of problems. However, the
    emphasis is not upon completeness but upon providing a metalogical
    framework within which a user may express his insight into proof
    methods and may delegate routine (but error-prone) work to the
    computer. This method of tactic composition is presented at the start
    of the paper in the form of an elementary theory of goal-seeking. A
    second ingredient of the methodology is the stratification of
    machine-assisted proof by an ancestry graph of applied theories, and
    the example illustrates this stratification. In the final section,
    some recent developments and applications of the method are cited.",
  paper = "Miln84.pdf"
}

@misc{Mohr14,
  author = "Mohring-Paulin, Christine",
  title = {{Introduction to the Calculus of Inductive Constructions}},
  year = "2014",
  link = "\url{https://hal.inria.fr/hal-01094195/file/CIC.pdf}",
  paper = "Mohr14.pdf"
}

@mastersthesis{Mort10,
  author = {M\"ortbert, Anders},
  title = {{Constructive Algebra in Functional Programming and Type Theory}},
  school = "University of Gothenburg, Department of Computer Science",
  year = "2010",
  month = "5",
  file = "Mort10.pdf",
  abstract = 
    "This thesis considers abstract algebra from a constructive point
    of view. The central concept of study is coherent rings -- algebraic
    structures in which it is possible to solve homogeneous systems of
    linear equations. Three different algebraic theories are considered;
    Bezout domains, Prufer domains and polynomial rings. The first two
    of these are non-Noetherian analogues of classical notions. The
    polynomial rings are presented from a constructive point of view with a
    treatment of Groebner bases. The goal of the thesis is to study the
    proofs that these theories are coherent and explore how the proofs can
    be implemented in functional programming and type theory."
}

@misc{Mour15,
  author = "de Moura, Leonardo and Avigad, Jeremy and Kong, Soonho and 
            Roux, Cody",
  title = {{Elaboration in Dependent Type Theory}},
  year = "2015",
  comment = "arXiv:1505.04324v2",
  abstract =
    "To be usable in practice, interactive theorem provers need to provide
    convenient and efficient means of writing expressions, definitions,
    and proofs. This involves inferring information that is often left
    implicit in an ordinary mathematical text, and resolving ambiguities
    in mathematical expressions. We refer to the process of passing from a
    quasi-formal and partially-specified expression to a completely
    precise formal one as {\sl elaboration}. We describe an elaboration
    algorithms for dependent type theory that has been implemented in the
    Lean theorem prover. Lean's elaborator supports higher-order
    unification, type class inference, ad hoc overloading, insertion of
    coercions, the use of tactics, and the computational reduction of
    terms. The interactions between these components are subtle and
    complex, and the elaboration algorithm has been carefully designed to
    balance efficiency and usability. We describe the central design
    goals, and the means by which they are achieved.",
  paper = "Mour15.pdf",
  keywords = "coercion, printed"
}  

@misc{Mour16,
  author = "de Moura, Leonardo and Avigad, Jeremy and Kong, Soonho and 
            Roux, Cody",
  title = {{Elaboration in Dependent Type Theory}},
  year = "2016",
  link = "\url{http://leodemoura.github.io/files/elaboration.pdf}",
  abstract =
    "We describe the elaboration algorithm that is used in {\sl Lean}, a
    new interactive theorem prover based on dependent type theory. To be
    practical, interactive theorem provers must provide mechanisms to
    resolve ambiguities and infer implicit type information, thereby
    supporting convenient input of expressions and proofs. Lean's
    elaborator supports higher-order unification, ad-hoc overloading,
    insertion of coercions, type class inference, the use of tactics, and
    the computational reduction of terms.  The interactions between these
    components are subtle and complex, and Lean's elaborator has been
    carefully designed to balance efficiency and usability.",
  paper = "Mour16.pdf",
  keywords = "coercion"
}  

@article{Naum06,
  author = "Naumowicz, Adam",
  title = {{An Example of Formalizing Recent Mathematical Results in Mizar}},
  journal = "Journal of Applied Logic",
  volume = "4",
  number = "4",
  year = "2006",
  pages = "396-413",
  abstract =
    "This paper describes an example of the successful formalization of
    quite advanced and new mathematics using the Mizar system. It shows
    that although much effort is required to formalize nontrivial facts in
    a formal computer deduction system, still it is possible to obtain the
    level of full logical correctness of all inference steps. We also
    discuss some problems encountered during the formalization, and try to
    point out some of the features of the Mizar system responsible for
    that. The formalization described in this paper allows also for
    contrasting the linguistic capability of the Mizar language and some
    of the phrases commonly used in “informal” mathematical papers that
    the Mizar system lacks, and consequently presents the methods of how
    to cope with it during the formalization. Yet, apart from the
    problems, this paper shows some definite benefits from using a formal
    computer system in the work of a mathematician.",
  paper = "Naum06.pdf"
}

@misc{Neup13,
  author = "Neuper, Walther",
  title = {{Computer Algebra implemented in Isabelle's Function Package
           under Lucas-Interpretation -- a Case Study}},
  year = "2013",
  link = "\url{http://ceur-ws.org/Vol-1010/paper-09.pdf}",
  paper = "Neup13.pdf",
  keywords = "CAS-Proof, printed"
}

@misc{Newc13,
  author = "Newcombe, Chris and  Rath, Tim and  Zhang, Fan and
            Munteanu, Bogdan and  Brooker, Marc and Deardeuff, Michael",
  title = {{Use of Formal Methods at Amazon Web Services}},
  link = "\url{http://research.microsoft.com/en-us/um/people/lamport/tla/formal-methods-amazon.pdf}",
  abstract = 
    "In order to find subtle bugs in a system design, it is necessary to
    have a precise description of that design. There are at least two
    major benefits to writing a precise design; the author is forced to
    think more clearly, which helps eliminate ``plausible hand-waving'',
    and tools can be applied to check for errors in the design, even while
    it is being written. In contrast, conventional design documents
    consist of prose, static diagrams, and perhaps pseudo-code in an ad
    hoc untestable language. Such descriptions are far from precise; they
    are often ambiguous, or omit critical aspects such as partial failure
    or the granularity of concurrency (i.e. which constructs are assumed
    to be atomic). At the other end of the spectrum, the final executable
    code is unambiguous, but contains an overwhelming amount of detail. We
    needed to be able to capture the essence of a design in a few hundred
    lines of precise description. As our designs are unavoidably complex,
    we need a highly-expressive language, far above the level of code, but
    with precise semantics. That expressivity must cover real-world
    concurrency and fault-tolerance. And, as we wish to build services
    quickly, we wanted a language that is simple to learn and apply,
    avoiding esoteric concepts. We also very much wanted an existing
    ecosystem of tools.  We found what we were looking for in TLA+, a
    formal specification language."
}

@book{Nipk14,
  author = "Nipkow, Tobias and Klein, Gerwin",
  title = {{Concrete Semantics}},
  isbn = "978-3-10542-0",
  publisher = "Springer",
  year = "2014"
}

@article{Nguy16,
  author = "Nguyen, Phuc C. and Tobin-Hochstadt, Sam and van Horn, David",
  title = {{Higher-order symbolic execution for contract verification and
           refutation}},
  journal = "arXiv",
  link = "\url{http://arxiv.org/pdf/1507.04817v2.pdf}",
  year = "2016",
  month = "February",
  abstract =
    "We present a new approach to automated reasoning about higher-order
    programs by endowing symbolic execution with a notion of higher-order,
    symbolic values.

    To validate our approach, we use it to develop and evaluate a system
    for verifying and refuting behavioral software contracts of components
    in a functional language, which we call {\sl soft contract
    verification}. In doing so, we discover a mutually beneficial relation
    between behavioral contracts and higher-order symbolic
    execution. Contracts aid symbolic execution by providing a rich
    language of specifications that can serve as the basis of symbolic
    higher-order values; the theory of blame enables modular verification
    and leads to the theorem that {\sl verified compnents can't be
    blamed}; and the run-time monitoring of contracts enables {\sl soft}
    verification whereby verified and unverified components can safely
    interact and verification is not an all-or-nothing
    proposition. Conversely, symbolic execution aids contracts by
    providing compile-time verification which increases assurance and
    enables optimizations; automated test-case generation for contracts
    with counter-examples; and engendering a virtuous cycle between
    verification and the gradual spread of contracts.

    Our system uses higher-order symbolic execution, leveraging contracts
    as a source of symbolic values including unknown behavioral values,
    and employs an updatable heap of contract invariants to reason about
    flow-sensitive facts. Whenever a contract is refuted, it reports a
    concrete {\sl counterexample} reproducing the error, which may involve
    solving for an unknown function. The approach is able to analyze
    first-class contracts, recursive data structures, unknown functions,
    and control-flow-sensitive refinement of values, which are all
    idiomatic in dynamic languages. It makes effective use of an
    off-the-shelf solver to decide problems without heavy encodings. Our
    counterexample search is sound and relatively complete with respect to
    a first-order solver for base type values. Therefore, it can form the
    basis of automated verification and bug-finding tools for higher-order
    programs. The approach is competitive with a wide range of existing
    tools -- including type systems, flow analyzers, and model checkers --
    on their own benchmarks. We have built a tool which analyzes programs
    written in Racket, and report on its effectiveness in verifying and
    refuting contracts.",
  paper = "Nguy16.pdf"
}

@book{Nord90,
  author = {Nordstr\"om, Bengt and Petersson, Kent and Smith, Jan M.},
  title = {{Programming in Martin-L\"of's Type Theory}},
  year = "1990",
  publisher = "Oxford University Press",
  paper = "Nord90.pdf",
  keywords = "printed"
}

@misc{OCon15,
  author = {O'Connor, Liam},
  title = {{Write Your Compiler by Proving It Correct}},
  year = "2015",
  link = "\url{http://liamoc.net/posts/2015-08-23-verified-compiler.html}",
  abstract =
    "Recently my research has been centered around the development of a
    self-certifying compiler for a functional language with linear types
    called Cogent (see O'Connor et al. [2016]). The compiler works by
    emitting, along with generated low-level code, a proof in Isabelle/HOL
    (see Nipkow et al. [2002]) that the generated code is a refinement of
    the original program, expressed via a simple functional semantics in HOL.

    As dependent types unify for us the language of code and proof, my
    current endeavour has been to explore how such a compiler would look
    if it were implemented and verified in a dependently typed programming
    language instead. In this post, I implement and verify a toy compiler
    for a language of arithmetic expressions and variables to an idealised
    assembly language for a virtual stack machine, and explain some of the
    useful features that dependent types give us for writing verified
    compilers."
}

@article{Owre92,
  author = "Owre, S. and Rushby, J.M. and Shankar, N.",
  title = {{PVS: A Prototype Verification System}},
  journal = "Lecture Notes in Computer Science",
  volume = "687",
  pages = "748-752",
  year = "1992",
  abstract =
    "This brief paper introduces the main ideas of PVS",
  paper = "Owre92.pdf"
}  

@article{Pada80,
  author = "Padawitz, Peter",
  title = {{New results on completeness and consistency of abstract data 
           types}},
  journal = "LNCS",
  volume = "88",
  pages = "460-473",
  year = "1980",
  abstract =
    "If an algebraic specification is designed in a structured way, a
    small specification is stepwise enriched by more complex operations
    and their defining equations. Based on normalization properties of
    term reductions we present sufficient ``local'' conditions for the
    completeness and consistency of enrichment steps, which can be
    efficiently verified in many cases where other attempts to prove the
    enrichment property ``syntactically'' have failed so far."
}

@book{Paol02,
  author = "Paoli, Francesco",
  title = {{Substructural Logics: A Primer}},
  publisher = "Springer",
  isbn = "978-90-481-6014-3",
  year = "2002",
  abstract = 
    "Substructural logics are by now one of the most prominent branches of
    the research field usually labelled as ``nonclassical logics'' - and
    perhaps of logic tout court. Over the last few decades a vast amount
    of research papers and even some books have been devoted to this
    subject. The aim of the present book is to give a comprehensive
    account of the ``state of the art'' of substructural logics, focusing
    both on their proof theory (especially on sequent calculi and their
    generalizations) and on their semantics (both algebraic and relational).",
  paper = "Paol02.pdf"
}

@inproceedings{Pare93,
  author = "Parent, Catherine",
  title = {{Developing Certified Programs in the System Coq: The Program
           Tactic}},
  booktitle = "Proc. Int. Workshop on Types for Proofs and Programs",
  publisher = "Springer-Verlag",
  isbn = "3-540-58085-9",
  pages = "291-312",
  year = "1993",
  abstract =
    "The system {\sl Coq} is an environment for proof development based on
    the Calculus of Constructions extended by inductive definitions. The
    specification of a program can be represented by a logical formula and
    the program itself can be extracted from the constructive proof of the
    specification. In this paper, we look at the possibility of inverting
    the specification and a program, builds the logical condition to be
    verified in order to obtain a correctness proof of the program. We
    build a proof of the specification from the program from which the
    program can be extracted. Since some information cannot automatically
    be inferred, we show how to annotate the program by specifying some of
    its parts in order to guide the search for the proof.",
  paper = "Pare93.pdf",
  keywords = "printed"
}

@techreport{Pare94,
  author = "Parent, Catherine",
  title = {{Synthesizing proofs from programs in the Calculus of Inductive
           Constructions}},
  year = "1994",
  institution = {Ecole Normale Sup\'erieure de Lyon},
  abstract =
    "In type theory, a proof can be represented as a typed $\lambda$-term.
    There exist methods to mark logical parts in proofs and extract their
    algorithmic contents. The result is a correct program with respect to
    a specification. This paper focuses on the inverse problem: how to 
    generate a proof from its specification. The framework is the Calculus
    of Inductive Constructions. A notion of coherence is introduced between
    a specification and a program containing types but no logical proofs.
    This notion is based on the definition of an extraction function called
    the weak extraction. Such a program can give a method to reconstruct a
    set of logical properties needed to have a proof of the initial 
    specification. This can be seen either as a method of proving programs
    or as a method of synthetically describing proofs.",
  paper = "Pare94.pdf"
}

@misc{Pare96,
  author = "Parent-Vigouroux, Catherine",
  title = {{Natural proofs versus programs optimization in the 
           Calculus of Inductive Constructions}},
  year = "1996",
  abstract =
    "This paper presents how to automatically prove that an 'optimized'
    program is correct with respect to a set of given properties that is a
    specification. Proofs of specifications contain logical and
    computational parts. Programs can be seen as computational parts of
    proofs. They can thus be extracted from proofs and be certified to be
    correct. The inverse problem can be solved: it is possible to
    reconstruct proof obligations from a program and its specification.
    The framework is a type theory where a proof can be represented as a
    typed $\lambda$-term and, particularly, the Calculus of Inductive
    Constructions. This paper shows how programs can be simplified in
    order to be written in a much closer way to the ML one's. Indeed,
    proofs structures are often much more heavy than program structures. 
    The problem is consequently to consider natural programs (in a ML sense) 
    and see how to retrieve natural structures of proofs from them.",
  paper = "Pare96.pdf"
}

@article{Pare97,
  author = "Parent-Vigouroux, Catherine",
  title = {{Verifying programs in the Calculus of Inductive Constructions}},
  year = "1997",
  journal = "Formal Aspects of Computing",
  volume = "9",
  number = "5",
  pages = "484-517",
  abstract =
    "This paper deals with a particular approach to the verification of
    functional programs. A specification of a program can be represented
    by a logical formula. In a constructive framework, developing a program
    then corresponds to proving this formula. Given a specification and a
    program, we focus on reconstructing a proof of the specification whose
    algorithmic contents corresponds to the given program. The best we can
    hope is to generate proof obligations on atomic parts of the program 
    corresponding to logical properties to be verified. First, this paper
    studies a weak extraction of a program from a proof that keeps track
    of intermediate specifications. From such a program, we prove the
    determinism of retrieving proof obligations. Then, heuristic methods
    are proposed for retrieving the proof from a natural program containing
    only partial annotations. Finally, the implementation of this methos as
    a tactic of the {\sl Coq} proof assistant is presented.",
  paper = "Pare97.pdf"
}

@inproceedings{Paul93,
  author = "Paulin-Mohring, Christine",
  title = {{Inductive Definitions in the system Coq -- Rules and
           Properties}},
  booktitle = "Proc '93 Int. Conf. on Typed Lambda Calculi and
              Applications", 
  year = "1993",
  pages = "328-345",
  isbn = "3-540-56517-5",
  abstract =
    "In the pure Calculus of Constructions, it is possible to represent
    data structures and predicates using higher-order
    quantification. However, this representation is not satisfactory, from
    the point of view of both the efficiency of the underlying programs
    and the power of the logical system. For these reasons, the calculus
    was extended with a primitive notion of inductive definitions
    [8]. This paper describes the rules for inductive definitions in the
    system Coq. They are general enough to be seen as one formulation of
    adding inductive definitions to a typed lambda-calculus. We prove
    strong normalization for a subsystem of Coq corresponding to the pure
    Calculus of Constructions plus Inductive Definitions with only weak
    eliminations"
}

@book{Paul94,
  author = "Paulson, Lawrence C.",
  title = {{ISABELLE: A Generic Theorem Prover}},
  year = "1994",
  publisher = "Springer",
  isbn = "978-3-540-58244-1",
}

@misc{Paul98,
  author = "Paulson, Lawrence C.",
  title = {{Introduction to Isabelle}},
  publisher = "Computer Laboratory, Univ. of Cambridge",
  year = "1998"
}

@article{Pela14,
  author = "Pelayo, Alvaro and Warren, Michael A.",
  title = {{Homotopy Type Theory and Voevodsky's Univalent Foundations}},
  journal = "Bulletin of the American Mathematical Society",
  volume = "51",
  number = "4",
  year = "2014",
  pages = "597-648",
  link = "\url{https://arxiv.org/pdf/1210.5658.pdf}",
  abstract =
    "Recent discoveries have been made connecting abstract homotopy
    theory and the field of type theory from logic and theoretical computer
    science. This has given rise to a new field, which has been christened
    {\sl homotopy type theory}. In this direction, Vladimir Voevodsky 
    observed that it is possible to model type theory using simpical sets
    and that this model satisfies an additional property, called the
    {\sl Univalence Axiom}, which has a number of striking consequences.
    He has subsequently advocated a program, which he calls {\sl univalent
    foundations}, of developing mathematics in the setting of type theory
    with the Univalence Axiom and possibly other additional axioms motivated
    by the simplical set model. Because type theory posses good computational
    properties, this program can be carried out in a computer proof assistant.
    In this paper we give an introduction to homotopy type theory in 
    Voevodsky's setting, paying attention to both theoretical and practical
    issues. In particular, the paper serves as an introduction to both the
    general ideas of homotopy type theory as well as to some of the concrete
    details of Voevodsky's work using the well-known proof assistant Coq. 
    The paper is written for a general audience of mathematicians with basic
    knowledge of algebraic topology; the paper does not assume any 
    preliminary knowledge of type theory, logic, or computer science. Because
    a defining characteristic of Voevodsky's program is that the Coq code has
    fundamental mathematical content, and many of the mathematical concepts
    which are efficiently captured in the code cannot be explained in 
    standard mathematical English without a length detour through type theory,
    the later sections of this paper (beginning with Section 3) make use of
    code; however, all notions are introduced from the beginning and in a
    self-contained fashion.",
  paper = "Pela14.pdf"
}

@inproceedings{Pell91,
  author = "Pelletier, Francis Jeffry",
  title = {{The Philosophy of Automated Theorem Proving}},
  booktitle = "Proc 12th IJCAI",
  pages = "538-543",
  year = "1991",
  publisher = "Morgan Kaufmann",
  paper = "Pell91.pdf",
  keywords = "printed"
}

@inproceedings{Pfen88,
  author = "Pfenning, Frank",
  title = {{Partial Polymorphic Type Inference and Higher-Order Unification}},
  booktitle = "Proc 1988 ACM Conf. on Lisp and Functional Programming",
  pages = "153-163",
  year = "1988",
  publisher = "ACM",
  isbn = "0-89791-273-X",
  abstract =
    "We show that the problem of partial type inference in the $n$-th
    order polymorphic $\lambda$-calculus is equivalent to $n$-th order
    unification. On the one hand, this means that partial type inference
    in polymorphic $\lambda$-calculi of order 2 or higher is
    undecidable. On the other hand, higher-order unification is often
    tractable in practice, and our translation entails a very useful
    algorithm for partial type inference in the $omega$-order polymorphic
    $\lambda$-calculus. We present an implementation in $\lambda$Prolog in
    full.",
  paper = "Pfen88.pdf",
  keywords = "printed"
}

@techreport{Pfen89,
  author = "Pfenning, Frank and Paulin-Mohring, Christine",
  title = {{Inductively Defined Types in the Calculus of Constructions}},
  institution = "Carnegie-Mellon University",
  year = "1989",
  number = "CMU-CS-89-209",
  link = "\url{http://repository.cmu.edu/cgi/viewcontent.cgi?article=2907&context=compsci}",
  abstract = 
    "We define the notion of an {\sl inductively defined type} in the
    Calculus of Constructions and show how inductively defined types can
    be represented by closed types. We show that all primitive recursive
    functional over these inductively defined types are also
    representable. This generalizes work by Bohm and Berarducci on
    synthesis of functions on term algebras in the second-order
    polymorphic $\lambda$-calculus ($F_2$). We give several applications
    of this generalization, including a representation of $F_2$-programs
    in $F_3$, along with a definition of functions {\bf reify}, {\bf
    reflect}, and {\bf eval} for $F_2$ in $F_3$. We also show how to
    define induction over inductively defined types and sketch some
    results that show that the extension of the Calculus of Construction
    by induction principles does not alter the set of functions in its
    computational fragment, $F_\omega$. This is because a proof by
    induction can be {\bf realized} by primitive recursion, which is
    already definable in $F_\omega$.",
  paper = "Pfen89.pdf"
}

@misc{Pfen17,
  author = "Pfenning, Frank",
  title = {{Logical Frameworks}},
  link = "\url{http://www.cs.cmu.edu/afs/cs.cmu.edu/user/fp/www/lfs.html}",
  year = "2017"
}

@incollection{Pfen92a,
  author = "Pfenning, Frank",
  title = {{Dependent Types in Logic Programming}},
  booktitle = "Types in Logic Programming",
  isbn = "9780262161312",
  publisher = "MIT Press",
  year = "1992",
  paper = "Pfen92a.pdf"
}  

@book{Pier00,
  author = "Pierce, Benjamin C.",
  title = {{Type Systems for Programming Languages}},
  year = "2000",
  publisher = "MIT Press",
  link = "\url{http://ropas.snu.ac.kr/~kwang/S20/pierce\_book.pdf}",
  paper = "Pier00.pdf"
}

@misc{Pier15,
  author = {Pierce, Benjamin C. and Casinghino, Chris and Gaboardi, Marco and
     Greenberg, Michael and Hritcu, Catalin and Sj\"oberg, Vilhelm and
     Yorgey, Brent},
  title = {{Software Foundations}},
  year = "2015",
  file = "Pier15.tgz",
  abstract = 
    "This electronic book is a course on Software Foundations, the
    mathematical underpinnings of reliable software. Topics include basic
    concepts of logic, computer-assisted theorem proving, the Coq proof
    assistant, functional programming, operational semantics, Hoare logic,
    and static type systems. The exposition is intended for a broad range
    of readers, from advanced undergraduates to PhD students and
    researchers. No specific background in logic or programming languages
    is assumed, though a degree of mathematical maturity will be helpful.

    The principal novelty of the course is that it is one hundred per cent
    formalized and machine-checked: the entire text is literally a script
    for Coq. It is intended to be read alongside an interactive session
    with Coq. All the details in the text are fully formalized in Coq, and
    the exercises are designed to be worked using Coq.

    The files are organized into a sequence of core chapters, covering
    about one semester's worth of material and organized into a coherent
    linear narrative, plus a number of appendices covering additional
    topics. All the core chapters are suitable for both upper-level
    undergraduate and graduate students."
}

@article{Piro05,
  author = "Piroi, Florina and Kutsiz, Temur",
  title = {{The Theorema Environment for Interactive Proof Development}},
  journal = "LNAI",
  volume = "3835",
  pages = "261-275",
  year = "2005",
  abstract =
    "We describe an environment that allows the users of the Theorema
    system to flexibly control aspects of computer-supported proof
    development. The environment supports the display and manipulation of
    proof trees and proof situations, logs the user activities (commands
    communicated with the system during the proving session), and presents
    (also unfinished) proofs in a human-oriented style. In particular, the
    user can navigate through the proof object, expand/remove proof
    branches, provide witness terms, develop several proofs concurrently,
    proceed step by step or automatically and so on. The environment
    enhances the effectiveness and flexibility of the reasoners of the
    Theorema system.",
  paper = "Piro05.pdf, printed"
}

@article{Plot77,
  author = "Plotkin, G.D.",
  title = {{LCF Considered as a Programming Language}},
  journal = "Theoretical Computer Science",
  volume = "5",
  year = "1977",
  pages = "223-255",
  link = "\url{http://homepages.inf.ed.ac.uk/gdp/publications/LCF.pdf}",
  abstract =
    "The paper studies connections between denotational and operational
    semantics for a simple programming language based on LCF. It begins
    with the connection between the behaviour of a program and its 
    denotation. It turns out that a program denotes $\bot$ in any of
    several possible semantics iff it does not terminate. From this it 
    follows that if two terms hae the same denotation in one of these
    semantics, they have the same behaviour in all contexts. The converse
    fails for all the semantics. If, however, the language is extended to
    allow certain parallel facilities behavioural equivalence does coincide
    with denotational equivalence in one of the semantics considered, which
    may therefore be called ``fully abstract''. Next a connection is given
    which actually determines the semantics up to isomorphism from the
    behaviour alone. Conversely, by allowing further parallel facilities, 
    every r.e. element of the fully abstract semantics becomes definable,
    thus characterising the programming language, up to interdefinability,
    from the set of r.e. elements of the domains of the semantics.",
  paper = "Plot77.pdf"
}

@misc{Poll98,
  author = "Poll, Erik and Thompson, Simon",
  title = {{Adding the axioms to Axiom. Toward a system of automated 
          reasoning in Aldor}},
  year = "1998",
  link =
    "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.7.1457}",
  abstract = 
    "This paper examines the proposal of using the type system of Axiom to
    represent a logic, and thus to use the constructions of Axiom to
    handle the logic and represent proofs and propositions, in the same
    way as is done in theorem provers based on type theory such as Nuprl
    or Coq.

    The paper shows an interesting way to decorate Axiom with pre- and
    post-conditions.",
  paper = "Poll98.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@misc{Poll99,
  author = "Poll, Erik",
  title = {{The Type System of Axiom}},
  year = "1999",
  link = "\url{http://www.cs.ru.nl/E.Poll/talks/axiom.pdf}",
  abstract = 
   "This is a slide deck from a talk on the correspondence between
    Axiom/Aldor types and Logic.",
  paper = "Poll99.pdf",
  keywords = "axiomref"
}

@misc{Poll99a,
  author = "Poll, Erik and Thompson, Simon",
  title = {{The Type System of Aldor}},
  link = "\url{http://www.cs.kent.ac.uk/pubs/1999/874/content.ps}",
  abstract = 
    "This paper gives a formal description of -- at least a part of --
    the type system of Aldor, the extension language of the Axiom.
    In the process of doing this a critique of the design of the system
    emerges.",
  paper = "Poll99a.pdf",
  keywords = "axiomref"
}

@article{Poll00,
  author = "Poll, Erik and Thompson, Simon",
  title = {{Integrating Computer Algebra and Reasoning through the Type 
           System of Aldor}},
  journal = "Lecture Notes in Computer Science",
  volume = "1794",
  pages = "136-150",
  year = "2000",
  abstract = 
    "A number of combinations of reasoning and computer algebra systems
    have been proposed; in this paper we describe another, namely a way to
    incorporate a logic in the computer algebra system Axiom. We examine
    the type system of Aldor -- the Axiom Library Compiler -- and show
    that with some modifications we can use the dependent types of the
    system to model a logic, under the Curry-Howard isomorphism. We give
    a number of example applications of the logic we construct and explain
    a prototype implementation of a modified type-checking system written
    in Haskell.",
  paper = "P0ll00.pdf",
  keywords = "axiomref, printed"
}

@InProceedings{Poll00a,
  author = "Poll, Erik and Thompson, Simon",
  title = {{Integrating Computer Algebra and Reasoning through the Type
           System of Aldor}},
  booktitle = "Frontiers of Combining Systems",
  series = "Lecture Notes in Artificial Intelligence",
  year = "2000",
  isbn = "3-540-67281-8",
  location = "Nancy, France",
  pages = "136-150",
  keywords = "axiomref"
}

@techreport{Poll97,
  author = "Pollack, Robert",
  title = {{How to Believe a Machine-Checked Proof}},
  type = "technical report",
  institution = "Univ. of Aarhus, Basic Research in Computer Science",
  number = "BRICS RS-97-18",
  year = "1997",
  abstract =
    "Suppose I say ``Here is a machine-checked proof of Fermat's last
    theorem (FLT)''. How can you use my putative machine-checked proof
    as evidence for belief in FLT? I start from the position that you
    must have some personal experience of understanding to attain
    belief, and to have this experience you must engage your intuition
    and other mental processes which are impossible to formalise.",
  paper = "Poll97.pdf",
  keywords = "printed"
}

@article{Prev02,
  author = "Prevosto, Virgile and Doligez, Damien",
  title = {{Algorithms and proofs inheritance in the FOC language}},
  journal = "J. Autom. Reasoning",
  volume = "29",
  number = "3-4",
  year = "2002",
  pages = "337-363",
  abstract =
    "In this paper, we present the FOC language, dedicated to the
    development of certified computer algebra libraries (that is sets of
    programs). These libraries are based on a hierarchy of implementations
    of mathematical structures. After presenting the core set of features
    of our language, we describe the static analyses, which reject
    inconsistent programs. We then show how we translate FOC definitions
    into OCAML and COQ, our target languages for the computational part
    and the proof checking, respectively.",
  paper = "Prev02.pdf",
  keywords = "axiomref, printed"
}

#InCollection{Rect89,
  author = "Rector, D. L.",
  title = {{Semantics in Algebraic Computation}},
  booktitle = "Computers and Mathematics",
  publisher = "Springer-Verlag",
  year = "1989",
  pages = "299-307",
  isbn = "0-387-97019-3",
  keywords = "axiomref"
}

@inproceedings{Reyn02,
  author = "Reynolds, John C.",
  title = {{Separation Logic: A Logic for Shared Mutable Data Structures}},
  booktitle = "Logic in Computer Science '02",
  year = "2002",
  pages = "55-74",
  isbn = "0-7695-1483-9",
  abstract =
    "In joint work with Peter O'Hearn and others, based on early ideas of
    Burstall, we have developed an extension of Hoare logic that permits
    reasoning about low-level imperativeprograms that use shared mutable
    data structure.The simple imperative programming language is extended
    with commands (not expressions) for accessing and modifying shared
    structures, and for explicit allocation and deallocation of
    storage. Assertions are extended by introducing a ``separating
    conjunction'' that asserts that its sub-formulas hold for disjoint
    parts of the heap, and a closely related ``separating
    implication''. Coupled with the inductive definition of predicates on
    abstract data structures, this extension permits the concise and
    flexible description of structures with controlled sharing.In this
    paper, we will survey the current development of this program logic,
    including extensions that permit unrestricted address arithmetic,
    dynamically allocated arrays, and recursive procedures. We will also
    discuss promising future directions.",
  paper = "Reyn02.pdf",
  keywords = "printed"
}

@article{Reyn05,
  author = "Reynolds, John C.",
  title = {{An Overview of Separation Logic}},
  year = "2005",
  journal = "LNCS",
  volume = "4171",
  pages = "460-469",
  abstract = 
    "After some general remarks about program verification, we introduce
    separation logic, a novel extension of Hoare logic that can strengthen
    the applicability and scalability of program verification for
    imperative programs that use shared mutable data structures or
    shared-memory concurrency",
  paper = "Reyn05.pdf",
  keywords = "printed"
}

@misc{Robe15,
 author = "Roberts, Siobhan",
 title = {{In Mathematics, Mistakes Aren't What They Used To Be}},
 year = 2015,
 link = "\url{http://nautil.us/issue/24/error/In-mathematics-mistakes-arent-what-they-used-to-be}"
}

@article{Rudn01,
  author = "Rudnicki, Piotr and Schwarzweller, Christoph and 
            Trybulec, Andrzej",
  title = {{Commutative algebra in the Mizar system}},
  journal = "J. Symb. Comput.",
  volume = "32",
  number = "1-2",
  pages = "143-169",
  year = "2001",
  link = "\url{https://inf.ug.edu.pl/~schwarzw/papers/jsc01.pdf}",
  abstract =
    "We report on the development of algebra in the Mizar system. This
    includes the construction of formal multivariate power series and
    polynomials as well as the definition of ideals up to a proof of the
    Hilbert basis theorem. We present how the algebraic structures are
    handled and how we inherited the past developments from the Mizar
    Mathematical Library (MML). The MML evolves and past contributions are
    revised and generalized. Our work on formal power series caused a
    number of such revisions. It seems that revising past developments
    with an intent to generalize them is a necessity when building a
    database of formalized mathematics. This poses a question: how much
    generalization is best?",
  paper = "Rudn01.pdf",
  keywords = "axiomref"
}

@techreport{Sack87,
  author = "Sacks, Elisha",
  title = {{Hierarchical Reasoning about Inequalities}},
  institution = "MIT",
  year = "1987",
  abstract =
    "This paper describes a program called BOUNDER that proves
    inequalities between functions over finite sets of constraints.
    Previous inequality algorithms perform well on some subset of the
    elementary functions, but poorly elsewhere.  To overcome this problem,
    BOUNDER maintains a hierarchy of increasingly complex algorithms.
    When one fails to resolve an inequality, it tries the next.  This
    strategy resolves more inequalities than any single algorithm.  It
    also performs well on hard problems without wasting time on easy
    ones.  The current hierarchy consists of four algorithms: bounds
    propagation, substitution, derivative inspection, and iterative
    approximation.  Propagation is an extension of interval arithmetic
    that takes linear time, but ignores constraints between variables
    and multiple occurrences of variables.  The remaining algorithms
    consider these factors, but require exponential time.  Substitution
    is a new, provably correct, algorithm for utilizing constraints
    between variables.  The final two algorithms analyze constraints
    between variables.  Inspection examines the signs of partial
    derivatives.  Iteration is based on several earlier algorithms
    from interval arithmetic.",
  paper = "Sack87.pdf"
}

@phdthesis{Schw97,
  author = "Schwarzweller, Christoph",
  title = {{MIZAR verification of generic algebraic algorithms}},
  school = "University of Tubingen",
  year = "1997",
  abstract =
    "Although generic programming founds more and more attention –
    nowadays generic programming languages as well as generic libraries
    exist – there are hardly approaches for the verification of generic
    algorithms or generic libraries. This thesis deals with generic
    algorithms in the field of computer algebra. We propose the Mizar
    system as a theorem prover capable of verifying generic algorithms on
    an appropriate abstract level. The main advantage of the MIZAR theorem
    prover is its special input language that enables textbook style
    presentation of proofs. For generic versions of Brown/Henrici addition
    and of Euclidean’s algorithm we give complete correctness proofs
    written in the MIZAR language.
    
    Moreover, we do not only prove algorithms correct in the usual
    sense. In addition we show how to check, using the MIZAR system, that
    a generic algebraic algorithm is correctly instantiated with a
    particular domain. Answering this question that especially arises if
    one wants to implement generic programming languages, in the field of
    computer algebra requires nontrivial mathematical knowledge.
    
    To build a verification system using the MIZAR theorem prover, we also
    implemented a generator which almost automatically computes for a
    given algorithm a set of theorems that imply the correctness of this
    algorithm.",
  paper = "Schw97.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@techreport{Sege91,
  author = "Seger, C. and Joyce, J.J.",
  title = {{A Two-level Formal Verification Methodology using HOL and COSMOS}},
  type = "technical report",
  year = "1991",
  number = "91-10",
  institution = "Dept. of Comp. Sci. Univ. of British Columbia",
  abstract =
    "Theorem-proving and symbolic simulation are both described as methods
    for the formal verification of hardware. They are both used to achieve
    a common goal -- correctly designed hardware -- and both are intended
    to be an alternative to conventional methods based on non-exhaustive
    simulation. However, they have different strengths and weaknesses. The
    main significance of this paper -- and its most original contribution
    -- is the suggestion that symbolic simulation and theorem-proving can
    be combined in a complementary manner.  We also outline our plans for
    the development of a mathematical interface between the two approaches
    -- in particular, a semantic link between the formulation of
    higher-order logic used in the Cambridge HOL system and the
    specification language used in the COSMOS system. We believe that this
    combination offers great potential as a practical formal verification
    methodology which combines the ability to accurately model circuit
    level behavior with the ability to reason about digital hardware at
    higher levels of abstraction"
}

@misc{Seli13,
  author = "Selinger, Peter",
  title = {{Lecture Notes on the Lambda Calculus}},
  year = "2013",
  link = "\url{https://www.irif.fr/~mellies/mpri/mpri-ens/biblio/Selinger-Lambda-Calculus-Notes.pdf}",
  abstract =
    "This is a set of lecture notes that developed out of courses on the
    lambda calculus that I taught at the University of Ottawa in 2001 and
    at Dalhousie University in 2007 and 2013. Topcis covered in these
    notes include the untyped lambda calculus, the Church-Rosser theorem,
    combinatory algebras, the simply-typed lambda calculus, the
    Curry-Howard isomorphism, weak and strong normalization, polymorphism,
    type inference, denotational semantics, complete partial orders, and
    the language PCF.",
  paper = "Seli13.pdf"
}

@book{Simm00,
  author = "Simmons, Harold",
  title = {{Derivation and Computation: Taking the Curry-Howard correspondence
           seriously}},
  year = "2000",
  publisher = "Cambridge University Press",
  isbn = "0-521-77173-0"
}

@misc{Smit60,
  author = "Smith, Brian Cantwell",
  title = {{The Limits of Correctness}},
  year = "1960",
  link = "\url{http://eliza.newhaven.edu/ethics/Resources/14.Reliability-Responsibility/LimitsOfCorrectness.pdf}",
  paper = "Smit60.pdf"
}

@article{Soze08,
  author = "Sozeau, Mattieu and Oury, Nicolas",
  title = {{First-Class Type Classes}},
  journal = "Lecture Notes in Computer Science",
  volume = "5170",
  publisher = "Springer",
  year = "2008",
  pages = "278-293",
  link = "\url{https://www.irif.fr/~sozeau/research/publications/First-Class\_Type\_Classes.pdf}",
  abstract =
    "Type Classes have met a large success in Haskell and Isabelle, as a
    solution for sharing notations by overloading and for specifying with
    abstract structures by quantificaiton on contexts. However, both
    systems are limited by second-class implementations of these
    constructs, and these limitations are only overcome by ad-hoc
    extensions to the respective systems. We propose an embedding of type
    classes into a dependent type theory that is first-class and supports
    some of the most popular extensions right away. The implementation is
    correspondingly cheap, general, and integrates well inside the system,
    as we have experimented in Coq. We show how it can be used to help
    structured programming and proving by way of examples.",
  paper = "Soze08.pdf"
}

@inproceedings{Soze12,
  author = "Sozeau, Mattieu",
  title = {{Coq with Classes}},
  booktitle = "JFLA 2012",
  link = "\url{https://www.irif.fr/~sozeau/research/publications/Coq\_with\_Classes-JFLA-040212.pdf}",
  year = "2012",
  paper = "Soze12.pdf"
}

@inproceedings{Spec18,
  author = "Spector-Zabusky, Antal and Breitner, Joachim and 
            Rizkallah, Christine and Weirich, Stephanie",
  title = {{Total Haskell is Reasonable Coq}},
  booktitle = "ACM SIGPLAN Int. Conf. on Certified Programs and
               Proofs",
  publisher = "ACM",
  year = "2018",
  abstract = 
    "We would like to use the Coq proof assistant to mechanically
    verify properites of Haskell programs. To that end, we present a
    tool, named {\tt hs-to-coq}, that translates total Haskell
    programs into Coq programs via a shallow embedding. We apply our
    tool in three case studies -- a lawful {\tt Monad} instance,
    ``Hutton's razor'', and an existing data structure library -- and
    prove their correctness. These examples show that this approach is
    viable: both that {\tt hs-to-coq} applies to existing Haskell
    code, and that the output it produces is amenable to
    verification.",
  paper = "Spec18.pdf"
}

@misc{Stac17a,
  author = "cstheory.stackexchange.com",
  title = {{Why does Coq have Prop?}},
  link = "\url{http://cstheory.stackexchange.com/questions/21836/why-does-coq-have-prop/21878\#21878}",
  year = "2017",
}

@misc{Ster17,
  author = "Sterling, Jonathan and Harper, Robert",
  title = {{Algebraic Foundations of Proof Refinement}},
  link = "\url{http://www.cs.cmu.edu/~rwh/papers/afpr/afpr.pdf}",
  year = "2017",
  abstract =
    "We contribute a general apparatus for {\sl dependent} tactic-based
    proof refinement in the LCF tradition, in which the statements of
    subgoals may express a dependency on the proofs of other subgoals;
    this form of dependency is extremely useful and can serve as an
    {\sl algorithmic} alternative to extensions of LCF based on non-local
    instantiation of schematic variables. Additionally, we introduce a
    novel behavioral distinction between {\sl refinement rules} and
    {\sl tactics} based on naturality. Our framework, called Dependent
    LCF, is already deployed in the nascent RedPRL proof assistant for
    computational cubical type theory.",
  paper = "Ster17.pdf"
}

@techreport{Stur95,
  author = "Sturm, T.",
  title = {{A REDUCE package for first-order logic}},
  type = "technical report",
  institution = "Universitat Passau",
  year = "1995",
}

@misc{Theo17,
  author = "Unknown",
  title = {{Theorema Project}},
  link = "\url{http://www.theorema.org}",
  year = "2017"
}

@article{Ther01,
  author = "Th\'ery, Laurent",
  title = {{A Machine-Checked Implementation of Buchberger's Algorithm}},
  journal = "Journal of Automated Reasoning",
  volume = "26",
  year = "2001",
  pages = "107-137",
  abstract = "We present an implementation of Buchberger's algorithm that
    has been proved correct within the proof assistant Coq. The 
    implementation contains the basic algorithm plus two standard
    optimizations.",
  paper = "Ther01.pdf"
}

@article{Ther98,
  author = "Thery, Laurent",
  title = {{A Certified Version of Buchberger's Algorithm}},
  journal = "LNCS",
  volume = "1421",
  pages = "349-364",
  year = "1998",
  abstract =
    "We present a proof of Buchberger's algorithm that has been developed
    in the Coq proof assistant. The formulation of the algorithm in Coq
    can be efficiently compiled and used to do computation",
  paper = "Ther98.pdf"
}

@phdthesis{Tomu98,
  author = "Tomuta, E.",
  title = {{Proof Control in the Theorema Project}},
  year = "1998",
  school = "RISC Linz"
}

@article{Tray11,
  author = "Traytel, Dmitriy and Berghofer, Stefan and Nipkow, Tobias",
  title = {{Extending Hindley-Milner Type Inference with Coercive
           Structural Subtyping}},
  journal = "LNCS",
  volume = "7078",
  pages = "89-104",
  year = "2011",
  abstract = 
    "We investigate how to add coercive structural subtyping to a type
    system for simply-typed lambda calculus with Hindley-Milner 
    polymorphism. Coercions allow to convert between different types, and
    their automatic insertion can greatly increase readability of
    terms. We present a type inference algorithm that, given a term
    without type information, computes a type assignment and determines at
    which positions in the term coercions have to be inserted to make it
    type-correct according to the standard Hindley-Milner system (without
    any subtypes). The algorithm is sound and, if the subtype relation
    on base types is a disjoint union of lattices, also complete. The
    algorithm has been implemented in the proof assistant Isabelle.",
  paper = "Tray11.pdf",
  keywords = "coercion"
}

@misc{Troe97,
  author = "Troelstra, A.S.",
  title = {{From constructivism to computer science}},
  volume = "211",
  year = "1999",
  pages = "232-252",
  abstract =
    "My field is mathematical logic, with a special interest in
    constructivism, and I would not dare to call myself a computer
    scientist. But some computer scientists regard my work as a
    contribution to their field; and in this text I shall try to explain
    how this is possible, by taking a look at the history of ideas.

    I want to describe how two interrelated ideas, connected with the
    constructivistic trend in the foundations of mathematics, developed
    within mathematical logic and ultimately diffused into computer
    science.

    It will be seen that this development has not been a quite
    straightforward one. In the history of ideas it often looks as if a
    certain idea has to be discovered several times, by different people,
    before it really enters inthe the ``consciousness'' of science",
  paper = "Troe99.pdf"
}

@misc{Tros13,
  author = "Trostle, Anne",
  title = {{An Algorithm for the Greatest Common Divisor}},
  link = "\url{http://www.nuprl.org/MathLibrary/gcd/}",
  year = "2013"
}

@article{Turn95,
  author = "Turner, D.A.",
  title = {{Elemntary Strong Functional Programming}},
  journal = "Lecture Notes in Computer Science",
  volume = "1022",
  pages = "1-13",
  year = "1995",
  abstract = 
    "Functional programming is a good idea, but we haven’t got it quite
    right yet.  What we have been doing up to now is weak (or partial)
    functional programming.  What we should be doing is strong (or
    total) functional programming - in which all computations terminate.
    We propose an elementary discipline of strong functional programming.
    A key feature of the discipline is that we introduce a type
    distinction between data, which is known to be finite, and codata,
    which is (potentially) infinite.",
  paper = "Turn95.pdf"
}

@inproceedings{Wadl88,
  author = "Wadler, Philip and Blott, Stephen",
  title = {{How to make ad-hoc polymorphism less ad hoc}},
  booktitle = "Proc 16th ACM SIGPLAN-SIGACT Symp. on Princ. of Prog. Lang",
  isbn = "0-89791-294-2",
  pages = "60-76",
  year = "1988",
  link = "\url{http://202.3.77.10/users/karkare/courses/2010/cs653/Papers/ad-hoc-polymorphism.pdf}",
  abstract =
    "This paper presents {\sl type classes}, a new approach to {\sl ad-hoc}
    polymorphism. Type classes permit overloading of arithmetic operators
    such as multiplication, and generalise the ``eqtype variables'' of
    Standard ML Type classes extend the Hindley/Milner polymorphic type
    system, and provide a new approach to issues that arise in object-oriented
    programming, bounded type quantification, and abstract data types. This 
    paper provides an informal introduction to type classes, and defines them
    formally by means of type inference rules",
  paper = "Wadl88.pdf",
  keywords = "printed"
}

@inproceedings{Wadl89,
  author = "Wadler, Philip",
  title = {{Theorems for free!}},
  booktitle = "4th Intl Conf. on Functional Programming",
  pages = "347-359",
  year = "1989",
  abstract =
    "From the type of a polymorphic function we can derive a theorem that
    it satisfies. Every function of the same type satisfies the same
    theorem.  This provides a free source of useful theorems, courtesy of
    Reynolds' abstraction theorem for the polymorphic lambda calculus.",
  paper = "Wadl89.pdf",
  keywords = "printed"
}

@misc{Wadl14,
  author = "Wadler, Philip",
  title = {{Propositions as Types}},
  year = "2014",
  link = "\url{http://homepages.inf.ed.ac.uk/wadler/papers/propositions-as-types/propositions-as-types.pdf}",
  paper = "Wadl14.pdf"
}

@article{Wald74,
  author = "Waldinger, R.J. and Levitt, K.N.",
  title = {{Reasoning about Programs}},
  journal = "Artificial Intelligence",
  volume = "5",
  number = "3",
  year = "1974",
  pages = "235-316",
  abstract =
    "This paper describes a theorem prover that embodies knowledge about
    programming constructs, such as numbers, arrays, lists, and
    expressions. The program can reason about these concepts and is used
    as part of a program verification system that uses the Floyd-Naur
    explication of program semantics. It is implemented in the qa4
    language; the qa4 system allows many pieces of strategic knowledge,
    each expressed as a small program, to be coordinated so that a program
    stands forward when it is relevant to the problem at hand. The
    language allows clear, concise representation of this sort of
    knowledge. The qa4 system also has special facilities for dealing with
    commutative functions, ordering relations, and equivalence relations;
    these features are heavily used in this deductive system. The program
    interrogates the user and asks his advice in the course of a
    proof. Verifications have been found for Hoare's FIND program, a
    real-number division algorithm, and some sort programs, as well as for
    many simpler algorithms. Additional theorems have been proved about a
    pattern matcher and a version of Robinson's unification algorithm. The
    appendix contains a complete, annotated listing of the deductive
    system and annotated traces of several of the deductions performed by
    the system.",
  paper = "Wald74.pdf"
}

@misc{Wals92,
  author = "Walsh, Toby and Nunes, Alex and Bundy, Alan",
  title = {{The Use of Proof Plans to Sum Series}},
  booktitle = "Proc. of 11th Int. Conf. on Automated Deduction",
  year = "1992",
  abstract =
    "We describe a program for finding closed form solutions to finite
    sums. The program was built to test the applicability of the proof
    planning search control technique in a domain of mathematics outwith
    induction. This experiment was successful. The series summing program
    extends previous work in this area and was built in a short time just
    by providing new series summing methods to our existing inductive
    theorem proving system CLAM. 

    One surprising discovery was the
    usefulness of the ripple tactic in summing series. Rippling is the key
    tactic for controlling inductive proofs, and was previously thought to
    be specialised to such proofs. However, it turns out to be the key
    sub-tactic used by all the main tactics for summing series. The only
    change required was that it had to be supplemented by a difference
    matching algorithm to set up some initial meta-level annotations to
    guide the rippling process. In inductive proofs these annotations are
    provided by the application of mathematical induction. This evidence
    suggests that rippling, supplemented by difference matching, will find
    wide application in controlling mathematical proofs. The research
    reported in this paper was supported by SERC grant GR/F/71799, a SERC
    PostDoctoral Fellowship to the first author and a SERC Senior
    Fellowship to the third author. We would like to thank the other
    members of the mathematical reasoning group for their feedback on this
    project.",
  paper = "Wals92.pdf"
}

@misc{Warn17,
  author = "Warner, Evan",
  title = {{Splash Talk: The Foundational Crisis of Mathematics}},
  year = "2017",
  link = "\url{web.stanford.edu/~ebwarner/SplashTalk.pdf}",
  abstract = 
    "This class will cover some of the mathematics, history, and
    philosophy of the co-called {\sl foundational crisis in mathematics}. 
    Broadly speaking, mathematics in the late nineteenth and early 
    twentieth centuries was marked by an increased awareness of
    ``foundational issues,'' prompted by a number of problems in the
    practice of mathematics that had accumulated over the years. We will
    discuss a few examples of some of these problems, and then discusssthe
    three major schools of thought that emerged to deal with them and
    provide a coherent philosophical and methodologicial underpinning for
    mathematics.",
  paper = "Warn17.pdf"
}

@misc{Wied07,
  author = "Wiedijk, Freek",
  title = {{The Seventeen Provers of the World}},
  link = "\url{http://www.cs.kun.nl/~freek/comparison/comparison.pdf}",
  year = "2007",
  abstract =
    "We compare the styles of several proof assistants for mathematics.
     We present Pythagoras' proof of the irrationality of $\sqrt{2}$ both
     informal and formalized in (1) HOL, (2) Mizar, (3) PVS, (4) Coq,
     (5) Otter/Ivy, (6) Isabelle/Isar, (7) Alfa/Agda, (8) ACL2, (9) PhoX,
     (10) IMPS, (11) Metamath, (12) Theorema, (13) Lego, (14) Nuprl,
     (15) $\Omega$mega, (16) B method, (17) Minlog",
  paper = "Wied07.pdf"
}

@misc{Wijn68,
  author = "Wijngarrden, A. van and Mailloux, B.J. and Peck, J.E.L. and
            Koster, C.H.A. and Sintzoff, M. and Lindsey, C.H. and
            Meertens, L.G.T. and Fisker, R.G.",
  title = {{Revised Report on the Algorithmic Language ALGOL 68}},
  link = "\url{http://www.eah-jena.de/~kleine/history/languages/algol68-revisedreport.pdf}",
  year = "1968",
  paper = "Wijn68.pdf"
}  

@misc{Wiki17,
  author = "Wikipedia",
  title = {{Calculus of constructions}},
  year = "2017",
  link = "\url{https://en.wikipedia.org/wiki/Calculus\_of\_constructions}"
}

@misc{WikiED,
  author = "Wikipedia",
  title = {{Euclidean Domain}},
  year = "2017",
  link = "\url{https://en.wikipedia.org/wiki/Euclidean\_domain}"
}

@article{Wind14,
  author = "Windsteiger, Wolfgang",
  title = {{Theorema 2.0: A System for Mathematical Theory
           Exploration}},
  journal = "LNCS",
  volume = "8592",
  pages = "49-52",
  year = "2014",
  abstract =
    "Theorema 2.0 stands for a re-design including a complete 
    re-implementation of the Theorema system, which was originally designed,
    developed, and implemented by Bruno Buchberger and his Theorema group
    at RISC. In this talk, we want to present the current status of the
    new implementation, in particular the new user interface of the
    system.",
  paper = "Wind14.pdf",
  keywords = "printed"
}

@article{Wind06,
  author = "Windsteiger, Wolfgang",
  title = {{An automated prover for Zermelo-Fraenkel set theory in
            Theorema}},
  journal = "J. of Symbolic Computation",
  volume = "41",
  pages = "435-470",
  year = "2006",
  abstract =
    "This paper presents some fundamental aspects of the design and
    implementation of an automated prover for Zermelo-Fraenkel set theory
    within the Theorema system. The method applies the 
    ``Prove-Compute-Solve'' paradigm as its major strategy for generating
    proofs in a natural style for statements involving constructs from set
    theory.",
  paper = "Wind06.pdf"
}

@book{Wins93,
  author = "Winskel, Glynn",
  title = {{The Formal Semantics of Programming Languages}},
  isbn = "978-0262731034",
  year = "1993",
  publisher = "MIT"
}

@book{Yasu71,
  author = "Yasuhara, Ann",
  title = {{Recursive Function Theory and Logic}},
  year = "1971",
  publisher = "Academic Press",
  isbn = "0-12-768950-8"
}

@misc{Boeh86,
  author = "Boehm, Hans-J. and Cartwright, Robert and Riggle, Mark and
            O'Donnell, Michael J.",
  title = {{Exact Real Arithmetic: A Case Study in Higher Order Programming}},
  year = "1986",
  link = "\url{http://dev.acm.org/pubs/citations/proceedings/lfp/319838/p162-boehm}",
  abstract =
    "Two methods for implementing {\sl exact} real arithmetic are explored
    One method is based on formulating real numbers as functions that map
    rational tolerances to rational approximations.  This approach, which
    was developed by constructive mathematicians as a concrete
    formalization of the real numbers, has lead to a surprisingly
    successful implementation.  The second method formulates real numbers
    as potentially infinite sequences of digits, evaluated on demand.
    This approach has frequently been advocated by proponents of lazy
    functional languages in the computer science community.  Ironically,
    it leads to much less satisfactory implementations.  We discuss the
    theoretical problems involved m both methods, give algortthms for the
    basic arithmetic operations, and give an empirical comparison of the
    two techniques.  We conclude wtth some general observations about the
    lazy evaluation paradigm and its implementation.",
  paper = "Boeh86.pdf"
}

@misc{Brig04,
  author = "Briggs, Keith",
  title = {{Exact real arithmetic}},
  link = "\url{http://keithbriggs.info/documents/xr-kent-talk-pp.pdf}",
  year = "2004",
  paper = "Bri04.pdf"
}

@misc{Fate94a,
  author = "Fateman, Richard J.; Yan, Tak W.",
  title ={{Computation with the Extended Rational Numbers and an 
           Application to Interval Arithmetic}},
  link = "\url{http://www.cs.berkeley.edu/~fateman/papers/extrat.pdf}",
  abstract = "
    Programming languages such as Common Lisp, and virtually every
    computer algebra system (CAS), support exact arbitrary-precision
    integer arithmetic as well as exect rational number computation.
    Several CAS include interval arithmetic directly, but not in the
    extended form indicated here. We explain why changes to the usual
    rational number system to include infinity and ``not-a-number'' may be
    useful, especially to support robust interval computation. We describe
    techniques for implementing these changes.",
  paper = "Fate94a.pdf"
}

@misc{Gust16,
  author = "Gustafson, John",
  title ={{A Radical Approach to Computation with Real Numbers}},
  link = "\url{http://www.johngustafson.net/presentations/Multicore2016-JLG.pdf}",
  ppt = "Gust16.pptx",
  abstract =
    "If we are willing to give up compatibility with IEEE 754 floats and
    design a number format with goals appropriate to 2016, we can achieve
    several goals simultaneously: Extremely high energy efficiency and
    information-per-bit, no penalty for decimal operations instead of
    binary, rigorous bounds on answers without the overly pessimistic
    bounds produced by interval methods, and unprecedented high speed up
    to some precision. This approach extends the ideas of unum arithmetic
    introduced two years ago by breaking completely from the IEEE
    float-type format, resulting in fixed bit size values, fixed execution
    time, no exception values or 'gradual underflow' issues, no wasted bit
    patterns, and no redundant representations (like 'negative zero'). As
    an example of the power of this format, a difficult 12-dimensional
    nonlinear robotic kinematics problem that has defied solvers to date
    is quickly solvable with absolute bounds. Also unlike interval
    methods, it becomes possible to operate on arbitrary disconnected
    subsets of the real number line with the same speed as operating on a
    simple bound.",
  paper = "Gust16.pdf",
}

@book{Gust16a,
  author = "Gustafson, John",
  title ={{The End of Error: Unum Computing}},
  publisher = "Chapman and Hall / CRC Computational Series",
  year = "2016",
  isbn = "978-1482239867" 
}

@article{Gust16b,
  author = "Gustafson, John",
  title = {{Unums 2.0 An Interview with John L. Gustafson}},
  publisher = "ACM",
  journal = "Ubiquity",
  year = "2016",
  paper = "Gust16b.pdf"
}

@incollection{Lamb06,
  author = "Lambov, Branimir",
  title = {{Interval Arithmetic Using SSE-2}},
  booktitle = "Lecture Notes in Computer Science",
  publisher = "Springer-Verlag",
  year = "2006",
  isbn = "978-3-540-85520-0",
  pages = "102-113"
}

@misc{Atki09,
  author = "Atkinson, Kendall and Han, Welmin and Stewear, David",
  title = {{Numerical Solution of Ordinary Differential Equations}},
  link =
     "\url{http://homepage.math.uiowa.edu/~atkinson/papers/NAODE_Book.pdf}",
  abstract = "
    This book is an expanded version of supplementary notes that we used
    for a course on ordinary differential equations for upper-division
    undergraduate students and beginning graduate students in mathematics,
    engineering, and sciences. The book introduces the numerical analysis
    of differential equations, describing the mathematical background for
    understanding numerical methods and giving information on what to
    expect when using them. As a reason for studying numerical methods as
    a part of a more general course on differential equations, many of the
    basic ideas of the numerical analysis of differential equations are
    tied closely to theoretical behavior associated with the problem being
    solved. For example, the criteria for the stability of a numerical
    method is closely connected to the stability of the differential
    equation problem being solved.",
  paper = "Atki09.pdf"
}

@book{Hamm62,
  author = "Hamming, R W.",
  title = {{Numerical Methods for Scientists and Engineers}},
  publisher = "Dover",
  year = "1973",
  isbn = "0-486-65241-6"
}

@article{Nord62,
  author = "Nordsieck, Arnold",
  title = {{On Numerical Integration of Ordinary Differential Equations}},
  journal = "Mathematics of Computations",
  volume = "XVI",
  year = "1962",
  pages = "22-49",
  abstract =
    "A reliable efficient general-purpose method for automatic digital
    computer integration of systems of ordinary differential equations is
    described. The method operates with the current values of the higher
    derivatives of a polynomial approximating the solution. It is
    thoroughly stable under all circumstances, incorporates automatic
    starting and automatic choice and revision of elementary interval
    size, approximately minimizes the amount of computation for a
    specified accuracy of solution, and applies to any system of
    differential equations with derivatives continuous or piecewise
    continuous with finite jumps. ILLIAC library subroutine F7, University
    of Illinois Digital Computer Laboratory, is a digital computer program
    applying this method."
}

@misc{Walt71,
  author = "Walther, J.S.",
  title = {{A Unified Algorithm for Elementary Functions}},
  link = "\url{}",
  year = "1971",
  abstract =
    "This paper describes a single unified algorithm for the calculation
    of elementary functions including multipli- cation, division, sin,
    cos, tan, arctan, sinh, cosh, tanh, arctanh, In, exp and square-root.
    The basis for the algorithm is coordinate rotation in a linear,
    circular, or hyperbolic coordinate system depending on which function
    is to be calculated.  The only operations re- quired are shifting,
    adding, subtracting and the recall of prestored constants.  The
    limited domain of con- vergence of the algorithm is calculated,
    leading to a discussion of the modifications required to extend the
    domain for floating point calculations.  
    
    A hardware floating point processor using the algo- rithm was built at
    Hewlett-Packard Laboratories.  The block diagram of the processor, the
    microprogram control used for the algorithm, and measures of actual
    performance are shown.",
  paper = "Walt71.pdf"
}

@misc{Kama15,
  author = "Kamareddine, Fairouz and Wells, Joe and Zengler, Christoph and
            Barendregt, Henk",
  title = {{Computerising Mathematical Text}},
  year = "2015",
  abstract = 
    "Mathematical texts can be computerised in many ways that capture
    differing amounts of the mathematical meaning. At one end, there is
    document imaging, which captures the arrangement of black marks on
    paper, while at the other end there are proof assistants (e.g. Mizar,
    Isabelle, Coq, etc.), which capture the full mathematical meaning and
    have proofs expressed in a formal foundation of mathematics. In
    between, there are computer typesetting systems (e.g. Latex and
    Presentation MathML) and semantically oriented systems (e.g. Content
    MathML, OpenMath, OMDoc, etc.). In this paper we advocate a style of
    computerisation of mathematical texts which is flexible enough to
    connect the diferent approaches to computerisation, which allows
    various degrees of formalsation, and which is compatible with
    different logical frameworks (e.g. set theory, category theory, type
    theory, etc.) and proof systems. The basic idea is to allow a
    man-machine collaboration which weaves human input with machine
    computation at every step in the way. We propose that the huge step from
    informal mathematics to fully formalised mathematics be divided into
    smaller steps, each of which is a fully developed method in which
    human input is minimal."
}

@misc{Leeu94a,
  author = {van Leeuwen, Andr\'e M.A.},
  title = {{Representation of mathematical object in interactive books}},
  abstract = "
    We present a model for the representation of mathematical objects in
    structured electronic documents, in a way that allows for interaction
    with applications such as computer algebra systems and proof checkers.
    Using a representation that reflects only the intrinsic information of
    an object, and storing application-dependent information in so-called
    {\sl application descriptions}, it is shown how the translation from
    the internal to an external representation and {\sl vice versa} can be
    achieved. Hereby a formalisation of the concept of {\sl context} is
    introduced. The proposed scheme allows for a high degree of
    application integration, e.g., parallel evaluation of subexpressions
    (by different computer algebra systems), or a proof checker using a
    computer algebra system to verify an equation involving a symbolic
    computation.",
  paper = "Leeu94a.pdf"
}

@InProceedings{Kalt84,
  author = "Kaltofen, E.",
  title = {{A Note on the {Risch} Differential Equation}},
  booktitle = "Proc. EUROSAM '84",
  pages = "359--366",
  year = "1984",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/84/Ka84_risch.ps.gz}",
  paper = "Kalt84.ps"
}

@misc{Abra01,
  author = "Abramov, Sergei and Bronstein, Manuel",
  title = {{On Solutions of Linear Functional Systems}},
  year = "2001",
  link = "\url{http://www-sop.inria.fr/cafe/Manuel.Bronstein/publications/mb_papers.html}",
  algebra =
  "\newline\refto{category OREPCAT UnivariateSkewPolynomialCategory}
   \newline\refto{category LODOCAT LinearOrdinaryDifferentialOperatorCategory}
   \newline\refto{domain AUTOMOR Automorphism}
   \newline\refto{domain ORESUP SparseUnivariateSkewPolynomial}
   \newline\refto{domain OREUP UnivariateSkewPolynomial}
   \newline\refto{domain LODO LinearOrdinaryDifferentialOperator}
   \newline\refto{domain LODO1 LinearOrdinaryDifferentialOperator1}
   \newline\refto{domain LODO2 LinearOrdinaryDifferentialOperator2}
   \newline\refto{package APPLYORE ApplyUnivariateSkewPolynomial}
   \newline\refto{package OREPCTO UnivariateSkewPolynomialCategoryOps}
   \newline\refto{package LODOF LinearOrdinaryDifferentialOperatorFactorizer}
   \newline\refto{package LODOOPS LinearOrdinaryDifferentialOperatorsOps}",
  abstract = "
    We describe a new direct algorithm for transforming a linear system of
    recurrences into an equivalent one with nonsingular leading or
    trailing matrix. Our algorithm, which is an improvement to the EG
    elimination method, uses only elementary linear algebra operations
    (ranks, kernels, and determinants) to produce an equation satisfied by
    the degress of the solutions with finite support. As a consequence, we
    can boudn and compute the polynomial and rational solutions of very
    general linear functional systems such as systems of differential or
    ($q$)-difference equations.",
  paper = "Abra01.pdf"
}

@inproceedings{Bron96b,
  author = "Bronstein, Manuel",
  title = {{On the Factorization of Linear Ordinary Differential Operators}},
  booktitle = "Mathematics and Computers in Simulation",
  volume = "42",
  pages = "387-389",
  year = "1996",
  abstract = 
    "After reviewing the arithmetic of linear ordinary differential
    operators, we describe the current status of the factorisation
    algorithm, specially with respect to factoring over non-algebraically
    closed constant fields. We also describe recent results from Singer
    and Ulmer that reduce determining the differential Galois group of an
    operator to factoring.",
  paper = "Bro96b.pdf"
}

@article{Bron96a,
  author = "Bronstein, Manuel and Petkovsek, Marko",
  title = {{An introduction to pseudo-linear algebra}},
  journal = "Theoretical Computer Science",
  volume = "157",
  pages = "3-33",
  year = "1966",
  link = "\url{http://www-sop.inria.fr/cafe/Manuel.Bronstein/publications/mb_papers.html}",
  algebra = "\newline\refto{category LORER LeftOreRing}",
  abstract = 
    "Pseudo-linear algebra is the study of common properties of linear
    differential and difference operators. We introduce in this paper its
    basic objects (pseudo-derivations, skew polynomials, and pseudo-linear
    operators) and describe several recent algorithms on them, which, when
    applied in the differential and difference cases, yield algorithms for
    uncoupling and solving systems of linear differential and difference
    equations in closed form.",
  paper = "Bron96a.pdf"
}

@inproceedings{Bron01,
  author = "Bronstein, Manuel",
  title = {{Computer Algebra Algorithms for Linear Ordinary Differential 
           and Difference equations}},
  link = "\url{http://www-sop.inria.fr/cafe/Manuel.Bronstein/publications/ecm3.pdf}",
  booktitle = "European Congress of Mathematics",
  series = "Progress in Mathematics",
  volume = "202",
  year = "2001",
  pages = "105-119",
  abstract = "
    Galois theory has now produced algorithms for solving linear ordinary
    differential and difference equations in closed form. In addition,
    recent algorithmic advances have made those algorithms effective and
    implementable in computer algebra systems. After introducing the
    relevant parts of the theory, we describe the latest algorithms for
    solving such equations.",
  paper = "Bron01.pdf"
}

@misc{Bron96,
  author = "Bronstein, Manuel",
  title = 
   {{$\sum^{IT}$ -- A strongly-typed embeddable computer algebra library}},
  link = "\url{http://www-sop.inria.fr/cafe/Manuel.Bronstein/publications/mb_papers.html}",
  abstract = 
    "We describe the new computer algebra library $\sum^{IT}$ and its
    underlying design. The development of $\sum^{IT}$ is motivated by the
    need to provide highly efficient implementations of key algorithms for
    linear ordinary differential and ($q$)-difference equations to
    scientific programmers and to computer algebra users, regardless of
    the programming language or interactive system they use. As such,
    $\sum^{IT}$ is not a computer algebra system per se, but a library (or
    substrate) which is designed to be ``plugged'' with minimal efforts
    into different types of client applications.",
  paper = "Bron96.pdf"
}

@inproceedings{Bron02,
  author = "Bronstein, Manuel and Lafaille, S\'ebastien",
  title = {{Solutions of linear ordinary differential equations in terms 
           of special functions}},
  booktitle = "Proc. ISSAC '02",
  publisher = "ACM Press",
  pages = "23-28",
  year = "2002",
  isbn = "1-58113-484-3",
  link = "\url{http://www-sop.inria.fr/cafe/Manuel.Bronstein/publications/issac2002.pdf}",
  url2 = "http://xena.hunter.cuny.edu/ksda/papers/bronstein2.pdf",
  paper2 = "Bron02x.pdf",
  abstract = 
    "We describe a new algorithm for computing special function solutions
    of the form $y(x) = m(x)F(\eta(x))$ of second order linear ordinary
    differential equations, where $m(x)$ is an arbitrary Liouvillian
    function, $\eta(x)$ is an arbitrary rational function, and $F$
    satisfies a given second order linear ordinary differential
    equations. Our algorithm, which is base on finding an appropriate
    point transformation between the equation defining $F$ and the one to
    solve, is able to find all rational transformations for a large class
    of functions $F$, in particular (but not only) the $_0F_1$ and $_1F_1$
    special functions of mathematical physics, such as Airy, Bessel,
    Kummer and Whittaker functions. It is also able to identify the values
    of the parameters entering those special functions, and can be
    generalized to equations of higher order.",
  paper = "Bron02.pdf"
}

@misc{Bron02a,
  author = "Bronstein, Manuel",
  title = {{$\Sigma^{it}$ User Guide and Reference Manual}},
  year = "2002",
  paper = "Bron02a.pdf"
}

\bibitem[Davenport 86]{Dav86} Davenport, J.H.
@article{Dave86,
  author = "Davenport, James H.",
  title = {{The Risch Differential Equation Problem}},
  year = "1986",
  journal = "SIAM J. COMPUT.",
  volume = "15",
  number = "4",
  comment = "Technical Report 83-4, Dept. Comp. Sci, Univ. Delaware",
  abstract = "
    We propose a new algorithm, similar to Hermite's method for the
    integration of rational functions, for the resolution of Risch
    differential equations in closed form, or proving that they have no
    resolution. By requiring more of the presentation of our differential
    fields (in particular that the exponentials be weakly normalized), we
    can avoid the introduction of arbitrary constants which have to be
    solved for later.

    We also define a class of fields known as exponentially reduced, and
    show that solutions of Risch differential equations which arise from
    integrating in these fields satisfy the ``natural'' degree constraints
    in their main variables, and we conjecture (after Risch and Norman)
    that this is true in all variables.",
  paper = "Dave86.pdf"
}

@article{Bund94,
  author = "Bundgen, Reinhard",
  title = {{Combining Computer Algebra and Rule Based Reasoning}},
  journal = "LNCS",
  volume = "958",
  pages = "209-223",
  year = "1994",
  abstract = 
    "We present extended term rewriting systems as a means to describe a
    simplification relation for an equational specification with a
    built-ln domain of external objects.  Even if the extended term
    rewriting system is canonical, the combined relation including
    built-in computations of 'ground terms' needs neither be terminating
    nor confluent.  We investigate restrictions on the extended term
    rewriting systems and the built-in domains under which these
    properties hold.  A very important property of extended term rewriting
    systems is decomposition freedom.  Among others decomposition free
    extended term rewriting systems allow for efficient simplifications.
    Some interesting algebraic applications of canonical simplification
    relations are presented.",
  paper = "Bund94.pdf"
}

@article{Fort87,
  author = "Fortenbacher, Albrecht",
  title = {{An Algebraic Approach to Unification Under Associativity and
           Commutativity}},
  journal = "J. Symbolic Computation",
  volume = "3",
  pages = "217-229",
  year = "1987",
  abstract =
    "From the work of Siekmann and Livesey, and Stickel it is known how to
    unify two terms in an associative and commutative theory: transfer the
    terms into Abelian strings, look for mappings which solve the problem
    in the Abelian monoid, and decide whether a mapping can be regarded as
    a unifier.  Very often most of the mappings are thus eliminated, and
    so it is crucial for efficiency either to not create these unnecessary
    solutions or to remove them as soon as possible.  The following work
    formalises the transformations between the free algebra and this
    monoid.  This leads to an algorithm which uses maximal information for
    its search for solutions in the monoid.  It is both very efficient and
    easily verifiable.  Some applications of this algorithm are shown in
    the appendix.",
  paper = "Fort87.pdf",
  keywords = "printed"
}

@inproceedings{Cavi76,
  author = "Caviness, Bob F. and Fateman, Richard J.",
  title = {{Simplification of Radical Expressions}},
  booktitle = "Proc. 1976 SYMSAC",
  pages = "329-338",
  year = "1976",
  abstract =
    "In this paper we discuss the problem of simplifying unnested radical
    expressions. We describe an algorithm implemented in MACSYMA that
    simplifies radical expressions and then follow this description with
    a formal treatment of the problem. Theoretical computing times for some
    of the algorithms are briefly discussed as is related work of other
    authors",
  paper = "Cavi76.pdf",
  keywords = "axiomref"
}

@article{Land93,
  author = "Landau, Susan",
  title = {{How to Tangle with a Nested Radical}},
  institution = "University of Massachusetts",
  journal = "The Mathematical Intelligencer",
  year = "1993",
  paper = "Land93.pdf"
}

@article{Shac90,
  author = "Shackell, John",
  title = {{Growth Estimates for Exp-Log Functions}},
  journal = "J. Symbolic Computation",
  volume = "10",
  year = "1990",
  pages = "611-632",
  abstract =
    "Exp-log functions are those obtained from the constant 1 and the
    variable X by means of arithmetic operations and the function symbols
    exp() and log(). This paper gives an explicit algorithm for
    determining eventual dominance of these functions modulo an oracle for
    deciding zero equivalence of constant terms. This also provides
    another proof that the dominance problem for exp-log functions is
    Turing-reducible to the identity problem for constant terms."
}

@article{Stou76,
  author = "Stoutemyer, David R.",
  title = {{Automatic Simplification for the Absolute-value Function and its
           Relatives}},
  journal = "ACM SIGSAM",
  volume = "10",
  number = "4",
  year = "1976",
  pages = "48-49",
  abstract =
    "Computer symbolic mathematics has made impressive progress for the
    automatic simplification of rational expressions, algebraic
    expressions, and elementary transcendental expressions. However,
    existing computer-algebra systems tend to provide little or no
    simplification for the absolute-value function or for its relatives
    such as the signum, unit ramp, unit step, max, min, modulo, and Dirac
    delta functions. Although these functions lack certain desireable
    properties that are helpful for canonical simplification, there are
    opportunities for some ad hoc simplification. Moreover, a perusal of
    most mathematics, engineering, and scientific journals or texts
    reveals that these functions are too prevalent to be ignored.This
    article describes specific simplification rules implemented in a
    program that supplements the built-in rules for the MACSYMA ABS and
    SIGNUM functions.",
  paper = "Stou76.pdf"
}

@misc{Bronxxa,
  author = "Bronstein, Manuel",
  title = {{Symbolic Integration: towards Practical Algorithms}},
  abstract =
    "After reviewing the Risch algorithm for the integration of elementary
    functions and the underlying theory, we descrbe the successive
    improvements in the field, and the current ``rational'' approach to
    symbolic integration. We describe how a technique discovered by
    Hermite a century ago can be efficiently applied to rational,
    algebraic, elementary transcendental and mixed elementary functions."
}

@article{Bron88,
  author = "Bronstein, Manuel",
  title = {{The Transcendental Risch Differential Equation}},
  journal = "J. Symbolic Computation",
  volume = "9",
  year = "1988",
  pages = "49-60",
  abstract =
    "We present a new rational algorithm for solving Risch differential
    equations in towers of transcendental elementary extensions.  In
    contrast to a recent algorithm of Davenport we do not require a
    progressive reduction of the denominators involved, but use weak
    normality to obtain a formula for the denominator of a possible
    solution.  Implementation timings show this approach to be faster than
    a Hermite-like reduction.",
  paper = "Bron88.pdf",
  keywords = "axiomref"
}

@article{Bron90a,
  author = "Bronstein, Manuel",
  title = {{Integration of Elementary Functions}},
  journal = "J. Symbolic Computation",
  volume = "9",
  pages = "117-173",
  year = "1990",
  abstract = 
    "We extend a recent algorithm of Trager to a decision procedure for the
    indefinite integration of elementary functions. We can express the
    integral as an elementary function or prove that it is not
    elementary. We show that if the problem of integration in finite terms
    is solvable on a given elementary function field $k$, then it is
    solvable in any algebraic extension of $k(\theta)$, where $\theta$ is
    a logarithm or exponential of an element of $k$. Our proof considers
    an element of such an extension field to be an algebraic function of
    one variable over $k$.

    In his algorithm for the integration of algebraic functions, Trager
    describes a Hermite-type reduction to reduce the problem to an
    integrand with only simple finite poles on the associated Riemann
    surface. We generalize that technique to curves over liouvillian
    ground fields, and use it to simplify our integrands.  Once the
    multipe finite poles have been removed, we use the Puiseux expansions
    of the integrand at infinity and a generalization of the residues to
    compute the integral. We also generalize a result of Rothstein that
    gives us a necessary condition for elementary integrability, and
    provide examples of its use.",
  paper = "Bron90a.pdf"
}

@article{Bron90c,
  author = "Bronstein, Manuel",
  title = {{On the integration of elementary functions}},
  journal = "Journal of Symbolic Computation",
  volume = "9",
  number = "2",
  pages = "117-173",
  year = "1990",
  month = "February"
}

@inproceedings{Bron93,
  author = "Bronstein, Manuel and Salvy, Bruno",
  title = {{Full partial fraction decomposition of rational functions}},
  booktitle = "Proc. ISSAC 1993",
  year = "1993",
  pages = "157-160",
  isbn = "0-89791-604-2",
  link = "\url{http://www.acm.org/pubs/citations/proceedings/issac/164081/}",
  algebra = "\newline\refto{domain FPARFRAC FullPartialFractionExpansion}",
  abstract =
    "We describe a rational algorithm that computes the full partial
    fraction expansion of a rational function over the algebraic closure
    of its field of definition. The algorithm uses only gcd operations
    over the initial field but the resulting decomposition is expressed
    with linear denominators. We give examples from its Axiom and Maple
    implementations.",
  paper = "Bron93.pdf",
  keywords = "axiomref",
  beebe = "Bronstein:1993:FPF"
}

@book{Bron97,
  author = "Bronstein, Manuel",
  title = {{Symbolic Integration I--Transcendental Functions}},
  publisher = "Springer, Heidelberg",
  year = "1997",
  isbn = "3-540-21493-3",
  link = "\url{http://evil-wire.org/arrrXiv/Mathematics/Bronstein,_Symbolic_Integration_I,1997.pdf}",
  paper = "Bron97.pdf"
}

@article{Bron06,
  author = "Bronstein, M.",
  title = {{Parallel integration}},
  journal = "Programming and Computer Software",
  year = "2006",
  issn = "0361-7688",
  volume = "32",
  number = "1",
  doi = "10.1134/S0361768806010075",
  link = "\url{http://dx.doi.org/10.1134/S0361768806010075}",
  publisher = "Nauka/Interperiodica",
  pages = "59-60",
  abstract = "
    Parallel integration is an alternative method for symbolic
    integration.  While also based on Liouville's theorem, it handles all
    the generators of the differential field containing the integrand ``in
    parallel'', i.e.  all at once rather than considering only the topmost
    one in a recursive fasion. Although it still contains heuristic
    aspects, its ease of implementation, speed, high rate of success, and
    ability to integrate functions that cannot be handled by the Risch
    algorithm make it an attractive alternative.",
  paper = "Bron06.pdf"
}

@article{Bron07,
  author = "Bronstein, Manuel",
  title = {{Structure theorems for parallel integration}},
  journal = "Journal of Symbolic Computation",
  volume = "42",
  number = "7",
  pages = "757-769",
  year = "2007",
  month = "July",
  abstract = "
    We introduce structure theorems that refine Liouville's Theorem on
    integration in closed form for general derivations on multivariate
    rational function fields. By predicting the arguments of the new
    logarithms that an appear in integrals, as well as the denominator of
    the rational part, those theorems provide theoretical backing for the
    Risch-Norman integration method.  They also generalize its applicability 
    to non-monomial extensions, for example the Lambert W function.",
  paper = "Bron07.pdf"
}

@article{Cher85,
  author = "Cherry, G.W.",
  title = {{Integration in Finite Terms with Special Functions: 
           The Error Function}},
  journal = "J. Symbolic Computation",
  year = "1985",
  volume = "1",
  pages = "283-302",
  abstract = 
    "A decision procedure for integrating a class of transcendental
    elementary functions in terms of elementary functions and error
    functions is described. The procedure consists of three mutually
    exclusive cases. In the first two cases a generalised procedure for
    completing squares is used to limit the error functions which can
    appear in the integral of a finite number.  This reduces the problem
    to the solution of a differential equation and we use a result of
    Risch (1969) to solve it.  The third case can be reduced to the
    determination of what we have termed $\sum$-decompositions. The resutl
    presented here is the key procuedure to a more general algorithm which
    is described fully in Cherry (1983).",
  paper = "Cher85.pdf"
}

@article{Coll69,
  author = "Collins, George E.",
  title =
    {{Algorithmic Approaches to Symbolic Integration and SImplification}},
  journal = "ACM SIGSAM",
  volume = "12",
  year = "1969",
  pages = "5016",
  abstract =
    "This panel session followed the format announced by SIGSAM Chairman
    Carl Engelman in the announcement published in SIGSAM Bulletin No. 10
    (October 1968). Carl gave a brief (five or ten minutes) introduction
    to the subject and introduced Professor Joel Moses (M. I. T.). Joel
    presented an excellent exposition of the recent research
    accomplishments of the other panel members, synthesizing their work
    into a single large comprehensible picture. His presentation was
    greatly enhanced by a series of 27 carefully prepared slides
    containing critical examples and basic formulas, and was certainly the
    feature of the show. A panel discussion followed, with some audience
    participation. Panel members were Dr. W. S. Brown (Bell Telephone
    Laboratories), Professor B. F. Caviness (Duke University), Dr. Daniel
    Richardson and Dr. R. H. Risch (IBM).",
  paper = "Coll69.pdf"
}

@book{Dave81,
  author = "Davenport, James H.",
  title = {{On the Integration of Algebraic Functions}},
  publisher = "Springer-Verlag",
  series = "Lecture Notes in Computer Science 102",
  isbn = "0-387-10290-6",
  year = "1981",
  abstract =
    "This work is concerned with the following question: ``{\sl When is an
    algebraic function integrable?}''. We can state this question in
    another form which makes clearer our interpretation of integration:
    ``If we are given an algebraic function, when can we find an
    expression in terms of algebraics, logarithms and exponentials whose
    derivative is the given function, and what is that expression?''.
    
    This question can be looked at purely mathematically, as a question in
    decidablility theory, but our interest in this question is more
    practical and springs from the requirements of computer algebra.  Thus
    our goal is ``{\sl Write a program which, when given an algebraic
    function, will produce an expression for its integral in terms of
    algebraics, exponentials and logarithms, or will prove that there is
    no such expression}''.",
  paper = "Dave81.pdf"
}

@article{Dave81c,
  author = "Davenport, James H.",
  title = {{Algebraic Computations}},
  publisher = "Springer-Verlag",
  journal = "Lecture Notes in Computer Science 102",
  pages = "14-29",
  isbn = "0-387-10290-6",
  year = "1981",
  abstract =
    "Algebraic relationships between variables and expressions are very
    common in computer algebra. Not only do they often occur explicitly,
    in forms like $sqrt(x^2+1)$, but well known difficulties such as
    $sin(x)^2+cos(x)^2=1$ (Stoutemyer, 1977) can be expressed in this
    form. Nevertheless it is difficult to compute with regard to these
    relationships. This chapter discusses the problem of such computig,
    and then enters the area of algebraic geometry, which is a natural
    outgrowth of attempts to perform such computations as readily as one
    computes without them.",
  paper = "Dave81c.pdf"
}

@article{Dave81d,
  author = "Davenport, James H.",
  title = {{Coates' Algorithm}},
  publisher = "Springer-Verlag",
  journal = "Lecture Notes in Computer Science 102",
  pages = "30-48",
  isbn = "0-387-10290-6",
  year = "1981",
  abstract =
    "In this chapter, we consider the problem of finding a function with a
    certain set of poles. That this problem is non-trivial in the case of
    algebraic functions (although it is trivial in the case of rational
    functions) can be seen from the fact that such functions need not
    always exist. For example, on the curve defined by $\sqrt{x^3+1}$,
    there is no function with a zero of order 1 at one place lying over
    the point $X=0$ and a pole of order 1 at infinity and no other poles
    or zeros, but there is one with divisor 3 times that (ie.e the divisor
    has order 3). On the curve defined by $Y^2=x^3-3X^2+X+1$, there are no
    functions with a zero on one place lying over $X=0$ and a pole at the
    other, both having the same order, and no other zeros or poles.",
  paper = "Dave81d.pdf"
}

@article{Dave81e,
  author = "Davenport, James H.",
  title = {{Risch's Theorem}},
  publisher = "Springer-Verlag",
  journal = "Lecture Notes in Computer Science 102",
  pages = "49-63",
  isbn = "0-387-10290-6",
  year = "1981",
  abstract =
    "This chapter describes an underlying body of theory to the area of
    finding (or proving non-existent) the elementary integrals of
    algebraic functions, where a function is {\sl algebraic} if it can be
    generated from the variable of integration and constants by the
    arithmetic operations and the taking of roots of equations (the theory
    does not require that these roots should be expressible in terms of
    radicals), possibly with nesting. By {\sl elementary} we mean
    denerated from the variable of integration and constants by the
    arithmetic operations and the taking of roots, exponentials and
    logarithms, possibly with nesting.",
  paper = "Dave81e.pdf"
}

@article{Dave81f,
  author = "Davenport, James H.",
  title = {{The Problem of Torsion Divisors}},
  publisher = "Springer-Verlag",
  journal = "Lecture Notes in Computer Science 102",
  pages = "64-75",
  isbn = "0-387-10290-6",
  year = "1981",
  abstract =
    "This chapter and the next three are concerned with the theory and
    practice of the FIND-ORDER procedure, which, as we saw in the last
    chapter, is a necessary part of our integration algorithm, and which
    turns out to be the mathematically most difficult. This chapter will
    outline the general nature of the problem, with special reference to
    the simplest non-trivial case, viz. problems involving the square root
    of one cubic or quartic and involving no constants other than the
    rationals.",
  paper = "Dave81f.pdf"
}

@article{Dave81g,
  author = "Davenport, James H.",
  title = {{Gauss-Manin Operators}},
  publisher = "Springer-Verlag",
  journal = "Lecture Notes in Computer Science 102",
  pages = "76-91",
  isbn = "0-387-10290-6",
  year = "1981",
  abstract =
    "This chapter is devoted to the case of integrands which contain a
    transcendental parameter apart from the variable of integration, so
    that we can consider our problem to be the integration of a function
    in $\{K(x,y) | F(u,x,y) = 0\}$, where $K$ is an algebraic extension of
    $k(u)$ for some field $k$ and $u$ transcendental over it.  We shall
    use this notation, with $u$ being the independent transcendental, as
    we shall use the prefix operator $D$ to denote differentiation with
    respect to $u$, and the suffix $\prime$ to denote differentiation with
    respect to $x$. THis case is often more tractable than the case when
    there is no such transcendental, for integration with respect to $x$
    and differentiation with respect to $u$ commute, so that if $G(u,x,y0$
    is integrable, then so is $DG(u,x,y)$, $D^2G(u,x,y)$ and so on.",
  paper = "Dave81g.pdf"
}

@article{Dave81h,
  author = "Davenport, James H.",
  title = {{Elliptic Integrals Concluded}},
  publisher = "Springer-Verlag",
  journal = "Lecture Notes in Computer Science 102",
  pages = "92-105",
  isbn = "0-387-10290-6",
  year = "1981",
  abstract =
    "The previous chapter (including the algorithm FIND\_ORDER\_MANIN)
    completely solved the problem of torsion divisors over ground fields
    containing a transcendental. We are therefore left with the case of
    ground fields all of whose elements are algebraic over the rationals,
    and this is the problem we will consider in this chapter (for elliptic
    curves) and the next. Furthermore, any particular definition of a
    curve and of a divisor can only involve a finite number of algebraics,
    so we can restrict our attention to fields which are generated from
    the rationals by extending with a finite number of algebraics, i.e.
    {\sl algebraic number fields}. Before we can explore the torsion
    divisor problem over them, we first need to know more about their
    structure and possible computer representations, and this we discuss
    in the next section, amplifying the discussion of general algebraic
    expression in Chapter 2.",
  paper = "Dave81h.pdf"
}

@article{Dave81i,
  author = "Davenport, James H.",
  title = {{Curves over Algebraic Number Fields}},
  publisher = "Springer-Verlag",
  journal = "Lecture Notes in Computer Science 102",
  pages = "106-118",
  isbn = "0-387-10290-6",
  year = "1981",
  abstract =
    "The case of curves of arbitrary genus is much more difficult than the
    case of curves of genus 1, and there are no well-developed algorithms
    for this case. I have not been able to code any significant program to
    deal with this case because of the large number of subsidiary
    algorithms for which I do not have programs, though such programs have
    been written elsewhere, or can readily be written. Presented here,
    therefore, are the outlines of techniques which will enable one to
    bound the torsion of curves of arbitrary genus over algebraic number
    fields",
  paper = "Dave81i.pdf"
}

@article{Dave79c,
  author = "Davenport, James H.",
  title = {{Algorithms for the Integration of Algebraic Functions}},
  journal = "Lecture Notes in Computer Science",
  volume = "72",
  pages = "415-425",
  year = "1979",
  abstract = "
    The problem of finding elementary integrals of algebraic functions has
    long been recognized as difficult, and has sometimes been thought
    insoluble. Risch stated a theorem characterising the integrands with
    elementary integrals, and we can use the language of algebraic
    geometry and the techniques of Davenport to yield an algorithm that will
    always produce the integral if it exists. We explain the difficulty in
    the way of extending this algorithm, and outline some ways of solving
    it. Using work of Manin we are able to solve the problem in all cases
    where the algebraic expressions depend on a parameter as well as on
    the variable of integration.",
  paper = "Dave79c.pdf"
}

@article{Dave82a,
  author = "Davenport, Jamess H.",
  title = {{The Parallel Risch Algorithm (I)}},
  journal = "Lecture Notes in Computer Science",
  volume = "144",
  pages = "144-157",
  year = "1982",
  abstract = 
    "In this paper we review the so-called ``parallel Risch'' algorithm for
    the integration of transcendental functions, and explain what the
    problems with it are. We prove a positive result in the case of
    logarithmic integrands.",
  paper = "Dave82a.pdf"
}

@article{Dave85b,
  author = "Davenport, Jamess H. and Trager, Barry M.",
  title = {{The Parallel Risch Algorithm (II)}},
  journal = "ACM TOMS",
  volume = "11",
  number = "4",
  pages = "356-362",
  year = "1985",
  abstract = 
    "It is proved that, under the usual restrictions, the denominator of
    the integral of a purely logarithmic function is the expected one,
    that is, all factors of the denominator of the integrand have their
    multiplicity decreased by one. Furthermore, it is determined which new
    logarithms may appear in the integration.",
  paper = "Dave85b.pdf"
}

@article{Dave82b,
  author = "Davenport, Jamess H.",
  title = {{The Parallel Risch Algorithm (III): use of tangents}},
  journal = "ACM SIGSAM",
  volume = "16",
  number = "3",
  pages = "3-6",
  year = "1982",
  abstract = 
    "In this note, we look at the extension to the parallel Risch
    algorithm (see, e.g., the papers by Norman and Moore [1977], Norman and
    Davenport [1979], Fitch [1981] or Davenport [1982] for a description
    of the basic algorithm) which represents trigonometric functions in
    terms of tangents, rather than instead of complex exponentials.",
  paper = "Dave82b.pdf"
}

@article{Dave16,
  author = "Davenport, James H.",
  title =
     {{Complexity of Integration, Special Values, and Recent Developments}},
  journal = "LNCS",
  volume = "9725",
  pages = "485-491",
  year = "2016",
  abstract =
    "Two questions often come up when the author discusses integration:
    what is the complexity of the integration process, and for what
    special values of parameters is an unintegrable function actually
    integrable. These questions have not been much considered in the
    formal literature, and where they have been, there is one recent
    development indicating that the question is more delicate than had
    been supposed.",
  paper = "Dave16.pdf",
  keywords = "printed"
}

@inproceedings{Gedd89,
  author = "Geddes, K. O. and Stefanus, L. Y.",
  title = {{On the Risch-norman Integration Method and Its Implementation 
           in MAPLE}},
  booktitle = "Proc. of the ACM-SIGSAM 1989 Int. Symp. on Symbolic and 
               Algebraic Computation",
  series = "ISSAC '89",
  year = "1989",
  isbn = "0-89791-325-6",
  location = "Portland, Oregon, USA",
  pages = "212--217",
  numpages = "6",
  link = "\url{http://doi.acm.org/10.1145/74540.74567}",
  doi = "10.1145/74540.74567",
  acmid = "74567",
  publisher = "ACM",
  address = "New York, NY, USA",
  abstract = "
    Unlike the Recursive Risch Algorithm for the integration of
    transcendental elementary functions, the Risch-Norman Method processes
    the tower of field extensions directly in one step. In addition to
    logarithmic and exponential field extensions, this method can handle
    extentions in terms of tangents. Consequently, it allows trigonometric
    functions to be treated without converting them to complex exponential
    form. We review this method and describe its implementation in
    MAPLE. A heuristic enhancement to this method is also presented.",
  paper = "Gedd89.pdf"
}

@incollection{Grad80,
  author = "Gradshteyn, I.S. and Ryzhik, I.M.",
  title = {{Definite Integrals of Elementary Functions}},
  booktitle = "Table of Integrals, Series, and Products",
  publisher = "Academic Press",
  year = "1980",
  comment = "Chapter 3-4"
}

@article{Hebi15,
  author = "Hebisch, Waldemer",
  title = {{Integration in terms of exponential integrals and incomplete
           gamma functions}},
  year = "2015",
  journal = "ACM Communications in Computer Algebra",
  volume = "49",
  Issue = "3",
  pages = "98-100",
  abstract =
    "Indefinite integration means that given $f$ in some set we want to
    find $g$ from possibly larger set such that $f = g^\prime$. When $f$
    and $g$ are required to be elementary functions due to work of among
    others Risch, Rothstein, Trager, Bronstein (see [1] for references)
    integration problem is now solved at least in theory. In his thesis
    Cherry gave algorithm to integrate transcendental elementary functions
    in terms of exponential integrals. In [2] he gave algorithm to
    integrate transcendental elementary functions in so called reduced
    fields in terms of error functions. Knowles [3] and [4] extended this
    allowing also liovillian integrands and weakened restrictions on the
    field containing integrands. We extend previous results allowing
    incomplete gamma function $\Gamma(a, x)$ with rational $a$. Also, our
    theory can handle algebraic extensions and is complete jointly (and
    not only separately for Ei and erf). In purely transcendental case our
    method should be more efficient and easier to implement than [2]. In
    fact, it seems that no system currently implements algorithm from [2],
    while partial implementation of our method in FriCAS works well enough
    to be turned on by default. With our approach non-reduced case from
    [2] can be handled easily. We hope that other classes of special
    functions can be handled in a similar way, in particular irrational
    case of incomplete gamma function and polylogarithms (however
    polylogarithms raise tricky theoretical questions).",
  paper = "Hebi15.pdf",
  keywords = "printed"
}

@misc{Herm1872,
  author = "Hermite, E.",
  title = {{Sur l'int\'{e}gration des fractions rationelles}},
  journal = "Nouvelles Annales de Math\'{e}matiques",
  volume = "11",
  pages = "145-148",
  year = "1872"
}

@misc{Kaha90,
  author = "Kahan, William",
  title = {{The Persistence of Irrationals in Some Integrals}},
  year = "1990",
  abstract =
    "Computer algebra systems are expected to simplify formulas they
    obtain for symbolic integrals whenever they can, and often they
    succeed. However, the formulas so obtained may then produce incorrect
    results for symblic definite integrals"
}

@TechReport{Kalt84b,
  author = "Kaltofen, E.",
  title = {{The Algebraic Theory of Integration}},
  institution = "RPI",
  address = "Dept. Comput. Sci., Troy, New York",
  year = "1984",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/84/Ka84_integration.pdf}",
  paper = "Kalt84b.pdf"
}

@article{Kano76,
  author = "Kanoui, Henry",
  title = {{Some Aspects of Symbolic Integration via Predicate Logic 
           Programming}},
  journal = "ACM SIGSAM",
  volume = "10",
  number = "4",
  year = "1976",
  pages = "29-42",
  abstract =
    "During the past years, various algebraic manipulations systems have
    been described in the literature. Most of them are implemented via
    ``classic'' programming languages like Fortran, Lisp, PL1 ... We propose
    an alternative approach: the use of Predicate Logic as a programming
    language.",
  paper = "Kano76.pdf"
}

@article{Kasp80,
  author = "Kasper, Toni",
  title = {{Integration in Finite Terms: The Liouville Theory}},
  journal = "ACM SIGSAM",
  volume = "14",
  number = "4",
  year = "1980",
  pages = "2-8",
  abstract =
    "The search for elementary antiderivatives leads from classical
    analysis through modern algebra to contemporary research in computer
    algorithms.",
  paper = "Kasp80.pdf"
}

@article{Know93,
  author = "Knowles, Paul",
  title = {{Integration of a Class of Transcendental Liouvillian Functions 
           with Error-Functions, Part I}},
  journal = "Journal of Symbolic Computation",
  volume = "13",
  number = "5",
  pages = "525-543",
  year = "1993",
  abstract =
    "This paper gives a decision-procedure for the symbolic integration of
    a certain class of transcendental Liouvillian functions in terms of
    elementary functions and error-functions. An example illustrating the
    use of the decision-procedure is given.",
  paper = "Know93.pdf"
}

@article{Know93a,
  author = "Knowles, Paul",
  title = {{Integration of a Class of Transcendental Liouvillian Functions 
           with Error-Functions, Part II}},
  journal = "Journal of Symbolic Computation",
  volume = "16",
  number = "3",
  year = "1993",
  pages = "227-239",
  abstract =
    "This paper extends the decision procedure for the symbolic
    integration of a certain class of transcendental Liouvillian functions
    in terms of elementary functions and error-functions given in Knowles
    (1992) to allow a much larger class of integrands. Examples
    illustrating the use of the decision procedure are given.",
  paper = "Know93a.pdf"
}

@article{Krag09,
  author = "Kragler, R.",
  title = {{On Mathematica Program for Poor Man's Integrator Algorithm}},
  journal = "Programming and Computer Software",
  volume = "35",
  number = "2",
  pages = "63-78",
  year = "2009",
  issn = "0361-7688",
  abstract = "
    In this paper by means of computer experiment we study advantages and
    disadvantages of the heuristical method of ``parallel integrator''. For
    this purpose we describe and use implementation of the method in
    Mathematica. In some cases we compare this implementation with the original
    one in Maple.",
  paper = "Krag09.pdf"
}

@article{Liou1833a,
  author = "Liouville, Joseph",
  title = {{Premier m\'{e}moire sur la d\'{e}termination des int\'{e}grales 
           dont la valeur est alg\'{e}brique}},
  journal = "Journal de l'Ecole Polytechnique",
  volume = "14",
  pages = "124-128",
  year = "1833"
}

@article{Liou1833b,
  author = "Liouville, Joseph",
  title = {{Second m\'{e}moire sur la d\'{e}termination des int\'{e}grales 
           dont la valeur est alg\'{e}brique}},
  journal = "Journal de l'Ecole Polytechnique",
  volume = "14",
  pages = "149-193",
  year = "1833"
}

@article{Lope99,
  author = {L\'opez, Jos\'e L.},
  title = {{Asymptotic expansions of integrals: The term-by-term integration
           method}},
  year = "1999",
  journal = "Journal of Computational and Applied Mathematics",
  volume = "102",
  pages = "181-194",
  abstract =
    "The classical term-by-term integration technique used for obtaining
    asymptotic expansions of integrals requires the integrand to have an
    uniform asymptotic expansion in the integration variable. A
    modification of this method is presented in which the uniformity
    conditions provides the term-by-term integration technique a large
    range of applicability. As a consequence of this generality, Watson's
    lemma and the integration by parts technique applied to Laplace's and
    a special family of Fourier's transforms become corollaries of the
    term-by-term integration method.",
  paper = "Lope99.pdf"
}

@book{Munr53,
  author = "Munroe, M.E.",
  title = {{Introduction to Measure and Integration}},
  publisher = "Addison-Wesley",
  year = "1953"
}

@article{Mose71,
  author = "Moses, Joel",
  title = {{Symbolic Integration: The Stormy Decade}},
  journal = "CACM",
  year = "1971",
  volume = "14",
  number = "8",
  pages = "548-560",
  link = 
  "\url{http://www-inst.eecs.berkeley.edu/~cs282/sp02/readings/moses-int.pdf}",
  abstract = 
    "Three approaches to symbolic integration in the 1960's are
    described. The first, from artificial intelligence, led to Slagle's
    SAINT and to a large degree to Moses' SIN. The second, from algebraic
    manipulation, led to Monove's implementation and to Horowitz' and
    Tobey's reexamination of the Hermite algorithm for integrating
    rational functions. The third, from mathematics, led to Richardson's
    proof of the unsolvability of the problem for a class of functions and
    for Risch's decision procedure for the elementary functions. 
    Generalizations of Risch's algorithm to a class of special
    functions and programs for solving differential equations and for
    finding the definite integral are also described.",
  paper = "Mos71a.pdf",
  keywords = "printed"
}

@article{Ngxx74,
  author = "Ng, Edward W.",
  title = {{Symbolic Integration of a Class of Algebraic Functions}},
  journal = "ACM SIGSAM",
  volume = "8",
  number = "3",
  year = "1974",
  pages = "99-102",
  abstract =
    "In this presentation we describe the outline of an algorithmic
    approach to handle a class of algebraic integrands. (It is important
    to stress that for an extended abstract of the present form, we can at
    best convey the flavor of the approach, with numerous details
    missing.) We shall label this approach Carlson's algorithm because it
    is based on a series of analyses rendered by Carlson and his
    associates in the last ten years (Refs. 2, 3, 4, 8, and 12). The class
    of integrands is of the form $r(x,y)$, where $y^2$ is a polynomial in $x$,
    and $r$ a rational function in $x$ and $y$. This is the type of integrand
    that classically led to the study of elliptic integrals. At first
    glance this is a rather restricted class of algebraic functions. But
    in fact many trigonometric and hyperbolic integrands reduce to this
    form. The richness of this class of integrands is exemplified by a
    recently published handbook of 3000 integral formulas (Ref. 1). Our
    proposed approach will cover fifty to seventy percent of the items in
    the handbook. Furthermore the non-classical approach we shall describe
    holds great promise of developing to the case where definite integrals
    can be evaluated in terms of a host of other well-known functions
    (e.g., Bessel and Legendre).",
  paper = "Ngxx74.pdf"
}

@techreport{Ngxx77,
  author = "Ng, Edward W.",
  title = {{Observations on Approximate Integrations}},
  year = "1977",
  paper = "Ngxx77.pdf"
}

@inproceedings{Norm90,
  author = "Norman, Arthur C.",
  title = {{A Critical-Pair/Completion based Integration ALgorithm}},
  booktitle = "ISSAC 90",
  pages = "201-205",
  year = "1990",
  isbn = "0-201-54892-5",
  abstract =
    "In 1976 Risch [1] proposed a scheme for finding the integrals of
    forms built up out of transcendental functions that viewed general
    functions as rational forms in a suitable differential field and
    represented the polynomial parts of those forms in a distributed
    rather than recursive way. By using a data representation where all
    variables were (more or less) equally important this new method seemed
    to side-step some of the complications that had appeared in his
    previous scheme [2] where various side-constraints had to be
    propagated between the levels present in a tower of separate
    extensions of differential fields, otherwise seen as levels in
    recursive data structures. 

    An initial implementation of the method was
    prepared in the context of the SCRATCHPAD/1 algebra system and
    demonstrated at the 1976 SYMSAC meeting at Yorktown Heights, a
    subsequent version for Reduce [3][5] came after that, and made it
    possible to try the method on a large range of integrals. These
    practical studies showed up some problems with the method and its
    implementation. 

    The presentation given here re-expresses the 1976
    Risch method in terms of rewrite rules, and thus exposes the major
    problem it suffers from as a manifestation of the fact that in certain
    circumstances the set of rewrites generated is not confluent. This
    difficulty is then attacked using a critical-pair/completion (CPC)
    approach. For very many integrands it is then easy to see that the
    initial set of rewrites used in the early implementations [1] and [3]
    do not need any extension, and this fact explains the high level of
    competence of the programs involved despite their shaky theoretical
    foundations. For a further large collection of problems even a simple
    CPC scheme converges rapidly; when the techniques presented here are
    applied to the REDUCE integration test suite in all applicable cases a
    short computation succeeds in completing the set of rewrites and hence
    gives a secure basis for testing for integrability. 

    This paper describes the implementation of the CPC process and
    discusses current limitations to and possible future extended
    applications of it.",
  paper = "Norm90.pdf",
  keywords = "axiomref"
}

@article{Renb82,
  author = "Renbao, Zhong",
  title = {{An Algorithm for Avoiding Complex Numbers in Rational Function
           Integration}},
  journal = "ACM SIGSAM",
  volume = "16",
  number = "3",
  pages = "30-32",
  year = "1982",
  abstract =
    "Given a proper rational function $A(x)/B(x)$ where $A(x)$ and $B(x)$
    both are in $R[x]$ with $gcd(A(x), B(x))= 1$, $B(x)$ monic and
    $deg(A(x)) < deg(B(x))$, from the Hermite algorithm for rational
    function integration in [3], we obtain 
    \[\int{frac{A(x)}{B(x)}~dx} = S(x)+\int{\frac{T(x)}{B^*(x)}~dx}\] 
    where $S(x)$ is a rational function
    which is called the rational part of the integral of $A(x)/B(x)$ in
    eq. (1), $B^*(x)$ is the greatest square-free factor of $B(x)$, and
    $T(x)$ is in $R[x]$ with $deg(T(x)) < deg(B^*(x))$.  The integral of
    $T(x)/B^*(x)$ is called the transcendental part of the integral of
    $A(x)/B(x)$ in eq. (1).",
  paper = "Renb82.pdf"
}

@techreport{Risc68,
  author = "Risch, Robert",
  title = {{On the integration of elementary functions which are built up 
           using algebraic operations}},
  type = "Research Report",
  number = "SP-2801/002/00",
  institution = "System Development Corporation, Santa Monica, CA, USA", 
  year = "1968"
}

@techreport{Risc69a,
  author = "Risch, Robert",
  title = {{Further results on elementary functions}},
  type = "Research Report",
  number = "RC-2042",
  institution = "IBM Research, Yorktown Heights, NY, USA",
  year = "1969"
}

@article{Risc69b,
  author = "Risch, Robert",
  title = {{The problem of integration in finite terms}},
  journal = "Transactions of the American Mathematical Society",
  volume = "139",
  year = "1969",
  pages = "167-189",
  abstract = "This paper deals with the problem of telling whether a
    given elementary function, in the sense of analysis, has an elementary
    indefinite integral.",
  paper = "Ris69b.pdf"
}

@article{Risc70,
  author = "Risch, Robert",
  title = {{The Solution of the Problem of Integration in Finite Terms}},
  journal = "Bull. AMS",
  year = "1970",
  issn = "0002-9904",
  volume = "76",
  number = "3",
  pages = "605-609",
  abstract = "
    The problem of integration in finite terms asks for an algorithm for
    deciding whether an elementary function has an elementary indefinite
    integral and for finding the integral if it does.  ``Elementary'' is
    used here to denote those functions build up from the rational
    functions using only exponentiation, logarithms, trigonometric,
    inverse trigonometric and algebraic operations.  This vaguely worded
    question has several precise, but inequivalent formulations. The
    writer has devised an algorithm which solves the classical problem of
    Liouville. A complete account is planned for a future publication. The
    present note is intended to indiciate some of the ideas and techniques
    involved.",
  paper = "Risc70.pdf"
}

@article{Risc79,
  author = "Risch, Robert",
  title = {{Algebraic properties of the elementary functions of analysis}},
  journal = "American Journal of Mathematics",
  volume = "101",
  pages = "743-759",
  year = "1979"
}

@article{Rose72,
  author = "Rosenlicht, Maxwell",
  title = {{Integration in finite terms}},
  journal = "American Mathematical Monthly",
  year = "1972",
  volume = "79",
  pages = "963-972",
  paper = "Rose72.pdf"
}

@article{Ro76a,
  author = "Rothstein, Michael and  Caviness, Bob F.",
  title = {{A structure theorem for exponential and primitive functions: 
           a preliminary report}},
  journal = "ACM Sigsam Bulletin",
  volume = "10",
  number = "4",
  year = "1976",
  abstract = 
    "In this paper a generalization of the Risch Structure Theorem is reported.
    The generalization applies to fields $F(t_1,\ldots,t_n)$ where $F$ 
    is a differential field (in our applications $F$ will be a finitely 
    generated extension of $Q$, the field of rational numbers) and each $t_i$ 
    is either algebraic over $F_{i-1}=F(t_1,\ldots,t_{i-1})$, is an 
    exponential of an element in $F_{i-1}$, or is an integral of an element 
    in $F_{i-1}$. If $t_i$ is an integral and can be expressed using 
    logarithms, it must be so expressed for the generalized structure 
    theorem to apply.",
  paper = "Ro76a.pdf"
}

@article{Roth77,
  author = "Rothstein, Michael",
  title = {{A new algorithm for the integration of exponential and 
           logarithmic functions}},
  journal = "Proceedings of the 1977 MACSYMA Users Conference",
  year = "1977",
  pages = "263-274",
  publisher = "NASA Pub CP-2012"
}

@article{Scho89,
  author = "Schou, Wayne C. and Broughan, Kevin A.",
  title = {{The Risch Algorithms of MACSYMA and SENAC}},
  journal = "ACM SIGSAM",
  volume = "23",
  number = "3",
  year = "1989",
  abstract =
    "The purpose of this paper is to report on a computer implementation
    of the Risch algorithm for the symbolic integration of rational
    functions containing nested exponential and logarithms. For the class
    of transcendental functions, the Risch algorithm [4] represents a
    practical method for symbolic integration. Because the Risch algorithm
    describes a decision procedure for transcendental integration it is an
    ideal final step in an integration package. Although the decision
    characteristic cannot be fully realised in a computer system, because
    of major algebraic problems such as factorisation, zero-equivalence
    and simplification, the potential advantages are considerable.",
  paper = "Scho89.pdf",
}

@article{Sing85,
  author = "Singer, Michael F. and  Saunders, B. David and Caviness, Bob F.",
  title = 
    {{An extension of Liouville's theorem on integration in finite terms}},
  journal = "SIAM J. of Comp.",
  volume = "14",
  pages = "965-990",
  year = "1985",
  link = "\url{http://www4.ncsu.edu/~singer/papers/singer_saunders_caviness.pdf}",
  abstract = 
    "In Part 1 of this paper, we give an extension of Liouville's Theorem
    and give a number of examples which show that integration with special
    functions involves some phenomena that do not occur in integration
    with the elementary functions alone. Our main result generalizes
    Liouville's Theorem by allowing, in addition to the elementary
    functions, special functions such as the error function, Fresnel
    integrals and the logarithmic integral (but not the dilogarithm or
    exponential integral) to appear in the integral of an elementary
    function. The basic conclusion is that these functions, if they
    appear, appear linearly. We give an algorithm which decides if an
    elementary function, built up using only exponential functions and
    rational operations has an integral which can be expressed in terms of
    elementary functions and error functions.",
  paper = "Sing85.pdf"
}

@article{Smit83,
  author = "Smith, Paul and Sterling, Leon",
  title = {{Of Integration by Man and Machine}},
  journal = "ACM SIGSAM",
  volume = "17",
  number = "3-4",
  year = "1983",
  abstract =
    "We describe a symbolic integration problem arising from an
    application in engineering. A solution is given and compared with the
    solution generated by the REDUCE integration package running at
    Cambridge. Nontrivial symbol manipulation, particularly
    simplification, is necessary to reconcile the answers.",
  paper = "Smit83.pdf"
}

@misc{Temmxx,
  author = "Temme, N.M.",
  title = {{Uniform Asymptotic Expansions of Integrals}},
  abstract =
    "The purpose of the paper is to give an account of several aspects of
    uniform asymptotic expansions of integrals. We give examples of
    standard forms, the role of critical points and methods to construct
    the experiences."
}

@article{Temm95,
  author = "Temme, N.M.",
  title = {{Uniform asymptotic expansions of integrals: a selection of
           problems}},
  journal = "Journal of Computational and Applied Mathematics",
  volume = "65",
  number = "1-3",
  year = "1995",
  pages = "395-417",
  abstract =
    "On the occasion of the conference we mention examples of Stieltjes'
    work on asymptotics of special functions. The remaining part of the
    paper gives a selection of asymptotic methods for integrals, in
    particular on uniform approximations. We discuss several “standard”
    problems and examples, in which known special functions (error
    functions, Airy functions, Bessel functions, etc.) are needed to
    construct uniform approximations. Finally, we discuss the recent
    interest and new insights in the Stokes phenomenon. An extensive
    bibliography on uniform asymptotic methods for integrals is given,
    together with references to recent papers on the Stokes phenomenon for
    integrals and related topics.",
  paper = "Temm95.pdf"

}

@mastersthesis{Tere09,
  author = "Terelius, Bjorn",
  title = {{Symbolic Integration}},
  school = "Royal Institute of Technology",
  address = "Stockholm, Sweden",
  year = "2009",
  abstract = 
    "Symbolic integration is the problem of expressing an indefinite integral
    $\int{f}$ of a given function $f$ as a finite combination $g$ of elementary
    functions, or more generally, to determine whether a certain class of
    functions contains an element $g$ such that $g^\prime = f$.

    In the first part of this thesis, we compare different algorithms for
    symbolic integration. Specifically, we review the integration rules
    taught in calculus courses and how they can be used systematically to
    create a reasonable, but somewhat limited, integration method. Then we
    present the differential algebra required to prove the transcendental
    cases of Risch's algorithm. Risch's algorithm decides if the integral
    of an elementary function is elementary and if so computes it. The
    presentation is mostly self-contained and, we hope, simpler than
    previous descriptions of the algorithm. Finally, we describe
    Risch-Norman's algorithm which, although it is not a decision
    procedure, works well in practice and is considerably simpler than the
    full Risch algorithm.

    In the second part of this thesis, we briefly discuss an
    implementation of a computer algebra system and some of the
    experiences it has given us.  We also demonstrate an implementation of
    the rule-based approach and how it can be used, not only to compute
    integrals, but also to generate readable derivations of the results.",
  paper = "Tere09.pdf"
}

@article{Trag76,
  author = "Trager, Barry",
  title = {{Algebraic factoring and rational function integration}},
  journal = "Proceedings of SYMSAC'76",
  year = "1976",
  pages = "219-226",
  abstract = "
    This paper presents a new, simple, and efficient algorithm for
    factoring polynomials in several variables over an algebraic number
    field. The algorithm is then used interatively to construct the
    splitting field of a polynomial over the integers. Finally the
    factorization and splitting field algorithms are applied to the
    problem of determining the transcendental part of the integral of a
    rational function. In particular, a constructive procedure is given
    for finding a least degree extension field in which the integral can
    be expressed.",
  paper = "Trag76.pdf"
}

@phdthesis{Trag84,
  author = "Trager, Barry",
  title = {{Integration of Algebraic Functions}},
  school = "MIT",
  year = "1984",
  link = "\url{http://www.dm.unipi.it/pages/gianni/public_html/Alg-Comp/thesis.pdf}",
  abstract = "
    We show how the ``rational'' approach for integrating algebraic
    functions can be extended to handle elementary functions. The
    resulting algorithm is a practical decision procedure for determining
    whether a given elementary function has an elementary antiderivative,
    and for computing it if it exists.",
  paper = "Trag76.pdf"
}

@phdthesis{Wang71,
  author = "Wang, Paul S.",
  title = {{Evaluation of Definite Integrals by Symbolic Manipulation}},
  school = "MIT",
  year = "1971",
  link = 
    "\url{http://publications.csail.mit.edu/lcs/pubs/pdf/MIT-LCS-TR-092.pdf}",
  comment = "MIT/LCS/TR-92",
  abstract =
    "A heuristic computer program for the evaluation of real definite
    integrals of elementary functions is described This program, called
    WANDERER, (WANg's DEfinite integRal EvaluatoR), evaluates many proper
    and improper integrals. The improper integrals may have a finite or
    infinite range of integration. Evaluation by contour integration and
    residue theory is among the methods used. A program called DELIMITER
    (DEfinitive LIMIT EvaluatoR) is used for the limit computations needed
    in evaluating some definite integrals. DELIMITER is a heuristic
    program written for computing limits of real or complex analytic
    functions. For real functions of a real variable, one-sided as well
    been implmented in the MACSYMA system, a symbolic and algebraic
    manipulation system being developed at Project MAC, MIT. A typical
    problem in applied mathematics, namely asymptotic analysis of a
    definite integral, is solved using MACSYMA to demonstrate the
    usefulness of such a system and the facilities provided by WANDERER.",
  paper = "Wang71.pdf"
}

@misc{Dele06,
  author = "Delenclos, Jonathon and Leroy, Andr\'e",
  title = {{Noncommutative Symmetric functions and $W$-polynomials}},
  link = "\url{http://arxiv.org/pdf/math/0606614.pdf}",
  algebra = "\newline\refto{category LORER LeftOreRing}",
  abstract = "
    Let $K$, $S$, $D$ be a division ring an endomorphism and a
    $S$-derivation of $K$, respectively. In this setting we introduce
    generalized noncommutative symmetric functions and obtain Vi\'ete
    formula and decompositions of different operators. $W$-polynomials
    show up naturally, their connetions with $P$-independency. Vandermonde
    and Wronskian matrices are briefly studied. The different linear
    factorizations of $W$-polynomials are analysed. Connections between
    the existence of LLCM (least left common multiples) of monic linear
    polynomials with coefficients in a ring and the left duo property are
    established at the end of the paper.",
  paper = "Dele06.pdf"
}

@mastersthesis{Bohl08,
  author = "Bohler, Per Reidar",
  title = {{Special number field sieve}},
  school = "Norwegian University of Science and Technology",
  year = "2008",
  link = "\url{http://www.diva-portal.org/smash/get/diva2:348611/FULLTEXT01.pdf}",
  abstract = 
    "Integer factorization is a problem not yet solved for arbitrary integers.
    Huge integers are therefore widely used for encrypting, e.g. in the RSA
    encryption scheme. The special number field sieve holds the current
    factorization record for factoring the number $2^{1039}+1$. The
    algorithms depends on arithmetic in an algebraic number fields and is
    a further development from the quadratic sieve factoring algorithm. 
    We therefor present the quadratic sieve first. Then the special number
    field is described. The key concepts is evaluated one bye one. 
    Everything is illustrated with the corresponding parts of an example
    factorization. The running time of the special number field sieve is
    then evaluated and compared against that of the quadratic sieve. The
    special number field sieve only applies to integers of a special form,
    but a generalization has been made, the general number field sieve. It
    is slower but all estimates suggests it is asymptotically faster than
    all other existing general purpose algorithms.",
  paper = "Bohl08.pdf"
}

@misc{Case16,
  author = "Case, Michael",
  title = {{A Beginner's Guide to the General Number Field Sieve}},
  year = "2016",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.219.2389}",
  paper = "Case16.pdf"
}

@misc{Hill11,
  author = "Hill, Joshua E.",
  title = {{The Number Field Sieve: An Extended Abstract}},
  year = "2011",
  link = "\url{http://www.untruth.org/~josh/math/NFS.pdf}",
  paper = "Hill11.pdf"
}

@InProceedings{Kalt89d,
  author = "Kaltofen, E. and Valente, T. and Yui, N.",
  title = {{An improved {Las Vegas} primality test}},
  booktitle = "Proc. 1989 Internat. Symp. Symbolic Algebraic Comput.",
  pages = "26--33",
  year = "1989",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/89/KVY89.pdf}",
  paper = "Kalt89d.pdf"
}

@InCollection{Kalt91b,
  author = "Kaltofen, E. and Yui, N.",
  editor = "D. V. Chudnovsky and G. V. Chudnovsky and H. Cohn and 
            M. B. Nathanson",
  title = {{Explicit construction of {Hilbert} class fields of imaginary 
           quadratic fields by integer lattice reduction}},
  booktitle = "Number Theory New York Seminar 1989--1990",
  pages = "150--202",
  publisher = "Springer-Verlag",
  year = "1991",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/91/KaYui91.pdf}",
  paper = "Kalt91b.pdf"
}

@InProceedings{Kalt84a,
  author = "Kaltofen, E. and Yui, N.",
  title = {{Explicit construction of the {Hilbert} class field of imaginary 
           quadratic fields with class number 7 and 11}},
  booktitle = "Proc. EUROSAM '84",
  pages = "310--320",
  year = "1984",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/84/KaYui84_eurosam.ps.gz}",
  paper = "Kalt84a.ps"
}

@article{Pome94,
  author = "Pomerance, Carl",
  title = {{The Number Field Sieve}},
  journal = "Proc. Symposia in Applied Mathematics",
  volume = "48",
  year = "1994",
  abstract =
    "The most exciting recent development in the integer factorization
    problem is the number field sieve. It has had some spectacular successes
    with integers in certain special forms, most notably the factorization in
    1990 of the 155 decimal digit number $2^{512}+1$. For arbitrary hard
    integers, it now appears to threaten the quadratic sieve as the algorithm
    of choice. In this paper the number field sieve, and the ideas behind it,
    are described",
  paper = "Pome94.pdf"
}

@misc{Sho08,
  author = "Shoup, Victor",
  title = {{A Computational Introduction to Number Theory}},
  link = "\url{http://shoup.net/ntb/ntb-v2.pdf}",
  paper = "Sho08.pdf"
}

@InProceedings{Kalt07a,
  author = "Kaltofen, Erich and Yang, Zhengfeng and Zhi, Lihong",
  title = {{On probabilistic analysis of randomization in hybrid 
           symbolic-numeric algorithms}},
  year = "2007",
  booktitle = "Proc. 2007 Internat. Workshop on Symbolic-Numeric Comput.",
  pages = "11--17",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/07/KYZ07.pdf}",
  paper = "Kalt07a.pdf"
}

@InProceedings{Kalt07b,
  author = "Kaltofen, Erich and Yang, Zhengfeng",
  title = {{On Exact and Approximate Interpolation of Sparse 
           Rational Functions}},
  year = "2007",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'07",
  pages = "203--210",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/07/KaYa07.pdf}",
  paper = "Kalt07b.pdf"
}

@Article{Gies03,
  author = "Giesbrecht, Mark and Kaltofen, Erich and Lee, Wen-shin",
  title = {{Algorithms for Computing Sparsest Shifts of Polynomials in
           Power, {Chebychev}, and {Pochhammer} Bases}},
  year = "2003",
  journal = "Journal of Symbolic Computation",
  volume = "36",
  number = "3--4",
  pages = "401--424",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/03/GKL03.pdf}",
  paper = "Gies03.pdf"
}

@InProceedings{Gies02,
  author = "Giesbrecht, Mark and Kaltofen, Erich and Lee, Wen-shin",
  title = {{Algorithms for Computing the Sparsest Shifts for Polynomials via 
           the Berlekamp/Massey Algorithm}},
  booktitle = "Proc. 2002 Internat. Symp. Symbolic Algebraic Comput.",
  pages = "101--108",
  year = "2002",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/02/GKL02.pdf}",
  paper = "Gies02.pdf"
}

@Article{Kalt03b,
  author = "Kaltofen, Erich and Lee, Wen-shin",
  title = {{Early Termination in Sparse Interpolation Algorithms}},
  year = "2003",
  journal = "Journal of Symbolic Computation",
  volume = "36",
  number = "3--4",
  pages = "365--400",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/03/KL03.pdf}",
  paper = "Kalt03b.pdf"
}

@InProceedings{Kalt00a,
  author = "Kaltofen, E. and Lee, W.-s. and Lobo, A.A.",
  title = {{Early termination in Ben-Or/Tiwari sparse interpolation
           and a hybrid of Zippel's algorithm}},
  booktitle = "Proc. 2000 Internat. Symp. Symbolic Algebraic Comput.",
  pages = "192--201",
  year = "2000",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/2K/KLL2K.pdf}",
  paper = "Kalt00a.pdf"
}

@InProceedings{Kalt10b,
  author = "Kaltofen, Erich L.",
  title = {{Fifteen years after {DSC} and {WLSS2} {What} parallel 
           computations {I} do today [{Invited} Lecture at {PASCO} 2010]}},
  year = "2010",
  booktitle = "Proc. 2010 Internat. Workshop on Parallel Symbolic Comput.",
  pages = "10--17",
  month = "July",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/10/Ka10_pasco.pdf}",
  paper = "Kalt10b.pdf"
}

@InProceedings{Kalt90,
  author = "Kaltofen, E. and Lakshman, Y.N. and Wiley, J.M.",
  editor = "S. Watanabe and M. Nagata",
  title = {{Modular rational sparse multivariate polynomial interpolation}},
  booktitle = "Proc. 1990 Internat. Symp. Symbolic Algebraic Comput.",
  pages = "135--139",
  publisher = "ACM Press",
  year = "1990",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/90/KLW90.pdf}",
  paper = "Kalt90.pdf"
}

@InProceedings{Kalt88a,
  author = "Kaltofen, E. and Yagati, Lakshman",
  title = {{Improved sparse multivariate polynomial interpolation algorithms}},
  booktitle = "Symbolic Algebraic Comput. Internat. Symp. ISSAC '88 Proc.",
  pages = "467--474",
  year = "1988",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/88/KaLa88.pdf}",
  paper = "Kalt88a.pdf"
}

@InCollection{Gren11,
  author = "Grenet, Bruno and Kaltofen, Erich L. and Koiran, Pascal 
            and Portier, Natacha",
  title = {{Symmetric Determinantal Representation of Formulas and Weakly 
           Skew Circuits}},
  booktitle = "Randomization, Relaxation, and Complexity in Polynomial 
               Equation Solving",
  year = "2011",
  editor = "Leonid Gurvits and Philippe P\'{e}bay and J. Maurice Rojas
            and David Thompson",
  pages = "61--96",
  publisher = "American Mathematical Society",
  address = "Providence, Rhode Island, USA",
  isbn = "978-0-8218-5228-6",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/10/GKKP10.pdf}",
  paper = "Gren11.pdf"
}

@InProceedings{Kalt08a,
  author = "Kaltofen, Erich and Koiran, Pascal",
  title = {{Expressing a Fraction of Two Determinants as a Determinant}},
  year = "2008",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'08",
  pages = "141--146",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/08/KaKoi08.pdf}",
  paper = "Kalt08a.pdf"
}

@Article{Hitz95,
  author = "Kitz, M.A. and Kaltofen, E.",
  title = {{Integer division in residue number systems}},
  journal = "IEEE Trans. Computers",
  year = "1995",
  volume = "44",
  number = "8",
  pages = "983--989",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/95/HiKa95.pdf}",
  paper = "Hitz95.pdf"
}

@InProceedings{Kalt92a,
  author = "Kaltofen, E.",
  title = {{On computing determinants of matrices without divisions}},
  booktitle = "Proc. 1992 Internat. Symp. Symbolic Algebraic Comput.",
  pages = "342--349",
  year = "1992",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/92/Ka92_issac.pdf}",
  paper = "Kalt92a.pdf"
}

@Article{Cant91,
  author = "Cantor, D.G. and Kaltofen, E.",
  title = {{On fast multiplication of polynomials over arbitrary algebras}},
  journal = "Acta Inform.",
  year = "1991",
  volume = "28",
  number = "7",
  pages = "693--701",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/91/CaKa91.pdf}",
  paper = "Cant91.pdf"
}

@Article{Kalt88b,
  author = "Kaltofen, E.",
  title = {{Greatest common divisors of polynomials given by 
           straight-line programs}},
  journal = "J. ACM",
  year = "1988",
  volume = "35",
  number = "1",
  pages = "231--264",
  link =
    "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/88/Ka88_jacm.pdf}",
  abstract =
    "Algorithms on multivariate polynomials represented by straight-line
    programs are developed.  First, it is shown that most algebraic
    algorithms can be probabilistically applied to data that are given by
    a straight-line computation.  Testing such rational numeric data for
    zero, for instance, is facilitated by random evaluations modulo random
    prime numbers.  Then, auxiliary algorithms that determine the
    coefficients of a multivariate polynomial in a single variable are
    constructed.  The first main result is an algorithm that produces the
    greatest common divisor of the input polynomials, all in straight-line
    representation.  The second result shows how to find a straight-line
    program for the reduced numerator and denominator from one for the
    corresponding rational function.  Both the algorithm for that
    construction and the greatest common divisor algorithm are in random
    polynomial time for the usual coefftcient fields and output a
    straight-line program, which with controllably high probability
    correctly determines the requested answer.  The running times are
    polynomial functions in the binary input size, the input degrees as
    unary numbers, and the logarithm of the inverse of the failure
    probability.  The algorithm for straight-line programs for the
    numerators and denominators of rational functions implies that every
    degree-bounded rational function can be computed fast in parallel,
    that is, in polynomial size and polylogarithmic depth.",
  paper = "Kalt88b.pdf"
}

@article{Abbo87,
  author = "Abbott, J.A. and Bradford, R.J. and Davenport, J.H.",
  title = {{factorisation of Polynomials: Old Ideas and Recent
            Results}},
  journal = "Lecture Notes in Computer Science",
  volume = "296",
  year = "1987",
  pages = "81-91",
  abstract =
    "The problem of factorising polynomials: that is to say, given a
    polynomial with integer coefficients, to find the irreducible
    polynomials that divide it, is one with a long history. While the last
    word has not been said on the subject, we can say that the past 15
    years have seen major break-throughs, and many computer algebra
    systems now include {\sl efficient} algorithms for this problem. When
    it comes to polynomials with algebraic number coefficients, the
    problem is far harder, and several major questions remain to be
    answered. Nevertheless, the last few years have seen substantial
    improvements, and such factorisations are now possible",
  paper = "Abbo87.pdf",
  keywords = "printed"
}

@InProceedings{Bern97a,
  author = "Bernardin, Laurent and Monagan, Michael B.",
  title = {{Efficient multivariate factorization over finite fields}},
  booktitle = "Applied algebra, algebraic algorithms and error-correcting 
              codes",
  series = "AAECC-12",
  year = "1997",
  location = "Toulouse, France",
  publisher = "Springer",
  pages = "15-28",
  link = "\url{http://www.cecm.sfu.ca/~monaganm/papers/AAECC.pdf}",
  abstract =
    "We describe the Maple implementation of multivariate factorization
    over general finite fields. Our first implementation is available in
    Maple V Release 3. We give selected details of the algorithms and show
    several ideas that were used to improve its efficiency. Most of the
    improvements presented here are incorporated in Maple V Release 4. In
    particular, we show that we needed a general tool for implementing
    computations in GF$(p^k)[x_1,x_2,\cdots,x_v]$.  We also needed an
    efficient implementation of our algorithms $\mathbb{Z}_p[y][x]$ in
    because any multivariate factorization may depend on several bivariate
    factorizations. The efficiency of our implementation is illustrated by
    the ability to factor bivariate polynomials with over a million
    monomials over a small prime field.",
  paper = "Bern97a.pdf",
  keywords = "axiomref"
}

@InProceedings{Diaz95,
  author = "Diaz, A. and Kaltofen, E.",
  title = {{On computing greatest common divisors with polynomials given by 
           black boxes for their evaluation}},
  booktitle = "Proc. 1995 Internat. Symp. Symbolic Algebraic Comput.",
  pages = "232--239",
  year = "1995",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/95/DiKa95.ps.gz}",
  paper = "Diaz95.ps"
}

@article{Gath01,
  author = "von zur Gathen, Joachim and Panario, Daniel",
  title = {{Factoring Polynomials Over Finite Fields: A Survey}},
  journal = "J. Symbolic Computation",
  year = "2001",
  volume = "31",
  pages = "3-17",
  link = "\url{http://people.csail.mit.edu/dmoshdov/courses/codes/poly-factorization.pdf}",
  keywords = "survey",
  abstract = 
    "This survey reviews several algorithms for the factorization of
    univariate polynomials over finite fields. We emphasize the main ideas
    of the methods and provide and up-to-date bibliography of the problem.
    This paper gives algorithms for {\sl squarefree factorization},
    {\sl distinct-degree factorization}, and {\sl equal-degree factorization}.
    The first and second algorithms are deterministic, the third is
    probabilistic.",
  paper = "Gath01.pdf"
}

@article{Gian88,
  author = "Gianni, Patrizia. and Trager, Barry. and Zacharias, Gail",
  title = {{Groebner Bases and Primary Decomposition of Polynomial Ideals}},
  journal = "J. Symbolic Computation",
  volume = "6", 
  pages = "149-167",
  year = "1988",
  link = "\url{http://www.sciencedirect.com/science/article/pii/S0747717188800403/pdf?md5=40c29b67947035884904fd4597ddf710&pid=1-s2.0-S0747717188800403-main.pdf}",
  algebra = "\newline\refto{package IDECOMP IdealDecompositionPackage}",
  paper = "Gian88.pdf"
}

@article{Gian96,
  author = "Gianni, P. and Trager, B.",
  title = {{Square-free algorithms in positive characteristic}},
  journal = 
    "J. of Applicable Algebra in Engineering, Communication and Computing",
  volume = "7",
  pages = "1-14",
  year = "1996",
}

@PhdThesis{Kalt82,
  author = "Kaltofen, E.",
  title = {{On the complexity of factoring polynomials with integer 
           coefficients}},
  school = "RPI",
  address = "Troy, N. Y.",
  year = "1982",
  month = "December",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/82/Ka82_thesis.pdf}",
  paper = "Kalt82.pdf"
}

@Article{Gath85,
  author = "{von zur Gathen}, Joachim and Kaltofen, E.",
  title = {{Factoring sparse multivariate polynomials}},
  journal = "J. Comput. System Sci.",
  year = "1985",
  volume = "31",
  pages = "265--287",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/85/GaKa85_mathcomp.ps.gz}",
  paper = "Gath85.ps"
}

@Article{Gath85b,
  author = "{von zur Gathen}, Joachim and Kaltofen, E.",
  title = {{Polynomial-Time Factorization of Multivariate Polynomials over
           Finite Fields}},
  journal = "Math. Comput.",
  year = "1985",
  volume = "45",
  pages = "251-261",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/85/GaKa85\_mathcomp.ps.gz}",
  paper = "Gath85.ps",
  abstract =
    "We present a probabilistic algorithm that finds the irreducible
    factors of a bivariate polynomial with coefficients from a finite
    field in time polynomial in the input size, i.e. in the degree of the
    polynomial and $log$(cardinality of field). The algorithm generalizes
    to multivariate polynomials and has polynomial running time for
    densely encoded inputs. Also a deterministic version of the algorithm
    is discussed whose running time is polynomial in the degree of the
    input polynomial and the size of the field."
}

@InCollection{Kalt11c,
  author = "Kaltofen, Erich and Lecerf, Gr{\'e}goire",
  title = {{Section 11.5. {Factorization} of multivariate polynomials}},
  booktitle = "Handbook of Finite Fields",
  publisher = "Springer",
  pages = "382--392",
  year = "2011",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/11/KL11.pdf}",
  paper = "Kalt11c.pdf"
}

@InProceedings{Kalt05b,
  author = "Kaltofen, Erich and Koiran, Pascal",
  title = {{On the complexity of factoring bivariate supersparse 
           (lacunary) polynomials}},
  year = "2005",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'05",
  pages = "208--215",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/05/KaKoi05.pdf}",
  paper = "Kalt05b.pdf"
}

@InProceedings{Kalt06a,
  author = "Kaltofen, Erich and Koiran, Pascal",
  title = {{Finding Small Degree Factors of Multivariate Supersparse
           (Lacunary) Polynomials Over Algebraic Number Fields}},
  year = "2006",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'06",
  pages = "162--168",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/06/KaKoi06.pdf}",
  paper = "Kalt06a.pdf"
}

@InProceedings{Kalt97a,
  author = "Kaltofen, E. and Shoup, V.",
  title = {{Fast polynomial factorization over high algebraic extensions of 
           finite fields}},
  booktitle = "Proc. 1997 Internat. Symp. Symbolic Algebraic Comput.",
  year = "1997",
  pages = "184--188",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/97/KaSh97.pdf}",
  paper = "Kalt97a.pdf"
}

@Article{Kalt98,
  author = "Kaltofen, E. and Shoup, V.",
  title = {{Subquadratic-time factoring of polynomials over finite fields}},
  journal = "Math. Comput.",
  month = "July",
  year = "1998",
  volume = "67",
  number = "223",
  pages = "1179--1197",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/98/KaSh98.pdf}",
  paper = "Kalt98.pdf"
}

@InProceedings{Kalt95a,
  author = "Kaltofen, E. and Shoup, V.",
  title = {{Subquadratic-time factoring of polynomials over finite fields}},
  booktitle = "Proc. 27th Annual ACM Symp. Theory Comput.",
  year = "1995",
  publisher = "ACM Press",
  address = "New York, N.Y.",
  pages = "398--406",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/95/KaSh95.ps.gz}",
  paper = "Kalt95a.ps"
}

@InProceedings{Kalt88,
  author = "Kaltofen, E. and Trager, B.",
  title = {{Computing with polynomials given by black boxes for their 
    evaluations: Greatest common divisors, factorization, separation of 
    numerators and denominators}},
  booktitle = "Proc. 29th Annual Symp. Foundations of Comp. Sci.",
  pages = "296--305",
  year = "1988",
  organization = "IEEE",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/88/focs88.ps.gz}",
  paper = "Kalt88.ps"
}

@InProceedings{Kalt85b,
  author = "Kaltofen, E.",
  title = {{Computing with polynomials given by straight-line programs {II}; 
           sparse factorization}},
  booktitle = "Proc. 26th Annual Symp. Foundations of Comp. Sci.",
  year = "1985",
  pages = "451--458",
  organization = "IEEE",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/85/Ka85_focs.ps.gz}",
  paper = "Kalt85b.ps"
}

@InProceedings{Kalt86,
  author = "Kaltofen, E.",
  title = {{Uniform closure properties of p-computable functions}},
  booktitle = "Proc. 18th Annual ACM Symp. Theory Comput.",
  year = "1986",
  pages = "330--337",
  organization = "ACM",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/86/Ka86_stoc.pdf}",
  paper = "Kalt86.pdf"
}

@InProceedings{Kalt87b,
  author = "Kaltofen, E.",
  title = {{Single-factor Hensel lifting and its application to the 
           straight-line complexity of certain polynomials}},
  booktitle = "Proc. 19th Annual ACM Symp. Theory Comput.",
  year = "1987",
  pages = "443--452",
  organization = "ACM",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/87/Ka87_stoc.pdf}",
  paper = "Kalt87b.pdf"
}

@InCollection{Kalt89,
  author = "Kaltofen, E.",
  editor = "S. Micali",
  title = {{Factorization of polynomials given by straight-line programs}},
  booktitle = "Randomness and Computation",
  pages = "375--412",
  publisher = "JAI Press Inc.",
  year = "1989",
  volume = "5",
  series = "Advances in Computing Research",
  address = "Greenwhich, Connecticut",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/89/Ka89_slpfac.pdf}",
  paper = "Kalt89.pdf"
}

@Article{Gao04,
  author = "Gao, Shuhong and Kaltofen, E. and Lauder, A.",
  title = {{Deterministic distinct degree factorization for polynomials 
           over finite fields}},
  year = "2004",
  journal = "Journal of Symbolic Computation",
  volume = "38",
  number = "6",
  pages = "1461--1470",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/01/GKL01.pdf}",
  paper = "Gao04.pdf"
}

@Article{Kalt87c,
  author = "Kaltofen, E.",
  title = {{Deterministic irreducibility testing of polynomials over 
           large finite fields}},
  journal = "Journal of Symbolic Computation",
  year = "1987",
  volume = "4",
  pages = "77--82",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/87/Ka87_jsc.ps.gz}",
  paper = "Kalt87c.ps"
}

@Article{Kalt95b,
  author = "Kaltofen, E.",
  title = {{Effective {Noether} irreducibility forms and applications}},
  journal =  "J. Comput. System Sci.",
  year = "1995",
  volume = "50",
  number = "2",
  pages = "274--295",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/95/Ka95_jcss.pdf}",
  paper = "Kalt95b.pdf"
}

@Article{Kalt85a,
  author = "Kaltofen, E.",
  title = {{Fast parallel absolute irreducibility testing}},
  journal = "Journal of Symbolic Computation",
  year = "1985",
  volume = "1",
  number = "1",
  pages = "57--67",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/85/Ka85_jsc.pdf}",
  paper = "Kalt85a.pdf"
}

@Article{Gath85a,
  author = "{von zur Gathen}, Joachim and Kaltofen, E.",
  title = {{Factoring multivariate polynomials over finite fields}},
  journal = "Math. Comput.",
  year = "1985",
  volume = "45",
  pages = "251--261",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/85/GaKa85_mathcomp.ps.gz}",
  paper = "Gath85a.ps"
}

@Article{Kalt85e,
  author = "Kaltofen, E.",
  title = {{Polynomial-time reductions from multivariate to bi- and univariate 
           integral polynomial factorization}},
  journal = "{SIAM} J. Comput.",
  year = "1985",
  volume = "14",
  number = "2",
  pages = "469--489",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/85/Ka85_sicomp.pdf}",
  paper = "Kalt85e.pdf"
}

@InProceedings{Kalt82a,
  author = "Kaltofen, E.",
  title = {{A polynomial-time reduction from bivariate to univariate 
           integral polynomial factorization}},
  booktitle = "Proc. 23rd Annual Symp. Foundations of Comp. Sci.",
  year = "1982",
  pages = "57--64",
  organization = "IEEE",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/82/Ka82_focs.pdf}",
  paper = "Kalt82a.pdf"
}

@InProceedings{Kalt03,
  author = "Kaltofen, Erich",
  title = {{Polynomial Factorization: a Success Story}},
  year = "2003",
  booktitle = "Symbolic Algebraic Comput. Internat. Symp. ISSAC '88 Proc.",
  pages = "3--4",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/03/Ka03.pdf}",
  keywords = "survey",
  paper = "Kalt03.pdf"
}

@InProceedings{Kalt92b,
  author = "Kaltofen, E.",
  title = {{Polynomial factorization 1987-1991}},
  booktitle = "Proc. LATIN '92",
  editor = "I. Simon",
  series = "Lect. Notes Comput. Sci.",
  volume = "583",
  pages = "294--313",
  publisher = "Springer-Verlag",
  year = "1992",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/92/Ka92_latin.pdf}",
  keywords = "survey",
  paper = "Kalt92b.pdf"
}

@InCollection{Kalt90c,
  author = "Kaltofen, E.",
  editor = "D. V. Chudnovsky and R. D. Jenks",
  title = {{Polynomial Factorization 1982-1986}},
  booktitle = "Computers in Mathematics",
  pages = "285--309",
  publisher = "Marcel Dekker, Inc.",
  year = "1990",
  volume = "125",
  series = "Lecture Notes in Pure and Applied Mathematics",
  address = "New York, N. Y.",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/90/Ka90_survey.ps.gz}",
  keywords = "survey",
  paper = "Kalt90c.ps"
}

@InCollection{Kalt82b,
  author = "Kaltofen, E.",
  title = {{Polynomial factorization}},
  editor = "B. Buchberger and G. Collins and R. Loos",
  booktitle = "Computer Algebra",
  edition = "2",
  pages = "95--113",
  publisher = "Springer-Verlag",
  year = "1982",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/82/Ka82_survey.ps.gz}",
  keywords = "survey",
  paper = "Kalt82b.ps"
}

@phdthesis{Sale04,
  author = "Salem, Fatima Khaled Abu",
  title = {{Factorisation Algorithms for Univariate and Bivariate Polynomials
           over Finite Fields}},
  school = "Meron College",
  year = "2004",
  link = "\url{http://www.cs.aub.edu.lb/fa21/Dissertations/My\_thesis.pdf}",
  abstract = 
    "In this thesis we address algorithms for polynomial factorisation
    over finite fields. In the univariate case, we study a recent
    algorithm due to Niederreiter where the factorisation problem is
    reduced to solving a linear system over the finite field in question,
    and the solutions are used to produce the complete factorisation of
    the polynomials into irreducibles. We develop a new algorithm for
    solving the linear system using sparse Gaussian elimination with the
    Markowitz ordering strategy, and conjecture that the Niederreiter
    linear system is not only initially sparse, but also preserves its
    sparsity throughout the Gaussian elimination phase. We develop a new
    bulk synchronous parallel (BSP) algorithm base on the approach of
    Gottfert for extracting the factors of a polynomial using a basis of
    the Niederreiter solution set of $\mathbb{F}_2$. We improve upon the
    complexity and performance of the original algorithm, and produce
    binary univariate factorisations of trinomials up to degree 400000.

    We present a new approach to multivariate polynomial factorisation
    which incorporates ideas from polyhedral geometry, and generalises
    Hensel lifting. The contribution is an algorithm for factoring
    bivariate polynomials via polytopes which is able to exploit to some
    extent the sparsity of polynomials. We further show that the polytope
    method can be made sensitive to the number of nonzero terms of the
    input polynomial. We describe a sparse adaptation of the polytope
    method over finite fields of prime order which requires fewer bit
    operations and memory references for polynomials which are known to be
    the product of two sparse factors. Using this method, and to the best
    of our knowledge, we achieve a world record in binary bivariate
    factorisation of a sparse polynomial of degree 20000. We develop a BSP
    variant of the absolute irreducibility testing via polytopes given in
    [45], producing a more memory and run time efficient method that can
    provide wider ranges of applicability. We achieve absolute
    irreducibility testing of a bivariate and trivariate polynomial of
    degree 30000, and of multivariate polynomials with up to 3000
    variables.",
  paper = "Sale04.pdf"
}

@InProceedings{Shou91,
  author = "Shoup, Victor",
  title = {{A Fast Deterministic Algorithm for Factoring Polynomials over
           Finite Fields of Small Characteristic}},
  booktitle = "Proc. ISSAC 1991",
  series = "ISSAC 1991",
  year = "1991",
  pages = "14-21",
  link = "\url{http://www.shoup.net/papers/quadfactor.pdf}",
  abstract =
    "We present a new algorithm for factoring polynomials over finite
    fields. Our algorithm is deterministic, and its running time is
    ``almost'' quadratic when the characteristic is a small fixed
    prime. As such, our algorithm is asymptotically faster than previously
    known deterministic algorithms for factoring polynomials over finite
    fields of small characteristic.",
  paper = "Shou91.pdf"
}

@inproceedings{Trev91,
  author = "Trevisan, Vilmar and Wang, Paul",
  title = {{Practical factorization of univariate polynomials over 
           finite fields}},
  booktitle = "Proc. ISSAC 1991",
  series = "ISSAC '91",
  publisher = "ACM",
  isbn = "0-89791-437-6",
  pages = "22-31",
  year = "1991",
  link = "\url{http://lib.org/by/\_djvu\_Papers/Computer\_algebra/Algebraic\%20numbers}",
  abstract =
    "Research presented here is part of an effort to establish
    state-of-the-art factoring routines for polynomials. The foundation of
    such algorithms lies in the efficient factorization over a finite
    field $GF(p^k)$. The Cantor-Zassenhaus algorithm together with
    innovative ideas suggested by others is compared with the Berlekamp
    algorithm. The studies led us to design a hybrid algorithm that
    combine the strengths of the different approaches. The algorithms are
    also implemented and machine timings are obtained to measure the
    performance of these algorithms.",
  paper = "Trev91.djvu"
}

@article{Beau03,
  author = "Beaumont, James and Bradford, Russell and Davenport, James H.",
  title = 
    {{Better simplification of elementary functions through power series}},
  journal = "2003 International Symposium on Symbolic and Algebraic Computation",
  series = "ISSAC'03",
  year = "2003",
  month = "August",
  abstract = "
    In [5], we introduced an algorithm for deciding whether a proposed
    simplification of elementary functions was correct in the presence of
    branch cuts. This algorithm used multivalued function simplification
    followed by verification that the branches were consistent.

    In [14] an algorithm was presented for zero-testing functions defined
    by ordinary differential equations, in terms of their power series.

    The purpose of the current paper is to investigate merging the two
    techniques. In particular, we will show an explicit reduction to the
    constant problem [16].",
  paper = "Beau03.pdf"
}

@article{Brad02,
  author = "Bradford, Russell and Corless, Robert M. and Davenport, James H. 
            Jeffrey, David J. and Watt, Stephen M.",
  title = {{Reasoning about the Elementary Functions of Complex Analysis}},
  journal = "Annals of Mathematics and Artificial Intelligence",
  year = "2002",
  issn = "1012-2443",
  volume = "36",
  number = "3",
  doi = "10.1023/A:1016007415899",
  link = "\url{http://dx.doi.org/10.1023/A%3A1016007415899}",
  publisher = "Kluwer Academic Publishers",
  keywords = "elementary functions; branch cuts; complex identities",
  pages = "303-318",
  abstract = 
    "There are many problems with the simplification of elementary
    functions, particularly over the complex plane, though not
    exclusively. Systems tend to make ``howlers'' or not to simplify
    enough. In this paper we outline the ``unwinding number'' approach to
    such problems, and show how it can be used to prevent errors and to
    systematise such simplification, even though we have not yet reduced
    the simplification process to a complete algorithm. The unsolved
    problems are probably more amenable to the techniques of artificial
    intelligence and theorem proving than the original problem of complex
    variable analysis.",
  paper = "Brad02.pdf"
}

@inproceedings{Chyz11,
  author = "Chyzak, Frederic and Davenport, James H. and 
            Koutschan, Christoph and Salvy, Bruno",
  title = {{On Kahan's Rules for Determining Branch Cuts}},
  booktitle = "Proc. 13th Int. Symp. on Symbolic and Numeric Algorithms 
               for Scientific Computing",
  year = "2011",
  isbn = "978-1-4673-0207-4",
  location = "Timisoara",
  pages = "47-51",
  doi = "10.1109/SYNASC.2011.51",
  acmid = "258794",
  publisher = "IEEE",
  abstract = 
    "In computer algebra there are different ways of approaching the
    mathematical concept of functions, one of which is by defining them as
    solutions of differential equations. We compare different such
    appraoches and discuss the occurring problems. The main focus is on
    the question of determining possible branch cuts. We explore the
    extent to which the treatment of branch cuts can be rendered (more)
    algorithmic, by adapting Kahan's rules to the differential equation
    setting.",
  paper1 = "Chyz11a.pdf",
  paper = "Chyz11.pdf"
}

@article{Dave10,
  author = "Davenport, James",
  title = {{The Challenges of Multivalued "Functions"}},
  journal = "Lecture Notes in Computer Science",
  volume = "6167",
  year = "2010",
  pages = "1-12",
  abstract = "
    Although, formally, mathematics is clear that a function is a
    single-valued object, mathematical practice is looser, particularly
    with n-th roots and various inverse functions. In this paper, we point
    out some of the looseness, and ask what the implications are, both for
    Artificial Intelligence and Symbolic Computation, of these practices.
    In doing so, we look at the steps necessary to convert existing tests
    into
    \begin{itemize}
    \item (a) rigorous statements
    \item (b) rigorously proved statements
    \end{itemize}
    In particular we ask whether there might be a constant ``de Bruij factor''
    [18] as we make these texts more formal, and conclude that the answer
    depends greatly on the interpretation being placed on the symbols.",
  paper = "Dave10.pdf"
}
  
@article{Phis11,
  author = "Phisanbut, Nalina and Bradford, Russell J. and 
            Davenport, James H.",
  title = {{Geometry of branch cuts}},
  journal = "ACM Communications in Computer Algebra",
  volume = "44",
  number = "3-4",
  pages = "132-135",
  year = "2011",
  abstract = 
    "'Simplification' is a key concept in Computer Algebra. But many
    simplification rules, such as $\sqrt{x}\sqrt{y} \rightarrow \sqrt{xy}$
    are not universally valid, due to the fact that many elementary
    functions are multi-valued. Hence a key question is ``Is this
    simplification correct?'', which involves algorithmic analysis of the
    branch cuts involved. In this paper, we look at variable ordering and
    pre-conditioning as supporting technologies for this analysis.",
  paper = "Phis11.pdf"
}

@article{Dave12,
  author = "Davenport, James H. and Bradford, Russell and England, Matthew 
            and Wilson, David",
  title = {{Program Verification in the presence of complex numbers, functions 
           with branch cuts etc}},
  journal = "14th Int. Symp. on Symbolic and Numeric Algorithms for 
            Scientific Computing",
  link = "\url{http://arxiv.org/pdf/1212.5417.pdf}",
  year = "2012",
  series = "SYNASC'12",
  pages = "83-88",
  publisher = "IEEE",
  abstract = "
    In considering the reliability of numerical programs, it is normal to
    ``limit our study to the semantics dealing with numerical precision''.
    On the other hand, there is a great deal of work on the reliability of
    programs that essentially ignores the numerics. The thesis of this
    paper is that there is a class of problems that fall between the two,
    which could be described as ``does the low-level arithmetic implement
    the high-level mathematics''. Many of these problems arise because
    mathematics, particularly the mathematics of the complex numbers, is
    more difficult than expected; for example the complex function log is
    not continuous, writing down a program to compute an inverse function
    is more complicated than just solving an equation, and many algebraic
    simplification rules are not universally valid.

    The good news is that these problems are theoretically capable of
    being solved, and are practically close to being solved, but not yet
    solved, in several real-world examples. However, there is still a long
    way to go before implementations match the theoretical possibilities.",
  paper = "Dave12.pdf"
}

@article{Engl13,
  author = "England, M. and Bradford, R. and Davenport, J. H. and 
            Wilson, D.",
  title = {{Understanding Branch Cuts of Expressions}},
  journal = "Intelligent Computer Mathematics",
  year = "2013",
  series = " LNCS 7961",
  publisher = "Springer, Berlin",
  pages = "136-151",
  isbn = "9783642393198",
  abstract = "
    We assume some standard choices for the branch cuts of a group of
    functions and consider the problem of then calculating the branch cuts
    of expressions involving those functions. Typical examples include the
    addition formulae for inverse trigonometric functions. Understanding
    these cuts is essential for working with the single-valued
    counterparts, the common approach to encoding multi-valued functions
    in computer algebra systems. While the defining choices are usually
    simple (typically portions of either the real or imaginary axes) the
    cuts induced by the expression may be surprisingly complicated. We
    have made explicit and implemented techniques for calculating the cuts
    in the computer algebra programme Maple. We discuss the issues raised,
    classifying the different cuts produced. The techniques have been
    gathered in the BranchCuts package, along with tools for visualising
    the cuts. The package is included in Maple 17 as part of the
    FunctionAdvisor tool.",
  paper = "Engl13.pdf"
}

@article{Jeff04,
  author = "Jeffrey, D. J. and Norman, A. C.",
  title = {{Not Seeing the Roots for the Branches: Multivalued Functions in 
           Computer Algebra}},
  journal = "SIGSAM Bull.",
  issue_date = "September 2004",
  volume = "38",
  number = "3",
  month = "September",
  year = "2004",
  issn = "0163-5824",
  pages = "57--66",
  numpages = "10",
  link = "\url{http://doi.acm.org/10.1145/1040034.1040036}",
  doi = "10.1145/1040034.1040036",
  acmid = "1040036",
  publisher = "ACM",
  address = "New York, NY, USA",
  abstract = "
    We discuss the multiple definitions of multivalued functions and their
    suitability for computer algebra systems. We focus the discussion by
    taking one specific problem and considering how it is solved using
    different definitions. Our example problem is the classical one of
    calculating the roots of a cubic polynomial from the Cardano formulae,
    which contains fractional powers. We show that some definitions of
    these functions result in formulae that are correct only in the sense
    that they give candidates for solutions; these candidates must then be
    tested. Formulae that are based on single-valued functions, in
    contract, are efficient and direct.",
  paper = "Jeff04.pdf"
}

@inproceedings{Kaha86,
  author = "Kahan, W.",
  title = {{Branch cuts for complex elementary functions}},
  booktitle = "The State of the Art in Numerical Analysis",
  year = "1986",
  month = "April",
  editor = "Powell, M.J.D and Iserles, A.",
  publisher = "Oxford University Press",
  paper1 = "Kaha86a.pdf",
  paper = "Kaha86.pdf"
}

@article{Rich96,
  author = "Rich, Albert D. and Jeffrey, David J.",
  title = {{Function Evaluation on Branch Cuts}},
  journal = "SIGSAM Bull.",
  issue_date = "June 1996",
  volume = "30",
  number = "2",
  month = "June",
  year = "1996",
  issn = "0163-5824",
  pages = "25--27",
  numpages = "3",
  link = "\url{http://doi.acm.org/10.1145/235699.235704}",
  doi = "10.1145/235699.235704",
  acmid = "235704",
  publisher = "ACM",
  address = "New York, NY, USA",
  abstract = "
    Once it is decided that a CAS will evaluate multivalued functions on
    their principal branches, questions arise concerning the branch
    definitions. The first questions concern the standardization of the
    positions of the branch cuts. These questions have largely been
    resolved between the various algebra systems and the numerical
    libraries, although not completely. In contrast to the computer
    systems, many mathematical textbooks are much further behind: for
    example, many popular textbooks still specify that the argument of a
    complex number lies between 0 and $2\pi$. We do not intend to discuss
    these first questions here, however. Once the positions of the branch
    cuts have been fixed, a second set of questions arises concerning the
    evaluation of functions on their branch cuts."
}

@article{Patt96,
  author = "Patton, Charles M.",
  title = {{A Representation of Branch-cut Information}},
  journal = "SIGSAM Bull.",
  issue_date = "June 1996",
  volume = "30",
  number = "2",
  month = "June",
  year = "1996",
  issn = "0163-5824",
  pages = "21--24",
  numpages = "4",
  link = "\url{http://doi.acm.org/10.1145/235699.235703}",
  doi = "10.1145/235699.235703",
  acmid = "235703",
  publisher = "ACM",
  address = "New York, NY, USA",
  abstract = "
    Handling (possibly) multi-valued functions is a problem in all current
    computer algebra systems. The problem is not an issue of technology.
    Its solution, however, is tied to a uniform handling of the issues by
    the mathematics community.",
  paper = "Patt96.pdf"
}

@article{Squi91,
  author = "Squire, Jon S.",
  title = {{Rationale for the Proposed Standard for a Generic Package of 
           Complex Elementary Functions}},
  journal = "Ada Lett.",
  issue_date = "Fall 1991",
  volume = "XI",
  number = "7",
  month = "September",
  year = "1991",
  issn = "1094-3641",
  pages = "166--179",
  numpages = "14",
  link = "\url{http://doi.acm.org/10.1145/123533.123545}",
  doi = "10.1145/123533.123545",
  acmid = "123545",
  publisher = "ACM",
  address = "New York, NY, USA",
  abstract = "
    This document provides the background on decisions that were made
    during the development of the specification for Generic Complex
    Elementary fuctions. It also rovides some information that was used to
    develop error bounds, range, domain and definitions of complex
    elementary functions.",
  paper = "Squi91.pdf"
}

@article{Squi91a,
  author = "Squire, Jon S.",
  title = {{Proposed Standard for a Generic Package of Complex 
           Elementary Functions}},
  journal = "Ada Lett.",
  issue_date = "Fall 1991",
  volume = "XI",
  number = "7",
  month = "September",
  year = "1991",
  issn = "1094-3641",
  pages = "140--165",
  numpages = "26",
  link = "\url{http://doi.acm.org/10.1145/123533.123544}",
  doi = "10.1145/123533.123544",
  acmid = "123544",
  publisher = "ACM",
  address = "New York, NY, USA",
  abstract = "
    This document defines the specification of a generic package of
    complex elementary functions called Generic Complex Elementary
    Functions. It does not provide the body of the package."
}

@misc{Unkn15,
  author = "Unknown",
  title = {{Branches of Functions}},
  link =
     "\url{http://scipp.ucsc.edu/~haber/ph116A/ComplexFunBranchTheory.pdf}",
  paper = "Unkn15.pdf"
}

@article{Bern97,
  author = "Bernardin, Laurent",
  title = {{On square-free factorization of multivariate polynomials over a 
           finite field}},
  journal = "Theoretical Computer Science",
  volume = "187",
  number = "1-2",
  year = "1997",
  month = "November",
  pages = "105-116",
  abstract = "
    In this paper we present a new deterministic algorithm for computing
    the square-free decomposition of multivariate polynomials with
    coefficients from a finite field.

    Our algorithm is based on Yun's square-free factorization algorithm
    for characteristic 0. The new algorithm is more efficient than
    existing, deterministic algorithms based on Musser's squarefree
    algorithm

    We will show that the modular approach presented by Yun has no
    significant performance advantage over our algorithm. The new
    algorithm is also simpler to implement and it can rely on any existing
    GCD algorithm without having to worry about choosing ``good'' evaluation
    points.

    To demonstrate this, we present some timings using implementations in
    Maple (Char et al. 1991), where the new algorithm is used for Release
    4 onwards, and Axiom (Jenks and Sutor, 1992) which is the only system
    known to the author to use and implementation of Yun's modular
    algorithm mentioned above.",
  paper = "Bern97.pdf",
  keywords = "axiomref"
}

@article{Chez07,
  author = "Ch\'eze, Guillaume and Lecerf, Gr\'egoire",
  title = {{Lifting and recombination techniques for absolute factorization}},
  journal = "Journal of Complexity",
  volume = "23",
  number = "3",
  year = "2007",
  month = "June",
  pages = "380-420",
  abstract = "
    In the vein of recent algorithmic advances in polynomial factorization
    based on lifting and recombination techniques, we present new faster
    algorithms for computing the absolute factorization of a bivariate
    polynomial. The running time of our probabilistic algorithm is less
    than quadratic in the dense size of the polynomial to be factored.",
  paper = "Chez07.pdf"
}

@article{Lece07,
  author = "Lecerf, Gr\'egoire",
  title = {{Improved dense multivariate polynomial factorization algorithms}},
  journal = "Journal of Symbolic Computation",
  volume = "42",
  number = "4",
  year = "2007",
  month = "April",
  pages = "477-494",
  abstract = "
    We present new deterministic and probabilistic algorithms that reduce
    the factorization of dense polynomials from several variables to one
    variable.  The deterministic algorithm runs in sub-quadratic time in
    the dense size of the input polynomial, and the probabilistic
    algorithm is softly optimal when the number of variables is at least
    three. We also investigate the reduction from several to two variables
    and improve the quantitative versions of Bertini's irreducibility 
    theorem.",
  paper = "Lece07.pdf"
}

@article{Wang77,
  author = "Wang, Paul S.",
  title = {{An efficient squarefree decomposition algorithm}},
  journal = "ACM SIGSAM Bulletin",
  volume = "11",
  number = "2",
  year = "1977",
  month = "May",
  pages = "4-6",
  abstract = "
    The concept of polynomial squarefree decomposition is an important one
    in algebraic computation. The squarefree decomposition process has
    many uses in computer symbolic computation. A recent survey by D. Yun
    [3] describes many useful algorithms for this purpose. All of these
    methods depend on computing the greated common divisor (gcd) of the
    polynomial to be decomposed and its first derivative (with repect to
    some variable). In the multivariate case, this gcd computation is
    non-trivial and dominates the cost for the squarefree decompostion.",
  paper = "Wang77.pdf"
}

@article{Wang79,
  author = "Wang, Paul S. and Trager, Barry M.",
  title = {{New Algorithms for Polynomial Square-Free Decomposition 
           over the Integers}},
  journal = "SIAM Journal on Computing",
  volume = "8",
  number = "3",
  year = "1979",
  publisher = "Society for Industrial and Applied Mathematics",
  issn = "00975397",
  abstract = "
    Previously known algorithms for polynomial square-free decomposition
    rely on greatest common divisor (gcd) computations over the same
    coefficient domain where the decomposition is to be performed. In
    particular, gcd of the given polynomial and its first derivative (with
    respect to some variable) is obtained to begin with. Application of
    modular homomorphism and $p$-adic construction (multivariate case) or
    the Chinese remainder algorithm (univariate case) results in new
    square-free decomposition algorithms which, generally speaking, take
    less time than a single gcd between the given polynomial and its first
    derivative. The key idea is to obtain one or several ``correct''
    homomorphic images of the desired square-free decomposition
    first. This provides information as to how many different square-free
    factors there are, their multiplicities and their homomorphic
    images. Since the multiplicities are known, only the square-free
    factors need to be constructed. Thus, these new algorithms are
    relatively insensitive to the multiplicities of the square-free factors.",
  paper = "Wang79.pdf"
}

@inproceedings{Yun76,
  author = "Yun, D.Y.Y",
  title = {{On square-free decomposition algorithms}},
  booktitle = "Proceedings of SYMSAC'76",
  year = "1976",
  keywords = "survey",
  pages = "26-35"
}

@article{Abra71,
  author = "Abramov, S.A.",
  title = {{On the summation of rational functions}},
  year = "1971",
  journal = "USSR Computational Mathematics and Mathematical Physics",
  volume = "11",
  number = "4",
  pages = "324--330",
  abstract = "
    An algorithm is given for solving the following problem: let
    $F(x_1,\ldots,x_n)$ be a rational function of the variables
    $x_i$ with rational (read or complex) coefficients; to see if
    there exists a rational function $G(v,w,x_2,\ldots,x_n)$ with
    coefficients from the same field, such that
    \[\sum_{x_1=v}^w{F(x_1,\ldots,x_n)} = G(v,w,x_2,\ldots,x_n)\]
    for all integral values of $v \le w$. If $G$ exists, to obtain it.
    Realization of the algorithm in the LISP language is discussed.",
  paper = "Abra71.pdf"
}

@article{Gosp78,
  author = "Gosper, R. William",
  title = {{Decision procedure for indefinite hypergeometric summation}},
  year = "1978",
  journal = "Proc. Natl. Acad. Sci. USA",
  volume = "75",
  number = "1",
  pages = "40--42",
  month = "January",
  abstract = "
    Given a summand $a_n$, we seek the ``indefinite sum'' $S(n)$
    determined (within an additive constant) by 
    \[\sum_{n=1}^m{a_n} = S(m)=S(0)\]
    or, equivalently, by
    \[a_n=S(n)-S(n-1)\]
    An algorithm is exhibited which, given $a_n$, finds those $S(n)$
    with the property
    \[\displaystyle\frac{S(n)}{S(n-1)}=\textrm{a rational function of n}\]
    With this algorithm, we can determine, for example, the three
    identities
    \[\displaystyle\sum_{n=1}^m{
    \frac{\displaystyle\prod_{j=1}^{n-1}{bj^2+cj+d}}
    {\displaystyle\prod_{j=1}^n{bj^2+cj+e}}=
    \frac{1-{\displaystyle\prod_{j=1}^m{\frac{bj^2+cj+d}{bj^2+cj+e}}}}{e-d}}\]
    \[\displaystyle\sum_{n=1}^m{
    \frac{\displaystyle\prod_{j=1}^{n-1}{aj^3+bj^2+cj+d}}
         {\displaystyle\prod_{j=1}^n{aj^3+bj^2+cj+e}}=
    \frac{1-{\displaystyle\prod_{j=1}^m{
    \frac{aj^3+bj^2+cj+d}{aj^3+bj^2+cj+e}}}}{e-d}}\]
    \[\displaystyle\sum_{n=1}^m{
    \displaystyle\frac{\displaystyle\prod_{j=1}^{n-1}{bj^2+cj+d}}
    {\displaystyle\prod_{j=1}^{n+1}{bj^2+cj+e}}=
    \displaystyle\frac{
    \displaystyle\frac{2b}{e-d}-
    \displaystyle\frac{3b+c+d-e}{b+c+e}-
    \left(
    \displaystyle\frac{2b}{e-d}-\frac{b(2m+3)+c+d-e}{b(m+1)^2+c(m+1)+e}
    \right)
    \displaystyle\prod_{j=1}^m{\frac{bj^2+cj+d}{bj^2+cj+e}}}
    {b^2-c^2+d^2+e^2+2bd-2de+2eb}}\]",
  paper = "Gosp78.pdf"
}

@article{Karr81,
  author = "Karr, Michael",
  title = {{Summation in Finite Terms}},
  journal = "Journal Association for Computing Machinery",
  year = "1981",
  volume = "28",
  number = "2",
  month = "April",
  issn = "0004-5411",
  pages = "305--350",
  link = "\url{http://doi.acm.org/10.1145/322248.322255}",
  publisher = "ACM",
  abstract = "
    Results which allow either the computation of symbolic solutions to
    first-order linear difference equations or the determination that
    solutions of a certain form do not exist are presented. Starting with
    a field of constants, larger fields may be constructed by the formal
    adjunction of symbols which behave like solutions to first-order
    linear equations (with a few restrictions). It is in these extension
    fields that the difference equations may be posed and in which the
    solutions are requested. The principal application of these results is
    in finding formulas for a broad class of finite sums or in showing the
    nonexistence of such formula.",
  paper = "Karr81"
}

@book{Lafo82,
  author = "Lafon, J.C.",
  title = {{Summation in Finite Terms}},
  year = "1982",
  publisher = "Springer-Verlag",
  isbn = "3-211-81776-X", 
  pages = "71--77",
  abstract = "
    A survey on algorithms for summation in finite terms is given. After a
    precise definition of the problem the cases of polynomial and rational
    summands are treated. The main concern of this paper is a description
    of Gosper's algorithm, which is applicable for a wide class of
    summands.  Karr's theory of extension difference fields and some
    heuristic techniques are touched on briefly.",
  keywords = "axiomref,survey"
}

@article{Abra85,
  author = "Abramov, S.A.",
  title = {{Separation of variables in rational functions}},
  year = "1985",
  journal = "USSR Computational Mathematics and Mathematical Physics",
  volume = "25",
  number = "5",
  pages = "99--102",
  abstract = 
    "The problem of expanding a rational function of several variables into
    terms with separable variables is formulated. An algorithm for solving
    this problem is given. Programs which implement this algorithm can
    occur in sets of algebraic alphabetical transformations on a computer
    and can be used to reduce the multiplicity of sums and integrals of
    rational functions for investigating differential equations with
    rational right-hand sides etc.",
  paper = "Abra85.pdf"
}

@Article{Karr85,
  author = "Karr, Michael",
  title = {{Theory of Summation in Finite Terms}},
  year = "1985",
  journal = "Journal of Symbolic Computation",
  volume = "1",
  number = "3",
  month = "September",
  pages = "303-315",
  abstract = "
    This paper discusses some of the mathematical aspects of an algorithm
    for finding formulas for finite sums. The results presented here
    concern a property of difference fields which show that the algorithm
    does not divide by zero, and an analogue to Liouville's theorem on
    elementary integrals.",
  paper = "Karr85.pdf"
}

@book{Koep98,
  author = "Koepf, Wolfram",
  title = {{Hypergeometric Summation}},
  publisher = "Springer",
  year = "1998",
  isbn = "978-1-4471-6464-7",
  abstract = "
    Modern algorithmic techniques for summation, most of which were
    introduced in the 1990s, are developed here and carefully implemented
    in the computer algebra system Maple.

    The algorithms of Fasenmyer, Gosper, Zeilberger, Petkovsek and van
    Hoeij for hypergeometric summation and recurrence equations, efficient
    multivariate summation as well as q-analogues of the above algorithms
    are covered.  Similar algorithms concerning differential equations are
    considered. An equivalent theory of hyperexponential integration due
    to Almkvist and Zeilberger completes the book.

    The combination of these results gives orthogonal polynomials and
    (hypergeometric and q-hypergeometric) special functions a solid
    algorithmic foundation. Hence, many examples from this very active
    field are given.

    The materials covered are sutiable for an introductory course on
    algorithmic summation and will appeal to students and researchers
    alike.",
  paper = "Koep98.pdf"
}

@article{Liso93,
  author = "Lisonek, Petr and Paule, Peter and Strehl, Volker",
  title = {{Improvement of the Degree Setting in Gosper's Algorithm}},
  journal = "J. Symbolic Computation",
  volume = "16",
  year = "1993",
  pages = "243-258",
  link = 
   "\url{http://www.sciencedirect.com/science/article/pii/S0747717183710436}",
  abstract =
    "A detailed study of the degree setting for Gosper's algorithm for
    indefinite hypergeometric summation is presented. In particular, we
    discriminate between rational and proper hypergeometric input. As a
    result, the critical degree bound can be improved in the former case.",
  paper = "Liso93.pdf"
}

@article{Manx93,
  author = "Man, Yiu-Kwong",
  title = {{On Computing Closed Forms for Indefinite Summations}},
  journal = "J. Symbolic Computation",
  volume = "16",
  pages = "335-376",
  year = "1993",
  link = 
   "\url{http://www.sciencedirect.com/science/article/pii/S0747717183710539}",
  abstract =
    "A decision procedure for finding closed forms for indefinite
    summation of polynomials, rational functions, quasipolynomials and
    quasirational functions is presented. It is also extended to deal with
    some non-hypergeometric sums with rational inputs, which are not
    summable by means of Gosper's algorithm. Discussion of its
    implementation, analysis of degree bounds and some illustrative
    examples are included.",
  paper = "Manx93.pdf"
}

@article{Paul95,
  author = "Paule, Peter",
  title = {{Greatest Factorial Factorization and Symbolic Summation}},
  journal = "Journal of Symbolic Computation",
  year = "1995",
  volume = "20",
  pages = "235-268",
  link = 
   "\url{http://www.sciencedirect.com/science/article/pii/S0747717185710498}",
  abstract = 
    "The greatest factorial factorization (GFF) of a polynomial provides
    an analogue to square-free factorization but with respect to integer
    shifts instead to multiplicities. We illustrate the fundamental role
    of that concept in the context of symbolic summation. Besides a
    detailed discussion of the basic GFF notions we present a new approach
    to the indefinite rational summation problem as well as to Gosper's
    algorithm for summing hypergeometric sequences.",
  paper = "Paul95.pdf"
}

@article{Petk92,
  author = "Petkovsek, Marko",
  title = {{Hypergeometric solutions of linear recurrences with
           polynomial coefficients}},
  journal = "J. Symbolic Computation",
  volume = "14",
  pages = "243-264",
  year = "1992",
  link =
   "\url{http://www.sciencedirect.com/science/article/pii/0747717192900386}",
  abstract =
    "We describe algorithm Hyper which can be used to find all
    hypergeometric solutions of linear recurrences with polynomial
    coefficients.",
  paper = "Petk92.pdf"
}  

@InProceedings{Schn00,
  author = "Schneider, Carsten",
  title = {{An implementation of Karr's summation algorithm in Mathematica}},
  year = "2000",
  booktitle = "S\'eminaire Lotharingien de Combinatoire",
  volume = "S43b",
  pages = "1-10",
  abstract = "
    Implementations of the celebrated Gosper algorithm (1978) for
    indefinite summation are available on almost any computer algebra
    platform. We report here about an implementation of an algorithm by
    Karr, the most general indefinite summation algorithm known. Karr's
    algorithm is, in a sense, the summation counterpart of Risch's
    algorithm for indefinite integration. This is the first implementation
    of this algorithm in a major computer algebra system. Our version
    contains new extensions to handle also definite summation problems. In
    addition we provide a feature to find automatically appropriate
    difference field extensions in which a closed form for the summation
    problem exists. These new aspects are illustrated by a variety of
    examples.",
  paper = "Schn00.pdf"
}

@phdthesis{Schn01,
  author = "Schneider, Carsten",
  title = {{Symbolic Summation in Difference Fields}},
  school = "RISC Research Institute for Symbolic Computation",
  year = "2001",
  link = "\url{http://www.risc.jku.at/publications/download/risc_3017/SymbSumTHESIS.pdf}",
  abstract = 
    "There are implementations of the celebrated Gosper algorithm (1978) on
    almost any computer algebra platform. Within my PhD thesis work I
    implemented Karr's Summation Algorithm (1981) based on difference
    field theory in the Mathematica system. Karr's algorithm is, in a
    sense, the summation counterpart of Risch's algorithm for indefinite
    integration.  Besides Karr's algorithm which allows us to find closed
    forms for a big clas of multisums, we developed new extensions to
    handle also definite summation problems. More precisely we are able to
    apply creative telescoping in a very general difference field setting
    and are capable of solving linear recurrences in its context.

    Besides this we find significant new insights in symbolic summation by
    rephrasing the summation problems in the general difference field
    setting. In particular, we designed algorithms for finding appropriate
    difference field extensions to solve problems in symbolic summation.
    For instance we deal with the problem to find all nested sum
    extensions which provide us with additional solutions for a given
    linear recurrence of any order. Furthermore we find appropriate sum
    extensions, if they exist, to simplify nested sums to simpler nested
    sum expressions. Moreover we are able to interpret creative
    telescoping as a special case of sum extensions in an indefinite
    summation problem. In particular we are able to determine sum
    extensions, in case of existence, to reduce the order of a recurrence
    for a definite summation problem.",
  paper = "Schn01.pdf"
}

@phdthesis{Scho95,
  author = "Schorn, Markus",
  title = {{Contributions to Symbolic Summation}},
  school = "Johannes Kepler University, RISC",
  year = "1995",
  link = "\url{http://www.risc.jku.at/publications/download/risc_2246/diplom.pdf}",
  paper = "Scho95.pdf"
}

@inproceedings{Gerh03,
  author = "Gerhard, J. and Giesbrecht, M. and Storjohann, A. and Zima, E.V.",
  title = {{Shiftless decomposition and polynomial-time rational summation}},
  booktitle = "Proceedings of ISSAC'03",
  year = "2003",
  pages = "119--126",
  abstract = "
    New algorithms are presented for computing the dispersion set of two
    polynomials over {\bf Q} and for {\sl shiftless} factorization. Together 
    with a summability criterion by Abramov, these are applied to get a 
    polynomial-time algorithm for indefinite rational summation, using a 
    sparse representation of the output.",
  paper = "Gerh03.pdf"
}

@article{Schn05,
  author = "Schneider, Carsten",
  title = {{A new Sigma approach to multi-summation}},
  year = "2005",
  journal = "Advances in Applied Mathematics",
  volume = "34",
  number = "4",
  pages = "740--767",
  abstract = "
    We present a general algorithmic framework that allows not only to
    deal with summation problems over summands being rational expressions
    in indefinite nested syms and products (Karr, 1981), but also over
    $\delta$-finite and holonomic summand expressions that are given by a
    linear recurrence. This approach implies new computer algebra tools
    implemented in Sigma to solve multi-summation problems efficiently.
    For instacne, the extended Sigma package has been applied successively
    to provide a computer-assisted proof of Stembridge's TSPP Theorem.",
  paper = "Schn05.pdf"
}

@article{Kaue08a,
  author = "Kauers, Manuel and Schneider, Carsten",
  title = {{Indefinite summation with unspecified summands}},
  year = "2006",
  journal = "Discrete Mathematics",
  volume = "306",
  number = "17",
  pages = "2073--2083",
  abstract = "
    We provide a new algorithm for indefinite nested summation which is
    applicable to summands involving unspecified sequences $x(n)$. More
    than that, we show how to extend Karr's algorithm to a general
    summation framework by which additional types of summand expressions
    can be handled. Our treatment of unspecified sequences can be seen as
    a first illustrative application of this approach.",
  paper = "Kaue08a.pdf"
}

@article{Kaue07,
  author = "Kauers, Manuel",
  title = {{Summation algorithms for Stirling number identities}},
  year = "2007",
  journal = "Journal of Symbolic Computation",
  volume = "42",
  number = "10",
  month = "October",
  pages = "948--970",
  abstract = 
    "We consider a class of sequences defined by triangular recurrence
    equations.  This class contains Stirling numbers and Eulerian numbers
    of both kinds, and hypergeometric multiples of those. We give a
    sufficient criterion for sums over such sequences to obey a recurrence
    equation, and present algorithms for computing such recurrence
    equations efficiently. Our algorithms can be used for verifying many
    known summation identities on Stirling numbers instantly, and also for
    discovering new identities.",
  paper = "Kaue07.pdf"
}

@InProceedings{Schn07,
  author = "Schneider, Carsten",
  title = {{Symbolic Summation Assists Combinatorics}},
  year = "2007",
  booktitle = "S\'eminaire Lotharingien de Combinatoire",
  volume = "56",
  article = "B56b",
  abstract = "
    We present symbolic summation tools in the context of difference
    fields that help scientists in practical problem solving. Throughout
    this article we present multi-sum examples which are related to
    combinatorial problems.",
  paper = "Schn07.pdf"
}

@article{Schn08,
  author = "Schneider, Carsten",
  title = {{A refined difference field theory for symbolic summation}},
  year = "2008",
  journal = "Journal of Symbolic Computation",
  volume = "43",
  number = "9",
  pages = "611--644",
  abstract = "
    In this article we present a refined summation theory based on Karr's
    difference field approach. The resulting algorithms find sum
    representations with optimal nested depth. For instance, the
    algorithms have been applied successively to evaluate Feynman
    integrals from Perturbative Quantum Field Theory",
  paper = "Schn08.pdf"
}

@article{Schn09,
  author = "Schneider, Carsten",
  title = {{Structural theorems for symbolic summation}},
  journal = "Proc. AAECC-2010",
  year = "2010",
  volume = "21",
  pages = "1--32",
  abstract = "
    Starting with Karr's structural theorem for summation - the discrete
    version of Liouville's structural theorem for integration - we work
    out crucial properties of the underlying difference fields. This leads
    to new and constructive structural theorems for symbolic summation. 
    E.g., these results can be applied for harmonic sums which arise 
    frequently in particle physics.",
  paper = "Schn09.pdf"
}

@article{Eroc10,
  author = {Er\"ocal, Bur\c{c}in},
  title = {{Summation in Finite Terms Using Sage}},
  journal = "ACM Commun. Comput. Algebra",
  volume = "44",
  number = "3/4",
  month = "January",
  year = "2011",
  issn = "1932-2240",
  pages = "190--193",
  link = "\url{http://doi.acm.org/10.1145/1940475.1940517}",
  publisher = "ACM",
  abstract = "
    The summation analogue of the Risch integration algorithm developed by
    Karr uses towers of difference fields to model nested indefinite sums
    and products, as the Risch algorithm uses towers of differential
    fields to model the so called {\sl elementary functions}. The
    algorithmic machinery developed by Karr, and later generalized and
    extended, allows one to find solutions of first order difference
    equations over such towers of difference fields, in turn simplifying
    expressions involving sums and products.

    We present an implementation of this machinery in the open source
    computer algebra system Sage. Due to the nature of open source
    software, this allows direct experimentation with the algorithms and
    structures involved while taking advantage of the state of the art
    primitives provided by Sage. Even though these methods are used behind
    the scenes in the summation package Sigma and they were previously
    implemented, this is the first open source implementation.",
  paper = "Eroc10.pdf"
}

@phdthesis{Eroc11,
  author = {Er\"ocal, Bur\c{c}in},
  title = {{Algebraic Extensions for Symbolic Summation}},
  school = "RISC Research Institute for Symbolic Computation",
  year = "2011",
  link = "\url{http://www.risc.jku.at/publications/download/risc_4320/erocal_thesis.pdf}",
  abstract = 
    "The main result of this thesis is an effective method to extend Karr's
    symbolic summation framework to algebraic extensions. These arise, for
    example, when working with expressions involving $(-1)^n$. An
    implementation of this method, including a modernised version of
    Karr's algorithm is presented.

    Karr's algorithm is the summation analogue of the Risch algorithm for
    indefinite integration. In the summation case, towers of specialized
    difference fields called $\prod\sum$-fields are used to model nested
    sums and products. This is similar to the way elementary functions
    involving nested logarithms and exponentials are represented in
    differential fields in the integration case.

    In contrast to the integration framework, only transcendental
    extensions are allowed in Karr's construction. Algebraic extensions of
    $\prod\sum$-fields can even be rings with zero divisors. Karr's
    methods rely heavily on the ability to solve first-order linear
    difference equations and they are no longer applicable over these
    rings.

    Based on Bronstein's formulation of a method used by Singer for the
    solution of differential equations over algebraic extensions, we
    transform a first-order linear equation over an algebraic extension to
    a system of first-order equations over a purely transcendental
    extension field. However, this domain is not necessarily a
    $\prod\sum$-field. Using a structure theorem by Singer and van der
    Put, we reduce this system to a single first-order equation over a
    $\prod\sum$-field, which can be solved by Karr's algorithm. We also
    describe how to construct towers of difference ring extensions on an
    algebraic extension, where the same reduction methods can be used.

    A common bottleneck for symbolic summation algorithms is the
    computation of nullspaces of matrices over rational function
    fields. We present a fast algorithm for matrices over $\mathbb{Q}(x)$
    which uses fast arithmetic at the hardware level with calls to BLAS
    subroutines after modular reduction. This part is joint work with Arne
    Storjohann.",
  paper = "Eroc11.pdf"
}

@article{Poly11,
  author = "Polyadov, S.P.",
  title = {{Indefinite summation of rational functions with factorization
           of denominators}},
  year = "2011",
  month = "November",
  journal = "Programming and Computer Software",
  volume = "37",
  number = "6",
  pages = "322--325",
  abstract = "
    A computer algebra algorithm for indefinite summation of rational
    functions based on complete factorization of denominators is
    proposed. For a given $f$, the algorithm finds two rational functions
    $g$, $r$ such that $f=g(x+1)-g(x)+r$ and the degree of the denominator
    of $r$ is minimal. A modification of the algorithm is also proposed
    that additionally minimizes the degree of the denominator of
    $g$. Computational complexity of the algorithms without regard to
    denominator factorization is shown to be $O(m^2)$, where $m$ is the
    degree of the denominator of $f$.",
  paper = "Poly11.pdf"
}

@article{Schn13,
  author = "Schneider, Carsten",
  title = {{Fast Algorithms for Refined Parameterized Telescoping in 
            Difference Fields}},
  journal = "CoRR",
  year = "2013",
  volume = "abs/1307.7887",
  keywords = "survey",
  abstract = "
    Parameterized telescoping (including telescoping and creative
    telescoping) and refined versions of it play a central role in the
    research area of symbolic summation. In 1981 Karr introduced
    $\prod\sum$-fields, a general class of difference fields, that enables
    one to consider this problem for indefinite nested sums and products
    covering as special cases, e.g., the (q-)hypergeometric case and their
    mixed versions. This survey article presents the available algorithms
    in the framework of $\prod\sum$-extensions and elaborates new results
    concerning efficiency.",
  paper = "Schn13.pdf"
}

@article{Zima13,
  author = "Zima, Eugene V.",
  title = {{Accelerating Indefinite Summation: Simple Classes of Summands}},
  journal = "Mathematics in Computer Science",
  year = "2013",
  month = "December",
  volume = "7",
  number = "4",
  pages = "455--472",
  abstract = "
    We present the history of indefinite summation starting with classics
    (Newton, Montmort, Taylor, Stirling, Euler, Boole, Jordan) followed by
    modern classics (Abramov, Gosper, Karr) to the current implementation
    in computer algebra system Maple. Along with historical presentation
    we describe several ``acceleration techniques'' of algorithms for
    indefinite summation which offer not only theoretical but also
    practical improvements in running time. Implementations of these
    algorithms in Maple are compared to standard Maple summation tools",
  paper = "Zima13.pdf"
}

@misc{Schn14,
  author = "Schneider, Carsten",
  title = {{A Difference Ring Theory for Symbolic Summation}},
  year = "2014",
  abstract = "
    A summation framework is developed that enhances Karr's difference
    field approach. It covers not only indefinite nested sums and products
    in terms of transcendental extensions, but it can treat, e.g., nested
    products defined over roots of unity. The theory of the so-called
    $R\prod\sum*$-extensions is supplemented by algorithms that support the
    construction of such difference rings automatically and that assist in
    the task to tackle symbolic summation problems. Algorithms are
    presented that solve parameterized telescoping equations, and more
    generally parameterized first-order difference equations, in the given
    difference ring. As a consequence, one obtains algorithms for the
    summation paradigms of telescoping and Zeilberger's creative
    telescoping. With this difference ring theory one obtains a rigorous
    summation machinery that has been applied to numerous challenging
    problems coming, e.g., from combinatorics and particle physics.",
  paper = "Schn14.pdf"
}

@phdthesis{Vazq14,
  author = "Vazquez-Trejo, Javier",
  title = {{Symbolic Summation in Difference Fields}},
  year = "2014",
  school = "Carnegie-Mellon University",
  abstract = "
    We seek to understand a general method for finding a closed form for a
    given sum that acts as its antidifference in the same way that an
    integral has an antiderivative. Once an antidifference is found, then
    given the limits of the sum, it suffices to evaluate the
    antidifference at the given limits. Several algorithms (by Karr and
    Schneider) exist to find antidifferences, but the apers describing
    these algorithms leave out several of the key proofs needed to
    implement the algorithms. We attempt to fill in these gaps and find
    that many of the steps to solve difference equations rely on being
    able to solve two problems: the equivalence problem and the homogenous
    group membership problem. Solving these two problems is essential to
    finding the polynomial degree bounds and denominator bounds for
    solutions of difference equations. We study Karr and Schneider's
    treatment of these problems and elaborate on the unproven parts of
    their work. Section 1 provides background material; section 2 provides
    motivation and previous work; Section 3 provides an outline of Karr's
    Algorithm; section 4 examines the Equivalance Problem, and section 5
    examines the Homogeneous Group Membership Problem. Section 6 presents
    some proofs for the denominator and polynomial bounds used in solving
    difference equations, and Section 7 gives some directions for future
    work.",
  paper = "Vazq14.pdf"
}  

@book{Petk97,
  author = "Petkov\v{s}ek, Marko and Wilf, Herbert S. and 
            Zeilberger, Doran",
  title = {{A=B}},
  publisher = "A.K. Peters, Ltd",
  year = "1997",
  paper = "Petk97.pdf"
}

@misc{Temm14,
  author = "Temme, N.M.",
  title = {{Bernoulli Polynomials Old and New}},
  abstract = 
    "We consider two problems on generalized Bernoulli polynomials
    $B_n^u(z)$.  One is connected with defining functions instead of
    polynomials by making the degree $n$ of the polynomial a complex
    variable. In the second problem we are concerned with the asymptotic
    behaviour of $B_n^u(z)$ when the degree $n$ tends to infinity.",
  paper = "Temm14.pdf"
}

@book{Cart06,
  author = {Cartan, Henri},
  title = {{Differential Forms}},
  year = "2006",
  location = {Mineola, N.Y},
  edition = {Auflage: Tra},
  isbn = {9780486450100},
  pagetotal = {166},
  publisher = {Dover Pubn Inc},
  date = {2006-05-26}
}

@book{Flan03a,
  author = "Flanders, Harley",
  title = {{Differential Forms with Applications to the Physical Sciences}},
  year = "2003",
  location = "Mineola, N.Y",
  isbn = "9780486661698",
  pagetotal = "240",
  publisher = "Dover Pubn Inc",
  date = "2003-03-28",
  algebra = "\newline\refto{domain DERHAM DeRhamComplex}"
}

@book{Whit12,
  author = {Whitney, Hassler},
  title = {{Geometric Integration Theory: Princeton Mathematical Series, 
           No. 21}},
  year = "2012",
  isbn = {9781258346386},
  shorttitle = {Geometric Integration Theory},
  pagetotal = {402},
  publisher = {Literary Licensing, {LLC}},
  date = {2012-05-01}
}

@book{Fede13,
  author = {Federer, Herbert},
  title = {{Geometric Measure Theory}},
  year = "2013",
  location = {Berlin ; New York},
  edition = {Reprint of the 1st ed. Berlin, Heidelberg, New York 1969},
  isbn = {9783540606567},
  pagetotal = {700},
  publisher = {Springer},
  date = {2013-10-04},
  abstract = 
    "This book is a major treatise in mathematics and is essential in the
    working library of the modern analyst. (Bulletin of the London
    Mathematical Society)"
}

@book{Abra93,
  author = "Abraham, Ralph and Marsden, Jerrold E. and Ratiu, Tudor",
  title = {{Manifolds, Tensor Analysis, and Applications}},
  year = "1993",
  location = "New York",
  edition = "2nd Corrected ed. 1988. Corr. 2nd printing 1993",
  isbn = "9780387967905",
  pagetotal = "656",
  publisher = "Springer",
  date = "1993-08-26",
  abstract = "
    The purpose of this book is to provide core material in nonlinear
    analysis for mathematicians, physicists, engineers, and mathematical
    biologists. The main goal is to provide a working knowledge of
    manifolds, dynamical systems, tensors, and differential forms. Some
    applications to Hamiltonian mechanics, fluid mechanics,
    electromagnetism, plasma dynamics and control theory are given using
    both invariant and index notation. The prerequisites required are
    solid undergraduate courses in linear algebra and advanced calculus."
}

@book{Lamb97a,
  author = {Lambe, L. A. and Radford, D. E.},
  title = {{Introduction to the Quantum Yang-Baxter Equation and 
           Quantum Groups: An Algebraic Approach}},
  year = "1997",
  location = {Dordrecht ; Boston},
  edition = {Auflage: 1997},
  isbn = {9780792347217},
  shorttitle = {Introduction to the Quantum Yang-Baxter Equation and 
                Quantum Groups},
  abstract = {
    Chapter 1 The algebraic prerequisites for the book are covered here
    and in the appendix. This chapter should be used as reference material
    and should be consulted as needed. A systematic treatment of algebras,
    coalgebras, bialgebras, Hopf algebras, and represen­ tations of these
    objects to the extent needed for the book is given. The material here
    not specifically cited can be found for the most part in [Sweedler,
    1969] in one form or another, with a few exceptions. A great deal of
    emphasis is placed on the coalgebra which is the dual of n x n
    matrices over a field. This is the most basic example of a coalgebra
    for our purposes and is at the heart of most algebraic constructions
    described in this book. We have found pointed bialgebras useful in
    connection with solving the quantum Yang-Baxter equation. For this
    reason we develop their theory in some detail. The class of examples
    described in Chapter 6 in connection with the quantum double consists
    of pointed Hopf algebras. We note the quantized enveloping algebras
    described Hopf algebras. Thus for many reasons pointed bialgebras are
    elsewhere are pointed of fundamental interest in the study of the
    quantum Yang-Baxter equation and objects quantum groups.},
  pagetotal = {300},
  publisher = {Springer},
  date = {1997-10-31}
}

@misc{Paga16,
  author = "Pagani, Kurt",
  title = {{SurfaceComplex}},
  year = "2016",
  link = "\url{https://groups.google.com/forum/\#!topic/fricas-devel/FRDGVFsoAKw}",
  abstract = 
    "This manual describes the FriCAS domains {\bf CellMap} and 
    {\bf SurfaceComplex}. These domains provide methods to compute various
    differential geometric properties of so-called $p$-surfaces in 
    $\mathbb{R}^n$, a notion which is used by Walter Rudin in his famous
    {\sl Principles of Mathematical Analysis}.",
  paper = "Paga16.pdf"
}

@misc{Paga16a,
  author = "Pagani, Kurt",
  title = {{DifferentialGeometry1}},
  year = "2016",
  link = "\url{https://groups.google.com/forum/\#!topic/fricas-devel/FRDGVFsoAKw}",
  abstract =
    "This manual describes the FriCAS package {\bf DifferentialGeometry1}.
    This package combines differential forms and cell mappings to provide
    methods which compute {\bf pull backs}, {\bf integrals} as well as some
    other quantities of differential forms living on a surface complex.",
  paper = "Paga16a.pdf"
}

@misc{Paga16b,
  author = "Pagani, Kurt",
  title = {{DifferentialForms}},
  year = "2016",
  link = "\url{https://groups.google.com/forum/\#!topic/fricas-devel/FRDGVFsoAKw}",
  abstract = "Reference manual for the package {\bf {\tt DifferentialForms}}",
  paper = "Paga16b.pdf"
}

@misc{Whee12,
  author = "Wheeler, James T.",
  title = {{Differential Forms}},
  year = "2012",
  month = "September",
  link = "\url{http://www.physics.usu.edu/Wheeler/ClassicalMechanics/CMDifferentialForms.pdf}",
  paper = "Whee12.pdf"
}

@inproceedings{Anai00,
  author = "Anai, Hirokazu and Weispfenning, Volker",
  title = {{Deciding linear-trigonometric problems}},
  booktitle = "Proc ISSAC'00",
  publisher = "ACM",
  isbn = "1-58113-218-2",
  year = "2000",
  pages = "14-22",
  abstract =
    "In this paper, we present a decision procedure for certain
    linear-trigonometric problems for the reals and integers formalized in
    a suitable first-order language. The inputs are restricted to
    formulas, where all but one of the quantified variables occur linearly
    and at most one occurs both linearly and in a specific trigonometric
    function. Moreover we may allow in addition the integer-part operation
    in formulas. Besides ordinary quantifiers, we allow also counting
    quantifiers. Furthermore we also determine the qualitative structure
    of the connected components of the satisfaction set of the mixed
    linear-trigonometric variable. We also consider the decision of these
    problems in subfields of the real algebraic numbers.",
  paper = "Anai00.pdf"
}

@phdthesis{Arno81,
  author = {Arnon, Dennis Soul\'e},
  title = {{Algorithms for the Geometry of Semi-algebraic Sets}},
  school = "University of Wisconsin-Madison",
  year = "1981",
  abstract =
    "Let A be a set of polynomials in r variables with integer
    coefficients. An $A$-invariant cylindrical algebraic decomposition 
    (cad) of $r$-dimensional Euclidean space (G. Collins, Lect. Notes 
    Comp. Sci., 33, Springer-Verlag, 1975, pp 134-183) is a certain 
    cellular decomposition of $r$-space, such that each cell is a 
    semi-algebraic set, the polynomials of $A$ are sign-invariant on 
    each cell, and the cells are arranged into cylinders. The cad 
    algorithm given by Collins provides, among other applications, 
    the fastest known decision procedure for real closed fields, a 
    cellular decomposition algorithm for semi-algebraic sets, and a 
    method of solving nonlinear (polynomial) optimization problems 
    exactly. The time-consuming calculations with real algebraic 
    numbers required by the algorithm have been an obstacle to its 
    implementation and use. The major contribution of this thesis 
    is a new version of the cad algorithm for $r \le 3$, in which 
    one works with maximal connected $A$-invariant collections of 
    cells, in such a way as to often avoid the most time-consuming 
    algebraic number calculations. Essential to this new cad 
    algorithm is an algorithm we present for determination of 
    adjacenies among the cells of a cad. Computer programs for 
    the cad and adjacency algorithms have been written, providing 
    the first complete implementation of a cad algorithm. Empirical 
    data obtained from application of these programs are presented 
    and analyzed."  
}

@techreport{Arno82,
  author = "Arnon, Dennis S. and Collins, George E. and McCallum, Scott",
  title = {{Cylindrical Algebraic Decomposition I: The Basic Algorithm}},
  year = "1982",
  institution = "Purdue University",
  type = "Technical Report",
  number = "82-427A",
  link = "\url{https://pdfs.semanticscholar.org/7643/4b54250f05ebf0dcc27c33b7dc250419fb94.pdf}",
  abstract = 
    "Given a set of r-variate integral polynomials, a {\sl cylindrical
    algebraic decomposition (cad)} of euclidean r-space $E^r$ into connected
    subsets compatible with the zeros of the polynomials. Collins gave a 
    cad construction algorithm in 1975, as part of a quantifier elimination
    procedure for real closed fields. The algorithm has subsequently found
    diverse applications (optimization, curve display); new applications
    have been proposed (term rewriting systems, motion planning). In the
    present two-part paper, we give an algorithm for determining the pairs
    of adjacent cells in a cad of $E^2$. This capability is often needed
    in applications. In Part I we describe the essential features of the
    r-space cad algorithm, to provide a framework for the adjacency algorithm
    in Part II.",
  paper = "Arno82.pdf"
}

@article{Arno82a,
  author = "Arnon, Dennis S. and McCallum, Scott",
  title = {{Cylindrical Algebraic Decomposition by Quantifier Eliminations}},
  journal = "Lecture Notes in Computer Science",
  volume = "144",
  pages = "215-222",
  year = "1982",
  abstract = 
    "Cylindrical algebraic decompositions were introduced as a major
    component of a new quantifier elimination algorithm for elementary
    algebra and geometry (G. Collins, ~973). In the present paper we turn
    the tables and show that one can use quantifier elimination for ele-
    mentary algebra and geometry to obtain a new version of the
    cylindrical algebraic decomposi- tion algorithm. A key part of our
    result is a theorem, of interest in its own right, that relates the
    multiplicities of the roots of a polynomial to their continuity.",
  paper = "Arno82a.pdf"
}

@article{Arno84,
  author = "Arnon, Dennis S. and Collins, George E. and McCallum, Scott",
  title = {{Cylindrical Algebraic Decomposition II: An Adjacency Algorithm
           for the Plane}},
  year = "1984",
  journal = "SIAM J. Comput.",
  volume = "13",
  number = "4",
  pages = "878-889",
  abstract = 
    "Given a set of r-variate integral polynomials, a {\sl cylindrical
    algebraic decomposition (cad)} of euclidean r-space $E^r$ partitions 
    $E^r$ into connected subsets compaitible with the zeros of the
    polynomials. Each subset is a {\sl cell}. Informally, two cells of
    a cad are {\sl adjacent} if they touch each other; formally, they are
    adjacent if their union is connected. In applications of cad's one
    often wishes to know the adjacent pairs of cells. Previous algorithms
    for cad construction (such as that given in Part I of this paper) have
    not actually determined them. We give here in Part II an algorithm
    which determines the pairs of adjacent cells as it constructs a cad
    of $E^2$.",
  paper = "Arno84.pdf"
}

@article{Arno88,
  author = "Arnon, D.S. and Mignotte, M.",
  title = {{On Mechanical Quantifier Elimination for Elementary Algebra 
           and Geometry}},
  journal = "J. Symbolic Computation",
  volume = "5", 
  pages = "237-259",
  year = "1988",
  abstract = "
    We give solutions to two problems of elementary algebra and geometry:
    (1) find conditions on real numbers $p$, $q$, and $r$ so that the
    polynomial function $f(x)=x^4+px^2+qx+r$ is nonnegative for all real
    $x$ and (2) find conditions on real numbers $a$, $b$, and $c$ so that
    the ellipse $\frac{(x-e)^2}{q^2}+\frac{y^2}{b^2}-1=0$ lies inside the
    unit circle $y^2+x^2-1=0$. Our solutions are obtained by following the
    basic outline of the method of quantifier elimination by cylindrical
    algebraic decomposition (Collins, 1975), but we have developed, and
    have been considerably aided by, modified versions of certain of its
    steps. We have found three equally simple but not obviously equivalent
    solutions for the first problem, illustrating the difficulty of
    obtaining unique ``simplest'' solutions to quantifier elimination
    problems of elementary algebra and geometry.",
  paper = "Arno88.pdf"

}

@misc{Arno88a,
  author = "Arnon, Dennis and Buchberger, Bruno",
  title = {{Algorithms in Real Algebraic Geometry}},
  publisher = "Academic Press",
  year = "1988",
  journal = "Journal of Symbolic Computation"
}

@article{Arno88b,
  author = "Arnon, Dennis S. and Collins, George E. and McCallum, Scott",
  title = {{An Adjacency algorithm for cylindrical algebraic decompositions
           of three-dimensional space}},
  journal = "J. Symbolic Computation",
  volume = "5",
  number = "1-2",
  pages = "163-187",
  year = "1988",
  abstract =
    "Let $A \subset \mathbb{Z}[x_1,\ldots,x_r]$
    be a finite set. An {\sl A-invariant cylindrical
    algebraic decomposition (cad)} is a certain partition of $r$-dimenslonal
    euclidean space $E^r$ into semi-algebraic cells such that the value of
    each $A_i \in A$  has constant sign (positive, negative, or zero) 
    throughout each cell. Two cells are adjacent if their union is 
    connected. We give an algorithm that determines the adjacent pairs 
    of cells as it constructs a cad of $E^3$. The general teehnlque 
    employed for $^3$ adjacency determination is “projection” into $E^2$, 
    followed by application of an existing $E^2$ adjacency elgorlthm 
    (Arnon, Collins, McCallum, 1984). Our algorithm has the following 
    properties: (1) it requires no coordinate changes, end (2) in any 
    cad of $E^1$, $E^2$, or $E^3$ that it builds, the boundary of each cell 
    is a (disjoint) union of lower-dlmenaionel cells.",
  paper = "Arno88b.pdf"
}

@article{Arno88c,
  author = "Arnon, Dennis S.",
  title = {{A bibliography of quantifier elimination for real closed fields}},
  journal = "J. of Symbolic Computation",
  volume = "5",
  number = "1-2",
  pages = "267-274",
  year = "1988",
  link = 
    "\url{http://www.sciencedirect.com/science/article/pii/S0747717188800166}",
  abstract =
    "A basic collection of literature relating to algorithmic quantifier
     elimination for real closed fields is assembled",
  paper = "Arno88c.pdf"
}

@misc{Bake90,
  author = "Baker, Henry G.",
  title = {{The Nimble Type Inferencer for Common Lisp-84}},
  link = "\url{http://home.pipeline.com/~hbaker1/TInference.html}",
  year = "1990",
  abstract = 
    "We describe a framework and an algorithm for doing type inference
    analysis on programs written in full Common Lisp-84 (Common Lisp
    without the CLOS object-oriented extensions). The objective of type
    inference is to determine tight lattice upper bounds on the range of
    runtime data types for Common Lisp program variables and
    temporaries. Depending upon the lattice used, type inference can also
    provide range analysis information for numeric variables. This lattice
    upper bound information can be used by an optimizing compiler to
    choose more restrictive, and hence more efficient, representations for
    these program variables. Our analysis also produces tighter control
    flow information, which can be used to eliminate redundant tests which
    result in dead code. The overall goal of type inference is to
    mechanically extract from Common Lisp programs the same degree of
    representation information that is usually provided by the programmer
    in traditional strongly-typed languages. In this way, we can provide
    some classes of Common Lisp programs execution time efficiency
    expected only for more strongly-typed compiled languages.
    
    The Nimble type inference system follows the traditional
    lattice/algebraic data flow techniques [Kaplan80], rather than the
    logical/theorem-proving unification techniques of ML [Milner78]. It
    can handle polymorphic variables and functions in a natural way, and
    provides for ``case-based'' analysis that is quite similar to that used
    intuitively by programmers. Additionally, this inference system can
    deduce the termination of some simple loops, thus providing
    surprisingly tight upper lattice bounds for many loop variables.
    
    By using a higher resolution lattice, more precise typing of primitive
    functions, polymorphic types and case analysis, the Nimble type
    inference algorithm can often produce sharper bounds than
    unification-based type inference techniques. At the present time,
    however, our treatment of higher-order data structures and functions
    is not as elegant as that of the unification techniques."
}

@article{Bake91,
  author = "Baker, Henry G.",
  title = {{Pragmatic Parsing in Common Lisp}},
  journal = "ACM Lisp Pointers",
  volume = "IV",
  number = "2",
  pages = "3-15",
  year = "1991",
  abstract =
    "We review META, a classic technique for building recursive descent
    parsers, that is both simple and efficient.  While META does not
    handle all possible regular or context-free grammars, it handles a
    surprisingly large fraction of the grammars encountered by Lisp
    programmers.  We show how META can be used to parse streams, strings
    and lists—including Common Lisp's hairy lambda expression parameter
    lists.  Finally, we compare the execution time of this parsing method
    to the built-in methods of Common Lisp.",
  paper = "Bake91.pdf",
  keywords = "printed"
}

@book{Basu06,
  author = "Basu, Saugata and Pollack, Richard and 
            Roy, Marie-Francoise",
  title = {{Algorithms in Real Algebraic Geometry}},
  publisher = "Springer",
  year = "2006",
  isbn = "978-3-540-33098-1"
}

@article{Beau07,
  author = "Beaumont, James C. and Bradford, Russell J. and 
            Davenport, James H. and Phisanbut, Nalina",
  title = {{Testing elementary function identities using CAD}},
  journal = "Applicable Algebra in Engineering, Communication and Computing",
  year = "2007",
  volume = "18",
  number = "6",
  issn = "0938-1279",
  publisher = "Springer-Verlag",
  pages = "513-543",
  abstract = "
    One of the problems with manipulating function identities in computer
    algebra systems is that they often involve functions which are
    multivalued, whilst most users tend to work with single-valued
    functions.  The problem is that many well-known identities may no
    longer be true everywhere in the complex plane when working with their
    single-valued counterparts. Conversely, we cannot ignore them, since
    in particular contexts they may be valid. We investigate the
    practicality of a method to verify such identities by means of an
    experiment; this is based on a set of test examples which one might
    realistically meet in practice.  Essentially, the method works as
    follows. We decompose the complex plane via means of cylindrical
    algebraic decomposition into regions with respect to the branch cuts
    of the functions. We then test the identity numerically at a sample
    point in the region. The latter step is facilitated by the notion of
    the {\sl adherence} of a branch cut, which was previously introduced
    by the authors. In addition to presenting the results of the
    experiment, we explain how adherence relates to the proposal of 
    {\sl signed zeros} by W. Kahan, and develop this idea further in order to
    allow us to cover previously untreatable cases. Finally, we discuss
    other ways to improve upon our general methodology as well as topics
    for future research.",
  paper = "Beau07.pdf"
}
   
@article{Beno86,
  author = "Ben-Or, Michael and Kozen, Dexter and Reif, John",
  title = {{The complexity of elementary algebra and geometry}},
  journal = "J. Computer and System Sciences",
  volume = "32",
  number = "2",
  year = "1986",
  pages = "251-264",
  abstract =
    "The theory of real closed fields can be decided in exponential space
    or parallel exponential time. In fixed dimesnion, the theory can be
    decided in NC.",
  paper = "Beno86.pdf"
}

@misc{Brad14,
  author = "Bradford, Russell and Chen, Changbo and Davenport, James H. and
            England, Matthew and Maza, Marc Moreno and Wilson, David",
  title = {{Truth Table Invariant Cylindrical Algebraic Decomposition by
           Regular Chains}},
  link = "\url{https://arxiv.org/pdf/1401.6310.pdf}",
  year = "2014",
  abstract =
    "A new algorithm to compute cylindrical algebraic decompositions
    (CADs) is presented, building on two recent advances. Firstly, the
    output is truth table invariant (a TTICAD) meaning given formulae have
    constant truth value on each cell of the decomposition.  Secondly, the
    computation uses regular chains theory to first build a cylindrical
    decomposition of complex space (CCD) incrementally by polynomial.
    Significant modification of the regular chains technology wa s used to
    achieve the more sophisticated invariance criteria. Experimental
    results on an implementation in the {\tt RegularChains} Library for Maple
    verify that combining these advances gives an algorithm superior to
    its individual components and competitive with the state of the art.",
  paper = "Brad14.pdf"
}

@misc{Brad15,
  author = "Bradford, Russell and Davenport, James H. and England, Matthew and
            McCallum, Scott",
  title = {{Truth Table Invariant Cylindrical Algebraic Decomposition}},
  link = "\url{https://arxiv.org/pdf/1401.0645.pdf}",
  year = "2015",
  abstract =
    "When using cylindrical algebraic decomposition (CAD) to solve a
    problem with respect to a set of polynomials, it is likely not the
    signs of those polynomials that are of paramount importance but rather
    the truth values of certain quantifier free formulae involving
    them. This observation motivates our article and definition of a Truth
    Table Invariant CAD (TTICAD).  In ISSAC 2013 the current authors
    presented an algorithm that can efficiently and directly construct a
    TTICAD for a list of formulae in which each has an equational
    constraint. This was achieved by generalising McCallum's theory of
    reduced projection operators. In this paper we present an extended
    version of our theory which can be applied to an arbitrary list of
    formulae, achieving savings if at least one has an equational
    constraint. We also explain how the theory of reduced projection
    operators can allow for further improvements to the lifting phase of
    CAD algorithms, even in the context of a single equational constraint.
    The algorithm is implemented fully in Maple and we present both
    promising results from experimentation and a complexity analysis
    showing the benefits of our contributions.",
  paper = "Brad15.pdf"
}

@phdthesis{Brow99,
  author = "Brown, Christopher W.",
  title = {{Solution Formula Construction for Truth Invariant CADs}},
  school = "University of Delaware",
  year = "1999",
  website = "http://www.usna.edu/CS/qepcadweb/B/impl/Implementation.html",
  link = "\url{http://www.usna.edu/Users/cs/wcbrown/research/thesis.ps.gz}",
  abstract = 
    "The CAD-based quantifier elimination algorithm takes a formula from
    the elementary theory of real closed fields as input, and constructs a
    CAD of the space of the formula's unquantified variables. This
    decomposition is truth invariant with respect to the input formula,
    meaning that the formula is either identically true or identically
    false in each cell of the decomposition. The method determines the
    truth of the input formula for each cell of the CAD, and then uses the
    CAD to construct a solution formula -- a quantifier free formula that
    is equivalent to the input formula. This final phase of the algorithm,
    the solution formula construction phase, is the focus of this thesis.

    An optimal solution formula construction algorithm would be {\sl
    complete} -- i.e. applicable to any truth-invariant CAD, would be {\sl
    efficient}, and would produce {\sl simple} solution formulas. Prior to
    this thesis, no method was available with even two of these three
    properties. Several algorithms are presented, all addressing problems
    related to solution formula construction. In combination, these
    provide an efficient and complete method for constructing solution
    formulas that are simple in a variety of ways.

    Algorithms presented in this thesis have been implemented using the
    SACLIB library, and integrated into QEPCAD, a SACLIB-based
    implementation of quantifier elimination by CAD. Example computations
    based on these implementations are discussed.",
  paper = "Brow99.pdf"
}

@misc{Brow01,
  author="Brown, Christopher W.",
  title = {{The McCallum projection, lifting, and order-invariance}},
  year="2001",
  link = "\url{http://www.usna.edu/Users/cs/wcbrown/research/MOTS2001.1.ps.gz}",
  abstract = 
    "The McCallum Projection for Cylindrical Algebraic Decomposition (CAD)
    produces a smaller projection factor set than previous projections,
    however it does not always produce a sign-invariant CAD for the set of
    input polynomials. Problems may arise when a ($k+1$)-level projection
    factor vanishes identically over a k-level cell. According to
    McCallum's paper, when this happens (and $k+1$ is not the highest
    level in the CAD) we do not know whether the projection is valid,
    i.e. whether or not a sign-invariant CAD for the set of input
    polynomials will be produced when lifting is performed in the usual
    way. When the $k$-level cell in question has dimension 0, McCallum
    suggests a modification of the lifting method that will ensure the
    validity of his projection, although to my knowledge this has never
    been implemented.

    In this paper we give easily computable criteria that often allow us
    to conclude that McCallum's projection is valid even though a
    projection factor vanishes identically over a cell. We also improve on
    McCallum's modified lifting method.

    We've incorporated the ideas contained in the paper into QEPCAD, the
    most complete implementation of CAD. When McCallum's projection is
    invalid because of a projection factor not being order-invariant over a
    region on which it vanishes identically, at least a warning message
    ought to be issued. Currently, QEPCAD may print warning messages that
    are not needed, and may fail to print warning messages when they are
    needed. Our implementation in QEPCAD ensures that warning messages are
    printed when needed, and reduces the number of times warning messages
    are printed when not needed. Neither McCallum's modified lifting
    method nor our improvement of it have been implemented in QEPCAD. The
    design of the system would make implementing such a feature quite
    difficult.",
  paper = "Brow01.pdf"
}

@article{Brow01a,
  author = "Brown, Christopher W.",
  title = {{Simple CAD Construction and its Applications}},
  journal = "J. Symbolic Computation",
  year = "2001",
  volume = "31",
  pages = "521-547",
  abstract = 
    "This paper presents a method for the simplification of truth-invariant
    cylindrical algebraic decompositions (CADs). Examples are given that
    demonstrate the usefulness of the method in speeding up the solutoin
    formula construction phase of the CAD-based quantifier elimination
    algorithm. Applications of the method to the construction of 
    truth-invariant CADs for very large quantifier-free formulas and
    quantifier elimination of non-prenex formulas are also discussed.",
  paper = "Brow01a.pdf"
}

@misc{Brow02,
  author = "Brown, Christopher W.",
  title = {{QEPCAD B -- A program for computing with semi-algebraic sets 
           using CADs}},
  year = "2002",
  abstract = 
    "This report introduces QEPCAD B, a program for computing with real
    algebraic sets using cylindrical algebraic decomposition (CAD). QEPCAD
    B both extends and improves upon the QEPCAD system for quantifier
    elimination by partial cylindrical algebraic decomposition written by
    Hoon Hong in the early 1990s. This paper briefly discusses some of the
    improvements in the implementation of CAD and quantifier elimination
    vis CAD, and provides somewhat more detail on extensions to the system
    that go beyond quantifier elimination. The author is responsible for
    most of the extended features of QEPCAD B, but improvements to the
    basic CAD implementation and to the SACLIB library on which QEPCAD is
    based are the results of many people's work.",
  paper = "Brow02.pdf"
}

@article{Brow11,
  author = "Brown, Christopher W.",
  title = {{Fast simplifications for Tarski formulas based on monomial
           inequalities}},
  year = "2011",
  journal = "Journal of Symbolic Computation",
  volume = "47",
  pages = "859-882",
  abstract = 
    "We define the ``combinatorial part'' of a Tarski formula in which
    equalities and inequalities are in factored or partially factored
    form.  The combinatorial part of a formula contains only ``monomial
    inequalities'', which are sign conditions on monomials. We give
    efficient algorithms for answering some basic questions about
    conjunctions of monomial inequalities and prove the
    NP-Completness/Hardness of some others. By simplifying the
    combinatorial part back to a Tarski formula, we obtain non-trivial
    simplifications without algebraic operations.",
  paper = "Brow11.pdf" 
}

@inproceedings{Cann87,
  author = "Canny, John",
  title = {{A new algebraic method of robot motion planning and 
            real geometry}},
  booktitle = "IEEE Symp. on Foundations of Comp. Sci.",
  pages = "39-48",
  year = "1987",
  abstract =
    "We present an algorithm which solves the findpath or generalized
    movers' problem in single exponential sequential time. This is the
    first algorithm for the problem whose sequential time bound is less
    than double exponential. In fact, the combinatorial exponent of the
    algorithm is equal to the number of degrees of freedom, making it
    worst-case optimal, and equaling or improving the time bounds of many
    special purpose algorithms. The algorithm accepts a formula for a
    semi-algebraic set S describing the set of free configurations and
    produces a one-dimensional skeleton or ``roadmap'' of the set, which is
    connected within each connected component of S. Additional points may
    be linked to the roadmap in linear time. Our method draws from results
    of singularity theory, and in particular makes use of the notion of
    stratified sets as an efficient alternative to cell decomposition. We
    introduce an algebraic tool called the multivariate resultant which
    gives a necessary and sufficient condition for a system of homogeneous
    polynomials to have a solution, and show that it can be computed in
    polynomial parallel time. Among the consequences of this result are
    new methods for quantifier elimination and an improved gap theorem for
    the absolute value of roots of a system of polynomials.",
  paper = "Cann87.pdf"
}

@inproceedings{Cann88,
  author = "Canny, John",
  title = {{Some algebraic and geometric computations in PSPACE}},
  booktitle = "Proc 20th ACM Symp. on the theory of computing",
  pages = "460-467",
  year = "1988",
  isbn = "0-89791-264-0",
  link = "\url{http://digitalassets.lib.berkeley.edu/techreports/ucb/text/CSD-88-439.pdf}",
  abstract =
    "We give a PSPACE algorithm for determining the signs of multivariate
    polynomials at the common zeros of a system of polynomial
    equations. One of the consequences of this result is that the
    ``Generalized Movers' Problem'' in robotics drops from EXPTIME into
    PSPACE, and is therefore PSPACE-complete by a previous hardness result
    [Rei]. We also show that the existential theory of the real numbers
    can be decided in PSPACE. Other geometric problems that also drop into
    PSPACE include the 3-d Euclidean Shortest Path Problem, and the ``2-d
    Asteroid Avoidance Problem'' described in [RS]. Our method combines the
    theorem of the primitive element from classical algebra with a
    symbolic polynomial evaluation lemma from [BKR]. A decision problem
    involving several algebraic numbers is reduced to a problem involving
    a single algebraic number or primitive element, which rationally
    generates all the given algebraic numbers.",
  paper = "Cann88.pdf"
}

@article{Cann93,
  author = "Canny, John",
  title = {{Improved algorithms for sign and existential quantifier 
           elimination}},
  journal = "The Computer Journal",
  volume = "36",
  pages = "409-418",
  year = "1993",
  abstract =
    "Recently there has been a lot of activity in algorithms that work
    over real closed fields, and that perform such calculations as
    quantifier elimination or computing connected components of
    semi-algebraic sets. A cornerstone of this work is a symbolic sign
    determination algorithm due to Ben-Or, Kozen and Reif. In this
    paper we describe a new sign determination method based on the earlier
    algorithm, but with two advantages: (i) It is faster in the univariate
    case, and (ii) In the general case, it allows purely symbolic
    quantifier elimination in pseudo-polynomial time. By purely symbolic,
    we mean that it is possible to eliminate a quantified variable from a
    system of polynomials no matter what the coefficient values are. The
    previous methods required the coefficients to be themselves
    polynomials in other variables. Our new method allows transcendental
    functions or derivatives to appear in the coefficients.
    
    Another corollary of the new sign-determination algorithm is a very
    simple, practical algorithm for deciding existentially-quantified
    formulae of the theory of the reals. We present an algorithm that has
    a bit complexity of $n^{k+1}d^{O(k)}(c log n)^{1+\epsilon}$ randomized, or
    $n^{n+1}d^{O(k^2)}c(1+\epsilon)$ deterministic, for any 
    $\epsilon>0$, where $n$ is the number
    of polynomial constraints in the defining formula, $k$ is the number of
    variables, $d$ is a bound on the degree, $c$ bounds the bit length of the
    coefficient. The algorithm makes no general position assumptions, and
    its constants are much smaller than other recent quantifier
    elimination methods.",
  paper = "Cann93.pdf"
}

@book{Cavi98,
  author = "Caviness, B. F. and Johnson, J. R.",
  title = {{Quantifier Elimination and Cylindrical Algebraic Decomposition}},
  publisher = "Springer",
  year = "1998",
  isbn = "3-221-82794-3",
  keywords = "axiomref"
}

@misc{Chen10,
  author = "Chen, Changbo and Davenport, James H. and May, John P. and
            Maza, Marc Moreno and Xia, Bican and Xiao, Rong",
  title = {{Triangular Decomposition of Semi-algebraic Systems}},
  year = "2010",
  link = "\url{https://arxiv.org/pdf/1002.4784.pdf}",
  abstract =
    "Regular chains and triangular decompositions are fundamental and
    well-developed tools for describing the complex solutions of
    polynomial systems. This paper proposes adaptations of these tools
    focusing on solutions of the real analogue: semi-algebraic systems.
    
    We show that any such system can be decomposed into finitely many
    regular semi-algebraic systems. We propose two specifications of such
    a decomposition and present corresponding algorithms. Under some
    assumptions, one type of decomposition can be computed in singly
    exponential time w.r.t. the number of variables. We implement our
    algorithms and the experimental results illustrate their
    effectiveness.",
  paper = "Chen10.pdf"
}

@misc{Chen12,
  author = "Chen, Changbo and Maza, Marc Moreno",
  title = {{An Incremental Algorithm for Computing Cylindrical Algebraic
           Decompositions}},
  link = "\url{https://arxiv.org/pdf/1210.5543.pdf}",
  year = "2012",
  abstract =
    "In this paper, we propose an incremental algorithm for computing
    cylindrical al gebraic decompositions. The algorithm consists of two
    parts: computing a complex cylindrical tree and refining this complex
    tree into a cylindrical tree in real space. The incrementality comes
    from the first part of the algorithm, where a complex cylindrical tree
    is constructed by refining a previous complex cylindrical tree with a
    polynomial constraint. We have implemented our algorithm in Maple. The
    experimentation shows that the proposed algorithm outperforms existing
    ones for many examples taken from the literature",
  paper = "Chen12.pdf"
}

@article{Coll71,
  author = "Collins, George E.",
  title = {{The Calculation of Multivariate Polynomial Resultants}},
  journal = "ACM SYMSAC",
  volume = "18",
  number = "4",
  year = "1971",
  pages = "515-532",
  abstract =
    "An efficient algorithm is presented for the exact calculation of
    resultants of multivariate polynomials with integer coefficients.
    The algorithm applies modular homomorphisms and the Chinese remainder
    theorem, evaluation homomorphisms and interpolation, in reducing
    the problem to resultant calculation for univariate polynomials 
    over GF(p), whereupon a polynomial remainder sequence algorithm is used.
    
    The computing time of the algorithm is analyzed theoretically as a
    function of the degrees and coefficient sizes of its inputs. As a very
    special case, it is shown that when all degrees are equal and the
    coefficient size is fixed, its computing time is approximately
    proportional to $\lambda^{2r+1}$, where $\lambda$ is the common 
    degree and $r$ is the number of variables.
    
    Empirically observed computing times of the algorithm are tabulated
    for a large number of examples, and other algorithms are compared.
    Potential application of the algorithm to the solution of systems of
    polynomial equations is discussed.",
  paper = "Coll71.pdf"
}

@article{Coll75,
  author = "Collins, George E.",
  title = {{Quantifier Elimination for Real Closed Fields by 
           Cylindrical Algebraic Decomposition}},
  year = "1975",
  journal = "Lecture Notes in Computer Science",
  volume = "33",
  pages = "134-183",
  abstract =
    "I.  Introduction.  Tarski in 1948, published a quantifier 
    elimination method for the elementary theory of real closed fields 
    (which he had discoverd in 1930).  As noted by Tarski, any quantifier
    elimination method for this theory also provides a decision method,
    which enables one to decide whether any sentence of the theory is
    true or false.  Since many important and difficult mathematical
    problems can be expressed in this theory, any computationally
    feasible quantifier elimination algorithm would be of utmost
    significance.  
    
    However, it became apparent that Tarski's method required too much
    computation to be practical except for quite trivial problems.
    Seidenberg in 1954, described another method which he thought
    would be more efficient.  A third method was published by Cohen in
    1969.  Some significant improvements of Tarski's method have been
    made by W. Boge, which are described in a thesis by Holthusen
    
    This paper describes a completely new method which I discoverd in
    February 1973.  This method was presented in a seminar at Stanford
    University in March 1973 and in abstract form at a symposium at 
    Carnegie-Mellon University in May 1973.  In August 1974 a full 
    presentation of the method was delivered at the EUROSAM 74 Conference 
    in Stockholm, and a preliminary version of the present paper was 
    published in the proceedings of that conference.",
  paper = "Coll75.pdf"
}

@article{Coll82a,
  author = "Collins, George E.",
  title = {{Factorization in Cylindrical Algebraic Decomposition - Abstract}},
  journal = "Lecture Notes in Computer Science",
  volume = "144",
  pages = "212-214",
  year = "1982",
  paper = "Coll82a.pdf"
}

@article{Coll91,
  author = "Collins, George E. and Hong, Hoon",
  title = {{Partial Cylindrical Algebraic Decomposition for Quantifier 
           Elimination}},
  journal = "J. Symbolic Computation",
  year = "1991",
  volume = "12",
  pages = "299-328",
  abstract =
    "The Cylindrical Algebraic Decomposition method (CAD) decomposes $R^r$
    into regions over which given polynomials have constant signs.  An
    important application of GAD is quantifier elimination in elementary
    algebra and geometry.  In this paper we present a method which
    intermingles CAD construction with truth evaluation so that parts of
    the CAD are constructed only as needed to further truth evaluation and
    aborts CAD construction as soon as no more truth evaluation is needed.
    The truth evaluation utilizes in an essential way any quantifiers
    which are present and additionally takes account of atomic formulas
    from which some variables are absent.  Preliminary observations show
    that the new method is always more efficient than the original, and
    often significantly more efficient.",
  paper = "Coll91.pdf"
}

@inproceedings{Coll98,
  author = "Collins, George E.",
  title = {{Quantifier Elimination by Cylindrical Algebraic Decomposition --
           Twenty Years of Progress}},
  booktitle = "Quantifier Elimination and Cylindrical Algebraic 
               Decomposition",
  isbn = "3-211-82794-3",
  year = "1998",
  pages = "8-23",
  abstract =
    "The CAD (cylindrical algebraic decomposition) method and its
    application to QE (quantifier elimination) for ERA (elementary real
    algebra) was announced by the author in 1973 at Carnegie Mellon
    University (Collins 1973b). In the twenty years since then several
    very important improvements have been made to the method which,
    together with a very large increase in available computational power,
    have made it possible to solve in seconds or minutes some interesting
    problems. In the following we survey these improvements and present
    some of these problems with their solutions."
}

@article{Coll02,
  author = "Collins, George E. and Johnson, Jeremy R. and Krandick, Werner",
  title = {{Interval arithmetic in cylindrical algebraic decomposition}},
  journal = "J. Symbolic Computation",
  volume = "34",
  number = "2",
  pages = "145-157",
  year = "2002",
  publisher = "Elsevier",
  abstract = 
    "Cylindrical algebraic decomposition requires many very time consuming
    operations, including resultant computation, polynomial factorization,
    algebraic polynomial gcd computation and polynomial real root
    isolation. We show how the time for algebraic polynomial real root
    isolation can be greatly reduced by using interval arithmetic instead
    of exact computation. This substantially reduces the overall time for
    cylindrical algebraic decomposition.",
  paper = "Coll02.pdf"
}

@techreport{Dave85a,
  author = "Davenport, J.H.",
  title = {{Computer Algebra for Cylindrical Algebraic Decomposition}},
  institution = "NADA Kth Stockholm / Bath Ccomputer Science",
  link = "\url{http://staff.bath.ac.uk/masjhd/TRITA.pdf}",
  type = "technical report",
  number = "88-10",
  year = "1985",
  abstract =
    "This report describes techniques for resolving systems of polynomial
    equations and inequalities. The general technique is {\sl cylindrical
    algebraic decomposition}, which decomposes space into a number of
    regions, on each of which the equations and inequalities have the
    same sign. Most of the report is spent describing the algebraic and
    algorithmic pre-requisites (resultants, algebraic numbers, Sturm
    sequences, etc.), and then describing the method, first in two 
    dimensions and then in an artitrary number of dimensions.",
  paper = "Dave85a.pdf"
}

@techreport{Dolz97a,
  author = "Dolzmann, Andreas and Sturm, Thomas and 
            Weispfenning, Volker",
  title = {{Real Quantifier Elimination in Practice}},
  type = "technical report",
  institution = "University of Passau",
  number = "MIP-9720",
  year = "1997",
  abstract =
    "We give a survey of three implemented real quantifier elimination
    methods: partial cylindrical algebraic decomposition, virtual
    substitution of test terms, and a combination of Groebner basis
    computations with multivariate real root counting. We examine the
    scope of these implementations for applications in various fields of
    science, engineering, and economics",
  paper = "Dolz97a.pdf"
}

@inproceedings{Dolz04,
  author = "Dolzmann, Andreas and Seidl, Andreas and Sturm, Thomas",
  title = {{Efficient projection orders for CAD}},
  booktitle = "proc ISSAC'04",
  year = "2004",
  pages = "111-118",
  publisher = "ACM",
  isbn = "1-58113-827-X",
  abstract =
    "We introduce an efficient algorithm for determining a suitable
    projection order for performing cylindrical algebraic
    decomposition. Our algorithm is motivated by a statistical analysis of
    comprehensive test set computations. This analysis introduces several
    measures on both the projection sets and the entire computation, which
    turn out to be highly correlated. The statistical data also shows that
    the orders generated by our algorithm are significantly close to optimal.",
  paper = "Dolz04.pdf"
}

@misc{Engl16,
  author = "England, Matthew and Davenport, James H.",
  title = {{Experience with Heuristics, Benchmarks and Standards for
           Cylindrical Algebraic Decomposition}},
  link = "\url{https://arxiv.org/pdf/1609.09269.pdf}",
  abstract =
    "n the paper which inspired the $SC^2$ project, [E. Abraham,
    Building Bridges between Symbolic Computation and Satisfiability
    Checking , Proc. ISSAC ’15, pp. 1–6, ACM, 2015] the author identified
    the use of sophisticated heuristics as a technique that the
    Satisfiability Checking community excels in and from which it is
    likely the Symbolic Computation community could learn and prosper.  To
    start this learning process we summarise our experience with heuristic
    development for the computer algebra algorithm Cylindrical Algebraic
    Decomposition. We also propose and discuss standards and benchmarks as
    another area where Symbolic Computation could prosper from
    Satisfiability Checking expertise, noting that these have been
    identified as initial actions for the new $SC^2$ community in the CSA
    project, as described in [E.́ Abraham et al., SC 2 : {\sl Satisfiability
    Checking meets Symbolic Computation (Project Paper)}, Intelligent
    Computer Mathematics (LNCS 9761), pp.  28–43, Springer, 2015].",
  paper = "Engl16.pdf"
}

@InProceedings{Emir04,
  author = "Emiris, Ioannis Z. and Tsigaridas, Elias P.",
  title = {{Comparing real algebraic numbers of small degree}},
  booktitle = "12th annual European symposium",
  series = "ESA 2004",
  year = "2004",
  isbn = "3-540-23025-4",
  location = "Bergen, Norway",
  pages = "652-663",
  link = "\url{http://www-polsys.lip6.fr/~elias/files//et-esa-04.pdf}",
  algebra = "\newline\refto{domain RECLOS RealClosure}",
  abstract =
    "We study polynomials of degree up to 4 over the rationals or a
    computable real subfield. Our motivation comes from the need to
    evaluate predicates in nonlinear computational geometry efficiently
    and exactly. We show a new method to compare real algebraic numbers by
    precomputing generalized Sturm sequences, thus avoiding iterative
    methods; the method, moreover handles all degenerate cases. Our first
    contribution is the determination of rational isolating points, as
    functions of the coefficients, between any pair of real roots. Our
    second contribution is to exploit invariants and Bezoutian
    subexpressions in writing the sequences, in order to reduce bit
    complexity. The degree of the tested quantities in the input
    coefficients is optimal for degree up to 3, and for degree 4 in
    certain cases. Our methods readily apply to real solving of pairs of
    quadratic equations, and to sign determination of polynomials over
    algebraic numbers of degree up to 4. Our third contribution is an
    implementation in a new module of library SYNAPS v2.1. It improves
    significantly upon the efficiency of certain publicly available
    implementations: Rioboo’s approach on AXIOM, the package of
    Guibas-Karavelas-Russel, and CORE v1.6, MAPLE v9, and SYNAPS
    v2.0. Some existing limited tests had shown that it is faster than
    commercial library LEDA v4.5 for quadratic algebraic numbers.",
  paper = "Emir04.pdf",
  keywords = "axiomref"
}

@misc{Engl14,
  author = "England, Matthew and Wilson, David and Bradford, Russell and
            Davenport, James H.",
  title = {{Using the Regular Chains Library to build cylindrical algebraic
           decompositions by projecting and lifting}},
  link = "\url{https://arxiv.org/pdf/1405.6090.pdf}",
  year = "2014",
  abstract =
    "Cylindrical algebraic decomposition (CAD) is an important tool, both
    for quantifier elimination over the reals and a range of other
    applications. Traditionally, a CAD is built through a process of
    projection and lifting to move the problem within Euclidean spaces of
    changing dimension. Recently, an alternative approach which first
    decomposes complex space using triangular decomposition before
    refining to real space has been introduced and implemented within the
    Regular-Chains Library of Maple. We here describe a freely available
    package ProjectionCAD which utilises the routines within the
    RegularChains Library to build CADs by projection and lifting. We
    detail how the projection and lifting algorithms were modified to
    allow this, discuss the motivation and survey the functionality of the
    package.",
  paper = "Engl14.pdf"
}

@misc{Engl14a,
  author = "England, Matthew and Bradford, Russell and Davenport, James H. and
            Wilson, David",
  title = {{Choosing a variable ordering for truth-table invariant cylindrical
           algebraic decomposition by incremental triangular decomposition}},
  link = "\url{https://arxiv.org/pdf/1405.6094.pdf}",
  year = "2014",
  abstract =
    "Cylindrical algebraic decomposition (CAD) is a key tool for solving
    problems in real algebraic geometry and beyond. In recent years a new
    approach has been developed, where regular chains technology is used
    to first build a decomposition in complex space. We consider the
    latest variant of this which builds the complex decomposition
    incrementally by polynomial and produces CADs on whose cells a
    sequence of formulae are truth-invariant. Like all CAD algorithms the
    user must provide a variable ordering which can have a profound impact
    on the tractability of a problem. We evaluate existing heuristics to
    help with the choice for this algorithm, suggest improvements and then
    derive a new heuristic more closely aligned with the mechanics of the
    new algorithm.",
  paper = "Engl14a.pdf"
}

@misc{Engl14b,
  author = "England, Matthew and Bradford, Russell and Chen, Changbo and
            Davenport, James H. and Maza, Marc Moreno",
  title = {{Problem formulation for truth-table invariant cylindrical
           algebraic decomposition by incremental triangular decomposition}},
  link = "\url{https://arxiv.org/pdf/1404.6371.pdf}",
  year = "2014",
  abstract =
    "Cylindrical algebraic decompositions (CADs) are a key tool for
    solving problems in real algebraic geometry and beyond.  We recently
    presented a new CAD algorithm combining two advances: truth-table
    invariance, making the CAD invariant with respect to the truth of
    logical formulae rather than the signs of polynomials; and CAD
    construction by regular chains technology, where first a complex
    decomposition is constructed by refining a tree incrementally by
    constraint. We here consider how best to formulate problems for input
    to this algorithm. We focus on a choice (not relevant for other CAD
    algorithms) about the order in which constraints are presented. We
    develop new heuristics to help make this choice and thus allow the
    best use of the algorithm in practice. We also consider other choices
    of problem formulation for CAD, as discussed in CICM 2013, revisiting
    these in the context of the new algorithm.",
  paper = "Engl14b.pdf"
}

@article{Engl15,
  author = "England, M. and Bradford, R. and Davenport, J. H.",
  title = {{Improving the use of equational constraints in cylindrical 
           algebraic decomposition}},
  journal = "ISSAC 15",
  year = "2015",
  series = "LNCS 7961",
  publisher = "ACM",
  link = "\url{http://opus.bath.ac.uk/42451/}",
  abstract = "
    When building a cylindrical algebraic decomposition (CAD) savings can
    be made in the presence of an equational constraint (EC): an equation
    logically implied by a formula.

    The present paper is concerned with how to use multiple ECs,
    propagating those in the input throughout the projection set. We
    improve on the approach of McCallum in ISSAC 2001 by using the reduced
    projection theory to make savings in the lifting phase (both to the
    polynomials we lift with and the cells lifted over). We demonstrate
    the benefits with worked examples and a complexity analysis.",
  paper = "Engl15.pdf"
}

@techreport{Fitc87,
  author = "Fitchas, N. and Galligo, A. and Morgenstern, J.",
  title = {{Algorithmes repides en s\'equential et en parallele pour
           l'\'elimination de quantificateurs en g\'eom\'etrie
           \'el\'ementaire}},
  type = "technical report",
  institution = {UER de Math\'ematiques Universite de Paris VII},
  year = "1987"
}

@inproceedings{Gonz89,
  author = "Gonzalex, Laureano and Henri, Lombardi and Recio, Tomas and
            Roy, Marie-Francoise",
  title = {{Sturm-Habicht sequence}},
  booktitle = "Proc. ACM-SIGSAM 1989",
  year = "1989",
  pages = "136-146",
  isbn = "0-89791-325-6",
  abstract =
    "Formal computations with inequalities is a subject of general interest
    in computer algebra. In particular it is fundamental in the
    parallelisation of basic algorithms and quantifier elimination for real
    closed filed ([BKR], [HRS]).
    
    In $\S{}I$ we give a generalisation of Sturm theorem essentially due to
    Sylvester which is the key for formal computations with inequalities.
    Our result is an improvement of previously known results (see [BKR])
    since no hypotheses have to be made on polynomials.
    
    In $\S{}II$ we study the subresultant sequence. We precise some of the
    classical definitions in order to avoid some problems appearing in the
    paper by Loos ([L]) and study specialisation properties in detail.
    
    In $\S{}III$ we introduce the Sturm-Habicht sequence, which generalises
    Habicht's work ([H]). This new sequence obtained automatically from a
    subresultant sequence has some remarkable properties:
    \begin{itemize}
    \item it gives the same information than the Sturm sequence, and this
    information may be recovered by looking only at its principal 
    coefficients
    \item it can be computed by ring operations and exact divisions only,
    in polynomial time in case of integer coefficients, eventually by
    modular methods
    \item it has goos specialisation properties
    \end{itemize}
    
    Finally in $\S{}IV$ we give some information about applications and
    implementation of the Sturm-Habicht sequence.",
  paper = "Gonz89.pdf"
}

@inproceedings{Gonz98,
  author = "Gonzalex-Vega, L.",
  title = {{A combinatorial algorithm solving some quantifier elimination
           problems}},
  booktitle = "Quantifier Elimination and Cylindrical Algebraic 
               Decomposition",
  isbn = "3-211-82794-3",
  year = "1998",
  pages = "365-374",
}

@article{Grig88,
  author = "Grigor'ev, D. Yu. and Vorobjov, N. N.",
  title = {{Solving systems of polynomial inequalities in 
            subexponential time}},
  journal = "J. Symbolic Computation",
  volume = "5",
  number = "1-2",
  pages = "37-64",
  year = "1988",
  abstract =
    "Let the polynomials $f_1,\ldots,f_k \in \mathbb{Z}[X_1,\ldots,X_n]$ 
    have degrees $deg(f_i)<d$ and absolute value of any coefficient of less
    than or equal to $s^M$ for all $1 \le i \le k$. We describe an algorithm
    which recognizes the existence of a real solution of the system of
    inequalities $f_1 > 0,\ldots,f_k \ge 0$. In the case of a positive
    answer the algorithm constructs a certain finite set of solutions
    (which is, in fact, a representative set for the family of components
    of connectivity of the set of all real solutions of the system). The
    algorithm runs in time polynomial in $M(kd)^{n^2}$. The previously
    known upper time bound for this problem was $(Mkd)^{2^{O(n)}}$.",
  paper = "Grig88.pdf"
}

@article{Grig88a,
  author = "Grigor'ev, D. Yu.",
  title = {{The complexity of deciding Tarski algebra}},
  journal = "J. Symbolic Computation",
  volume = "5",
  number = "1-2",
  pages = "65-108",
  year = "1988",
  abstract =
    "Let a formula of Tarski algebra contain $k$ atomic subformulas of the
    kind $(f_i \ge 0)$, $1 \le i \le k$, where the polynomials
    $f_i \in \mathbb{Z}[X_1,\ldots,X_n]$ have degrees $deg(f_i)<d$, let
    $2^M$ be an upper bound for the absolute value of every coefficient
    of the polynomials $f_i$, $1 \le i \le k$, let $a \le n$ be the number
    of quantifier alternations in the prenex form of the formula. A decision
    method for Tarski algebra is described with the running time polynomial in 
    $M(kd)^{(O(n))^{4a-2}}$. Previously known decision procedures have a
    time complexity polynomial in $(Mkd)^{2^{O(n)}}$",
  paper = "Grig88a.pdf"
}

@inproceedings{Hein89,
  author = "Heintz, J. and Roy, M-F. and Solerno, P.",
  title = {{On the complexity of semialgebraic sets}},
  booktitle = "Proc. IFIP",
  pages = "293-298",
  year = "1989"
}

@phdthesis{Hong90a,
  author = "Hong, Hoon",
  title = {{Improvements in CAD-based Quantifier Elimination}},
  school = "Ohio State University",
  year = "1990",
  abstract =
    "Many important mathematical and applied mathematical problems can be
    formulated as quantifier elimination problems (QE) in elementary
    algebra and geometry. Among several proposed QE methods, the best one
    is the CAD-based method due to G. E. Collins. The major contributions
    of this thesis are centered around improving each phase of the
    CAD-based method: the projection phase, the truth-invariant CAD
    construction phase, and the solution formula construction phase
    
    Improvements in the projection phase. By generalizing a lemma on which
    the proof of the original projection operation is based, we are able
    to reduce the size of the projection set significantly, and thus speed
    up the whole QE process.
    
    Improvements in the truth-invariant CAD construction phase. By
    intermingling the stack constructions with the truth evaluations, we
    are usually able to complete quantifier elimination with only a
    partially built CAD, resulting in significant speedup.
    
    Improvements in the solution formula construction phase. By
    constructing defining formulas for a collection of cells instead of
    individual cells, we are often able to produce simple solution
    formulas, without incurring the enormous cost of augmented projection.
    
    The new CAD-based QE algorithm, which integrates all the improvements
    above, has been implemented and tested on ten QE problems from diverse
    application areas. The overall speedups range from 2 to perhaps
    300,000 times at least
    
    We also implemented D. Lazard's recent improvement on the projection
    phase. Tests on the ten QE problems show additional speedups by up to
    1.8 times."
}

@inproceedings{Hong90,
  author = "Hong, Hoon",
  title = {{An improvement of the projection operator in cylindrical 
           algebraic decomposition}},
  booktitle = "ISSAC'90",
  publisher = "ACM",
  pages = "261-264",
  year = "1990",
  abstract = 
    "The cylindrical algebraic decomposition (CAD) of Collins (1975)
    provides a potentially powerful method for solving many important
    mathematical problems, provided that the required amount of
    computation can be sufficiently reduced. An important component 
    of the CAD method is the projection operation. Given a set $A$ of 
    $r$-variate polynomials, the projection operation produces a 
    certain set $P$ of ($r − l$)-variate polynomials such that a CAD 
    of $r$-dimensional space for $A$ can be constructed from a CAD 
    of ($r − 1$)-dimensional space for $P$. The CAD algorithm begins 
    by applying the projection operation repeatedly, beginning 
    with the input polynomials, until univariate polynomials are
    obtained. This process is called the projection phase."
}

@inproceedings{Hong93,
  author = "Hong, Hoon",
  title = {{Quantifier elimination for formulas constrained by 
           quadratic equations}},
  booktitle = "Proc. ISSAC'93",
  year = "1993",
  pages = "264-274",
  publisher = "ACM",
  isbn = "0-89791-604-2",
  abstract =
    "An algorithm is given for constructing a quantifier free formula
    (a boolean expression of polynomial equations and inequalitier)
    equivalent to a given formula of the form:
    $(\exists x \in \mathbb{R})[a_x^2+a_1x+a_0=0 \land F]$, where $F$
    is a quantifier free formula in $x_1,\ldots,x_r,x,$ and
    $a_2,a_1,a_0$ are polynomials in $x_1,\ldots,x_r$ with real
    coefficients such that the system
    $\{a_2=0,a_1=0,a_0=0\}$ has no solution in $\mathbb{R}^r$.
    Formulas of this form frequently occur in the context of
    constraint logic programming over the real numbers. The output
    formulas are made of resultants and two variants, which we call
    {\sl trace} and {\sl slope} resultants. Both of these variant
    resultants can be expressed as determinants of certain matrices.",
  paper = "Hong93.pdf"
}

@article{Hong93a,
  author = "Hong, Hoon",
  title = {{Quantifier Elimination for Formulas Constrained by 
           Quadratic Equations via Slope Resultants}},
  journal = "The Computer Journal",
  volume = "36",
  number = "5",
  year = "1993",
  pages = "439-449",
  abstract =
    "
    An algorithm is given for eliminating the quantifier from a formula
    $(\exists x \in \mathbb{R})[a_x^2+a_1x+a_0=0 \land F]$, where $F$
    is a quantifier free formula in $x_1,\ldots,x_r,x$ and
    $a_2,a_1,a_0$ are polynomials in $x_1,\ldots,x_r$ with real
    coefficients such that the system 
    $\{a_2=0,a_1=0,a_0=0\}$ has no solution in $\mathbb{R}^r$.
    The output formulas are made of resultants and their variants,
    which we call {\sl slope} resultants. The slope resultants can be,
    like the resultants, expressed as determinants of certain matrices.
    If we allow the determinant symbol in the output the computing time
    of the algorithm is {\sl linear} in the length of the input. If not,
    the computing time is dominated by $N(n^{2r+1}\ell+n^{2r}\ell^2)$
    where $N$ is the number of polynomials in the input formula, $r$
    is the number of variables, $n$ is the maximum of the degrees for
    every variable, and $\ell$ is the maximum of the integer 
    coefficient bit lengths. Experimants with implementation suggest
    that the algorithm is sufficiently efficient to be useful in practice.",
  paper = "Hong93a.pdf"
}

@article{Hong94,
  author = "Hong, Hoon and Stahl, V.",
  title = {{Safe start regions by fixed points and tightening}},
  journal = "Computing",
  year = "1994",
  volume = "53",
  number = "3",
  pages = "323-335",
  abstract = 
    "In this paper, we present a method for finding safe starting regions
    for a given system of non-linear equations. The method is an
    improvement of the usual method which is based on the fixed point
    theorem. The improvement is obtained by enclosing the components of
    the equation system by univariante interval polynomials whose zero
    sets are found. This operation is called ``ightening''. Preliminary
    experiments show that the tightening operation usually reduces the
    number of bisections, and thus the computing time. The reduction seems
    to become more dramatic when the number of variables increases."
}

@inproceedings{Hong98,
  author = "Hong, Hoon",
  title = {{Simple Solution Formula Construction in Cylindrical
           Algebraic Decomposition Based Quantifier Elimination}},
  booktitle = "Quantifier Elimination and Cylindrical Algebraic
               Decomposition",
  publisher = "Springer",
  year = "1998",
  isbn = "3-211-82794-3",
  pages = "201-219",
  comment = "also ISSAC'92 pages 177-188, 1992",
  abstract =
    "In this paper, we present several improvements to the last step
    (solution formula construction step) of Collins' cylindrical
    algebraic decomposition based quantifier elimination algorithm.
    The main improvements are
    \begin{itemize}
    \item that it does {\sl not} use the expensive augmented projection
    used by Collins' original algorithm, and
    \item that it produces {\sl simple} solution formulas by using
    three-valued logic minimization
    \end{itemize}
    
    For example, for the quadratic problem studied by Arnon, Mignotte,
    and Lazard, the solution formula produced by the original algorithm
    consists of 401 atomic formulas, but that by the improved algorithm
    consists of 5 atomic formulas.",
  paper = "Hong98.pdf"
}

@inproceedings{Hong98a,
  author = "Hong, Hoon and Sendra, J. Rafael",
  title = {{Computation of Variant Results}},
  booktitle = "Quantifier Elimination and Cylindrical Algebraic
               Decomposition",
  publisher = "Springer",
  year = "1998",
  isbn = "3-211-82794-3",
  pages = "327-337"
}

@misc{Hong05,
  author = "Hong, Hoon",
  title = {{Tutorial on CAD}},
  year = "2005",
  paper = "Hong05.pdf"
}

@article{Hong12,
  author = "Hong, Hoon",
  title = {{Variant Quantifier Elimination}},
  journal = "J. of Symbolic Computation",
  volume = "47",
  number = "7",
  year = "2012",
  pages = "883-901",
  abstract =
    "We describe an algorithm (VQE) for a variant of the real quantifier
    elimination problem (QE). The variant problem requires the input to
    satisfy a certain {\sl extra condition}, 
    and allows the output to be {\sl almost}
    equivalent to the input. The motivation/rationale for studying such a
    variant QE problem is that many quantified formulas arising in
    applications do satisfy the extra conditions. Furthermore, in most
    applications, it is sufficient that the output formula is almost
    equivalent to the input formula. The main idea underlying the
    algorithm is to substitute the repeated projection step of CAD by a
    single projection without carrying out a parametric existential
    decision over the reals. We find that the algorithm can tackle
    important and challenging problems, such as numerical stability
    analysis of the widely-used MacCormack’s scheme. The problem has been
    practically out of reach for standard QE algorithms in spite of many
    attempts to tackle it. However, the current implementation of VQE can
    solve it in about 12 hours. This paper extends the results reported at
    the conference ISSAC 2009.",
  paper = "Hong12.pdf"
}

@misc{Jova12,
  author = "Jovanovic, Dejan and de Moura, Leonardo",
  title = {{Solving Non-Linear Arithmetic}},
  year = "2012",
  link = "\url{http://cs.nyu.edu/~dejan/nonlinear/ijcar2012_extended.pdf}",
  comment = "see: http://cs.nyu.edu/~dejan/nonlinear/",
  abstract =
    "We present a new algorithm for deciding satisfiability of non-linear
    arithmetic constraints. The algorithm performs a Conflict-Driven
    Clause Learning (CDCL)- style search for a feasible assignment, while
    using projection operators adapted from cylindrical algebraic
    decomposition to guide the search away from the conflicting states.",
  paper = "Jova12.pdf"
}

@article{Kaue11,
  author = "Kauers, Manuel",
  title = {{How to use Cylindrical Algebraic Decomposition}},
  journal = {S\'emminaire Lotharingien de Combinatoire},
  volume = "65",
  year = "2011",
  comment = "Article B65a",
  abstract =
    "We take some items from a textbook on inequalities and show how to
    prove them with computer algebra using the Cylindrical Algebraic
    Decomposition algorithm.  This is an example collection for standard
    applications of this algorithm, intended as a guide for potential users.",
  paper = "Kaue11.pdf",
  keywords = "printed"
}

@book{LaVa06,
  author = "LaValle, Steven M.",
  title = {{Planning Algorithms}},
  year = "2006",
  publisher = "Cambridge University Press"
}

@article{Loos93,
  author = "Loos, Rudiger and Weispfenning, Volker",
  title = {{Applying linear quantifier elimination}},
  journal = "The Computer Journal",
  volume = "36",
  number = "5",
  year = "1993",
  abstract =
    "The linear quantifier elimination algorithm for ordered fields in
    [15] is applicable to a wide range of examples involving even
    non-linear parameters. The Skolem sets of the original algorithm
    are generalized to elimination sets whose size is linear in the
    number of atomic formulas, whereas the size of Skolem sets is 
    quadratic in this number. Elimiation sets may contain non-standard
    terms which enter the computation symoblically. Many cases can be
    treated by special methods improving further the empirical computing
    times.",
  paper = "Loos93.pdf"
}

@article{Mahb07,
  author = "Mahboubi, Assia",
  title = {{Implementing the cylindrical algebraic decomposition within 
           the Coq system}},
  journal = "Math. Struct. in Comp. Science",
  volume = "17",
  year = "2007",
  pages = "99-127",
  abstract = "The Coq system is a Curry-Howard based proof assistant. 
    Therefore, it contains a full functional, strongly typed programming
    language, which can be used to enhance the system with powerful
    automation tools through the implementation of reflexive tactics. 
    We present the implementation of a cylindrical algebraic decomposition
    algorithm within the Coq system, whose certification leads to a proof
    producing decision procedure for the first-order theory of real numbers.",
  paper = "Mahb07.pdf"
}

@phdthesis{Mcca84,
  author = "McCallum, Scott",
  title = {{An Improved Projection Operator for Cylindrical 
           Algebraic Decomposition}},
  school = "University of Wisconsin-Madison",
  year = "1984",
  comment = "Computer Sciences Technical Report \#578",
  link = "\url{ftp://ftp.cs.wisc.edu/pub/techreports/1985/TR578.pdf}",
  abstract =
    "A fundamental algorithm pertaining to the solution of polynomial
    equations in several variables is the {\sl cylindrical algebraic
    decomposition (cad)} algorithm due to G.E. Collins. Given as input
    a set $A$ of integral polynomials in $r$ variables, the cad 
    algorithm produces a decomposition of the euclidean space of $r$
    dimensions into cells, such that each polynomial in $A$ is 
    invariant in sign throughout each of the cells in the decomposition.
    
    A key component of the cad algorithm is the projection operation:
    the {\sl projection} of a set $A$ of $r$-variate polynomials is
    defined to be a certain set $P$ of $(r-1)$-variate polynomials.
    The solution set, or variety, of the polynomials in $P$ comprises
    a projection in the geometric sense of the variety of $A$. The cad
    algorithm proceeds by forming successive projections of the input
    set $A$, each projection resulting in the elimination of one
    variable.
    
    This thesis is concerned with a refinement to the cad algorithm,
    and to its projection operation in particular. It is shown, using
    a theorem from real algebraic geometry, that the original 
    projection set that Collins used can be substantially reduced in
    size, without affecting its essential properties. The results of
    theoretical analysis and empirical observations suggest that the
    reduction in the projection set size leads to an overall decrease
    in the computing time of the cad algorithm.",
  paper = "Mcca84.pdf"
}

@article{Mcca88,
  author = "McCallum, Scott",
  title = {{An improved projection operation for cylindrical algebraic
           decomposition of three-dimensional space}},
  journal = "J. Symbolic Computation",
  volume = "5",
  number = "1-2",
  year = "1998",
  pages = "141-161",
  abstract = 
    "A key component of the cylindrical algebraic decomposition (cad)
    algorithm of Collins (1975) is the projection operation: the
    {\sl projection} of a set $A$ of $r$-variate polynomials is defined 
    to be a certain set or $(r-1)$-variate polynomials. Tile zeros of the
    polynomials in the projection comprise a ``shadow'' of the critical
    zeros of $A$. The cad algorithm proceeds by forming successive
    projections of the input set $A$, each projection resulting in the
    elimination of one variable. This paper is concerned with a refinement
    to the cad algorithm, and to its projection operation in
    particular. It is shown, using a theorem from complex analytic
    geometry, that the original projection set for trivariate polynomials
    that Collins used can be substantially reduced in size, without
    affecting its essential properties. Observations suggest that the
    reduction in the projection set size leads to a substantial decrease
    in the computing time of the cad algorithm.",
  paper = "Mcca88.pdf"
}

@article{Mcca93,
  author = "McCallum, Scott",
  title = {{Solving Polynomial Strict Inequalities Using Cylindrical 
           Algebraic Decomposition}},
  journal = "The Computer Journal",
  volume = "36",
  number = "5",
  pages = "432-438",
  year = "1993",
  abstract =
    "We consider the problem of determining the consistency over the real
    number of a system of integral polynomial strict inequalities. This
    problem has applications in geometric modelling. The cylindrical
    algebraic decomposition (cad) algorithm [2] can be used to solve this
    problem, though not very efficiently. In this paper we present a less
    powerful version of the cad algorithm which can be used to solve the
    consistency problem for conjunctions of strict inequalities, and which
    runs considerably faster than the original method applied to this
    problem. In the case that a given conjunction of strict inequalities
    is consistent, the modified cad algorithm constructs solution points
    with rational coordinates.",
  paper = "Mcca93.pdf"
}

@article{Mcca02,
  author = "McCallum, Scott and Collins, George E.",
  title = {{Local box adjacency algorithms for cylindrical algebraic 
           decompositions}},
  journal = "J. of Symbolic Computation",
  volume = "33",
  number = "3",
  year = "2002",
  pages = "321-342",
  publisher = "Elsevier",
  abstract = 
    "We describe new algorithms for determining the adjacencies between
    zero-dimensional cells and those one-dimensional cells that are
    sections (not sectors) in cylindrical algebraic decompositions
    (cad). Such adjacencies constitute a basis for determining all other
    cell adjacencies. Our new algorithms are local, being applicable to a
    specified 0D cell and the 1D cells described by specified
    polynomials. Particularly efficient algorithms are given for the 0D
    cells in spaces of dimensions two, three and four. Then an algorithm
    is given for a space of arbitrary dimension. This algorithm may on
    occasion report failure, but it can then be repeated with a modified
    isolating interval and a likelihood of success.",
  paper = "Mcca02.pdf"
}

@inproceedings{Pede93,
  author = "Pedersen, P. and Roy, F.-F. and Szpirglas, A.",
  title = {{Counting real zeroes in the multivariate case}},
  booktitle = "Proc. MEGA'92: Computational Algebraic Geometry",
  volume = "109",
  pages = "203-224",
  year = "1993",
  abstract =
    "In this paper we show, by generalizing Hermite’s theorem to the
    multivariate setting, how to count the number of real or complex
    points of a discrete algebraic set which lie within some algebraic
    constraint region. We introduce a family of quadratic forms determined
    by the algebraic constraints and defined in terms of the trace from
    the coordinate ring of the variety to the ground field, and we show
    that the rank and signature of these forms are sufficient to determine
    the number of real points lying within a constraint region. In all
    cases we count geometric points, which is to say, we count points
    without multiplicity. The theoretical results on these quadratic forms
    are more or less classical, but forgotten too, and can be found also
    in [3].
    
    We insist on effectivity of the computation and complexity analysis:
    we show how to calculate the trace and signature using Gröbner bases,
    and we show how the information provided by the individual quadratic
    forms may be combined to determine the number of real points
    satisfying a conjunction of constraints. The complexity of the
    computation is polynomial in the dimension as a vector space of the
    quotient ring associated to the defining equations. In terms of the
    number of variables, the complexity of the computation is singly
    exponential. The algorithm is well parallelizable.
    
    We conclude the paper by applying our machinery to the problem of
    effectively calculating the Euler characteristic of a smooth
    hypersurface."
}  

@article{Rats02,
  author = "Ratschan, Stefan",
  title = {{Approximate quantified constraint solving by cylindrical box 
           decomposition}},
  year = "2002",
  journal = "Reliable Computing",
  volume = "8",
  number = "1",
  pages = "21-42",
  abstract =
    "This paper applies interval methods to a classical problem in
    computer algebra. Let a quantified constraint be a first-order formula
    over the real numbers. As shown by A. Tarski in the 1930's, such
    constraints, when restricted to the predicate symbols $<$, $=$ and
    function symbols $+$, $\times$, are in general solvable. 
    However, the problem
    becomes undecidable, when we add function symbols like
    sin. Furthermore, all exact algorithms known up to now are too slow
    for big examples, do not provide partial information before computing
    the total result, cannot satisfactorily deal with interval constants
    in the input, and often generate huge output. As a remedy we propose
    an approximation method based on interval arithmetic. It uses a
    generalization of the notion of cylindrical decomposition—as
    introduced by G. Collins. We describe an implementation of the method
    and demonstrate that, for quantified constraints without equalities,
    it can efficiently give approximate information on problems that are
    too hard for current exact methods."
}

@article{Rene92,
  author = "Renegar, James",
  title = {{On the computational complexity and geometry of the first-order
           theory of the reals. Part I: Introduction}},
  journal = "J. of Symbolic Computation",
  volume = "13",
  number = "3",
  year = "1992",
  pages = "255-299",
  link = 
    "\url{http://www.sciencedirect.com/science/article/pii/S0747717110800033}",
  abstract =
    "This series of papers presents a complete development and complexity
    analysis of a decision method, and a quantifier elimination method,
    for the first order theory of the reals. The complexity upper bounds
    which are established are the best presently available, both for
    sequential and parallel computation, and both for the bit model of
    computation and the real number model of computation; except for the
    bounds pertaining to the sequential decision method in the bit model
    of computation, all bounds represent significant improvements over
    previously established bounds.",
  paper = "Rene92.pdf"
}

@article{Rene92a,
  author = "Renegar, James",
  title = {{On the computational complexity and geometry of the first-order
           theory of the reals. Part II: The general decision problem}},
  journal = "J. of Symbolic Computation",
  volume = "13",
  number = "3",
  year = "1992",
  pages = "301-327",
  link = 
    "\url{http://www.sciencedirect.com/science/article/pii/S0747717110800045}",
  abstract =
    "This series of papers presents a complete development and complexity
    analysis of a decision method, and a quantifier elimination method,
    for the first order theory of the reals. The complexity upper bounds
    which are established are the best presently available, both for
    sequential and parallel computation, and both for the bit model of
    computation and the real number model of computation; except for the
    bounds pertaining to the sequential decision method in the bit model
    of computation, all bounds represent significant improvements over
    previously established bounds.",
  paper = "Rene92a.pdf"
}

@article{Rene92b,
  author = "Renegar, James",
  title = {{On the computational complexity and geometry of the first-order
           theory of the reals. Part III: Quantifier elimination}},
  journal = "J. of Symbolic Computation",
  volume = "13",
  number = "3",
  year = "1992",
  pages = "329-352",
  link = 
    "\url{http://www.sciencedirect.com/science/article/pii/S0747717110800057}",
  abstract =
    "This series of papers presents a complete development and complexity
    analysis of a decision method, and a quantifier elimination method,
    for the first order theory of the reals. The complexity upper bounds
    which are established are the best presently available, both for
    sequential and parallel computation, and both for the bit model of
    computation and the real number model of computation; except for the
    bounds pertaining to the sequential decision method in the bit model
    of computation, all bounds represent significant improvements over
    previously established bounds.",
  paper = "Rene92b.pdf"
}

@InCollection{Rich98,
  author = "Richardson, Daniel",
  title = {{Local Theories and Cylindrical Decomposition}},
  booktitle = "Quantifier Elimination and Cylindrical Algebraic Decomposition",
  publisher = "Springer",
  year = "1998",
  isbn = "3-211-82794-3",
  abstract =
    "There are many interesting problems which can be expressed in the
    language of elementary algebra, or in one of its extensions, but which
    do not really depend on the coordinate system, and in which the 
    variables can be restricted to an arbitrary small neighborhood of some
    point. It seems that it ought to be possible to use cylindrical
    decomposition techniques to solve such problems, taking advantage
    of their special features. This article attempts to do this, but
    many unsolved problems remain.",
  keywords = "axiomref"
}

@misc{Riobxx,
  author = "Rioboo, Renaud",
  title = {{Cylindrical Algebraic Decomposition class notes}},
  link = "\url{http://people.bath.ac.uk/masjhd/TRITA.pdf}",
  year = "2016",
  abstract =
    "This report describes techniques for resolving systems of polynomial
    equations and inequalities. The general technique is {\sl cylindrical
    algebraic decomposition}, which decomposes space into a number of
    regions, on each of which the equations and inequalities have the same
    sign. Most of the report is spent describing the algebraic and
    algorithmic pre-requisites (resultants, algebraic numbers, Sturm
    sequences, etc.), and then describing the method, first in two
    dimensions and then in an arbitrary number of dimensions",
  paper = "Riobxx.pdf"
}

@InProceedings{Riob92,
  author = "Rioboo, Renaud",
  title = {{Real algebraic closure of an ordered field, 
           Implementation in Axiom}},
  booktitle = "Proc. ISSAC 92",
  series = "ISSAC 92",
  year = "1992",
  isbn = "978-3-540-75186-1",
  location = "Berkeley, California",
  pages = "206-215",
  algebra = "\newline\refto{domain RECLOS RealClosure}",
  abstract = 
    "Real algebraic numbers appear in many Computer Algebra problems.  For
    instance the determination of a cylindrical algebraic decomposition
    for an euclidean space requires computing with real algebraic numbers.
    This paper describes an implementation for computations with the real
    roots of a polynomial. This process is designed to be recursively
    used, so the resulting domain of computation is the set of all real
    algebraic numbers. An implementation for the real algebraic closure
    has been done in Axiom (previously called Scratchpad).",
  paper = "Riob92.pdf",
  keywords = "axiomref",
  beebe = "Rioboo:1992:RAC"
}

@article{Seid54,
  author = "Seidenberg, A.",
  title = {{A New Decision Method for Elementary Algebra}},
  journal = "Annals of Mathematics",
  volume = "60",
  number = "2",
  year = "1954",
  pages = "365-374",
  abstract =
    "A. Tarski [4] has given a decision method for elementary algebra. In
     essence this comes to giving an algorithm for deciding whether a
     given finite set of polynomial inequalities has a solution. Below we
     offer another proof of this result of Tarski. The main point of our
     proof is accomplished upon showing how to decide whether a given
     polynomial $f(x,y)$ in two variables, defined over the field 
     $\mathbb{R}$ 
     of rational numbers, has a zero in a real-closed field $\mathbb{K}$ 
     containing $R^1$.
     This is done in \S{}2, but for purposes of induction it is necessary to
     consider also the case that the coefficients of $f(x,y)$ involve
     parameters; the remarks in \S{}3 will be found sufficient for this
     point. In \S{}1, the problem is reduced to a decision for equalities,
     but an induction (on the number of unknowns) could not possibly be
     carried out on equalities alone; we consider a simultaneous system
     consisting of one equality $f(x,y) = 0$ and one inequality $F(x) \ne 0$.
     Once the decision for this case is achieved, at least as in \S{}3,
     the induction is immediate.",
  paper = "Seid54.pdf"
}

@misc{Tars48,
  author = "Tarski, Alfred",
  title = {{A Decision Method for Elementary Algebra and Geometry}},
  year = "1948",
  link = 
   "\url{https://www.rand.org/content/dam/rand/pubs/reports/2008/R109.pdf}",
  paper = "Tars48.pdf"
}

@article{Weis88,
  author = "Weispfenning, V.",
  title = {{The complexity of linear problems in fields}},
  journal = "J. of Symbolic Computation",
  volume = "5",
  number = "1-2",
  year = "1988",
  pages = "3-27",
  link = "\url{http://www.sciencedirect.com.proxy.library.cmu.edu/science/article/pii/S0747717188800038}",
  abstract =
    "We consider linear problems in fields, ordered fields, discretely
    valued fields (with finite residue field or residue field of
    characteristic zero) and fields with finitely many independent
    orderings and discrete valuations. Most of the fields considered will
    be of characteristic zero. Formally, linear statements about these
    structures (with parameters) are given by formulas of the respective
    first-order language, in which all bound variables occur only
    linearly. We study symbolic algorithms 
    ({\sl linear elimination procedures})
    that reduce linear formulas to linear formulas of a very simple form,
    i.e. quantifier-free linear formulas, and algorithms 
    ({\sl linear decision procedures}) 
    that decide whether a given linear sentence holds in all
    structures of the given class. For all classes of fields considered,
    we find linear elimination procedures that run in double exponential
    space and time. As a consequence, we can show that for fields (with
    one or several discrete valuations), linear statements can be
    transferred from characteristic zero to prime characteristic $p$,
    provided $p$ is double exponential in the length of the statement. (For
    similar bounds in the non-linear case, see Brown, 1978.) We find
    corresponding linear decision procedures in the Berman complexity
    classes $\bigcup_{c\in N} STA(*,2^{cn},dn)$
    for $d = 1, 2$. In particular, all these procedures run in
    exponential space. The technique employed is quantifier elimination
    via Skolem terms based on Ferrante and Rackoff (1975). Using ideas of
    Fischer and Rabin (1974), Berman (1977), Fürer (1982), we establish
    lower bounds for these problems showing that our upper bounds are
    essentially tight. For linear formulas with a bounded number of
    quantifiers all our algorithms run in polynomial time. For linear
    formulas of bounded quantifier alternation most of the algorithms run
    in time $2^{O(n^k)}$ for fixed $k$.",
  paper = "Weis88.pdf"
}

@article{Weis92,
  author = "Weispfenning, V.",
  title = {{Comprehensive Groebner bases}},
  journal = "J. Symbolic Computation",
  volume = "14",
  number = "1",
  year = "1992",
  pages = "1-29",
  abstract =
    "Let $K$ be an integral domain and let $S$ be the polynomial ring
    $K[U_1,\ldots,U_m; X_1,\ldots,X_n]$. For any finite $F \subseteq S$,
    we construct a comprehensive Groebner basis of the ideal $Id(F)$, a
    finite ideal basis of $Id(F)$ that is a Groebner basis of $Id(F)$
    in $K^\prime[X_1,\ldots,X_n]$ for every specialization of the 
    parameters $U_1,\ldots,U_m$ in an arbitrary field $K^1$. We show
    that this construction can be performed with the same worst case
    degree bounds in the main variable $X_i$, as for ordinary Groebner
    bases; moreover, examples computed in an ALDES/SAC-2 implementation
    show that the construction is of practical value. Comprehensive
    Groebner bases admit numerous applications to parametric problems
    in algebraic geometry; in particular, they yield a fast elimination
    of quantifier-blocks in algebraically closed fields",
  paper = "Weis92.pdf"
}

@inproceedings{Weis94,
  author = "Weispfenning, V.",
  title = {{Quantifier elimination for real algebra – the cubic case}},
  booktitle = "Proc ISSAC'94",
  publisher = "ACM",
  isbn = "0-89791-638-7",
  pages = "258-263",
  year = "1994",
  abstract =
    "We present a special purpose quantifier elimination method that 
    eliminates a quantifier $\exists x$ in formulas $\exists x(\rho)$
    where $\rho$ is a boolean combination of polynomial inequalities of
    degree $\le 3$ with respect to $x$. The method extends the virtual
    substitition of parameterized test points developed in 
    [Weispfenning 1, Loos and Weispf.] for the linear case and in 
    [Weispfenning 2] for the quadratic case. It has similar upper
    complexity bounds and offers similar advantages (relatively large
    preprocessing part, explicit parametric solutions). Small examples
    suggest that the method will be of practical significance.",
  paper = "Weis94.pdf"
}

@article{Weis97,
  author = "Weispfenning, V.",
  title = {{Quantifier elimination for real algebra - 
           the quadratic case and beyond}},
  journal = 
    "Applicable Algebra in Engineering, Communication and Computing",
  volume = "8",
  number = "2",
  year = "1997",
  pages = "85-101",
  link = "\url{https://link-springer-com.proxy.library.cmu.edu/article/10.1007%2Fs002000050055}",
  abstract =
    "We present a new, ``elementary'' quantifier elimination method for
    various special cases of the general quantifier elimination problem
    for the first-order theory of real numbers. These include the
    elimination of one existential quantifier $\exists x$ in front of
    quantifier-free formulas restricted by a non-trivial quadratic
    equation in $x$ (the case considered also in [7]), and more generally in
    front of arbitrary quantifier-free formulas involving only polynomials
    that are quadratic in $x$. The method generalizes the linear quantifier
    elimination method by virtual substitution of test terms in [9]. It
    yields a quantifier elimination method for an arbitrary number of
    quantifiers in certain formulas involving only linear and quadratic
    occurrences of the quantified variables. Moreover, for existential
    formulas $\phi$ of this kind it yields sample answers to the query
    represented by $\phi$. The method is implemented in REDUCE as part of the
    REDLOG package (see [4, 5]). Experiments show that the method is
    applicable to a range of benchmark examples, where it runs in most
    cases significantly faster than the QEPCAD package of Collins and
    Hong. An extension of the method to higher degree polynomials using
    Thom’s lemma is sketched.",
  paper = "Weis97.pdf"
}

@InCollection{Weis98,
  author = "Weispfenning, V.",
  title = {{A New Approach to Quantifier Elimination for Real Algebra}},
  booktitle = "Quantifier Elimination and Cylindrical Algebraic Decomposition",
  publisher = "Springer",
  year = "1998",
  isbn = "3-211-82794-3",
  abstract =
    "Quantifier elimination for the elementary formal theory of real
    numbers is a facinating area of research at the intersection of
    various field of mathematics and computer science, such as
    mathematical logic, commutative algebra and algebraic geometry,
    computer algebra, computational geometry and complexity
    theory. Originally the method of quantifier elimination was invented
    (among others by Th. Skolem) in mathematical logic as a technical tool
    for solving the decision problem for a formalized mathematical
    theory. For the elementary formal theory of real numbers (or more
    accurately of real closed fields) such a quantifier elimination
    procedure was established in the 1930s by A. Tarski, using an
    extension of Sturm's theorem of the 1830s for counting the number of
    real zeros of a univariate polynomial in a given interval. Since then
    an abundance of new decision and quantifier elimination methods for
    this theory with variations and optimizations has been published with
    the aim both of establishing the theoretical complexity of the problem
    and of finding methods that are of practical importance (see Arnon
    1988a and the discussion and references in Renegar 1992a, 1992b, 1992c
    for a comparison of these methods). For subproblems such as
    elimination of quantifiers with respect to variables, that are
    linearly or quadratically restricted, specialized methods have been
    developed with good success (see Weispfenning 1988, Loos and
    Weispfenning 1993; Hong 1992d; Weispfenning 1997).",
  keywords = "axiomref"
}

@misc{Wils14,
  author = "Wilson, David and Bradford, Russell and Davenport, James H. and
            England, Matthew",
  title = {{Cylindrical Algebraic Sub-Decompositions}},
  link = "\url{https://arxiv.org/pdf/1401.0647.pdf}",
  year = "2014",
  abstract = 
    "Cylindrical algebraic decompositions (CADs) are a key tool in real
    algebraic geometry, used primarily for eliminating quantifiers over
    the reals and studying semi-algebraic sets. In this paper we
    introduce cylindrical algebraic sub-decompositions (sub-CADs), which
    are subsets of CADs containing all the information needed to specify a
    solution for a given problem.  We define two new types of sub-CAD:
    variety sub-CADs which are those cells in a CAD lying on a designated
    variety; and layered sub-CADs which have only those cells of
    dimension higher than a specified value. We present algorithms to
    produce these and describe how the two approaches may be combined with
    each other and the recent theory of truth-table invariant CAD.  We
    give a complexity analysis showing that these techniques can offer
    substantial theoretical savings, which is supported by experimentation
    using an implementation in Maple.",
  paper = "Wils14.pdf"
}

@article{Zhao11,
  author = "Zhao, Ting and Wang, Dongming and Hong, Hoon",
  title = {{Solution formulats for cubic equations without or 
            with constraints}},
  journal = "J. Symbolic Computation",
  volume = "46",
  pages = "904-918",
  year = "2011",
  abstract =
    "We present a convention (for square/cubic roots) which provides
    correct interpretations of the Lagrange formula for all cubic
    polynomial equations with real coefficients. Using this convention, we
    also present a real solution formula for the general cubic equation
    with real coefficients under equality and inequality constraints.",
  paper = "Zhao11.pdf"
}

@article{Bern96,
  author = "Benardin, Laurent",
  title = {{A review of symbolic solvers}},
  journal = "SIGSAM Bull.",
  volume = "30",
  number = "1",
  pages = "9-20",
  year = "1996",
  abstract =
    "Solving equations and systems of equations symbolically is a key
    feature of every Computer Algebra System. This review examines the
    capabilities of the six best known general purpose systems to date in
    the area of general algebraic and transcendental equation
    solving. Areas explicitly not covered by this review are differential
    equations and numeric or polynomial system solving as special purpose
    systems exist for these kinds of problems. The aim is to provide a
    benchmark for comparing Computer Algebra Systems in a specific
    domain. We do not intend to give a rating of overall capabilities as
    for example in [9]. 1 The Contestants We compare six major Computer
    Algebra Systems. Axiom 2.0 [7], Derive 3.06 [1], Macsyma 420 [8],
    Maple V R4 [3], Mathematica 2.2 [10], MuPAD 1.2.9 [5] and Reduce 3.6
    [6]. When available, we tried to use the latest shipping version of
    each system. 2 The Problem Set The following table presents the set of
    80 problems that we used to evaluate the different solvers...",
  paper = "Bern96.pdf",
  keywords = "axiomref"
}

@article{Kajl98,
  author = "Kajler, Norbert and Soiffer, Neil",
  title = {{A Survey of User Interfaces for Computer Algebra Systems}},
  journal = "J. Symbolic Computation",
  volume = "25",
  pages = "127-159",
  year = "1998",
  abstract = 
    "This paper surveys work within the Computer Algebra community (and
    elsewhere) directed towards improving user interfaces for scientific
    computation during the period 1963--1994. It is intended to be useful
    to two groups of people: those who wish to know what work has been
    done and those who would like to do work in the field. It contains an
    extensive bibliography to assist readers in exploring the field in more
    depth. Work related to improving human interaction with computer
    algebra systems is the main focus of the paper. However, the paper
    includes additional materials on some closely related issues such as
    structured document editing, graphics, and communication protocols.",
  paper = "Kajl98.pdf",
  keywords = "axiomref"
}

@misc{Open11,
  author = "Dos Reis, G.",
  title = {{OpenAxiom}},
  link = "\url{http://www.open-axiom.org}",
  year = "2011"
}

@misc{West95,
  author = "Wester, Michael J.",
  title = {{A Review of CAS Mathematical Capabilities}},
  year = "1995",
  link = "\url{http://math.unm.edu/~wester/cas/Paper.ps}",
  abstract =
    "Computer algebra systems (CASs) have become an important
    computational tool in the last decade. General purpose CASs, which are
    designed to solve a wide variety of problems, have gained special
    prominance. In this paper, the capabilities of seven major general
    purpose CASs (Axiom, Derive, Macsyma, Maple, Mathematica, MuPAD, and
    Reduce) are reviewed on 131 short problems covering a broad range of
    (primarily) symbolic mathematics.
    
    A demo was developed for each CAS, run and the results
    evaluated. Problems were graded in terms of whether it was easy or
    difficult or possible to produce an answer and if an answer was
    produced, whether it was correct. It is the author's hope that this
    review will encourage the development of a comprehensive CAS test
    suite.",
  paper = "West95.pdf",
  keywords = "axiomref"
}

@misc{West99a,
  author = "Wester, Michael J.",
  title = {{A Critique of the Mathematical Abilities of CA Systems}},
  year = "1999",
  link = "\url{http://math.unm.edu/~wester/cas/book/Wester.pdf}",
  url2 = "http://math.unm.edu/~wester/cas_review.html",
  abstract =
    "Computer algebra systems (CASs) have become an essential computational
    tool in the last decade. General purpose CASs, which are designed to
    solve a wide variety of problems, have gained special prominence. In
    this chapter, the capabilities of seven major general purpose CASs
    (Axiom, Derive, Macsyma, Maple, Mathmatica, MuPAD and Reduce) are
    reviewed on 542 short problems covering a broad range of (primarily)
    symbolic mathematics.",
  paper = "West99a.pdf",
  keywords = "axiomref"
}

@article{Ashx89,
  author = "Ash, D.W. and Black, I.F. and Vanstone, S.A.",
  title = {{Low Complexity Normal Bases}},
  journal = "Discrete Applied Mathematics",
  volume = "25",
  pages = "191-210",
  year = "1989"
}

@article{Beth91,
  author = "Beth, T. and Geiselmann, W. and Meyer, F.",
  title = {{Finding (Good) Normal Bases in Finite Fields}},
  journal = "Proc. ISSAC '91",
  year = "1991"
}

@article{Gath90,
  author = "Gathen, J. von zur and Giesbrecht, M.",
  title =  {{Constructing normal bases in finite fields}},
  journal = "J. Symb. Comp.",
  volume = "10",
  pages = "547-570",
  year = "1990"
}

@article{Geis89,
  author = "Geiselmann, W. and Gollmann, D.",
  title = {{Symmetry and Duality in Normal Basis Multiplication}},
  journal = "Proc. AAECC-6, LNCS",
  volume = "357",
  year = "1989"
}

@article{Hube90,
  author = "Huber, K.",
  title = {{Some Comments on Zech's Logarithm}},
  journal = "IEEE Trans. Information Theory",
  volume = "IT-36",
  pages = "946-950",
  year = "1990"
}

@article{Itoh88,
  author = "Itoh, T. and Tsujii, S.",
  title = {{A fast algorithm for computing multiplicative inverses in 
           $GF(2^m)$ using normal bases}},
  journal = "Inf. and Comp.",
  volume = "78", 
  pages = "171-177",
  year = "1988",
  algebra = "\newline\refto{package INBFF InnerNormalBasisFieldFunctions}",
  abstract = 
    "This paper proposes a fast algorithm for computing multiplicative
    inverses in $GF(2^m)$ using normal bases. Normal bases have the
    following useful property: In the case that an element $x$ in
    $GF(2^m)$ is represented by normal bases, $2^k$ power operation of an
    element $x$ in $GF(2^m)$ can be carried out by $k$ times cyclic shift
    of its vector representation.  C.C. Wang et al. proposed an algorithm
    for computing multiplicative inverses using normal bases, which
    requires $(m-2)$ multiplications in $GF(2^m)$ and $(m-1)$ cyclic
    shifts. The fast algorithm proposed in this paper also uses normal
    bases, and computes multiplicative inverses iterating multiplications
    in $GF(2^m)$. It requires at most $2[log_2(m-1)]$ multiplications in
    $GF(2^m)$ and $(m-1)$ cyclic shifts, which are much less than those
    required in Wang's method. The same idea of the proposed fast
    algorithm is applicable to the general power operation in $GF(2^m)$
    and the computation of multiplicative inverses in $GF(q^m)$
    $(q=2^n)$.",
  paper = "Itoh88.pdf"
}

@book{Jaco85,
  author = "Jacobson, N.",
  title = {{Basic Algebra I, 2nd ed.}},
  publisher = "W.H. Freeman and Co.",
  year = "1985"
}

@article{Lens82,
  author = "Lenstra, A.K.",
  title = {{Factorization of Polynomials, Comp. Methods in Number Theory
           (part 1)}},
  journal = "Math. Centre Tracts",
  volume = "154",
  year = "1982"
}

@article{Lens91,
  author = "Lenstra Jr., H.W.",
  title = {{Finding Isomorphisms between Finite Fields}},
  journal = "Math. of Comp.",
  volume = "56",
  number = "193",
  pages = "329-347",
  year = "1991"
}

@article{Lens87,
  author = "Lenstra, H. W. and Schoof, R. J.",
  title = {{Primitive Normal Bases for Finite Fields}},
  journal = "Mathematics of Computation",
  volume = "48",
  number = "177",
  year = "1987",
  pages = "217-231",
  link = "\url{http://www.math.leidenuniv.nl/~hwl/PUBLICATIONS/}",
  algebra = "\newline\refto{package FFPOLY FiniteFieldPolynomialPackage}",
  abstract =
    "It is proved that any finite extension of a finite field has a normal
    basis consisting of primitive roots",
  paper = "Lens87.pdf"
}

@book{Lidl83,
  author = "Lidl, Rudolf and Niederreiter, Harald",
  title = {{Finite Field, Encyclopedia of Mathematics and Its Applications}},
  volume = "20",
  publisher = "Cambridge Univ. Press", 
  year = "1983",
  isbn = "0-521-30240-4",
  algebra = 
   "\newline\refto{category FAXF FiniteAlgebraicExtensionField}
    \newline\refto{domain FF FiniteField}
    \newline\refto{domain FFCG FiniteFieldCyclicGroup}
    \newline\refto{domain FFCGX FiniteFieldCyclicGroupExtension}
    \newline\refto{domain FFCGP FiniteFieldCyclicGroupExtensionByPolynomial}
    \newline\refto{domain FFX FiniteFieldExtension}
    \newline\refto{domain FFP FiniteFieldExtensionByPolynomial}
    \newline\refto{domain FFNB FiniteFieldNormalBasis}
    \newline\refto{domain FFNBX FiniteFieldNormalBasisExtension}
    \newline\refto{domain FFNBP FiniteFieldNormalBasisExtensionByPolynomial}
    \newline\refto{domain IFF InnerFiniteField}
    \newline\refto{domain IPF InnerPrimeField}
    \newline\refto{domain PF PrimeField}
    \newline\refto{package INBFF InnerNormalBasisFieldFunctions}
    \newline\refto{package FFPOLY2 FiniteFieldPolynomialPackage2}
    \newline\refto{package FFPOLY FiniteFieldPolynomialPackage}
    \newline\refto{package FFHOM FiniteFieldHomomorphisms}
    \newline\refto{package FFF FiniteFieldFunctions}"
}

@book{Lips81,
  author = "Lipson, John D.",
  title = {{Elements of Algebra and Algebraic Computing}},
  publisher = "Addison-Wesley Educational Publishers",
  year = "1981",
  isbn = "978-0201041156",
  algebra = "\newline\refto{category FFIELDC FiniteFieldCategory}"
}

@misc{Lune87,
  author = {L\"uneburg, H},
  title = {{On the Rational Normal Form of Endomorphisms}},
  comment = "BI-Wissenschaftsverlag",
  year = "1987"
}

@techreport{Grab92,
  author = "Grabmeier, Johannes and Scheerhorn, Alfred",
  title = {{Finite fields in Axiom}},
  type = "technical report",
  number = "AXIOM Technical Report TR7/92 (ATR/5)(NP2522)",
  institution = "Numerical Algorithms Group, Inc.", 
  address = "Downer's Grove, IL, USA and Oxford, UK", 
  year = "1992",
  link = "\url{http://www.nag.co.uk/doc/TechRep/axiomtr.html}",
  algebra =
   "\newline\refto{category CHARNZ CharacteristicNonZero}
    \newline\refto{category FPC FieldOfPrimeCharacteristic}
    \newline\refto{category XF ExtensionField}
    \newline\refto{category FFIELDC FiniteFieldCategory}
    \newline\refto{category FAXF FiniteAlgebraicExtensionField}
    \newline\refto{domain SAE SimpleAlgebraicExtension}
    \newline\refto{domain IPF InnerPrimeField}
    \newline\refto{domain PF PrimeField}
    \newline\refto{domain FFP FiniteFieldExtensionByPolynomial}
    \newline\refto{domain FFCGP FiniteFieldCyclicGroupExtensionByPolynomial}
    \newline\refto{domain FFNBP FiniteFieldNormalBasisExtensionByPolynomial}
    \newline\refto{domain FFX FiniteFieldExtension}
    \newline\refto{domain FFCGX FiniteFieldCyclicGroupExtension}
    \newline\refto{domain FFNBX FiniteFieldNormalBasisExtension}
    \newline\refto{domain IFF InnerFiniteField}
    \newline\refto{domain FF FiniteField}
    \newline\refto{domain FFCG FiniteFieldCyclicGroup}
    \newline\refto{domain FFNB FiniteFieldNormalBasis}
    \newline\refto{package DLP DiscreteLogarithmPackage}
    \newline\refto{package FFF FiniteFieldFunctions}
    \newline\refto{package INBFF InnerNormalBasisFieldFunctions}
    \newline\refto{package FFPOLY FiniteFieldPolynomialPackage}
    \newline\refto{package FFPOLY2 FiniteFieldPolynomialPackage2}
    \newline\refto{package FFHOM FiniteFieldHomomorphisms}
    \newline\refto
        {package FFFACTSE FiniteFieldFactorizationWithSizeParseBySideEffect}",
  abstract =
    "Finite fields play an important role for many applications (e.g. coding
    theory, cryptograpy). There are different ways to construct a finite 
    field for a given prime power. The paper describes the different 
    constructions implemented in AXIOM. These are {\sl polynomial basis
    representation}, {\sl cyclic group representation}, and {\sl normal
    basis representation}. Furthermore, the concept of the implementation,
    the used algorithms and the various datatype coercions between these
    representations are discussed.",
  paper = "Grab92.pdf",
  keywords = "axiomref",
  beebe = "Grabmeier:1992:FFA"
}

@article{Mull88,
  author = "Mullin, R.C. and Onyszchuk, I.M. and Vanstone, S.A.",
  title = {{Optimal Normal Bases in $GF(p^n)$}},
  journal = "Discrete Applied Mathematics",
  volume = "22",
  pages = "149-161",
  year = "1988"
}

@misc{Nick88,
  author = "Nickel, W.",
  title = {{Endliche K\"orper in dem gruppentheoretischen Programmsystem GAP}},
  comment = "Diplomarbeit, RWTH Aachen",
  year = "1988"
}

@article{Odly85,
  author = "Odlyzko, A.M.",
  title = {{Discrete logarithms in finite fields and their cryptographic
           significance}},
  journal = "Proc. Eurocrypt '84, LNCS",
  volume = "209",
  publisher = "Springer-Verlag",
  pages = "224-314",
  year = "1985"
}

@article{Pinc89,
  author = "Pincin, A",
  title = {{Bases for finite fields and a canonical decomposition for a
           normal basis generator}},
  journal = "Communications in Algebra",
  volume = "17",
  number = "6",
  pages = "1337-1352",
  year = "1989"
}

@article{Pohl78,
  author = "Pohlig, S.C. and Hellman, M.",
  title = {{An improved algorith for computing logarithms over $GF(p)$
           and its cryptographic significance}},
  journal = "IEEE Trans Information Theory",
  volume = "IT-24",
  pages = "106-110",
  year = "1978"
}

@article{Rybo89,
  author = "Rybowicz, M",
  title = {{Search of primitive polynomials over finite fields}},
  journal = "J. Pure Appl.",
  volume = "65",
  pages = "139-151",
  year = "1989"
}

@misc{Salo16a,
  author = "Salomone, Matthew",
  title = {{Group Theory II}},
  year = "2016",
  institution = "Bridgewater State University",
  link = "\url{http://axiom-developer.org/axiom-website/GroupTheoryII/Salomone.html}"
}

@misc{Sche92,
  author = "Scheerhorn, A.",
  title = {{Trace- and Norm-Compatible Extensions of Finite Fields}},
  journal = "Appl. Alg. in Eng., Comm. and Comp.",
  year = "1992"
}

@misc{Sche93,
  author = "Scheerhorn, Alfred",
  title = {{Presentation of the algebraic closure of finite fields and 
           trace-compatible polynomial sequences}},
  comment = "Darstellungen des algebraischen Abschlusses endlicher Korper
             und spur-kompatible Polynomfolgen",
  year = "1993",
  abstract =
    "For numerical experiments concerning various problems in a finite
    field $\mathbb{F}_q$ it is useful to have an explicit data
    presentation $\mathbb{F}_{q^m}$ of for large $m$, and a method for the
    construction of towers
    \[\mathbb{F}_q \subset \mathbb{F}_{q^{d_1}} \subset \cdots \subset
    \mathbb{F}_{q^{d_k}} = \mathbb{F}_{q^m}\]
    In order to avoid the identification problem it is advantageous to 
    have all fields in the tower presented by properly chosen normal bases, 
    whereby the embedding 
    $\mathbb{F}_{q^{d_i}} \subset \mathbb{F}_{q^{d_{i+1}}}$
    is given by the trace function.
    
    The following notion is introduced: A sequence of polynomials 
    $\{f_n | n \ge 1\}$ with degree$(f_n)=n$ called trace-compatible over 
    $\mathbb{F}_q$ if (1) $f_n$ is a normal polynomial over $\mathbb{F}_q$, 
    (2) if $\alpha_n \in \mathbb{F}_{q^n}$ is a root of $f_n$, then for any 
    proper divisor $d$ of $n$ the trace of $\alpha_n$ over $\mathbb{F}_{q^d}$ 
    is a root of $f_d$.
    
    The main goal of the dissertation is to give algorithms for
    construction of sequences of trace-compatible polynomials and to
    present explicit numerical data. An analogous notion of
    norm-compatible sequences is also introduced and studied.
    
    The dissertation consists of four chapters and a supplement, as
    follows: (1) Basic notions (1-31). (2) Presentation of the algebraic
    closure of a finite field (32-59). (3) Sequences of polynomials and
    sequences of elements (60-115). (4) Implementations (118-139). (5)
    Supplement (142-171).
    
    In chapters (1)–(3) various known results and algorithms are
    collected, and new results are added and compared with those
    previously used. 
    
    The numerical results in the supplement contain sequences of
    trace-compatible polynomials of degree $n$, where $n \le 100$, and
    $q=2,3,5,7,11,13$. For implementation, the computer-algebra system
    AXIOM has been used. The details contained in this dissertation are
    not readily describable in a short review.",
  keywords = "axiomref"
}

@article{Shou92,
  author = "Shoup, V.",
  title = {{Searching for Primitive Roots in Finite Fields}},
  journal = "Math. of Comp.",
  volume = "58",
  number = "197",
  pages = "369-380",
  year = "1992"
}

@article{Stin90,
  author = "Stinson, D.R.",
  title = {{Some observations on parallel Algorithms for fast exponentiation 
           in $GF(2^n)$}},
  journal = "Siam J. Comp.",
  volume = "19",
  number = "4",
  pages = "711-717",
  year = "1990",
  algebra = "\newline\refto{package INBFF InnerNormalBasisFieldFunctions}",
  abstract = 
    "A normal basis represention in $GF(2^n)$ allows squaring to be
    accomplished by a cyclic shift. Algorithms for multiplication in
    $GF(2^n)$ using a normal basis have been studied by several
    researchers. In this paper, algorithms for performing exponentiation
    in $GF(2^n)$ using a normal basis, and how they can be speeded up by
    using parallelization, are investigated.",
  paper = "Stin90.pdf"
}

@article{Vars81,
  author = "Varshamov, Gamkrelidze",
  title = {{Method of construction of primitive polynomials over 
            finite fields}},
  journal = "Soobsheh. Akad. Nauk Gruzin.",
  volume = "99",
  pages = "61-64",
  year = "1981"
}

@article{Wass89,
  author = "Wassermann, A.",
  title = {{Konstruktion von Normalbasen}},
  journal = "Bayreuther Math. Schriften",
  volume = "31",
  pages = "1-9",
  year = "1989"
}

@article{Calm92,
  author = "Calmet, J. and Campbell, J.A.",
  title = {{Artificial Intelligence and Symbolic Mathematical
           Computations}},
  journal = "LNCS",
  volume = "737",
  pages = "1-19",
  year = "1992",
  abstract =
    "This introductory paper summarizes the picture of the territory
    common to AI and SMC that has evolved from discussions following the
    presentation of papers given at the 1992 Karlsruhe conference.  Its
    main objective is to highlight some patterns that can be used to guide
    both sketches of the state of the art in this territory and
    suggestions for future research activities.",
  paper = "Calm92.pdf"
}

@misc{Davexx,
  author = "Davenport, J.H.",
  title = {{Computer algebra -- past, present and future}},
  abstract =
    "Computer algebra started in 1953, and there were several systems in
    existence in the 1960's. Those inspired by physical applications
    largely implemented ``high school'' algebra and, from the point of
    view of today's much larger machines and more sophisticated
    programming languages, the miracle is that they worked at all, or as
    efficiently as they did.
    
    By the end of the 1960's it was clear that more sophisticated
    algorithms were necessary, either to solve problems for which the
    ``high school'' algorithms were inefficient on large data (e.g. gcd or
    factorization), or problems for which the ``high school'' techniques
    were not really algorithms at all (e.g. integration, solution of sets
    of equations).
    
    Hence the 1970's (and indeed much of the 1980's) were the ``age of
    algorithms''. It rapidly became obvious that these algorithms required
    more complex data structures and mathematical objects: finite fields,
    ideals, algebraic curves, divisor class groups to name but a few. This
    led to the growth of new systems, such as Axiom (formerly Scratchpad),
    Maple, Mathematica and Reduce 3. It is currently the case that many
    more algorithms are known than are implemented, and certainly that few
    systems implement even a reasonable cross-section of the known
    algorithms.
    
    At the present, there are two main trends. One is the rush to
    implement, which is causing a lot of duplication of work, but there is
    also a realisation that these systems need to be able to communicate,
    and that it is inherently impossible to have all the best algorithms
    in one system. To take an example, why implement from scratch enough
    group theory to analyse blocks of imprimitivity in a permutation
    group, when Cayley has all this and much more? However, parts of
    integration theory require this analysis.
    
    The other trend is the tendency to more ``structure-oriented''
    algorithms, i.e. algorithms which take accound of the structure of the
    problem. To name two, there is Gatemann's work on polynomial equation
    systems with symmetry, and Richardson's work on roots of polynomials
    which can be written as $p(x,x^n)$ with $p$ of low degree.
    
    The paper concludes with some speculations on the future of computer
    algebra.",
  paper = "Davexx.pdf",
  keywords = "axiomref"
}  

@phdthesis{Fate72,
  author = "Fateman, Richard J.",
  title = {{Essays in Algebraic Simplification}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/tr-95.pdf}",
  school = "MIT",
  comment = "MAC TR-95 technical report",
  year = "1972",
  abstract =
    "This thesis consists of essays on several aspects of the problem
    of algebraic simplification by computer. We first discuss a pattern
    matching system intended to recognize non-obvious occurrences of
    patterns within Algebraic expression. A user of such a system can
    ``teach'' the computer new simplification rules. Then we report on
    new applications of canonical simplification of rational functions.
    These applications include techniques for picking out coefficients,
    and for substituting for summs, products, quotients, etc. Our final
    essay is on a new, practical, canonical simplification algorithms
    for radical expressions (i.g. algebraic expressions including roots
    of polynomials). The effectiveness of the procedure is assured
    through proofs of appropriate properties of the simplified forms.
    Two appendices describe MACSYM, a computer system for algebraic
    manipulations, which served as the basis for this work.",
  paper = "Fate72.pdf"
}

@misc{Fate08b,
  author = "Fateman, Richard J.",
  title = {{Applications and Methods for Recognition of (Anti)-Symmetric
           Functions}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/symmetry.pdf}",
  year = "2008",
  abstract =
    "One of the important advantages held by computer algebra systems (CAS)
    over purely-numerical computational frameworks is that the CAS can
    provide a higher-level ``symbolic'' viewpoint for problem
    solving. Sometimes this can convert apparently impossible problems to
    trivial ones. Sometimes the symbolic perspective can provide
    information about questions which cannot be directly answered, or
    questions which might be hard to pose. For example, we might be able
    to analyze the asymptotic behavior of a solution to a differential
    equation even though we cannot solve the equation. One route to
    implicitly solving problems is the use of symmetry arguments. In this
    paper we suggest how, through symmetry, one can solve a large class of
    definite integration problems, including some that we found could not
    be solved by computer algebra systems. One case of symmetry provides
    for recognition of periodicity, and this solves additional problems,
    since removal of periodic components can be important in integration
    and in asymptotic expansions.",
  paper = "Fate08b.pdf"
}

@misc{Fate03b,
  author = "Fateman, Richard J.",
  title = {{Manipulation of Matrices Symbolically}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/symmat2.pdf}",
  year = "2003",
  abstract =
    "Traditionally, matrix algebra in computer algebra systems is 
    ``implemented'' in three ways:
    \begin{itemize}
    \item numeric explicit computation in a special arithmetic domain: 
    exact rational or integer, high-precision software floating-point, 
    interval, or conventional hardware floating-point.
    \item ‘symbolic’ explicit computation with polynomial or other 
    expression entries,
    \item (implicit) matrix computation with symbols defined over a 
    (non-commuting) ring.
    \end{itemize}
    Manipulations which involve matrices of indefinite size (n × m) or
    perhaps have components which are block submatrices of indefinite size
    have little or no support in general-purpose computer algebra systems,
    in spite of their importance in theorems, proofs, and generation of
    programs. We describe some efforts to design and implement tools for
    this mode of thinking about matrices in computer systems.",
  paper = "Fate03b.pdf"
}

@misc{Fate09a,
  author = "Fateman, Richard J.",
  title = {{Simplifying RootSum Expressions}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/rootsum.pdf}",
  year = "2009",
  abstract =
    "It's useful to sum an expression with a parameter varying over all
    the roots of a given polynomial. Here's a defense of that statement
    and a method to do the task.",
  paper = "Fate09a.pdf"
}

@misc{Fate09,
  author = "Fateman, Richard J.",
  title = {{Rational Integration, Simplified}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/root-integ.pdf}",
  year = "2009",
  abstract =
    "After all this computer algebra stuff, and several PhD theses in 
    the last few decades, what more could we say about symbolic
    rational function integration?

    How about a closed formula for the result, subject to a few algebraic
    side-conditions, which works even with parameters in the denominator?",
  paper = "Fate09.pdf"
}

@misc{Fate99b,
  author = "Fateman, Richard J.",
  title = {{Generation and Optimization of Numerical Programs by
           Symbolic Mathematical Methods}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/RIMS.pdf}",
  year = "1999",
  abstract =
    "Symbolic mathematical methods and systems
    \begin{itemize}
    \item support scientific and engineering ``problem solving environments'' 
    (PSEs),
    \item The specific manipulation of mathematical models as a precursor 
    to the coding of algorithms
    \item Expert system selection of modules from numerical libraries and 
    other facilities
    \item The production of custom numerical software such as derivatives 
    or non-standard arithmetic code-generation packages,
    \item The complete solution of certain classes of mathematical problems 
    that simply cannot be handled solely by conventional floating-point 
    computation.
    \end{itemize}

    Viewing computational objects and algorithms from a symbolic
    perspective and then specializing them to numerical or graphical views
    provides substantial additional flexibility over a more conventional view.
    
    We also consider interactive symbolic computing as a tool to provide
    an organizing principle or glue among otherwise dissimilar components.",
  paper = "Fate99b.pdf",
  keywords = "axiomref"
}

@misc{Fate07,
  author = "Fateman, Richard J.",
  title = {{Rational Function Computing with Poles and Residues}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/qd.pdf}",
  year = "2007",
  abstract = 
    "In a numerical calculation sometimes we need higher-than
    double-precision floating-point arithmetic to allow us to be confident
    of a result. One alternative is to rewrite the program to use a
    software package implementing arbitrary-precision extended
    floating-point arithmetic such as ARPREC or MPFR, and try to choose a
    suitable precision.  
    
    Such an arithmetic scheme, in spite of helpful tools, may be
    inconvenient to write. There are also facilities in computer algebra
    systems (CAS) for such software-implemented ``bigfloats.'' These
    facilities are convenient if one is already using the CAS. In any of
    these situations the bigfloats may be rather slow, a cost of its
    generality.
    
    There are possibilities intermediate between the largest hardware
    floating-point format and the general arbitrary-precision software
    which combine a considerable (but not arbitrary) amount of extra
    precision with a (relatively speaking) modest factor loss in
    speed. Sometimes merely doubling the number of bits in a
    double-floating-point fraction is enough, in which case arithmetic on
    double-double (DD) operands would suffice. Another possibility is to
    go for yet another doubling to quad-double (QD) arithmetic: instead of
    using the machine double-floats to give about 16 decimal digits of
    precision, QD supplies about 64 digits. DD and QD as used here provide
    the same exponent range as ordinary double.
    
    Here we describe how we incorporated QD arithmetic implemented in a
    library into a Common Lisp system, providing a smooth interface while
    adding only modest overhead to the run-time costs (compared to
    accessing the library from C or C++). One advantage is that we keep
    the program text almost untouched while switching from double to
    quad-double. Another is that the programs can be written, debugged,
    and run in an interactive environment. Most of the lessons from QD can
    be used for other versions of arithmetic which can be embedded in
    Lisp, including MPFR, for indefinite (arbitrary) precision, should QD
    provide inadequate precision or range.",
  paper = "Fate07.pdf",
  keywords = "axiomref"
}

@misc{Fate13a,
  author = "Fateman, Richard J.",
  title = {{Rational Function Computing with Poles and Residues}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/openmathcrit.pdf}",
  year = "2013",
  abstract = 
    "Computer algebra systems (CAS) usually support computation with exact
    or approximate rational functions as ratios of polynomials in
    ``expanded form'' with explicit coefficients. We examine the
    consequences of introducing a partial-fraction type of form in which
    some of the usual rational operations can be implemented in
    substantially faster times. In this form an expression in one
    variable, say $x$, is expressed as a polynomial in $x$ plus a sum of
    terms each of which has a denominator $x-c$ perhaps to an integer
    power, where $c$ is in general a complex constant. We show that some
    common operations including rational function addition,
    multiplication, and matrix determinant calculation can be performed
    many times faster than in the conventional representation. Polynomial
    GCD operations, the costliest part of rational additions, are entirely
    eliminated. Applicaiton of Cauchy's integral theorem allow for trivial
    integration of an expression around a closed contour. In some cases
    the approximate evaluation of transcendental functions can be
    accelerated, especially in parallel, by evaluation of a formula in
    pole+residue form.",
  paper = "Fate13a.pdf"
}

@misc{Fate01a,
  author = "Fateman, Richard J.",
  title = {{A Critique of OpenMath and Thoughts on Encoding Mathematics}},
  year = "2001",
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/openmathcrit.pdf}",
  algebra =
    "\newline\refto{category OM OpenMath}
     \newline\refto{domain COMPLEX Complex}
     \newline\refto{domain DFLOAT DoubleFloat}
     \newline\refto{domain FLOAT Float}
     \newline\refto{domain FRAC Fraction}
     \newline\refto{domain INT Integer}
     \newline\refto{domain LIST List}
     \newline\refto{domain SINT SingleInteger}
     \newline\refto{domain STRING String}
     \newline\refto{domain SYMBOL Symbol}
     \newline\refto{package OMEXPR ExpressionToOpenMath}
     \newline\refto{package OMSERVER OpenMathServerPackage}",
  abstract =
    "The OpenMath project, as portrayed in the Special Issue of the SIGSAM
    Bulletin (volume 34 no. 2), seems to have a number of problems to
    face. One of them is the (apparently implicit) assumption that
    OpenMath designers, through dint of mathematical thought and the
    advice of the members of the Open Math Society, have solved, in their
    domain, one of the most pressing problems of software engineering
    today, namely software re-use. After six years there is insufficient
    evidence on which to base any claims of success and it appears that
    most substantive practical issues of mathematical representation and
    communication have yet to be addressed. We also raise questions about
    related computational mathematical goals and mathematical encodings.",
  paper = "Fate01a.pdf"
}

@misc{Fate08a,
  author = "Fateman, Richard J.",
  title = {{Verbs, Nouns, and Computer Algebra, or What's Grammar Got
           to do with Math?}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/nounverbmac.pdf}",
  year = "2008",
  abstract =
    "Computer algebra systems (CAS) are supposed to do mathematics. 
    Unfortunately, much of human mathematics presentation and
    manipulation depends on humans to understand what is going on more
    generally. This context is often not made available to the computer
    system, and so it will sometimes fail to make the right guess at the
    meaning of constructions, unless they are made unambiguous. Among many
    issues, one that has been raised and resolved, is the need for
    attributing properties to operators that distinguish between ``the name
    of the operation'' and ``the operation itself''.  
    
    As a simple example, referential transparency requires that if $x = y$,
    then any true statement $P(x)$ about $x$ must be true about $y$, namely
    $P(y)$ is also true.
    
    Consider now the well-known equation $sin(0) = 0$, and the statement 
    $P(r) :=$ ``r = 0 is a rule to simplify the sin() function''.  The
    statement $P(sin(0))$ is clearly true. Therefore $P(0)$ should be true
    as well. It is not, since $0 = 0$ is not a rule to simplify $sin()$.
    
    The point here is that $sin(0)$ is not equal to 0 if sin is treated as
    the name of an operation. Perhaps it is a character string, or a data
    structure of some sort: a noun phrase. The corresponding expression
    including the verb form of sin is one in which $sin(0)$ can be
    simplified to something equivalent, but in some sense preferable. Here
    it is reasonable to replace it by 0.",
  paper = "Fate08a.pdf"
}

@misc{Fate97c,
  author = "Fateman, Richard J.",
  title = {{Excerpts from a proposal to the National Science Foundation
           on Programming Environments and Tools for Advanced Scientific
           Computation}},
  year = "1997",
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/newpse.pdf}",
  abstract =
    "We propose to concentrate on investigations of technology in support
    of advanced scientific computation, based on a higher-level view of the
    relationship of mathematical modeling to computation. This is based on
    the following considerations:
    \begin{enumerate}
    \item Symbolic mathematical manipulation and computer algebra systems: 
    These are tools and techniques that support problem solving, code 
    development, comprehension, debugging and partial evaluation
    (the principle behind many optimization techniques).
    \item Data modeling: in particular, the use of IEEE floating-point 
    number representations and operations.
    \item Closed form or exact solutions: For some sub-problems of 
    scientific interest, numerical solutions are not the only paradigm. 
    The availability of exact solutions, or symbolic-approximation solutions 
    has advanced along with computing technology too. 
    \item The use of scripting languages: the linkage of high-performance 
    library routines to graphical interfaces for data analysis and model setup
    can be enhanced by symbolic support and high-level scripting languages.
    \item Extending the use of remote computation. We are now using standard 
    network connections for the acquisition of complex data (in this case, 
    symbolic solution of definite integrals).
    \end{enumerate}",
  paper = "Fate97c.pdf",
  keywords = "axiomref"
}

@misc{Fate98,
  author = "Fateman, Richard J.",
  title = {{Open Problems in Human-Computer Interaction with Specific
           Application to Computer-Aided Mathematics}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/open4.ps}",
  year = "1998",
  abstract =
    "It is our intention, in this short paper, to indicate some of the
    problems of human-computer interactions at the boundaries of current
    research is algebraic manipulation as we see the field now, some 35
    years from the development of its first software systems (the
    now-ancient Macsyma was announced in 1971, and it had precurors.)
    
    Because computers, algorithms, and software systems have advanced so
    substantially, there is a tendency for computer scientists to think
    that all of mathematics can be done by computer, or at least that we
    are on track for the continued ``arithmetization of mathematics.''
    Working mathematicians tend to disagree with the characterization that
    we are at all close now, nor that we are headed toward any goal of
    being able to do or understand mathematics.
    
    Most of the problems which we indicate below are actually quite
    general problems in computer-human communication. For specificity (and
    the possibility that one could qunatify a solution!) we will count our
    discussion of in terms of applications in the area of algebraic
    manipulation. If we cannot handle allegedly well-formed mathematical
    issues, how can we expect to handle ill-formed ambigious
    ``natural''communication?
    
    Some of the problems are representative of strategic mathemaical
    manipulation procedures which lack totally algorithmic approaches at
    this time, and so are more of an interactive problem-solving protocol
    than a fixed recipe.",
  paper = "Fate98.pdf",
  keywords = "axiomref"
}

@misc{Fate00a,
  author = "Fateman, Richard J.",
  title = {{Improving Exact Integral From Symbolic Algebra Systems}},
  year = "2000",
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/nform2col.pdf}",
  abstract =
    "Programs in symbolic algebraic manipulation systems can compute
    certain classes of symbolic indefinite integrals in closed form. Although
    these answers are ordinarily formally correct algebraic anti-derivatives,
    their form is often unsuitable for further numerical or even analytical 
    processing. In particular, we address cases in which such ``exact answers''
    when numerically evaluated may give less-accurate answers than numerical
    approximations from first principles! The symbolic formulas may also
    behave inappropriately near singularities. We discuss techniques, based
    in part on the calculus of divided differences, for improving the form of
    results of symbolic mathematics systems. In particular, computer alge
    algebra systems must take explicit account of the possibility that they are
    producing not “mathematics” but templates of programs consisting of se
    sequences of arithmetic operations. In brief, mathematical correctness is
    not enough. Forms produced by rational integration programs are used
    for examples.",
  paper = "Fate00a.pdf"
}

@misc{Fate03,
  author = "Fateman, Richard J.",
  title = {{Continuity and Limits of Programs}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/limit.pdf}",
  year = "2003",
  abstract =
    "In demonstrations of proofs [3] of correctness for programs that are
    designed to compute mathematical functions we attempted to show that
    programs have the right properties, namely the same properties as the
    mathematical functions they are alleged to compute. In the cited paper
    we were forced to hand-wave (our excuse was the need for brevity) in
    at least two places, trying to side-step sticky issues. Here we try to
    address these issues by clarifying two concepts: (a) computational
    continuity and (b) equality in a domain where floating-point
    computations can be done to variable (presumably high) precision. We
    introduce notations p-representable and p-negligible where p denotes
    precision, and show how this helps in our applications.",
  paper = "Fate03.pdf",
  keywords = "CAS-Proof, printed"
}

@misc{Fate15,
  author = "Fateman, Richard J.",
  title = {{Interval Arithmetic, Extended Numbers and Computer Algebra
           Systems}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/interval.pdf}",
  year = "2015",
  abstract =
    "Many ambitious computer algebra systems were initially designed in a
    flush of enthusiasm, with the goal of automating any symbolic
    mathematical manipulation “correctly.” Historically, this approach
    resulted in programs that implicitly used certain identities to
    simplify expressions. These identities, which very likely seemed
    universally true to the programmers in the heat of writing the CAS,
    (and often were true in well-known abstract algebraic domains) later
    needed re-examination when such systems were extended for dealing with
    kinds of objects unanticipated in the original design. These new
    objects are generally introduced to the CAS by extending “generically”
    the arithmetic or other operations.  For example, approximate floats
    do not have the mathematical properties of exact integers or
    rationals.  Complex numbers may strain a system designed for
    real-valued variables. In the situation examined here, we consider two
    categories of “extended” numbers: ∞ or undefined, and real
    intervals. We comment on issues raised by these two troublesome
    notions, how their introduction into a computer algebra system may
    require a (sometimes painful) reconsideration and redesign of parts of
    the program, and how they are related. An alternative (followed most
    notably by the Axiom system is to essentially envision a “meta” CAS
    defined in terms of categories and inheritance with only the most
    fundamental built-in concepts; from these one can build many variants
    of specific CAS features. This approach is appealing but can fails to
    accommodate extensions that violate some mathematical tenets in the
    cause of practicality.",
  paper = "Fate15.pdf"
}

@misc{Fate14,
  author = "Fateman, Richard J.",
  title = {{Fun with Filon Quadrature - a Computer Algebra Perspective}},
  year = "2014",
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/fun-filon.pdf}",
  abstract =
    "Filon quadrature, useful for oscillatory integrals, is easily
    implemented in a computer algebra system (CAS) using exact or
    approximate arithmetic. Although quadrature is almost always used in
    the context of approximation, we see that various properties are
    exhibited by running an exact algorithm on exact {\sl symbolic}
    inputs. This experimental approach to the mathematics allows us to
    prove the implementation of the algorithm has the expected
    mathematical correctness properties, and is therefore more likely to
    be, itself, a correct implementation.",
  paper = "Fate14.pdf"
}

@misc{Fate97a,
  author = "Fateman, Richard J.",
  title = {{The Fast Fourier Transform}},
  course = "Berkeley CS 292, Fall 1997, Handout 2",
  year = "1997",
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/fftnotes.pdf}",
  paper = "Fate97a.pdf"
}

@misc{Fate97b,
  author = "Fateman, Richard J.",
  title = {{More Versatile Scientific Documents Over-Extended Abstract,
           working paper}},
  year = "1997",
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/mvsd.ps}",
  abstract =
    "The electronic representation of scientific documents: journals,
    technical reports, program documentation, laboratory notebooks,
    etc. present challenges in several distinct communities. We see five
    distinct groups concerned with electronic versions of scientific
    documents:
    \begin{itemize}
    \item Publishers of journals, texts, reference works, and their authors.
    \item Software publishers for OCR/document analysis, document formatting
    \item Software publishers whose products access ``contents semantics''
    from documents, including library keyword search programs, natural language
    search programs, data-base systems, visual presentation systems, 
    mathematical computation systems, etc.
    \item Institutions maintain access to electronic libraries, which must
    be broadly construed to include data and programs of all sorts.
    \item Individuals and their programs acting as their agents who need to
    use these libraries, to identify, locate, and retrieive relevant 
    documents
    \end{itemize}
    
    We need to have a convergence in design and standards for encoding new
    or pre-existing (typically paper-based) documents in order to most
    efficiently meet the needs of all these groups. Various effort, some
    loosely coordinated, but just as often competing, are trying to set
    standards and build tools.
    
    We approach this by first dividing the task into visual, syntactic,
    and semantic components. These specifications can focus attention on
    the addressing requirements of the different groups. Additionally, 
    these components rely on a structure for documents that incorporates
    existing library models and extends them to new modes of operation.
    Berkeley's MVD is one plausible structure which will allow prototype
    development and explorations.",
  paper = "Fate97b.pdf"
}

@article{Gent74a,
  author = "Gentleman, W. Morven",
  title = {{Experience with Truncated Power Series}},
  journal = "ACM SIGSAM",
  volume = "8",
  number = "3",
  year = "1974",
  pages = "61-62",
  abstract =
    "The truncated power series package in ALTRAN has been available for
    over a year now, and it has proved itself to be a useful and exciting
    addition to the armoury of symbolic algebra. A wide variety of
    problems have been attacked with this tool: moreover, through use in
    the classroom, we have had the opportunity to observe how a large
    number of people react to the availability of this tool.",
  paper = "Gent74a.pdf"
}

@article{Harr79,
  author = "Harrington, Steven J.",
  title = {{A Symbolic Limit Evaluation Program in REDUCE}},
  journal = "ACM SIGSAM",
  volume = "13",
  number = "1",
  year = "1979",
  pages = "27-31",
  abstract =
    "A method for the automatic evaluation of algebraic limits is
    described. It combines many of the techniques previously employed,
    including top-down recursive evaluation, power series expansion, and
    L'Hôpital's rule. It introduces the concept of a special algebraic
    form for limits. The method has been implemented in MODE-REDUCE.",
  paper = "Harr79.pdf"
}

@article{Heck97,
  author = "Heckmann, Reinhold and Wilhelm, Reinhard",
  title = {{A Functional Description of TeX's Formula Layout}},
  journal = "J. Functional Programming",
  volume = "7",
  number = "5",
  pages = "451-485",
  year = "1997",
  link = "\url{http://http.cs.berkeley.edu/~fateman/temp/neuform.pdf}",
  comment = "ftp://ftp.cs.uni-sb.de/formulae/formulae.tar.gz",
  abstract =
    "While the quality of the results of TeX's mathematica formula layout
    algorithm is convincing, its original description is hard to
    understand since it is presented as an imperative program with complex
    control flow and destructive manipulations of the data structures
    representing formulae. In this paper, we present a re-implementation
    of TeX's formula layout algorithm in the functional language SML,
    thereby providing a more readable description of the algorithm,
    extracted from the monolithical TeX system.",
  paper = "Heck97.pdf"
}

@misc{Jeffxx,
  author = "Jeffrey, David",
  title = {{Real Integration on Domains of Maximum Extent}},
  abstract =
    "General purpose computer algebra systems are used by people with
    widely varying backgrounds. Amongst the many difficulties that face
    the developer because of this, one that is particularly relevant to
    the subject of this talk is the fact that different users attach
    different meanings, or definitions, to the same symbols. When a user
    asks a CAS to integrate a function, it is not clear which definition
    of integral should be used. Some of the disagreements over the
    ``correct'' value of an integral reduce to the fact that the different
    parties are using different defintions. This talk therefore starts by
    defining my version of integration. According to this definition,
    functions returned as integrals should not only differentiate to the
    function supplied by the user, they should also satisfy global
    continuity properties. In order to achieve these properties, the idea
    of a rectifying transform is intrduced. For the problem of integrating
    a rational trigonometric function, a new rectifying transform is
    described."
}

@misc{Jeffxxa,
  author = "Jeffrey, D. J.",
  title = {{The Integration of Functions Containing Fractional Powers}},
  abstract =
    "An algorithm is developed for integrating functions that contain
    fractional powers. The algorithm addresses the following points. The
    integral must be valid for all possible values of the variable,
    including those values of the variable that make the integrand, and
    hence the integral, take complex values. The algorithm must allow for
    the fact that there are two possible interpretations of the cube root
    as a real number (in fact of any odd root), and produce correct
    integrals for both interpretations (it is shown that it is possible
    for the functiona form of the integral to change with the
    interpretation). Finally, all simplifications, especially of complex
    quantities, must follow correct rules, what are here derived using the
    concept of the unwinding number."
}

@misc{Jeffxxb,
  author = "Jeffrey, D.J. and Corless, R.M.",
  title = {Explorations of uses of the unwinding number $\Kappa$},
}

@misc{Kohl08,
  author = "Kohlhase, Michael",
  title = {{Using Latex as a Semantic Markup Format}},
  year = "2008",
  link = "\url{https://kwarc.info/kohlhase/papers/mcs08-stex.pdf}",
  abstract =
    "One of the great problems of Mathematical Knowledge Management (MKM)
    systems is to obtain access to a sufficiently large corpus of
    mathematical knowledge to allow the management / search / naviation
    techniques developed by the community to display their strength. Such
    systems usually expect the mathematical knowledge they operate on in
    the form of semantically enhanced documents, but mathematicians and
    publishers in Mathematics have heavily invested into the Tex/Latex
    format and workflow.
    
    We analyze the current practice of semi-semantic markup in Latex
    documents and extend it by a markup infrastructure that allows to
    embed semantic annotations into latex documents without changing their
    visual appearance. This collection of tex macro packages is called
    stex (semantic tex) as it allows to markup latex documents
    semantically without leaving the time-tried tex/latex workflow,
    essentially turning latex into an MKM format. At the heart of stex is
    a definition mechanism for semantic macros for mathematical objects
    and a non-standard scoping construct for them, which is oriented at
    the semantic dependency relation rather than the document structure.
    
    We evaulate the stex macro collection on a large case study: the
    course materials of a two-semester course in Computer Science was
    annotated semantically and coverted to the OMDOC MKM format by Bruce
    Miller's LatexML system.",
  paper = "Kohl08.pdf"
}

@article{Miol91,
  author = "Miola, Alfonso",
  title = {{Symbolic Computation and Artificial Intelligence}},
  journal = "LNCS",
  volume = "535",
  pages = "243-255",
  year = "1991",
  abstract = 
    "The paper presents an overview of the research achievements on issues
    of common interest for Symbolic Computation and Artificial
    Intelligence.  Common methods and techniques of non-numerical
    information processing and of automated problem solving are underlined
    together with specific applications.  A qualitative analysis of the
    symbolic computation systems currently available is presented in
    view of the design and implementation of a new system.  This system
    allows both formal algebraic and analytical computations and automated
    deduction to prove properties of the computation. ",
  paper = "Miol91.pdf"
}

@misc{Nore08,
  author = "Norell, Ulf and Chapman, James",
  title = {{Dependently Typed Programming in Agda}},
  link = "\url{http://www.cse.chalmers.se/~ulfn/papers/afp08/tutorial.pdf}",
  year = "2008",
  paper = "Nore08.pdf",
  keywords = "printed"
}

@phdthesis{Paul14,
  author = "Paule, Peter",
  title = {{Complex Variables Visualized}},
  school = "RISC Linz",
  year = "2014",
  link =
  "\url{http://www.risc.jku.at/publications/download/risc_5011/DiplomaThesisPonweiser.pdf}",
  abstract =
    "The aim of this diploma thesis is the visualization of some
    fundamental results in the context of the theory of the modular group
    and modular functions. For this purpose the computer algebra software
    Mathematica is utilized.  

    The thesis is structured in three parts. In
    Chapter 1, we summarize some important basic concepts of group theory
    which are relevant to this work.  Moreover, we introduce obius
    transformations and study their geometric mapping properties. 

    Chapter 2 is devoted to the study of the modular group from an
    algebraic and geometric point of view. We introduce the canonical
    fundamental region which gives rise to the modular tessellation of
    the upper half-plane. Additionally, we present a general method
    for nding fundamental regions with respect to subgroups of the
    modular group based on the concepts of 2-dimensional hyperbolic
    geometry.

    In Chapter 3 we give some concrete examples how the developed results and
    methods can be exploited for the visualization of certain mathematical
    results. Besides the visualization of function graphs of modular
    functions, a particularly nice result is the connection between
    modular transformations and continued fraction expansions.",
  paper = "Paul14.pdf"
}  

@article{Stou79,
  author = "Stoutemyer, David R.",
  title = {{LISP Based Symbolic Math Systems}},
  journal = "Byte Magazine",
  volume = "8",
  pages = "176-192",
  year = "1979",
  link = "\url{https://ia902603.us.archive.org/30/items/byte-magazine-1979-08/1979_08_BYTE_04-08_LISP.pdf}",
  comment =
    "SCRATCHPAD is a very large computer-algebra system implemented by the
    IBM Thomas J. Watson Research Center. It is available there on an IBM
    370, and it is available from other IBM corporate sites via
    telephone. Regrettably, this fine system has not yet been released to
    the public, but it is discussed here because of its novel features.
    
    In its entirety, the system occupies about 1600K bytes on an IBM 370
    with virtual storage, for which an additional minimum of 100 K bytes
    is recommeded for workspace. The variety of built-in transformations
    currently lies between that of REDUCE and MACSYMA. However, each of
    the three systems has features that none of the others possess, and
    one of these features may be a decisive advantage for a particular
    application. Here are some highlights of the SCRATCHPAD system:
    \begin{itemize}
    \item The system provides single-precision floating-point arithmetic
    as well as indefinite-precision rational arithmetic
    \item The built-in unavoidable and optional algebraic transformations
    are approximately similar to those of MACSYMA.
    \item The built-in exponential, logarithmic, and trigonometric
    transformations are approximately similar to those of REDUCE.
    \item Besides built-in symbolic matrix algebra, APL like array
    operations are included, and they are even further generalized to
    permit symbolic operations of nonhomogeneous arrays and on arrays
    of indefinite or infinite size.
    \item Symbolic differentiation and integration are built-in, with
    the latter employing the powerful Risch-Normal algorithm.
    \item There is a particularly elegant built-in facility for
    determining Taylor series expansions.
    \item There is a built-in SOLVE function capable of determining the
    exact solution to a system of linear equations.
    \item There is a powerful pattern-matching facility which serves as
    the primary mechanism for user level extensions. The associated syntax
    is at a very high level, being the closest of all computer algebra
    systems to the declarative, nonprocedural notation of mathematics.
    To implement the trigonometric multiple-angle expansions, we can
    merely enter the rewrite rules:
    \[cos(n*x) == 2*cos(x)*cos((n-1)*x)-cos((n-2)*x), n{\rm\ in\ }
    (2,3,...), x{\rm\ arb}\]
    \[sin(n*x) == 2*cos(x)*sin((n-1)*x)-sin((n-2)*x), n{\rm\ in\ }
    (2,3,...),x{\rm\ arb}\]
    Then, whenever we subsequently enter an expression such as 
    $cos(4*b)$, the response will be a corresponding expanded
    expression such as
    \[8*cos(B) - 8*cos^2(B)+1\]
    Thus, programs resemble a collection of math formulae, much as they
    would appear in a book or article.
    \item SCRATCHPAD has a particularly powerful yet easily used mechanism
    for controlling the output format of expressions.. For example, the user
    can specify that an expression be displayed as a power series in x,
    with coefficients which are factored rational functions in b and c,
    etc. For large expressions, such fine control over the output may
    mean the difference between an important new discovery and an
    inconprehensible mess.
    \end{itemize}
    
    This generalized recursive format idea is so natural and effective
    that SCRATCHPAD is now absorbing the idea into an internal 
    representation. A study of the polynomial additional algorithm in
    the previous section reveals that it is written to be applicable
    to any coefficient domain which has the algebraic properties of a
    {\sl ring}. The coefficients could be matrices, power-series, etc.
    That coefficient domain could in turn have yet another coefficient
    domain, and so on. With a careful modular design, packages to treat
    each of these domains can be dynamically linked together so that
    code can be shared and combined in new ways without extensive
    rewriting and duplication. Then not only the output, but also the
    internal computations can be selected most suitably for a particular
    application.
    
    For further information about SCRATCHPAD, contact Richard Jenks
    at the IBM Thomas J. Watson Research Center, Yorktown Heights, NY
    10598",
  paper = "Stou79.pdf",
  keywords = "axiomref"
}

@misc{Vaki98,
  author = "Vakil, Ravi",
  title = {{A Beginner's Guide to Jet Bundles fromthe Point of View of
           Algebraic Geometry}},
  link = "\url{http://math.stanford.edu/~vakil/files/jets.pdf}",
  year = "1998",
  paper = "Vaki98.pdf"
}

@article{Wang74,
  author = "Wang, Paul S.",
  title = {{The Undecidability of the Existence of Zeros of Real Elementary
           Functions}},
  journal = "J. ACM",
  volume = "21",
  number = "4",
  pages = "586-589",
  year = "1974",
  abstract =
    "From Richardson's undecidability results, it is shown that the predicate
    ``there exists a real number $r$ such that $G(r)=0$'' is recursively
    undecidable for $G(x)$ in a class of functions which involves polynomials
    and the sine function. The deduction follows that the convergence of a
    class of improper integrals is recursively undecidable.",
  paper = "Wang74.djvu"
}

@article{Lang02,
  author = "Langley, Simon and Richardson, Daniel",
  title = {{What can we do with a Solution?}},
  journal = "Electronic Notes in Theoretical Computer Science",
  volume = "66",
  number = "1",
  year = "2002",
  link = "\url{http://www.elsevier.nl/locate/entcs/volume66.html}",
  abstract =
    "If $S=0$ is a system of $n$ equations and unknowns over $\mathbb{C}$
    and $S(\alpha)=0$ to what extent can we compute with the point $\alpha$?
    In particular, can we decide whether or not a polynomial expressions 
    in the components of $\alpha$ with integral coefficients is zero?
    This question is considered for both algebraic and elementary systems
    of equations.",
  paper = "Lang02.pdf"
}

@article{John71,
  author = "Johnson, S.C.",
  title = {{On the Problem of Recognizing Zero}},
  journal = "J. ACM",
  volume = "18",
  number = "4",
  year = "1971",
  pages = "559-565",
  paper = "John71.djvu"
}

@article{Rich07,
  author = "Richardson, Daniel",
  title = {{How to Recognize Zero}},
  journal = "J. Symbolic Computation",
  volume = "24",
  number = "6",
  year = "2007",
  pages = "627-645",
  abstract =
    "An elementary point is a point in complex $n$ space, which is an
    isolated, non-singular solution of $n$ equations in $n$ variables,
    each equation being either or the form $p=0$, where $p$ is a polynomial
    in $\mathbb{Q}[x_1,\ldots,x_n]$, or of the form $x_j=e^{x_i}=0$. An
    elementary number is the polynomial image of an elementary point. In
    this article a semi-algorithm is given to decide whether or not a
    given elementary number is zero. It is proved that this semi-algorithm
    is an algorithm, i.e. that it always terminates, unless it is given
    a problem containing a counter example to Schanuel's conjecture.",
  paper = "Rich07.pdf"
}

@inproceedings{Hurx00,
  author = "Hur, Namhyun and Davenport, James H.",
  title = {{An exact real algebraic arithmetic with equality determination}},
  booktitle = "Proc. ISSAC 2000",
  series = "ISSAC '00",
  pages = "169-174",
  year = "2000",
  abstract =
    "We describe a new arithmetic model for real algebraic numbers with
    an exact equality determination. The model represents a real algebraic
    number as a pair of an arbitrary precision numerical value and a
    symbolic expression. For the numerical part we currently (another
    representation could be used) use the dyadic exact real number and 
    for the symbolic part we use a square-free polynomial for the real
    algebraic number. In this model we show that we can decide exactly
    the equality of real algebraic numbers.",
  paper = "Hurx00.djvu"
}

@article{Chow99,
  author = "Chow, Timothy Y.",
  title = {{What is a closed-form number?}},
  journal = "The American Mathematical Monthly",
  volume = "106",
  number = "5",
  pages = "440-448",
  year = "1999",
  paper = "Chowxx.pdf"
}

@book{Yapx00,
  author = "Yap, Chee-Keng",
  title = {{Fundamental Problems in Algorithmic Algebra}},
  publisher = "John Wiley and Sons",
  year = "2000",
  link = "\url{http://www.csie.nuk.edu.tw/~cychen/gcd/Fundamental%20Problems%20in%20Algorithmic%20Algebra.pdf}",
  abstract =
    "The author shows an interesting way to write arithmetic operations. 
    Given 
    \[z(x,y)=\frac{axy+bx+cy+d}
                  {a^{\prime}xy+b^{\prime}x+c^{\prime}y+d^{\prime}}\]
    we call the numerical constants $a,b,\ldots,c^{\prime},d^{\prime}$ 
    {\sl state variables} and use the compact notation
    \[z(x,y)=\frac{(a,b,c,d)}{(a^{\prime},b^{\prime},c^{\prime},d^{\prime})}
    {x \choose y}\]
    The arithmetic operations can be recovered by suitable choices for
    the state variables:
    \[x+y=\frac{(0,1,1,0)}{(0,0,0,1)}{x \choose y}\]
    \[x-y=\frac{(0,1,-1,0)}{(0,0,0,1)}{x \choose y}\]
    \[xy=\frac{(1,0,0,0)}{(0,0,0,1)}{x \choose y}\]
    \[x/y=\frac{(0,1,0,0)}{(0,0,1,0)}{x \choose y}\]
    \[\frac{ax+b}{cx+d}=\frac{(0,a,0,b)}{(0,c,0,d)}{x \choose y}\]
    and if
    \[x=q+\frac{p}{x^{\prime}}\]
    then
    \[\begin{array}{ccc}
    z(x,y)&=&\displaystyle\frac{a(q+p/x^{\prime})y+b(q+p/x^{\prime})+cy+d}
                  {a^{\prime}(q+p/x^{\prime})y+
                   b^{\prime}(q+p/x^{\prime})+
                   c^{\prime}y+d^{\prime}}\\
    &=&\displaystyle\frac{(aq+c,bq+d,ap,bp)}
             {(a^{\prime}q+c^{\prime},b^{\prime}q+d^{\prime},a^{\prime}p,
               b^{\prime}p)}{x^{\prime} \choose {y}}
    \end{array}\]",
  paper = "Yapx00.pdf"
}

@misc{Yap02a,
  author = "Yap, Chee-Keng",
  title = {{Problem of Algebra Lecture 0: Introduction}},
  year = "2002",
  link = 
    "\url{https://people.eecs.berkeley.edu/~fateman/282/readings/yap-0.pdf}",
  paper = "Yap02a.pdf"
}

@misc{Yap02b,
  author = "Yap, Chee-Keng",
  title = {{Lecture II: The GCD}},
  year = "2002",
  link = 
    "\url{https://people.eecs.berkeley.edu/~fateman/282/readings/yap-2.pdf}",
  paper = "Yap02b.pdf"
}

@article{Burn00,
  author = "Burnikel, C. and Fleischer, R. and Mehlhom, K. and Schirra, S.",
  title = {{A Strong and Easily Computable Separation Bound for Arithmetic
           Expressions Involving Radicals}},
  journal = "Algorithmica",
  volume = "27",
  pages = "87-99",
  year = "2000",
  abstract =
    "We consider arithmetic expressions over operators $+$, $-$, $*$, $/$,
    and $\sqrt{k}$, with integer operands. For an expression $E$ having 
    value $\eta$, a separation bound $sep(E)$ is a positive real number with
    the property that $\eta \ne 0$ implies $\vert\eta\vert \ge sep(E)$. We
    propose a new separation bound that is easy to compute and stronger
    than previous bounds.",
  paper = "Burn00.pdf"
}

@article{Corl97a,
  author = "Corless, Robert M. and Jeffrey, David J.",
  title = {{The Turing Factorization of a Rectangular Matrix}},
  journal = "ACM SIGSAM Bulletin",
  volume = "31",
  number = "3",
  pages = "28-35",
  year = "1997",
  abstract =
    "The Turing factorization is a generalization of the standard LU
    factoring of a square matrix. Among other advantages, it allows us to
    meet demands that arise in a symbolic context. For a rectangular
    matrix A, the generalized factors are written PA = LDU R, where R is
    the row-echelon form of A. For matrices with symbolic entries, the LDU
    R factoring is superior to the standard reduction to row-echelon form,
    because special case information can be recorded in a natural
    way. Special interest attaches to the continuity properties of the
    factors, and it is shown that conditions for discontinuous behaviour
    can be given using the factor D. We show that this is important, for
    example, in computing the Moore-Penrose inverse of a matrix containing
    symbolic entries.We also give a separate generalization of LU
    factoring to fraction-free Gaussian elimination.",
  paper = "Corl97a.pdf"
}

@inproceedings{Corl97,
  author = "Corless, Robert M. and Jeffrey, David J. and Knuth, Donald E.",
  title = {{A Sequence of Series for The Lambert W Function}},
  booktitle = "Proc. ISSAC 1997",
  series = "ISSAC '97",
  pages = "197-204",
  year = "1997",
  abstract =
    "We give a uniform treatment of several series expansions for the 
    Lambert $W$ function, leading to an infinite family of new series.
    We also discuss standardization, complex branches, a family of
    arbitrary-order iterative methods for computation of $W_i$, and
    give a theorem showing how to correctly solve another simple and
    frequently occurring nonlinear equation in terms of $W$ and the
    unwinding number",
  paper = "Corl97.pdf"
}

@article{Corl96,
  author = "Corless, Robert M. and Jeffrey, David J.",
  title = {{Editor's Corner: The Unwinding Number}},
  journal = "SIGSAM Bulletin",
  volume = "30",
  number = "1",
  issue = "115",
  pages = "28-35",
  year = "1996",
  abstract =
    "From the Oxford English Dictionary we find that {\sl to unwind}
    can mean ``to become free from a convoluted state''. Further down
    we find the quationation ``The solutoin of all knots, and unwinding
    of all intricacies'', from H. Brooke (The Fool of Quality, 1809).
    While we do not promise that the unwinding number, defined below,
    will solve {\sl all} intricacies, we do show that it may help for
    quite a few problems.",
  paper = "Corl96.pdf"
}

@article{Corl98,
  author = "Corless, Robert M. and Jeffrey, David J.",
  title = {{Graphing Elementary Riemann Surfaces}},
  journal = "SIGSAM Bulletin",
  volume = "32",
  number = "1",
  pages = "11-17",
  year = "1998",
  abstract =
    "This paper discusses one of the prettiest pieces of elementary
    mathematics or computer algebra, that we have ever had the pleasure
    to learn. The tricks that we discuss here are certainly ``well-known''
    (that is, in the literature), but we didn't know them until recently,
    and none of our immediate colleagues knew them either. Therefore we
    believe that it is useful to publicize them further. We hope that
    you find these ideas as pleasant and useful as we do.",
  paper = "Corl98.pdf"
}

@article{Wang93,
  author = "Wang, Dongming",
  title = {{An Elimination Method for Polynomial Systems}},
  journal = "J. Symbolic Computation",
  volume = "16",
  number = "2",
  pages = "83-114",
  year = "1993",
  abstract =
    "We present an elimination method for polynomial systems, in the form
    of three main algorithms. For any given system [$\mathbb{P}$,$\mathbb{Q}$] 
    of two sets of multivariate polynomials, one of the algorithms computes a 
    sequence of triangular forms $\mathbb{T}_1,\ldots,\mathbb{T}_e$ and 
    polynomial sets $\mathbb{U}_1,\ldots,\mathbb{U}_e$ such that
    Zero($\mathbb{P}$/$\mathbb{Q}$) 
    $= \cup_{i=1}^e {\rm\ Zero}(\mathbb{T}_i/\mathbb{U}_i)$, 
    where Zero($\mathbb{P}$/$\mathbb{Q}$) denotes the set of common zeros of 
    the polynomials in $\mathbb{P}$ which are not zeros of any polynomial in 
    $\mathbb{Q}$, and similarly for Zero($\mathbb{T}_i$/$\mathbb{U}_i$). 
    The two other algorithms compute the same zero decomposition but with nicer
    properties such as Zero$(\mathbb{T}_i/\mathbb{U}_i) \ne 0$ for each $i$. 
    One of them, for which the computed triangular systems 
    [$\mathbb{T}_i$, $\mathbb{U}_i$] possess the projection property, provides
    a quantifier elimination procedure for algebraically closed fields. 
    For the other, the computed triangular forms $\mathbb{T}_i$ are 
    irreducible. The relationship between our method and some existing 
    elimination methods is explained. Experimental data for a set of test 
    examples by a draft implementation of the method are provided, and show 
    that the efficiency of our method is comparable with that of some 
    well-known methods. A few encouraging examples are given in detail for 
    illustration.",
  paper = "Wang93.pdf"
}

@article{Wang94,
  author = "Wang, Dongming",
  title = {{Differentiation and Integration of Indefinite Summations with
           Respect to Indexed Variables - Some Rules and Applications}},
  journal = "J. Symbolic Computation",
  volume = "18",
  number = "3",
  pages = "249-263",
  year = "1994",
  abstract = 
    "In this paper we present some rules for the differentiation and
    integration of expressions involving indefinite summations with
    respect to indexed variables which have not yet been taken into
    account of current computer algebra systems. These rules, together
    with several others, have been implemented in MACSYMA and MAPLE as a
    toolkit for manipulating indefinite summations. We discuss some
    implementation issues and report our experiments with a set of typical
    examples. The present work is motivated by our investigation in the
    computer-aided analysis and derivation of artificial neural systems. 
    The application of our rules to this subject is briefly explained.",
  paper = "Wang94.pdf"
}

@article{Wang95a,
  author = "Wang, Dongming",
  title = {{A Method for Proving Theorems in Differential Geometry and
           Mechanics}},
  journal = "J. Universal Computer Science",
  volume = "1",
  number = "9",
  pages = "658-673",
  year = "1995",
  link = "\url{http://www.jucs.org/jucs\_1\_9/a\_method\_for\_proving}",
  abstract =
    "A zero decomposition algorithm is presented and used to devise a
    method for proving theorems automatically in differential geometry and
    mechanics. The method has been implemented and its practical
    efficiency is demonstrated by several non-trivial examples including
    Bertrand s theorem, Schell s theorem and Kepler-Newton s laws.",
  paper = "Wang95a.pdf"
}

@misc{Wang98,
  author = "Wang, Dongming",
  title = {{Decomposing Polynomial Systems into Simple Systems}},
  volume = "25",
  number = "3",
  pages = "295-314",
  year = "1998",
  abstract =
    "A simple system is a pair of multivariate polynomial sets (one set
    for equations and the other for inequations) ordered in triangular
    form, in which every polynomial is squarefree and has non-vanishing
    leading coefficient with respect to its leading variable. This paper
    presents a method that decomposes any pair of polynomial sets into
    finitely many simple systems with an associated zero decomposition. 
    The method employs top-down elimination with splitting and the 
    formation of subresultant regular subchains as basic operation.",
  paper = "Wang98.pdf"
}

@article{Wang99,
  author = "Wang, Dongming",
  title = {{Polynomial Systems from Certain Differential Equations}},
  journal = "J. Symbolic Computation",
  volume = "28",
  number = "1-2",
  pages = "303-315",
  year = "1999",
  abstract = 
    "In this paper, combined elimination techniques are applied to
    establish relations among center conditions for certain cubic
    differential systems initially investigated by Kukles in 1944. The
    obtained relations clarify recent rediscoveries of some known
    conditions of Cherkas. The computational difficulties of establishing
    the complete center conditions for Kukles’ system, a problem that is
    still open, are illustrated by interactive elimination. Some generated
    polynomial systems that need be solved are made available
    electronically for other developers to test elimination algorithms and
    their implementations.",
  paper = "Wang99.pdf"
}

@article{Wang00,
  author = "Wang, Dongming",
  title = {{Computing Triangular Systems and Regular Systems}},
  journal = "J. Symbolic Computation",
  volume = "30",
  number = "2",
  pages = "221-236",
  year = "2000",
  abstract = 
    "A previous algorithm of computing simple systems is modified and
    extended to compute triangular systems and regular systems from any
    given polynomial system. The resulting algorithms, based on the
    computation of subresultant regular subchains, have a simple structure
    and are efficient in practice. Preliminary experiments indicate that
    they perform at least as well as some of the known algorithms. Several
    properties about regular systems are also proved.",
  paper = "Wang00.pdf"
}

@article{Wang04,
  author = "Wang, Dongming",
  title = {{A simple method for implicitizing rational curves and surfaces}},
  journal = "J. Symbolic Computation",
  volume = "38", 
  number = "1",
  pages = "899-914",
  year = "2004",
  abstract = 
    "This paper presents a simple method for converting rational
    parametric equations of curves and surfaces into implicit
    equations. The method proceeds via writing out the implicit polynomial
    $F$ of estimated degree with indeterminate coefficients $u_i$,
    substituting the rational expressions for the given parametric curve
    or surface into $F$ to yield a rational expression $g/h$ in the
    parameter $s$ (or $s$ and $t$), equating the coefficients of $g$ in
    terms of $s$ (and $t$) to 0 to generate a sparse, partially triangular
    system of linear equations in $u_i$ with constant coefficients, and
    finally solving the linear system for ui. If a nontrivial solution is
    found, then an implicit polynomial is obtained; otherwise, one repeats
    the same process, increasing the degree of $F$. Our experiments show
    that this simple method is efficient. It performs particularly well in
    the presence of base points and may detect the dependency of
    parameters incidentally.",
  paper = "Wang04.pdf"
}

@inproceedings{Brad92,
  author = "Bradford, Russell",
  title = {{Algebraic Simplification of Multiple-Valued Functions}},
  booktitle = "Proc. DISCO 92",
  series = "Lecture Notes in Computer Science 721",
  year = "1992",
  abstract =
    "Many current algebra systems have a lax attitude to the
    simplification of expressions involving functions like log and
    $\sqrt{}$, leading the the ability to ``prove'' equalities like $-1=1$
    in such systems. In fact, only a little elementary arithmetic is
    needed to devise what the correct simplification should be. We detail
    some of these simplification rules, and outline a method for their
    incorporation into an algebra system.",
  paper = "Brad92.djvu"
}

@misc{Kais09,
  author = "Kaisler, Stephen H. and Madey, Gregory",
  title = {{Complex Adaptive Systems: Emergence and Self-Organization}},
  year = "2009",
  institution = "University of Notre Dame",
  comment = "source for Sweeney.eps",
  link = "\url{http://www3.nd.edu/~gmadey/Activities/CAS-Briefing.pdf}"
}

@article{Kalm01,
  author = "Kalman, Dan",
  title = {{A Generalized Logarithm for Exponential-Linear Equations}},
  journal = "The College Mathematics Journal",
  volume = "32",
  number = "1",
  year = "2001",
  abstract =
    "How do you solve the equation
    \[1.6^x = 5054.4 - 122.35*x\]
    We will refer to equations of this type, with an exponential
    expression on one side and a linear one on the other, as
    {\sl exponential-linear} equations. Numerical approaches such as Newton’s
    method or bisection quickly lead to accurate approximate solutions of
    exponential-linear equations. But in terms of the elementary functions
    of calculus and college algebra, there is no analytic solution.
    
    One approach to remedying this situation is to introduce a special
    function designed to solve exponential-linear equations. Quadratic
    equations, by way of analogy, are solvable in terms of the special
    function $\sqrt{x}$ , which in turn is simply the inverse of a very
    special and simple quadratic function. Similarly, exponential
    equations are solvable in terms of the natural logarithm {\sl log},
    and that too is the inverse of a very special function. So it is
    reasonable to ask whether there is a special function in terms of
    which exponential-linear equations might be solved. Furthermore, an
    obvious strategy for finding such a function is to invert some simple
    function connected with exponential-linear equations.
    
    This line of thinking proves to be immediately successful, and leads
    to a function I call {\sl glog} (pronounced {\sl gee-log}) which is a
    kind of generalized logarithm. As intended, glog can be used to solve
    exponential-linear equations. But that is by no means all it is good
    for. For example, with glog you can write a closed-form expression for
    the iterated exponential ($x^{x^{x^.}}$), and solve $x + y = x^y$ for
    $y$. The glog function is also closely related to another special
    function, called the Lambert $W$ function in [3] and [6], whose study
    dates to work of Lambert in 1758 and of Euler in 1777. Interesting
    questions about glog arise at every turn, from symbolic integration,
    to inequalities and estimation, to numerical computation. Elaborating
    these points is the goal of this paper.",
  paper = "Kalm01.pdf"
}

@phdthesis{Dewa91,
  author = "Dewar, Michael C.",
  title = {{Interfacing algebraic and numeric computation}},
  year = "1991",
  school = "University of Bath, UK, England"
}

@misc{Fate92,
  author = "Fateman, Richard J. and Einwohner, Theodore H.",
  title = {{A Proposal for Automated Integral Tables (Work in Progress)}},
  year = "1992",
  link = "\url{http://people.eecs.berkeley.edu/~fateman/papers/intable.pdf}",
  abstract =
    "One of the long-term general goals of algebraic manipulation systems
    has been the automation of difficult or tedious, yet common, symbolic
    mathematical operations. Prominent amount these has been symbolic
    integration. Although some effective algorithms have been devised for
    integration expecially for those problems solvable in terms of
    elementary functions and a few additional special functions, the vast
    majority of entires in large tables of indefinite and definite
    integrals remain out of reach of current machine algorithms. We
    propose techniques for introducing the information in such tables to
    computers, extending such tables, and measuring the success of such
    automation.
    
    Similar tabular data concerning simplifications, summation identities,
    and similar formulas could also be treated by some of the same
    techniques.",
  paper = "Fate92.pdf"
}

@misc{Atha03,
  author = "Athale, Rahul Ramesh and Diaz, Glauco Alfredo Lopez",
  title = {{Explaining Schwarz's LODEF algorithm with examples}},
  link = "\url{http://www.risc.jku.at/publications/download/risc\_1539/03-11.ps.gz}",
  year = "2003",
  abstract =
    "To every linear homogeneous ordinary differential equation
    \[L(y)=y^{(n)}+a_1y^{(n-1)}+\cdots+a_{n-1}y^{\prime}+a_ny=0\]
    one can associate the linear operator:
    \[L(D)[y]=(D^n+a_1D^{n-1}+\cdots+a_{n-1}D+a_n)[y]\]
    Here, $D^i$ is another notation for the $i$-th derivative of $y$,
    and the coefficients $a_i$ belong to a differential field $K$.
    Such an operator is called a linear homogeneous differential operator
    either over $K$, or with coefficients in $K$. Linear homogeneous
    differential operators over $K$ form a ring under the usual addition
    of operators and composition as multiplication.
    
    {\bf Problem}: Decompose $L$ as a product of operators of lower degree
    in $D$. We allow expressions algebraic over $K$.",
  paper = "Atha03.pdf"
}

@techreport{Chou89,
  author = "Chou, Shang-Ching and Gao, Xiao-Shan",
  title = {{A Collection of 120 Computer Solved Geometry Problems in
           Mechanical Formula Derivation}},
  institution = "University of Texas, Austin",
  link = "\url{http://www.cs.utexas.edu/ftp/techreports/tr89-22.pdf}",
  type = "technical report",
  number = "tr-89-22",
  year = "1989",
  abstract = 
    "This is a collection of 120 geometric problems mechanically solved by
    a program based on the methods introduced by us. Researchers can use
    this collection to experiment with their methods/programs similar to
    ours. It consists of two parts: the exact specification of the input
    to our program and a collection of 120 examples. A typical example
    consists of an informal description of the geometric problem, the
    input to the program which is the exact specification of the problem,
    the result of the problem, and a diagram.",
  paper = "Chou89.pdf"
}  

@book{Chou94,
  author = "Chou, Shang-Ching and Gao, Xiao-Shan and Zhang, Jing-Zhong",
  title = {{Machine Proofs in Geometry: Automated Production of Readable
           Proofs for Geometry Theorems}},
  publisher = "World Scientific",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.374.11778}",
  year = "1994",
  paper = "Chou94.pdf"
}  

@misc{Stur00,
  author = "Sturmfels, Bernd",
  title = {{Solving Systems of Polynomial Equations}},
  link = "\url{https://math.berkeley.edu/~bernd/cbms.pdf}",
  year = "2000",
  abstract =
    "One of the most classical problems of mathematics is to solve systems
    of polynomial equations in several unknowns. Today, polynomial
    models are ubiquitous and widely applied across the sciences. They
    arise in robotics, coding theory, optimization, mathematical
    biology, computer vision, game theory, statistics, machine learning,
    control theory, and numerous other areas.  The set of solutions to a
    system of polynomial equations is an algebraic variety, the basic
    object of algebraic geometry. The algorithmic study of algebraic 
    varieties is the central theme of computational algebraic
    geometry. Exciting recent developments in symbolic algebra and
    numerical software for geometric calculations have revolutionized
    the field, making formerly inaccessible problems tractable, and
    providing fertile ground for experimentation and conjecture.
    
    The first half of this book furnishes an introduction and represents a
    snapshot of the state of the art regarding systems of polynomial
    equations.  Afficionados of the well-known text books by Cox, Little,
    and O’Shea will find familiar themes in the first five chapters:
    polynomials in one variable, Groebner bases of zero-dimensional
    ideals, Newton polytopes and Bernstein’s Theorem, multidimensional
    resultants, and primary decomposition.
    
    The second half of this book explores polynomial equations from a
    variety of novel and perhaps unexpected angles. Interdisciplinary
    connections are introduced, highlights of current research are
    discussed, and the author’s hopes for future algorithms are
    outlined. The topics in these chapters include computation of Nash
    equilibria in game theory, semidefinite programming and the real
    Nullstellensatz, the algebraic geometry of statistical models, the
    piecewise-linear geometry of valuations and amoebas, and the
    Ehrenpreis-Palamodov theorem on linear partial differential equations
    with constant coefficients.
    
    Throughout the text, there are many hands-on examples and exercises,
    including short but complete sessions in the software systems maple,
    matlab, Macaulay 2, Singular, PHC, and SOStools . These examples
    will be particularly useful for readers with zero background in
    algebraic geometry or commutative algebra. Within minutes, anyone can
    learn how to type in polynomial equations and actually see some
    meaningful results on the computer screen.",
  paper = "Stur00.pdf"
}

@misc{NTCI16,
  author = "NTCIR",
  title = {{Axiom (computer algebra system)}},
  link = "\url{http://ntcir11-wmc.nii.ac.jp/index.php/Axiom\_(computer_algebra_system)}",
  year = "2016",
  keywords = "axiomref"
}

@misc{Vers16,
  author = "Verstraete, Jacques",
  title = {{Combinatorial Calculus of Formal Power Series}},
  comment = "264A Lecture B",
  link = "\url{http://www.math.ucsd.edu/~jverstra/264A-LECTUREB.pdf}",
  paper = "Vers16.pdf"
}  

@article{Koep92,
  author = "Koepf, Wolfram",
  title = {{Power Series in Computer Algebra}},
  journal = "J. Symbolic Computation",
  volume = "13",
  pages = "581-603",
  year = "1992",
  abstract = 
    "Formal power series (FPS) of the form
    $\sum_{k=0}^{\infty}{a_k(x-x_0)^k}$ are important in calculus and
    complex analysis.  In some Computer Algebra Systems (CASs) it is
    possible to define an FPS by direct or recursive definition of its
    coefficients.  Since some operations cannot be directly supported
    within the FPS domain, some systems generally convert FPS to finite
    truncated power series (TPS) for operations such as addition,
    multiplication, division, inversion and formal substitution.  This
    results in a substantial loss of information.  Since a goal of
    Computer Algebra is - in contrast to numerical programming - to work
    with formal objects and preserve such symbolic information, CAS should
    be able to use FPS when possible.

    There is a one-to-one correspondence between FPS with positive radius
    of convergence and corresponding analytic functions.  It should be
    possible to automate conversion between these forms.  Among CASs
    only MACSYMA provides a procedure {\tt powerseries} to calculate FPS from
    analytic expressions in certain special cases, but this is rather
    limited.
    
    Here we give an algorithmic approach for computing an FPS for a
    function from a very rich family of functions including all of the
    most prominent ones that can be found in mathematical dictionaries
    except those where the general coefficient depends on the Bernoulli,
    Euler, or Eulerian numbers.  The algorithm has been implemented by the
    author and A.  Rennoch in the CAS MATHEMATICA, and by D.  Gruntz in
    MAPLE.
    
    Moreover, the same algorithm can sometimes be reversed to calculate a
    function that corresponds to a given FPS, in those cases when a
    certain type of ordinary differential equation can be solved.",
  paper = "Koep92.pdf"
}

@article{Asla96,
  author = "Aslaksen, Helmer",
  title = {{Multiple-valued complex functions and computer algebra}},
  journal = "SIGSAM Bulletin",
  volume = "30",
  number = "2",
  year = "1996",
  pages = "12-20",
  link = "\url{http://www.math.nus.edu.sg/aslaksen/papers/cacas.pdf}",
  abstract =
    "I recently taught a course on complex analysis. That forced me to
    think more carefully about branches. Being interested in computer
    algebra, it was only natural that I wanted to see how such programs
    dealt with these problems. I was also inspired by a paper by
    Stoutemyer.
    
    While programs like Derive, Maple, Mathematica and Reduce are very
    powerful, they also have their fair share of problems. In particular,
    branches are somewhat of an Achilles' heel for them. As is well-known,
    the complex logarithm function is properly defined as a
    multiple-valued function. And since the general power and exponential
    functions are defined in terms of the logarithm function, they are
    also multiple-valued. But for actual computations, we need to make
    them single valued, which we do by choosing a branch. In Section 2, we
    will consider some transformation rules for branches of
    multiple-valued complex functions in painstaking detail.
    
    The purpose of this short article is not to do a comprehensive
    comparative study of different computer algebra systems. My goal is
    simply to make the readers aware of some of the problems, and to
    encourage the readers to sit down and experiment with their favourite
    programs.",
  paper = "Asla96.pdf"
}

@article{Tonixx,
  author = "Tonisson, Eno",
  title = {{Branch Completeness in School Mathematics and in Computer Algebra
           Systems}},
  journal = "The Electronic Journal of Mathematics and Technology",
  volume = "1",
  number = "1",
  year = "2000",
  issn = "1933-2823",
  link = "\url{https://php.radford.edu/~ejmt/deliveryBoy.php?paper=eJMT_v1n3p5}",
  abstract =
    "In many cases when solving school algebra problems (e.g. simplifying
    an expression, solving an equation), the solution is separable into
    branches in some manner. The paper describes some approaches to
    branches that are used in school textbooks and computer algebra
    systems and compares them with mathematically branch-complete
    solutions. It tries to identify possible reasons behind different
    approaches and also indicate some ideas how such differences could be
    explained to the students.",
  paper = "Tonixx.pdf"
}

@article{Stou91,
  author = "Stoutemyer, David R.",
  title = {{Crimes and misdemeanors in the computer algebra trade}},
  journal = "Notices of the American Mathematical Society",
  volume = "38",
  number = "7",
  pages = "778-785",
  year = "1991"
}

@misc{Sang10a,
  author = "Sangwin, Chris",
  title = {{Intriguing Integrals: Part I and II}},
  year = "2010",
  link = "\url{https://plus.maths.org/issue54/features/sangwin/2pdf/index.html/op.pdf}",
  paper = "Sang10a.pdf",
}

@misc{Sang10b,
  author = "Sangwin, Chris",
  title = {{Intriguing Integrals: Part I and II}},
  year = "2010",
  link = "\url{https://plus.maths.org/issue54/features/sangwin2/2pdf/index.html/op.pdf}",
  paper = "Sang10b.pdf"
}

@misc{Evanxx,
  author = "Evans, Brian",
  title = {{History of CA Systems}},
  link = "\url{http://felix.unife.it/Root/d-Mathematics/d-The-mathematician/d-History-of-mathematics/t-History-of-computer-algebra}",
  paper = "Evanxx.txt"
}

@misc{Apel94,
  author = "Apel, Joachim and Klaus, Uwe",
  title = {{Representing Polynomials in Computer Algebra Systems}},
  year = "1994",
  abstract =
    "There are discussed implementational aspects of the special-purpose
    computer algebra system FELIX designed for computations in
    constructive algebra. In particular, data types developed for the
    representation of and computation with commutative and non-commuative
    polynomials are described. Furthermore, comparison of time and memory
    requirements of different polynomial representations are reported.",
  paper = "Apel94.pdf"
}

@book{Juds15,
  author = "Judson, Thomas W.",
  title = {{Abstract Algebra: Theory and Applications}},
  year = "2015",
  link = "\url{http://abstract.ups.edu/aata/colophon-1.html}",
  publisher = "Website"
}

@InProceedings{Kalt83,
  author = "Kaltofen, E.",
  title = {{On the complexity of finding short vectors in integer lattices}},
  booktitle = "Proc. EUROCAL '83",
  series = "Lect. Notes Comput. Sci.",
  year = "1983",
  volume = "162",
  pages = "236--244",
  publisher = "Springer-Verlag",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/83/Ka83_eurocal.pdf}",
  paper = "Kalt83.pdf"
}

@InProceedings{Kalt85,
  author = "Kaltofen, E.",
  title = {{Effective {Hilbert} Irreducibility}},
  booktitle = "Proc. EUROSAM '84",
  pages = "275--284",
  year = "1985",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/85/Ka85_infcontr.ps.gz}",
  paper = "Kalt85.ps"
}
 
@TechReport{Kalt85c,
  author = "Kaltofen, E.",
  title = {{Sparse Hensel lifting}},
  institution = "RPI",
  address = "Dept. Comput. Sci., Troy, N. Y.",
  year = "1985",
  number = "85-12",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/85/Ka85_techrep.pdf}",
  paper = "Kalt85c.pdf"
}

@InProceedings{Kalt85d,
  author = "Kaltofen, E.",
  title = {{Sparse Hensel lifting}},
  booktitle = "EUROCAL 85 European Conf. Comput. Algebra Proc. Vol. 2",
  pages = "4--17",
  year = "1985",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/85/Ka85_eurocal.pdf}",
  paper = "Kalt85d.pdf"
}

@Article{Mill88,
  author = "Miller, G.L. and Ramachandran, V. and Kaltofen, E.",
  title = {{Efficient parallel evaluation of straight-line code and 
           arithmetic circuits}},
  journal = "SIAM J. Comput.",
  year = "1988",
  volume = "17",
  number = "4",
  pages = "687--695",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/88/MRK88.pdf}",
  paper = "Mill88.pdf"
}

@Article{Greg88,
  author = "Gregory, B.; Kaltofen, E.",
  title = {{Analysis of the binary complexity of asymptotically fast 
           algorithms for linear system solving}},
  journal = "SIGSAM Bulletin",
  year = "1988",
  month = "April",
  volume = "22",
  number = "2",
  pages = "41--49",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/88/GrKa88.pdf}",
  paper = "Grey88.pdf"
}

@Article{Kalt89a,
  author = "Kaltofen, E.; Rolletschek, H.",
  title = {{Computing greatest common divisors and factorizations in 
           quadratic number fields}},
  journal = "Math. Comput.",
  year = "1989",
  volume = "53",
  number = "188",
  pages = "697--720",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/89/KaRo89.pdf}",
  paper = "Kalt89a.pdf"
}

@Unpublished{Kalt89b,
  author = "Kaltofen, E.",
  title = {{Processor efficient parallel computation of polynomial greatest 
           common divisors}},
  note = "unknown",
  year = "1989",
  month = "July",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/89/Ka89_gcd.ps.gz}",
  paper = "Kalt89b.ps"
}

@TechReport{Kalt89c,
  author = "Kaltofen, E.",
  title = {{Parallel Algebraic Algorithm Design}},
  institution = "RPI",
  address = "Dept. Comput. Sci., Troy, New York",
  year = "1989",
  month = "July",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/89/Ka89_parallel.ps.gz}",
  paper = "Kalt89c.ps"
}

@InProceedings{Cann89,
  author = "Canny, J. and Kaltofen, E. and Yagati, Lakshman",
  title = {{Solving systems of non-linear polynomial equations faster}},
  booktitle = "Proc. 1989 Internat. Symp. Symbolic Algebraic Comput.",
  pages = "121--128",
  year = "1989",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/89/CKL89.pdf}",
  paper = "Cann89.pdf"
}

@Article{Kalt90b,
  author = "Kaltofen, E.",
  title = {{Computing the irreducible real factors and components of an 
           algebraic curve}},
  journal = "Applic. Algebra Engin. Commun. Comput.",
  year = "1990",
  volume = "1",
  number = "2",
  pages = "135--148",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/90/Ka90_aaecc.pdf}",
  paper = "Kalt90b.pdf"
}

@Article{Kalt90d,
  author = "Kaltofen, E.; Trager, B.",
  title = {{Computing with polynomials given by black boxes for their 
    evaluations: Greatest common divisors, factorization, separation of 
    numerators and denominators}},
  journal = "J. Symbolic Comput.",
  year = "1990",
  volume = "9",
  number = "3",
  pages = "301--320",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/90/KaTr90.pdf}",
  paper = "Kalt90d.pdf"
}

@InProceedings{Kalt91a,
  author = "Kaltofen, E. and Singer, M.F.",
  editor = "D. V. Shirkov and V. A. Rostovtsev and V. P. Gerdt",
  title = {{Size efficient parallel algebraic circuits for partial 
            derivatives}},
  booktitle = 
    "IV International Conference on Computer Algebra in Physical Research",
  pages = "133--145",
  publisher = "World Scientific Publ. Co.",
  year = "1991",
  address = "Singapore",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/91/KaSi91.pdf}",
  paper = "Kalt91a.pdf"
}

@InProceedings{Kalt93,
  author = "Kaltofen, E.",
  title = {{Computational Differentiation and Algebraic Complexity Theory}},
  booktitle = "Workshop Report on First Theory Institute on Computational 
               Differentiation",
  editor = "C. H. Bischof and A. Griewank and P. M. Khademi",
  publisher = "Argonne National Laboratory",
  address = "Argonne, Illinois",
  series = "Tech. Rep.",
  volume = "ANL/MCS-TM-183",
  month = "December",
  year = "1993",
  pages = "28--30",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/93/Ka93_diff.pdf}",
  paper = "Kalt93.pdf"
}

@Article{Kalt93b,
  author = "Kaltofen, E.",
  title = {{Direct proof of a theorem by Kalkbrener, Sweedler, and Taylor}},
  journal = "SIGSAM Bulletin",
  year = "1993",
  volume = "27",
  number = "4",
  pages = "2",
  link = 
"\url{http://www.math.ncsu.edu/~kaltofen/bibliography/93/Ka93_sambull.ps.gz}",
  paper = "Kalt93b.ps"
}

@InProceedings{Kalt94,
  author = "Kaltofen, E. and Pan, V.",
  title = {{Parallel solution of Toeplitz and Toeplitz-like linear 
           systems over fields of small positive characteristic}},
  booktitle = "Proc. First Internat. Symp. Parallel Symbolic Comput.",
  pages = "225--233",
  year = "1994",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/94/KaPa94.pdf}",
  paper = "Kalt94.pdf"
}

@article{Raab13,
  author = "Raab, C.G.",
  title = {{Generalization of Risch's Algorithm to Special Functions}},
  journal = "CoRR",
  volume = "abs/1305.1481",
  year = "2013",
  link = "\url{http://arxiv.org/pdf/1305.1481v1.pdf}",
  abstract =
    "Symbolic integration deals with the evaluation of integrals in closed
    form. We present an overview of Risch's algorithm including recent
    developments. The algorithms discussed are suited for both indefinite
    and definite integration. They can also be used to compute linear
    relations among integrals and to find identities for special functions
    given by parameter integrals. The aim of this presentation is twofold:
    to introduce the reader to some basic ideas of differential algebra in
    the context of integration and to raise awareness in the physics
    community of computer algebra algorithms for indefinite and definite
    integration. ",
  paper = "Raab13.pdf"
}

@InProceedings{Sama95,
  author = "Samadani, M. and Kaltofen, E.",
  title = {{Prediction based task scheduling in distributed computing}},
  booktitle = "Languages, Compilers and Run-Time Systems for Scalable 
               Computers",
  editor = "B. K. Szymanski and B. Sinharoy",
  publisher = "Kluwer Academic Publ.",
  address = "Boston",
  pages = "317--320",
  year = "1996",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/95/SaKa95_poster.ps.gz}",
  paper = "Sama95.ps"
}

@InProceedings{Erli96,
  author = "Erlingsson, U. and Kaltofen, E. and Musser, D.",
  title = {{Generic {Gram}-{Schmidt} Orthogonalization by Exact Division}},
  booktitle = "Proc. 1996 Internat. Symp. Symbolic Algebraic Comput.",
  year = "1996",
  pages = "275--282",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/96/EKM96.pdf}",
  paper = "Erli96.pdf"
}

@InProceedings{Kalt96,
 author = "Kaltofen, E. and Lobo, A.",
 title = {{On rank properties of {Toeplitz} matrices over finite fields}},
  booktitle = "Proc. 1996 Internat. Symp. Symbolic Algebraic Comput.",
  year = "1996",
  pages = "241--249",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/96/KaLo96_issac.pdf}",
  paper = "Kalt96.pdf"
}

@Article{Kalt97,
  author = "Kaltofen, E.",
  title = {{Teaching Computational Abstract Algebra}},
  journal = "Journal of Symbolic Computation",
  volume = "23",
  number = "5-6",
  pages = "503--515",
  year = "1997",
  note = "Special issue on education, L. Lambe, editor.",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/97/Ka97_jsc.pdf}",
  abstract = "
    We report on the contents and pedagogy of a course in abstract algebra
    that was taught with the aid of educational software developed within
    the Mathematica system. We describe the topics covered and the
    didactical use of the corresponding Mathematica packages, as well as
    draw conclusions for future such courses from the students' comments
    and our own experience.",
  paper = "Kalt97.pdf",
  keywords = "axiomref"
}

@Unpublished{Hitz97,
  author = "Hitz, M. A. and Kaltofen, E.",
  title = {{The {Kharitonov} theorem and its applications in symbolic 
           mathematical computation}},
  note = "unknown",
  year = "1997",
  month = "May",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/97/HiKa97_kharit.pdf}",
  paper = "Hitz97.pdf"
}

@InProceedings{Bern99,
  author = "Bernardin, L. and Char, B. and Kaltofen, E.",
  title = {{Symbolic Computation in {Java}: an Appraisement}},
  booktitle = "Proc. 1999 Internat. Symp. Symbolic Algebraic Comput.",
  year = "1999",
  pages = "237--244",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/99/BCK99.pdf}",
  paper = "Bern99.pdf"
}

@InProceedings{Kalt02,
  author = "Kaltofen, Erich and McLean, Michael and Norris, Larry",
  title = {{`{Using} {Maple} to Grade {Maple}' Assessment Software from 
            {North Carolina State University}}},
  booktitle = "Proceedings 2002 Maple Workshop",
  year = "2002",
  publisher = "Waterloo Maple Inc.",
  address = "Waterloo, Canada",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/02/KMN02.pdf}",
  paper = "Kalt02.pdf"
}

@Book{Grab03,
  editor = "Grabmeier, J. and Kaltofen, E. and Weispfenning, V.",
  title = {{Computer Algebra Handbook}},
  publisher = "Springer-Verlag",
  year = "2003",
  note =  "637 + xx~pages + CD-ROM. Includes E. Kaltofen and V. Weispfenning
    \S1.4 Computer algebra -- impact on research, pages 4--6;
    E. Kaltofen
    \S2.2.3 Absolute factorization of polynomials, page 26;
    E. Kaltofen and B. D. Saunders
    \S2.3.1 Linear systems, pages 36--38;
    R. M. Corless, E. Kaltofen and S. M. Watt 
    \S2.12.3 Hybrid methods, pages 112--125;
    E. Kaltofen
    \S4.2.17 FoxBox and other blackbox systems, pages 383--385.",
  isbn = "3-540-65466-6",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/01/symnum.pdf}",
  paper = "Grab03.pdf",
  keywords = "axiomref",
  beebe = "Grabmeier:2003:CAH"
}

@InProceedings{Kalt07,
  author = "Kaltofen, Erich and Li, Bin and Sivaramakrishnan, Kartik and
            Yang, Zhengfeng and Zhi, Lihong",
  title = {{Lower bounds for approximate factorizations via semidefinite 
           programming (extended abstract)}},
  year = "2007",
  booktitle = 
    "SNC'07 Proc. 2007 Internat. Workshop on Symbolic-Numeric Comput.",
  pages = "203--204",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/07/KLSYZ07.pdf}",
  paper = "Kalt07.pdf"
}

@Article{Borw07,
  author = "Borwein, Peter and Kaltofen, Erich and Mossinghoff, Michael J.",
  title = {{Irreducible Polynomials and {Barker} Sequences}},
  journal = "{ACM} Communications in Computer Algebra",
  volume = "162",
  number = "4",
  year = "2007",
  pages = "118--121",
  month = "December",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/07/BKM07.pdf}",
  paper = "Borw07.pdf"
}

@Article{Kalt10,
  author = "Kaltofen, Erich and Lavin, Mark",
  title = {{Efficiently Certifying Non-Integer Powers}},
  journal = "Computational Complexity",
  year = "2010",
  volume = "19",
  number = "3",
  month = "September",
  pages = "355--366",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/09/KaLa09.pdf}",
  paper = "Kalt10.pdf"
}

@InProceedings{Kalt11,
  author = "Kaltofen, Erich L. and Nehring, Michael",
  title = {{Supersparse black box rational function interpolation}},
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'11",
  month = "June",
  year = "2011",
  pages = "177--185",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/11/KaNe11.pdf}",
  paper = "Kalt11.pdf"
}

@InProceedings{Gren11a,
  author = "Grenet, Bruno and Kaltofen, Erich L. and Koiran, Pascal 
            and Portier, Natacha",
  title = {{Symmetric Determinantal Representation of Weakly Skew Circuits}},
  booktitle = "Proc. 28th Internat. Symp. on Theoretical Aspects of Computer 
               Science, STACS 2011",
  pages = "543--554",
  year = "2011",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/11/GKKP11.pdf}",
  paper = "Gren11a.pdf"
}

@InProceedings{Kalt11a,
  author = "Kaltofen, Erich L. and Nehring, Michael and Saunders, David B.",
  title = {{Quadratic-Time Certificates in Linear Algebra}},
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'11",
  month = "June",
  year = "2011",
  pages = "171--176",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/11/KNS11.pdf}",
  paper = "Kalt11a.pdf"
}

@InProceedings{Kalt11b,
  author = "Kaltofen, Erich L. and Lee, Wen-shin and Yang, Zhengfeng",
  title = {{Fast estimates of {Hankel} matrix condition numbers
           and numeric sparse interpolation}},
  booktitle = "Proc. 2011 Internat. Workshop on Symbolic-Numeric Comput.",
  month = "June",
  year = "2011",
  pages = "130--136",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/11/KLY11.pdf}",
  paper = "Kalt11b.pdf"
}

@InProceedings{Guo12,
  author = "Guo, Feng and Kaltofen, Erich L. and Zhi, Lihong",
  title = {{Certificates of Impossibility of {Hilbert}-{Artin} Representations
           of a Given Degree for Definite Polynomials and Functions}},
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'12",
  month = "July",
  year = "2012",
  pages = "195--202",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/12/GKZ12.pdf}",
  paper = "Guo12.pdf"
}

@InProceedings{Come12a,
  author = "Comer, Matthew T. and Kaltofen, Erich L. and Pernet, Cl{\'e}ment",
  title = {{Sparse Polynomial Interpolation and {Berlekamp}/\allowbreak 
           {Massey} Algorithms That Correct Outlier Errors in Input Values}},
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'12",
  month = "July",
  year = "2012",
  pages = "138--145",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/12/CKP12.pdf}",
  paper = "Come12a.pdf"
}

@InProceedings{Boye13,
  author = "Boyer, Brice and Comer, Matthew T. and Kaltofen, Erich L.",
  title =  {{Sparse Polynomial Interpolation by Variable Shift in
            the Presence of Noise and Outliers in the Evaluations}},
  booktitle = 
    "Proc. Tenth Asian Symposium on Computer Mathematics (ASCM 2012)",
  year = "2013",
  month = "October",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/13/BCK13.pdf}",
  paper = "Boye13.pdf"
}

@InProceedings{Kalt13b,
  author = "Kaltofen, Erich and Yang, Zhengfeng",
  title = {{Sparse multivariate function recovery from values with noise and 
           outlier errors}},
  year = "2013",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'13",
  pages = "219--226",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/13/KaYa13.pdf}",
  paper = "Kalt13b.pdf"
}

@InProceedings{Kalt13c,
  author = "Kaltofen, Erich L.",
  title = {{Symbolic Computation and Complexity Theory Transcript of My Talk}},
  booktitle = 
    "Proc. Tenth Asian Symposium on Computer Mathematics (ASCM 2012)",
  year = "2013",
  month = "October",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/13/Ka13.pdf}",
  paper = "Kalt13c.pdf"
}

@InProceedings{Kalt14,
  author = "Kaltofen, Erich L. and Yang, Zhengfeng",
  title = {{Sparse Multivariate Function Recovery With a High Error Rate 
           in Evaluations}},
  year = "2014",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'14",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/14/KaYa14.pdf}",
  paper = "Kalt14.pdf"
}

@InProceedings{Kalt14a,
  author = "Kaltofen, Erich L. and Pernet, Cl{\'e}ment",
  title = {{Sparse Polynomial Interpolation Codes and Their Decoding
           Beyond Half the Minimal Distance}},
  year = "2014",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'14",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/14/KaPe14.pdf}",
  paper = "Kalt14a.pdf"
}

@InProceedings{Duma14,
  author = "Dumas, Jean-Guillaume and Kaltofen, Erich L.",
  title = {{Essentially Optimal Interactive Certificates In Linear Algebra}},
  year = "2014",
  booktitle = "Internat. Symp. Symbolic Algebraic Comput. ISSAC'14",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/14/DuKa14.pdf}",
  paper = "Duma14.pdf"
}

@InProceedings{Boye14,
  author = "Boyer, Brice B. and Kaltofen, Erich L.",
  title = {{Numerical Linear System Solving With Parametric Entries By 
           Error Correction}},
  year = "2014",
  booktitle = "SNC'14 Proc. 2014 Int. Workshop on Symbolic-Numeric Comput.",
  link = "\url{http://www.math.ncsu.edu/~kaltofen/bibliography/14/BoKa14.pdf}",
  paper = "Boye14.pdf"
}

@book{Abla96,
  author = "Ablamowicz, Rafal and Lounesto, Pertti and Para, Josep M.",
  title = {{Clifford algebras with numeric and symbolic computations}},
  publisher = "Birkhauser",
  year = "1996",
  abstract =
    "The 20 articles of this volume will be reviewed individually. They
    have been grouped into the following four sections: I) Verifying and
    falsifying conjectures (1); II) Differential geometry quantum
    mechanics, spinors and conformal group (9); III) Generalized Clifford
    algebras and number systems, projective geometry and crystallography
    (7); IV) Numerical methods in Clifford algebras (3).
    
    Selected electronic materials submitted by our contributors can be
    found at a Web site: 
    http://www.birkhauser.com/books/ISBN/0-8176-3907-1
    
    These materials contain original packages, worksheets, notebooks,
    computer programs, etc., that were used in deriving results presented
    in this book.",
  keyword = "axiomref"
}

@misc{ACA00,
  author = "Kokol-voljc, Vlasta and Kutzler, Bernhard",
  title = {{Computer Algebra Meets Education}},
  conference = "Sessions of ACA2000",
  year = "2000",
  abstract =
    "Education has become one of the fastest growing application areas for
    computers in general and computer algebra in particular. Computer 
    algebra tools such as TI-92/89, Derive, Mathematica, Maple, Axiom,
    Reduce, Macsyma, or Mupad make powerful teaching tools in mathematics,
    physics, chemistry, biology, economy.

    The goal of this session is to exchange ideas and experiences, to hear
    about classroom experiments, and to discuss all issues related to the
    use of computer algebra tools in the classroom (such as assessment,
    change of curricula, new support material, ...)",
  keywords = "axiomref"
}

@misc{ACA15,
  author = "Martinez-Moro, Edgar Kotsireas, Ilias",
  title = {{21st Conference on Applications of Computer Algebra}},
  conference = "Sessions of ACA2015",
  location = "Kalamata, Greece",
  year = "2015",
  link = "\url{http://www.singacom.uva.es/ACA2015/latex/ACAproc.pdf}",
  paper = "ACA15.pdf",
  keywords = "axiomref"
}

@misc{ACM89,
  author = "ACM",
  title = {{Proceedings of the ACM-SIGSAM 1989 International
           Symposium on Symbolic and Algebraic Computation, ISSAC '89}},
  year = "1989",
  isbn = "0-89791-325-6",
  link = "\url{http://doi.acm.org/10.1145/74540.74567}",
  doi = "10.1145/74540.74567",
  acmid = "74567",
  publisher = "ACM Press",
  address = "New York, NY, USA",
  keywords = "axiomref"
}

@misc{ACM94,
  author = "ACM",
  title = {{Proceedings of the ACM-SIGSAM 1989 International
           Symposium on Symbolic and Algebraic Computation, ISSAC '94}},
  year = "1994",
  isbn = "0-89791-638-7",
  publisher = "ACM Press",
  address = "New York, NY, USA",
  keywords = "axiomref"
}

@InProceedings{Adam99a,
  author = "Adams, A.A. and Gottliebsen, H. and Linton, S.A. and Martin, U.",
  title = {{VSDITLU: A verifiable symbolic definite integral table look-up}},
  booktitle = "Automated Deduction",
  series = "CADE 16",
  year = "1999",
  location = "Trento, Italy",
  pages = "112-126",
  link = "\url{http://www.a-cubed.info/Publications/CADE99.pdf}",
  abstract =
    "We present a verifiable symbolic definite integral table lookup: a
    system which matches a query, comprising a definite integral with
    parameters and side conditions, against an entry in a verifiable table
    and uses a call to a library of facts about the reals in the theorem
    prover PVS to aid in the transformation of the table entry into an
    answer. Our system is able to obtain correct answers in cases where
    standard techniques implemented in computer algebra systems fail. We
    present the full model of such a system as well as a description of
    our prototype implementation showing the efficacy of such a system:
    for example, the prototype is able to obtain correct answers in cases
    where computer algebra systems do not. We extend upon Fateman’s
    web-based table by including parametric limits of integration and
    queries with side conditions.",
  paper = "Adam99a.pdf",
  keywords = "axiomref"
}

@article{Akba08,
  author = "Akbar Hussain, D.M. and Haq, Shaiq A. and Khan, Zafar Ullah
            and Ahmed, Zaki",
  title = {{Simple object oriented designed computer algebra system}},
  journal = "J. Comput. Mathods Sci. Eng.",
  volume = "8",
  number = "3",
  pages = "195-211",
  year = "2008",
  abstract = 
    "Computer Algebra System (CAS) is a software program that facilitates
    symbolic mathematics. The core functionality of a typical CAS is
    manipulation of mathematical expressions in symbolic forms. The
    expressions manipulated by CAS normally include polynomials in
    multiple variables, standart trigonometric and exponential functions,
    various special functions for example gamma, zeta, Bessel, etc. and
    also arbitrary functions like derivatives, integrals, sums, and
    products of expressions. Our implementation of a CAS tool provides an
    object orienged design framework. The system is portable to other
    platforms and highly scalable. The other key features include a very
    simple and interactive user GUI support for a formula editor, making
    it a self contained system, additionally the formula editor provides a
    real-time syntax checking for expressions.",
  keywords = "axiomref"
}

@article{Aitk99,
  author = "Aitken, William E. and Constable, Robert L. and 
            Underwood, Judith L.",
  title = {{Metalogical frameworks. II: Developing a reflected decision
           procedure}},
  journal = "J. Autom. Reasoning",
  volume = "22",
  number = "2",
  pages = "171-221",
  year = "1999",
  link = "\url{http://www.nuprl.org/documents/Constable/MetalogicalFrameworksII.pdf}",
  abstract = 
    "Proving theorems is a creative act demanding new combinations of ideas
    and on occasion new methods of argument. For this reason, theorem
    proving systems need to be extensible. The provers should also remain
    correct under extension, so there must be a secure mechanism for doing
    this. The tactic-style provers pioneered by Edinburgh LCF provide a
    very effective way to achieve secure extensions, but in such systems,
    all new methods must be reduced to tactics. This is a drawback because
    there are other useful proof generating tools such as decision
    procedures; these include, for example, algorithms which reduce a
    deduction problem, such as arithmetic provability, to a computation on
    graphs.

    The Nuprl system pioneered the combination of fixed decision
    procedures with tactics, but the issue of securely adding new ones was
    not solved. In this paper, we show how to safely include user-defined
    decision procedures in theorem provers. The idea is to prove
    properties of the procedure inside the prover’s logic and then invoke
    a reflection rule to connect the procedure to the system. We also show
    that using a rich underlying logic permits an abstract account of the
    approach so that the results carry over to different implementations
    and other logics.",
  paper = "Aitk99.pdf",
  keywords = "axiomref"
}

@misc{America,
  author = "america.pink",
  title = {{Axiom (computer algebra system)}},
  year = "2016",
  link = "\url{http://america.pink/axiom-computer-algebra-system_526647.html}",
  keywords = "axiomref"
}

@article{Alad03,
  author = "Aladjev, Victor",
  title = {{Computer Algebra System Maple: A New Software Library}},
  journal = "LNCS",
  year = "2003",
  pages = "711-717",
  abstract = 
    "The paper represents Maple library containing more than 400 procedures
    expanding possibilities of the Maple package of releases 6,7 and 8.
    The library is structurally organized similarly to the main Maple
    library. The process of the library installing is simple enough as a
    result of which the above library will be logically linked with the
    main Maple library, supporting access to software located in it
    equally with standard Maple software. The demo library is delivered
    free of charge at request to addresses mentioned above.",
  paper = "Alad03.pdf",
  keywords = "axiomref"
}

@article{Andr87,
  author = "Andrews, George",
  title = {{Applications of Scratchpad to Problems in Special
           Functions and Combinatorics}},
  journal = "Lecture Notes in Computer Science",
  volume = "296",
  year = "1987",
  pages = "158-166",
  abstract = 
    "Within the last few years, there have been numerous applications of
    computer algebra to special functions. G. Gasper (Norwestern
    University) has studied classical hypergeometric functions, and
    W. Gosper (Symbolics Inc.) has developed a large variety of
    spectacular transformation and summation techniques for MACSYMA. The
    purpose of this note is to explore some of the interface between
    computer algebra and special functions. In Section 2 we examine an
    application of MACSYMA which inadequately relied, in my opinion, on
    what was readily available in the literature on hypergeometric
    series. In Section 3 we consider classical observations on sums of
    powers of binomial coefficients. In Section 4 we consider a problem of
    D.M. Jackson wherein SCRATCHPAD and classical hypergeometric series
    interact nicely. We close with a problem inspired by work in
    statistical mechanics which leads us to questions about algorithms
    that would be useful in computer algebra applications.",
  paper = "Andr87.pdf",
  keywrods = "axiomref"
}

@inproceedings{Andr90,
  author = "Andrews, George and Baxter, R.J.",
  title = {{SCRATCHPAD explorations for elliptic theta functions}},
  booktitle = "Computers in Mathematics",
  series = "Lecture Notes in Pure and Appl. Math 125",
  pages = "17-33",
  year = "1990",
  keywords = "axiomref"
}

@article{Andr09,
  author = "Andrews, George",
  title = {{The Meaning of Ramanujan Now and for the Future}},
  journal = "Ramanujan Journal",
  volume = "20",
  pages = "257-273",
  year = "2009",
  link = "\url{http://www.personal.psu.edu/gea1/pdf/274.pdf}",
  abstract =
    "December 22, 2010 marks the 123th anniversary of Ramanujan's
    birth. In this paper we pay homage to this towering figure whose
    mathematical discoveries so affected mathematics throughout the
    twentieth century and into the twenty-first.",
  paper = "Andr09.pdf",
  keywords = "axiomref"
}

@article{Anto01,
  author = "Antoine, Xavier",
  title = {{Microlocal diagonalization of strictly hyperbolic 
          pseudo-differential systems and application to the design of 
          radiation conditions in electromagnetism}},
  journal = "SIAM J. Appl. Math",
  volume = "61",
  number = "6",
  pages = "1877-1905",
  year = "2001",
  abstract =
    "In [Commun. Pure Appl. Math. 28, 457-478 (1975; Zbl 0332.35058)],
    M. E. Taylor described a constructive diagonalization method to
    investigate the reflection of singularities of the solution to
    first-order hyperbolic systems. According to further works initiated
    by Engquist and Majda, this approach proved to be well adapted to the
    construction of artificial boundary conditions. However, in the case
    of systems governed by pseudodifferential operators with variable
    coefficients, Taylor’s method involves very elaborate calculations of
    the symbols of the operators. Hence, a direct approach may be
    difficult when dealing with high-order conditions.

    This motivates the first part of this paper, where a recursive
    explicit formulation of Taylor’s method is stated for microlocally
    strictly hyperbolic systems. Consequently, it provides an algorithm
    leading to symbolic calculations which can be handled by a computer
    algebra system. In the second part, an application of the method is
    investigated for the construction of local radiation boundary
    conditions on arbitrarily shaped surfaces for the transverse electric
    Maxwell system. It is proved that they are of complete order, provided
    the introduction of a new symbols class well adapted to the Maxwell
    system. Next, a complete second-order condition is designed, and when
    used as an on-surface radiation condition [G. A. Kriegsmann,
    A. Taflove, and K. R. Umashankar, IEEE Trans. Antennas Propag. 35,
    153-161 (1987; Zbl 0947.78571)], it can give rise to an ultrafast
    numerical approximate solution of the scattered field.",
  keywords = "axiomref"
}

@article{Arna95a,
  author = "Arnault, Francois",
  title = {{Rabin-Miller primality test: Composite numbers which pass it}},
  journal = "Mathematics of  Computation",
  volume = "64",
  number = "209",
  pages = "355-361",
  year = "1995",
  link = "\url{https://www.jointmathematicsmeetings.org/mcom/1995-64-209/S0025-5718-1995-1260124-2/S0025-5718-1995-1260124-2.pdf}",
  abstract =
    "The Rabin-Miller primality test is a probabilistic test which can be
    found in several algebraic computing systems (such as Pari, Maple,
    ScratchPad) because it is very easy to implement and, with a
    reasonable amount of computing, indicates whether a number is
    composite or ``probably prime'' with a very low probability of error. In
    this paper, we compute composite numbers which are strong pseudoprimes
    to several chosen bases. Because these bases are those used by the
    ScratchPad implementation of the test, we obtain, by a method which
    differs from a recent one by G. Jaeschke [ibid. 61, 915-926 (1993; Zbl
    0802.11001)], composite numbers which are found to be ``probably prime''
    by this test.",
  paper = "Arna95a.pdf",
  keywords = "axiomref"
}

@article{Atki95,
  author = "Atkinson, M. D. and Linton, S. A. and Walker, L. A.",
  title = {{Priority queues and multisets}},
  journal = "J. Comb",
  volume = "2",
  pages = "385-402",
  year = "1995",
  link = "\url{http://www.combinatorics.org/ojs/index.php/eljc/article/download/v2i1r24.pdf}",
  abstract = 
    "A priority queue, a container data structure equipped with the
    operations insert and delete-minimum, can re-order its input in
    various ways, depending both on the input and on the sequence of
    operations used. If a given input $\sigma$ can produce a particular
    output $\tau$ then $(\sigma,\tau)$ is said to be an allowable pair. It
    is shown that allowable pairs on a fixed multiset are in one-to-one
    correspondence with certain $k$-way trees and, consequently, the
    allowable pairs can be enumerated. Algorithms are presented for
    determining the number of allowable pairs with a fixed input
    component, or with a fixed output component. Finally, generating
    functions are used to study the maximum number of output components
    with a fixed input component, and a symmetry result is derived.",
  paper = "Atki95.pdf",
  keywords = "axiomref"
}

@inproceedings{Augo91,
  author = "Augot, D. and  Charpin, P. and Sendrier, N.",
  title = {{The miniumum distance of some binary codes via the 
           Newton's identities}},
  booktitle = "Int. Symp. on Coding Theory and Applications",
  year = "1991",
  pages = "65-73",
  isbn = "0-387-54303-1",
  abstract =
    "In this paper, we give a natural way of deciding whether a given
    cyclic code contains a word of given weight. The method is based on
    the manipulation of the locators and of the locator polynomial of a
    codeword $x$.
    
    Because of the dimensions of the problem, we need to use a symbolic
    computation software, like Maple or Scratchpad II. The method can be
    ineffective when the length is too large.
    
    The paper contains two parts: In the first part we will present the main
    definitions and properties we need.
    
    In the second part, we will explain how to use these properties, and, as
    illustration, we will prove the following facts:
    \begin{itemize}
    \item The dual of the BCH code of length 63 and designed distance 9
    has true minimum distance 14 (which was already known).
    \item The BCH code of length 1023 and designed distance of 253 has
    minimum distance 253.
    \item The cyclic codes of length $2^11-1$, $2^13-1$, $2^17-1$, with
    generator polynomial $m_1(x)$ and $m_7(x)$ have minimum distance 4.
    \end{itemize}",
  paper = "Augo91.pdf",
  keywords = "axiomref",
  beebe = "Augot:1991:MDS"
}

@InProceedings{Andr84,
  author = "Andrews, George E.",
  title = {{Ramanujan and SCRATCHPAD}},
  booktitle = "Proc. of 1984 MACSYM Users' Conference, July 1984",
  location = "General Electric, Schenectady, NY",
  year = "1984",
  pages = "383-408",
  keywords = "axiomref"
}

@InProceedings{Andr88,
  author = "Andrews, G. E.",
  title = {{Application of Scratchpad to problems in special functions and 
           combinatorics}},
  booktitle = "Trends in Computer Algebra",
  publisher = "Springer",
  series = "Lecture Notes in Comp. Sci. 296",
  year = "1988",
  isbn = "3-540-18928-9",
  pages = "159-166",
  abstract =
    "Within the last few years, there have been numerous applications of
    computer algebra to special functions. G. Gasper (Northwestern
    University) has studied classical hypergeometric functions, and
    W. Gosper (Symbolics, Inc.) has developed a large variety of
    spectacular transformation and summation techniques for MACSYMA. The
    purpose of this note is to explore some of the interface between
    computer algebra and special functions. In section 2 we examine an
    application of MACSYMA which inadequately relied, in my opinion, on
    what was readily available in the literature on hypergeometric
    series. In Section 3 we consider classical observations on sums of
    powers of binomial coefficients. In Section 4 we consdier a problem of
    D.M. Jackson wherein SCRATCHPAD and classical hypergeometric series
    interact nicely. We close with a problem inspired by work in
    statistical mechanics which leads us to question about algorithsm that
    would be useful in computer algebra applications.

    In this brief survey, we have illustrated some of the uses of computer
    algebra. It might be objected that our work could well be carried out
    in almost any computer language; so why bother with SCRATCHPAD? The
    answer, of course, lies in the naturalness and simplicity of computer
    algebra approaches to these problems. Expressions like (2.2), (3.1)
    and (4.1) can be coded in SCRATCHPAD in one line exactly as they are
    written. They can then be studied with minimal thought about the
    computer and maximal concentration on what is happening. Often
    mathematical research consists of sifting low grade ore, and when such
    sifting requires ingenious programming skills it is likely not to be
    carried out.",
  keywords = "axiomref",
  beebe = "Andrews:1988:ASP"
}

@book{Anon91,
  author = "Anonymous",
  title = {{Challenges of a Changing World (2 Volumes)}},
  publisher = "American Society for Engineering Education",
  year = "1991",
  keywords = "axiomref"
}

@book{Anon95,
  author = "Anonymous",
  title = {{Zeitschrift fur Angewandte Mathematik und Physik, 75 (suppl. 2)}},
  publisher = "Unknown",
  year = "1995",
  issn = "0044-2267",
  keywords = "axiomref"
}

@Article{Arna95,
  author = "Arnault, Francois",
  title = {{Constructing Carmichael numbers which are strong pseudoprimes to 
           several bases}},
  year = "1995",
  journal = "Journal of Symbolic Computation",
  volume  = "20",
  number = "2",
  pages = "151-161",
  link = "\url{http://ac.els-cdn.com/S0747717185710425/1-s2.0-S0747717185710425-main.pdf}",
  algebra = "\newline\refto{package PRIMES IntegerPrimesPackage}",
  abstract = 
    "We describe here a method of constructing Carmichael numbers which
    are strong pseudoprimes to some set of prime bases. We apply it to
    find composite numbers which are found to be prime by the Rabin-Miller
    test of packages as Axiom or Maple. We also use a variation of this
    method to construct strong Lucas pseudoprimes with respect to several
    pairs of parameters.",
  paper = "Arna95.pdf",
  keywords = "axiomref",
  beebe = "Arnault:1995:CCN"
}

@article{Augo97,
  author = "Augot, Daniel and Camion, Paul",
  title = {{On the computation of minimal polynomials, cyclic vectors,
           and Frobenius forms}},
  journal = "Linear Algebra Appl.",
  volume = "260",
  pages = "61-94",
  year = "1997",
  abstract = 
    "Algorithms related to the computation of the minimal polynomial of an
    $x\times n$ matrix over a field $K$ are introduced. The complexity of
    the first algorithm, where the complete factorization of the 
    characteristic polynomial is needed, is $O(\sqrt{n}\cdot n^3)$. An
    iterative algorithm for finding the minimal polynomial has complexity
    $O(n^3+n^2m^2)$, where $m$ is a parameter of the shift Hessenberg
    matrix used. The method does not require the knowlege of the
    characteristic polynomial. The average value of $m$ is $O(log n)$.

    Next methods are discussed for finding a cyclic vector for a matrix.
    The authors first consider the case when its characteristic polynomial
    is squarefree. Using the shift Hessenberg form leads to an algorithm
    at cost $O(n^3 + n^2m^2)$. A more sophisticated recurrent procedure
    gives the result in $O(n^3)$ steps. In particular, a normal basis for
    an extended finite field of size $q^n$ will be obtained with complexity
    $O(n^3+n^2 log q)$.

    Finally, the Frobenius form is obtained with asymptotic average
    complexity $O(n^3 log n)$.",
  paper = "Augo97.pdf",
  keywords = "axiomref"
}

@misc{AxiomPressRelease,
  author = "Numerical Algorithms Group",
  title = {{Axiom Press Release}},
  link = "\url{http://www.dorn.org/uni/sls/kap05/e08\_01.htm}",
  year = "1996",
  month = "July",
  day = "24",
  paper = "AxiomPressRelease.tgz",
  keywords = "axiomref"
}

@inproceedings{Babo16,
  author = "Babovsky, Hans and Grabmeier, Johannes",
  title = {{Calculus and design of discrete velocity models using
           computer algebra}},
  booktitle = "AIP Conference Proceedings",
  volume = "1786",
  isbn = "978-0-7354-1448-8",
  link = "\url{http://aip.scitation.org/doi/pdf/10.1063/1.4967672}",
  year = "2016",
  abstract =
    "In [2,3], a framework for a calculus with Discrete Velocity Models
    (DVM) has been derived. The rotational symmetry of the discrete
    velocities can be modelled algebraically by the action of the cyclic
    group $C_4$ -- or including reflections of the dihedral group $D_4$.
    Taking this point of view, the linearized collision operator can be
    represented in a compact form as a matrix of elements in the group
    algebra. Or in other words, by choosing a special numbering it
    exhibits a certain block structure which lets it appear as a matrix
    with entries in a certain polynomial ring. A convenient way for
    approaching such a structure is the use of a computer algebra system
    able to treat these (predefined) algebraic structures. We use the
    computer algebra system FriCAS/AXIOM [4,5] for the generation of the
    velocity and the collision sets and for the analysis of the structure
    of the collision operator. Concerning the fluid dynamic limit, the
    system provides the characterization of sets of collisions and their
    contribution to the flow parameters. It allows the design of
    rotationally invariant symmetric models for prescribed Prandtl
    numbers.  The implementation in FriCAS/AXIOM is explained and its
    results for a 25-velocity model are presented.",
  paper = "Babo16.pdf",
  keywords = "axiomref"
}

@article{Bacl14,
  author = "Baclawski, Krystian",
  title = {{SPAD language type checker}},
  journal = "unknown",
  year = "2014",
  link = "\url{http://github.com/cahirwpz/phd}",
  abstract = "
   The project aims to deliver a new type checker for SPAD language.
   Several improvements over current type checker are planned.
   \begin{itemize}
   \item introduce better type inference
   \item introduce modern language constructs
   \item produce understandable diagnostic messages
   \item eliminate well known bugs in the type system
   \item find new type errors
   \end{itemize}",
  keywords = "axiomref"
}

@misc{Bake16,
  author = "Baker, Martin",
  title = {{Axiom Maths Program}},
  year = "2016",
  link = "\url{http://www.euclideanspace.com/prog/scratchpad/axiom/index.htm}",
  keywords = "axiomref"
}

@misc{Bake16a,
  author = "Baker, Martin",
  title = {{Axioms in Axiom}},
  year = "2016",
  link = "\url{http://www.euclideanspace.com/prog/scratchpad/axiomsinAxiom}",
  keywords = "axiomref"
}

@article{Ball14,
  author = "Ballarin, Clemens",
  title = {{Locales: a module system for mathematical theories}},
  journal = "J. Autom. Reasoning",
  volume = "52",
  number = "2",
  pages = "123-153",
  year = "2014",
  link = "\url{http://www21.in.tum.de/~ballarin/publications/jar2013.pdf}",
  abstract = 
    "Locales are a module system for managing theory hierarchies in a
    theorem prover through theory interpretation. They are available for
    the theorem prover Isabelle. In this paper, their semantics is defined
    in terms of local theories and morphisms. Locales aim at providing
    flexible means of extension and reuse. Theory modules (which are
    called locales) may be extended by definitions and
    theorems. Interpretation of Isabelle's global theories and proof
    contexts is possible via morphisms. Even the locale hierarchy may be
    changed if declared relations between locales do not adequately
    reflect logical relations, which are implied by the locales'
    specifications. By discussing their design and relating it to more
    commonly known structuring mechanisms of programming languages and
    provers, locales are made accessible to a wider audience beyond the
    users of Isabelle. The discussed mechanisms include ML-style functors,
    type classes and mixins (the latter are found in modern
    object-oriented languages).",
  paper = "Ball14.pdf",
  keywords = "axiomref"
}

@article{Barn02,
  author = "Barnett, Michael P.",
  title = {{Computer algebra in the life sciences}},
  journal = "SIGSAM Bulletin",
  volume = "36",
  number = "4",
  pages = "5-31",
  year = "2002",
  link = "\url{https://notendur.hi.is/vae11/\%C3\%9Eekking/Systems\%20Biology/Biological\%20Algebra.PDF}",
  abstract = 
    "This note (1) provides references to recent work that applies computer
    algebra (CA) to the life sciences, (2) cites literature that explains
    the biological background of each application, (3) states the
    mathematical methods that are used, (4) mentions the benefits of CA,
    and (5) suggests some topics for future work.",
  paper = "Barn02.pdf",
  keywords = "axiomref"
}

@misc{Beeb14,
  author = "Beebe, Nelson H. F.",
  title = {{A Bibliography of Publications about the AXIOM 
           (formerly, Scratchpad) Symbolic Algebra Language}},
  year = "2014",
  link = "\url{http://www.netlib.org/bibnet/journals/axiom.ps.gz}",
  paper = "Beeb14.pdf"
}

@book{Benk98,
  author = "Benker, Hans",
  title = {{Engineering mathematics with computer algebra systems}},
  publisher = "Unknown",
  year = "1998",
  comment = "german",
  keywords = "axiomref",
  beebe = "Benker:1998:ICS"
}

@misc{Bern14,
  author = "Bernard, Joey",
  title = {{Open Axiom}},
  link = "\url{http://www.linuxjournal.com/content/open-axiom}",
  year = "2014",
  keywords = "axiomref"
}

@book{Bett99,
  author = "Betten, Anton and Kohnert, Axel and Laue, Reinhard and 
            Wassermann, Alfred",
  title = {{Algebraic Combinatorics and Applications}},
  year = "1999",
  publisher = "Springer",
  isbn = "978-3-540-41110-9",
  keywords = "axiomref"
}

@inproceedings{Blai70,
  author = "Blair, Fred W. and Griesmer, James H. and Jenks, Richard D.",
  title = {{An interactive facility for symbolic mathematics}},
  booktitle = "Proc. International Computing Symposium, Bonn, Germany",
  year = "1970",
  pages = "394-419",
  abstract =
    "The SCRATCHPAD/1 system is designed to provide an interactive symbolic
    coputational facility for the mathematician user. The system features
    a user language designed to capture the style and succinctness of
    mathematical notation, together with a facility for conveniently
    introducing new notations into the language. A comprehensive system
    library incorporates symbolic capabilities provided by such systems as
    SIN, MATHLAB, and REDUCE.",
  keywords = "axiomref"
}

@article{Blai70a,
  author = "Blair, Fred W. and Griesmer, James H. and Jenks, Richard D.",
  title = {{An interactive facility for symbolic mathematics}},
  journal = "ACM SIGSAM",
  volume = "14",
  year = "1970",
  pages = "17-18",
  abstract =
    "This paper describes a system designed to provide an interactive
    symbolic computational facility for the mathematician user. To carry
    out this objective, an experimental LISP system has been implemented
    for IBM System/360 computers. Using this LISP system as a base,
    portions of several systems have been combined and augmented to
    provide the following facilities to a user:(1) rational function
    manipulation and simplification; symbolic differentiation (Anthony
    Hearn's REDUCE)(2) symbolic integration (Joel Moses' SIN)(3)
    polynomial factorization, solution of linear differential equations,
    direct and inverse symbolic Laplace transforms (Carl Engelman's
    MATHLAB, including Knut Korsvold's simplification system)(4) unlimited
    precision integer arithmetic(5) manipulation of arrays containing
    symbolic entries(6) two-dimensional output on IBM 2741 terminals or
    IBM 2250 displays (William Martin's Symbolic Mathematical Laboratory,
    and Jonathan Millen's CHARYBDIS program from MATHLAB)(7)
    self-extending language facility (META/LISP).The user language created
    for the system incorporates a subset of ``customary'' mathematical
    notation. Data objects include sequences (both finite and infinite)
    and arrays of arbitrary rank. Assignment statements are the
    fundamental commands in the user language; they may contain
    ``for''-clauses which restrict the domain for which the assignment is
    valid and permit ``piecewise'' and recursive definition of new operators
    and functions. The user may also enter syntax definition statements in
    order to introduce new notations into the system.Expressions appearing
    in assignment statements may include ``where''-clauses which allow user
    control over the ``environment'' used in evaluation. Otherwise,
    evaluation of expressions occurs in the current environment created by
    the successive user commands, with certain operations such as
    integration, differentiation, and simplification performed
    automatically.Translators for the user language and for a resident
    higher-level procedural language facility are written in META/LISP, a
    new self-compiling translator-writing system. As a result, all input
    language facilities as well as the underlying manipulation routines
    may be interactively extended by an experienced user.",
  paper = "Blai70a.pdf",
  keywords = "axiomref, printed"
}

@InProceedings{Bosm94,
  author = "Bosma, Wieb and Cannon, John and Matthews, Graham",
  title = {{Programming with algebraic structures: Design of the Magma
           language}},
  booktitle = "Proc. ISSAC 94",
  series = "ISSAC 94",
  year = "1994",
  publisher = "ACM Press",
  location = "Baltimore, MD",
  pages = "52-57",
  link = "\url{http://www.math.ru.nl/~bosma/pubs/ISSAC94.pdf}",
  abstract =
    "MAGMA is a new software system for computational algebra, number
    theory and geometry whose design is centred on the concept of
    algebraic structure (magma). The use of algebraic structure as a
    design paradigm provides a natural strong typing mechanism. Further,
    structures and their morphisms appear in the language as first class
    objects. Standard mathematical notions are used for the basic data
    types. The result is a powerful, clean language which deals with
    objects in a mathematically rigorous manner. The conceptual and
    implementation ideas behind MAGMA will be examined in this paper. This
    conceptual base differs significantly from those underlying other
    computer algebra systems.",
  paper = "Bosm94.pdf",
  keywords = "axiomref"
}

@article{Bosm97,
  author = "Bosma, Wieb and Cannon, John and Playoust, Catherine",
  title = {{The Magma Algebra System I: The User Language}},
  journal = "J. Symbolic Computation",
  volume = "24",
  pages = "235-265",
  year = "1997",
  link = "\url{http://lib.org.by/_djvu/_Papers/Computer_algebra/CAS%20systems/}",
  abstract = 
    "In the first of two papers on MAGMA, a new system for computational
    algebra, we present the MAGMA language, outline the design principles
    and theoretical background, and indicate its scope and use. Particular
    attention is given to the constructors for structures, maps, and sets.",
  paper = "Bosm97.djvu",
  keywords = "axiomref"
}

@misc{Boyl88,
  author = "Boyle, Ann and Caviness, B.F. and Hearn, Anthony C.",
  title = {{Future Directions for Research in Symbolic Computation}},
  publisher = "Soc. for Industrial and Applied Mathematics", 
  year = "1988",
  link = "\url{http://www.eecis.udel.edu/~caviness/wsreport.pdf}",
  paper = "Boyl88.pdf",
  keywords = "axiomref"
}

@InProceedings{Brad09,
  author = "Bradford, Russell and Davenport, James H. and 
            Sangwin, Christopher J.",
  title = {{A comparison of equality in computer algebra and correctness
           in mathematical pedagogy}},
  year = "2009",
  booktitle = "Intelligent Computer Mathematics, 16th symposium",
  series = "Calculemus 2009",
  pages = "75-89",
  isbn = "978-3-642-02613-3",
  link = "\url{http://opus.bath.ac.uk/15188/1/RJBJHDCJSv2.pdf}",
  abstract = 
    "How do we recognize when an answer is ``right''? This is a question
    that has bedevilled the use of computer systems in mathematics (as
    opposed to arithmetic) ever since their introduction. A computer
    system can certainly say that some answers are definitely wrong, in
    the sense that they are provably not an answer to the question
    posed. However, an answer can be mathematically right without being
    pedagogically right. Here we explore the differences and show that,
    despite the apparent distinction, it is possible to make many of the
    differences amenable to formal treatment, by asking ``under which
    congruence is the pupil's answer equal to the teacher's?''.",
  paper = "Brad09.pdf",
  keywords = "axiomref"
}

@misc{Brem08,
  author = "Bremner, Murray R. and Murakami, Lucia I. and Shestakov, Ivan P.",
  title = {{Nonassociative Algebras}},
  year = "2008",
  algebra = "\newline\refto{category NARNG NonAssociativeRng}",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.146.2665}",
  abstract =
    "One of the earliest surveys on nonassociative algebras is the article
    by Shirshov which introduced the phrase ``rings that are nearly
    associative''. The first book in the English language devoted to a
    systematic study of nonassociative algebras is Schafer (Scha66). A
    comprehensive exposition of the work of the Russian School is
    Zhevlakov, Slinko, Shestakov and Shirshov. A collection of open
    research problems in algebra, including many problems on
    nonassociative algebra, is the {\sl Dniester Notebook}; the survey
    article by Kuzmin and Shetakov is from the same period. Three books on
    Jordan algebras which contain substantial material on general
    nonassociative algebras are Braun and Koecher, Jacobson, and
    McCrimmon. Recent research appears in the Proceedings of the
    International Conferences on Nonassociative Algebras and its
    Applications. The present section provides very limited information on
    Lie algebras, since they are the subject of Section 16.4. The last
    part (section 9) of the present section presents three applications of
    computational linear algebra to the study of polynomial identiies for
    nonassociative algebras: pseudorandom vectors in a nonassociative
    algebra, the expansion matrix for a nonassociative operation, and the
    representation theory of the symmetric group."
}  

@InProceedings{Breu98,
  author = "Breuer, Thomas and Linton, Steve",
  title = {{The GAP 4 type system organising algebraic algorithms}},
  booktitle = "Proc. ISSAC 98",
  series = "ISSAC 98",
  year = "1998",
  publisher = "ACM Press",
  location = "Rostock, Germany",
  pages = "13-15",
  link = "\url{http://www.gap-system.org/Doc/Talks/paper.ps}",
  abstract =
    "Version 4 of the GAP (Groups, Algorithms, Programming) system for
    computational discrete mathematics has a number of novel features. In
    this paper, we describe the type system, and the way in which it is
    used for method selection. This system is central to the organization
    of the library which is the main part of the GAP system. Unlike
    simpler object-oriented systems, GAP allows method selection based on
    the types of all arguments and on certain aspects of the relationship
    between the arguments. In addition, the type of an object can change,
    in a controlled way, during its life. This reflects information about
    the object which has been computed and stored. Individual methods can
    be written and installed independently. Furthermore, most checking of
    the arguments is done in a uniform way by the method selection system,
    making individual methods simpler and less prone to error. The methods
    are combined automatically to produce a powerful and usable system for
    interactive use or programming.",
  paper = "Breu98.pdf",
  keywords = "axiomref"
}

@inproceedings{Broa95,
  author = "Broadbery, P. A. and G{\'o}mez-D{\'\i}az, T. and Watt, S. M.",
  title = {{On the Implementation of Dynamic Evaluation}},
  booktitle = "Unknown",
  publisher = "Unknown",
  year = "1995",
  pages = "77-84",
  isbn = "0-89791-699-9",
  link = "\url{http://pdf.aminer.org/000/449/014/on_the_implementation_of_dynamic_evaluation.pdf}",
  abstract = "
    Dynamic evaluation is a technique for producing multiple results
    according to a decision tree which evolves with program execution.
    Sometimes it is desired to produce results for all possible branches
    in the decision tree, while on other occasions, it may be sufficient
    to compute a single result which satisfies certain properties. This
    techinique finds use in computer algebra where computing the correct
    result depends on recognizing and properly handling special cases of
    parameters. In previous work, programs using dynamic evaluation have
    explored all branches of decision trees by repeating the computations
    prior to decision points.

    This paper presents two new implementations of dynamic evaluation
    which avoid recomputing intermediate results. The first approach uses
    Scheme ``continuations'' to record state for resuming program
    execution. The second implementation uses the Unix ``fork'' operation
    to form new processes to explore alternative branches in parallel.",
  paper = "Broa95.pdf",
  keywords = "axiomref",
  beebe = "Broadbery:1995:IDE"
}

@misc{Bronxx,
  author = "Bronstein, Manuel",
  title = {{Symbolic Integration in Computer Algebra}},
  link = "\url{http://www.iaea.org/inis/collection/NCLCollectionStore/_Public/26/042/26042580.pdf}",
  year = "1990",
  abstract =
    "One major goal of symbolic integrators is to determine under what
    circumstances the integral of the elementary functions of calculus can
    themselves be expressed as elementary functions.  While using tables
    and the ad hoc tricks taught in calculus courses can have some limited
    success, a decision procedure is necessary in all but the most trivial
    cases.  The first complete algorithm for solving this problem was
    presented by Risch in 1969, but its complexity, specially when
    algebraic functions are present in the integrand, has prevented it
    from being fully implemented.  Over the past 20 years, the Risch
    integration algorithm has been completed, extended, and improved to
    such a point that recent computer algebra systems can integrate
    elementary functions without using any of the heuristics traditionally
    taught in calculus courses and used by older systems.  In this talk,
    we give an overview and description of the algorithms used in the
    Scratchpad symbolic integrator, and illustrate them with integrals
    drawn from the physical sciences.",
  paper = "Bron90.pdf",
  keywords = "axiomref"
}

\article{Brun04,
  author = "Brunelli, J.C.",
  title = {{PSEUDO: applications of streams and lazy evaluation to integrable
           models}},
  journal = "Comput. Phys. Commun.",
  volume = "163",
  number = "1",
  pages = "22-40",
  year = "2004",
  abstract =
    "Procedures to manipulate pseudo-differential operators in MAPLE are
    implemented in the program PSEUDO to perform calculations with
    integrable models. We use lazy evaluation and streams to represent and
    operate with pseudo-differential operators. No order of truncation is
    needed since terms are produced on demand. We give a series of
    concrete examples.",
  keywords = "axiomref"
}

@article{Boeh89,
  author = "Boehm, Hans-J.",
  title = {{Type Inference in the Presence of Type Abstraction}},
  journal = "ACM SIGPLAN Notices",
  volume = "24",
  number = "7",
  month = "July",
  year = "1989",
  pages = "192-206",
  link = "\url{http://www.acm.org/pubs/citations/proceedings/pldi/73141/p192-boehm}",
  abstract = "
    A number of recent programming language designs incorporate a type
    checking system based on the Girard-Reynolds polymorphic
    $\lambda$-calculus.  This allows the construction of general purpose,
    reusable software without sacrificing compile-time type checking. A
    major factor constraining the implementation of these languages is the
    difficulty of automatically inferring the lengthy type information
    that is otherwise required if full use is made of these
    languages. There is no known algorithm to solve any natural and fully
    general formulation of the ``type inference'' problem. One very
    reasonable formulation of the problem is known to be undecidable.

    Here we define a restricted version of the type inference problem and
    present an efficient algorithm for its solution. We argue that the
    restriction is sufficiently weak to be unobtrusive in practice.",
  paper = "Boeh89.pdf",
  keywords = "axiomref",
  beebe = "Boehm:1989:TIP"
}

@inproceedings{BHGM04,
  author = "Boulton, Richard and Hardy, Ruth and Gottliebsen, Hanne 
            and Martin, Ursula",
  title = {{Design verification for control engineering}},
  year = "2004",
  month = "April",
  booktitle = "Proc 4th Int. Conf. on Integrated Formal Methods",
  abstract = "
    We introduce control engineering as a new domain of application for
    formal methods. We discuss design verification, drawing attention to
    the role played by diagrammatic evaluation criteria involving numeric
    plots of a design, such as Nichols and Bode plots. We show that
    symbolic computation and computational logic can be used to discharge
    these criteria and provide symbolic, automated, and very general
    alternatives to these standard numeric tests. We illustrate our work
    with reference to a standard reference model drawn from military
    avionics.",
  keywords = "axiomref"
}

@misc{Bou93a,
  author = "Boulanger, Jean-Louis",
  title = {{Axiom, language fonctionnel \`a d\'evelopement objet}},
  year = "1993",
  month = "October",
  paper = "Bou93a.pdf",
  keywords = "axiomref"
}

@misc{Boul93b,
  author = "Boulanger, Jean-Louis",
  title = {{AXIOM, A Functional Language with Object Oriented Development}},
  year = "1993",
  abstract = 
    "We present in this paper, a study about the computer algebra system
    Axiom, which gives us many very interesting Software engineering
    concepts.  This language is a functional language with an Object
    Oriented Development.  This feature is very important for modeling the
    mathematical world (Hierarchy) and provides a running with
    mathematical sense. (All objects are functions). We present many
    problems of running and development in Axiom. We can note that Aiom is
    the only system of this category.",
  paper = "Boul93b.pdf",
  keywords = "axiomref"
}

@misc{Boul95,
  author = "Boulanger, J.L.",
  title = {{Object Oriented Method for Axiom}},
  year = "1995",
  month = "February",
  pages = "33-41",
  publisher = "ACM SIGPLAN Notices, 30(2) CODEN SINODQ ISSN 0362-1340",
  abstract = "
    Axiom is a very powerful computer algebra system which combines two
    language paradigms (functional and OOP). Mathematical world is complex
    and mathematicians use abstraction to design it. This paper presents
    some aspects of the object oriented development in Axiom. The Axiom
    programming is based on several new tools for object oriented
    development, it uses two levels of class and some operations such that
    {\sl coerce}, {\sl retract}, or {\sl convert} which permit the type
    evolution. These notions introduce the concept of multi-view.",
  paper = "Boul95.pdf",
  keywords = "axiomref",
  beebe = "Boulanger:1995:OOM"
}

@inproceedings{Bron89,
  author = "Bronstein, Manuel",
  title = {{Simplification of real elementary functions}},
  booktitle = "Proc. ISSAC 1989",
  series = "ISSAC 1989",
  year = "1989",
  pages = "207-211",
  isbn = "0-89791-325-6",
  doi = "https://dx.doi.org/10.1145/74540.74566",
  abstract = "
    We describe an algorithm, based on Risch's real structure theorem, that
    determines explicitly all the algebraic relations among a given set of
    real elementary functions. We also provide examples from its
    implementation that illustrate the advantages over the use of complex
    logarithms and exponentials.",
  paper = "Bron89.djvu",
  keywords = "axiomref",
  beebe = "Bronstein:1989:SRE"
}

@inproceedings{Bron91a,
  author = "Bronstein, M.",
  title = {{The Risch Differential Equation on an Algebraic Curve}},
  booktitle = "Proc. 1991 Int. Symp. on Symbolic and Algebraic Computation",
  series = "ISSAC'91",
  year = "1991",
  pages = "241-246",
  isbn = "0-89791-437-6",
  publisher = "ACM, NY",
  abstract = 
    "We present a new rational algorithm for solving Risch differential
    equations over algebraic curves. This algorithm can also be used to
    solve $n^{th}$-order linear ordinary differential equations with
    coefficients in an algebraic extension of the rational functions. In
    the general (``mixed function'') case, this algorithm finds the
    denominator of any solution of the equation.",
  paper = "Bro91a.pdf",
  keywords = "axiomref",
  beebe = "Bronstein:1991:RDE"
}

@misc{Bron92,
  author = "Bronstein, Manuel",
  title = {{Linear Ordinary Differential Equations: Breaking Through the 
           Order 2 Barrier}},
  booktitle = "Proc. ISSAC 1992",
  series = "ISSAC 92",
  publisher = "ACM",
  pages = "42-48",
  year = "1992",
  link = "\url{http://www-sop.inria.fr/cafe/Manuel.Bronstein/publications/issac92.ps.gz}",
  algebra =
   "\newline\refto{package ODECONST ConstantLODE}
    \newline\refto{package LODEEF ElementaryFunctionLODESolver}
    \newline\refto{package ODEPAL PureAlgebraicLODE}
    \newline\refto{package ODERAT RationalLODE}
    \newline\refto{package ODERED ReduceLODE}",
  abstract = "
    A major subproblem for algorithms that either factor ordinary linear
    differential equations or compute their closed form solutions is to
    find their solutions $y$ which satisfy $y^{'}/y \in \overline{K}(x)$
    where $K$ is the constant field for the coefficients of the equation.
    While a decision procedure for this subproblem was known in the
    $19^{th}$ century, it requires factoring polynomials over
    $\overline{K}$ and has not been implemented in full generality. We
    present here an efficient algorithm for this subproblem, which has
    been implemented in the AXIOM computer algebra system for equations of
    arbitrary order over arbitrary fields of characteristic 0. This
    algorithm never needs to compute with the individual complex
    singularities of the equation, and algebraic numbers are added only
    when they appear in the potential solutions.  Implementation of the
    complete Singer algorithm for $n=2,3$ based on this building block is
    in progress.",
  paper = "Bron92.pdf",
  keywords = "axiomref"
}

@article{Bron92a,
  author = "Bronstein, Manuel",
  title = {{Formulas for Series Computation}},
  journal = "Applied Algebra in Engineering, Communication and Computing",
  volume = "2",
  pages = "195-206",
  year = "1992",
  abstract =
    "We describe an algorithm that computes polynomials whose roots are
    the coefficients of any specific order of the Laurent series of a
    rational function. The algorithm uses only rational operations in the
    coefficient field of the function. This allows us to compute with the
    principal parts of the Laurent series, in particular with the residues
    of the function, without factoring or splitting its denominator. As
    applications, we get a generalisation of the residue formulas used in
    symbolic integration algorithms to $n^{th}$-order formulas. We also
    get an algorithm that computes explicitly the principlal parts at all
    the poles simultaneously, while computing in the field generated by
    the coefficients of the series instead of the one generated by the
    poles of the function. This yields an improved version of the
    necessary conditions for the various cases of Kovacic's algorithm,
    that expresses those conditions in the smallest possible extension
    field."
}

@misc{Bro92a,
  author = "Bronstein, Manuel",
  title = {{Integration and Differential Equations in Computer Algebra}},
  year = "1992",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.576}",
  abstract = "
    We describe in this paper how the problems of computing indefinite
    integrals and solving linear ordinary differential equations in closed
    form are now solved by computer algebra systems. After a brief review
    of the mathematical history of those problems, we outline the two
    major algorithms for them (respectively the Risch and Singer
    algorithms) and the recent improvements on those algorithms which has
    allowed them to be implemented.",
  paper = "Bro92a.pdf",
  keywords = "axiomref"
}

@article{Bene94,
  author = "Beneke, T. and Schwippert, W.",
  title = {{Double-track into the future: MathCAD will gain new users with 
           Standard and Plus versions}},
  journal = "Elektronik",
  volume = "43",
  number = "15",
  pages = "107-110",
  year = "1994",
  month = "July",
  abstract =
   "MathCAD software is a type of `intelligent scratchpad with a pocket
    calculator function'. Hitherto it has been suitable only to a limited
    extent for engineering mathematics. The new Version 5.0 is now offered
    in two implementations: as an inexpensive basic package and in a
    considerably more costly Plus version. The authors question whether
    MathCAD can catch up with the classical Maple and Mathematica products.",
  keywords = "axiomref",
  beebe = "Beneke:1994:DFM",
}

@inproceedings{Bron97a,
  author = "Bronstein, Manuel and Weil, Jacques-Arthur",
  title = {{On Symmetric Powers of Differential Operators}},
  booktitle = "Proc. of ISSAC 1997",
  series = "ISSAC'97",
  year = "1997",
  pages = "156-163",
  link = "\url{http://www-sop.inria.fr/cafe/Manuel.Bronstein/publications/mb_papers.html}",
  publisher = "ACM, NY",
  abstract = "
    We present alternative algorithms for computing symmetric powers of
    linear ordinary differential operators. Our algorithms are applicable
    to operators with coefficients in arbitrary integral domains and
    become faster than the traditional methods for symmetric powers of
    sufficiently large order, or over sufficiently complicated coefficient
    domains. The basic ideas are also applicable to other computations
    involving cyclic vector techniques, such as exterior powers of
    differential or difference operators.",
  paper = "Bro97a.pdf",
  keywords = "axiomref"
}

@article{Brow94,
  author = "Brown, Ronald and Tonks, Andrew",
  title = {{Calculations with simplicial and cubical groups in AXIOM}},
  journal = "Journal of Symbolic Computation",
  volume =  "17",
  number = "2",
  pages = "159-179",
  year = "1994",
  month = "February",
  misc = "CODEN JSYCEH ISSN 0747-7171",
  abstract = 
    "Work on calculations with simplicial and cubical groups in AXIOM was
    carried out using loan equipment and software from IBM UK and guidance
    from L. A. Lambe. We report on the results of this work, and present
    the AXIOM code written by the second author during this period. This
    includes an implementation of the monoids which model cubes and
    simplices, together with a new AXIOM category of near-rings with which
    to carry out non-abelian calculations. Examples of the use of this
    code in interactive AXIOM sessions are also given.",
  paper = "Brow94.pdf",
  keywords = "axiomref"
}

@misc{Brow95,
  author = "Brown, Ronald and Dreckmann, Winfried",
  title = {{Domains of data and domains of terms in AXIOM}},
  year = "1995",
  link = "\url{http://axiom-wiki.newsynthesis.org/public/refs/brown-free-c-g.pdf}",
  abstract = "
    The main new concept we wish to illustrate in this paper is a
    distinction between ``domains of data'' and ``domains of terms'', and
    its use in the programming of certain mathematical structures.
    Although this distinction is implicit in much of the programming work
    that has gone into the construction of Axiom categories and domains,
    we believe that a formalisation of this is new, that standards and
    conventions are necessary and will be useful in various other
    contexts. We shall show how this concept may be used for the coding of
    free categories and groupoids on directed graphs.",
  paper = "Brow95.pdf",
  keywords = "axiomref"
}

@misc{Buch11,
  author = "Buchberger, Bruno",
  title = {{Groebner Bases: A Short Introduction for System Theorists}},
  year = "2011",
  abstract =
    "In this paper, we give a brief overview on Groebner bases theory,
    addressed to novices without prior knowledge in the field. After
    explaining the general strategy for solving problems via the Groebner
    approach, we develop the concept of Groebner bases by studying
    uniqueness of polynomial division (``reduction''). For explicitly
    constructing Groebner bases, the crucial notion of S-polynomials is
    introduced, leading to the complete algorithmic solution of the
    construction problem. The algorithm is applied to examples from
    polynomial equation solving and algebraic relations. After a short
    discussion of complexity issues, we conclude the paper with some
    historical remarks and references."
}

@book{Buch85,
  author = "Buchberger, Bruno and Caviness, Bob F. (eds)",
  title = {{Lecture Notes in Computer Science, Vol 204}},
  isbn = "0-387-15983-5 (vol 1), 0-387-15984-3 (vol 2)",
  year = "1985",
  month = "April",
  publisher = "Springer-Verlag, Berlin, Germany",
  keywords = "axiomref"
}

@misc{Buh05, 
  author = "Buhl, Soren L.",
  title = {{Some Reflections on Integrating a Computer Algebra System in R}},
  year = "2005",
  keywords = "axiomref"
}

@InProceedings{Burg91,
  author = "Burge, W.H.",
  title = {{Scratchpad and the Rogers-Ramanujan identities}},
  booktitle = "Proc. ISSAC 1991",
  year = "1991",
  pages = "189-190",
  abstract =
    "This note sketches the part played by Scratchpad in obtaining new
    proofs of Euler's theorem and the Rogers-Ramanujan Identities.",
  paper = "Burg91.pdf",
  keywords = "axiomref",
  beebe = "Burge:1991:SRI"
}

@techreport{Burg87,
  author = "Burge, William and Watt, Stephen",
  title = {{Infinite structures in SCRATCHPAD II}},
  year = "1987",
  institution = "IBM Research",
  type = "Technical Report",
  number = "RC 12794 (\#57573)",
  link = "\url{http://www.csd.uwo.ca/~watt/pub/reprints/1987-eurocal-infinite.pdf}",
  abstract =
    "An {\sl infinite structure} is a data structure which cannot be fully
    constructed in any fixed amount of space. Several varieties of
    infinite structures are currently supported in Scratchpad II: infinite
    sequences, radix expansions, power series and continued fractions. Two
    basic methods are employed to represent infinite structures:
    self-referential data structures and lazy evaluation. These may be
    employed separately or in conjunction.
    
    This paper presents recently developed facilities in Scratchpad II for
    manipulating infinite structures. General techniques for manipulating
    infinite structures are covered, as well as the higher level
    manipulations on the various types of mathematical objects represented
    by infinite structures.",
  paper = "Burg87.pdf",
  keywords = "axiomref"
}

@inproceedings{Burg87b,
  author = "Burge, William H. and Watt, Stephen M.",
  title = {{Infinite structures in Scratchpad II}},
  booktitle = "EUROCAL '87. European Conference on Computer Algebra
               Proceedings",
  year = "1987",
  pages = "138-148",
  isbn = "3-540-51517-8",
  abstract = 
    "An infinite structure is a data structure which cannot be fully
    constructed in any fixed amount of space.  Several varieties of
    infinite structures are currently supported in Scratchpad II: infinite
    sequences, radix expansions, power series and continued fractions. Two
    basic methods are employed to represent infinite structures:
    self-referential data structures and lazy evaluation. These may be
    employed either separately or in conjunction. This paper presents
    recently developed facilities in Scratchpad II for manipulating
    infinite structures. General techniques for manipulating infinite
    structures are covered, as well as the higher level manipulations on
    the various types of mathematical objects represented by infinite
    structures.",
  keywords = "axiomref",
  beebe = "Burge:1989:ISS"
}

@article{Calm87,
  author = "Calmet, Jacques",
  title = {{Intelligent Computer Algebra System: Myth, Fancy or
             Reality?}},
  journal = "Lecture Notes in Computer Science",
  volume = "296",
  year = "1987",
  pages = "2-11",
  paper = "Calm87.pdf",
  keywords = "axiomref, printed"
}

@article{Calm97,
  author = "Calmet, J. and Campbell, J.A.",
  title = {{A perspective on symbolic mathematical computing and 
           artificial intelligence}},
  journal = "Ann. Math. Artif. Intell.",
  volume = "19",
  number = "3-4",
  pages = "261-277",
  year = "1997",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.5425}",
  abstract =
    "The nature and history of the research area common to artificial
    intelligence and symbolic mathematical computation are examined, with
    particular reference to the topics having the greatest current amount
    of activity or potential for further development: mathematical
    knowledge-based computing environments, autonomous agents and
    multi-agent systems, transformation of problem descriptions in logics
    into algebraic forms, exploitation of machine learning, qualitative
    reasoning, and constraint-based programming. Knowledge representation,
    for mathematical knowledge, is identified as a central focus for much
    of this work. Several promising topics for further research are stated.",
  paper = "Calm97.pdf",
  keywords = "axiomref"
}

@techreport{Cami92,
  author = "Camion, Paul and Courteau, Bernard and Montpetit, Andre",
  title = {{A combinatorial problem in Hamming Graphs and its solution 
           in Scratchpad}},
  comment = {Un probl\`eme combinatoire dans les graphies de Hamming et sa
             solution en Scratchpad},
  year = "1992",
  month = "January",
  link = "\url{https://hal.inria.fr/inria-00074974/document}",
  type = "Research report",
  number = "1586",
  institution = "Institut National de Recherche en Informatique et en 
                 Automatique, Le Chesnay, France",
  abstract =
    "We present a combinatorial problem which arises in the determination
    of the complete weight coset enumerators of error-correcting codes
    [1]. In solving this problem by exponential power series with
    coefficients in a ring of multivariate polynomials, we fall on a
    system of differential equations with coefficients in a field of
    rational functions. Thanks to the abstraction capabilities of
    Scratchpad this differential equation may be solved simply and
    naturally, which seems not to be the case for the other computer
    algebra systems now available.",
  paper = "Cami92.pdf",
  keywords = "axiomref",
  beebe = "Camion:1992:PCG"
}

@misc{Capr99a,
  author = "Capriotti, O. and Carlisle, D.",
  title = {{OpenMath and MathML: Semantic Mark Up for Mathematics}},
  year = "1999",
  link = "\url{http://www.acm.org/crossroads/xrds6-2/openmath.html}",
  keywords = "axiomref"
}

@misc{Capr99,
  author = "Capriotti, Olga and Cohen, Arjeh M. and Cuypers, Hans and 
            Sterk, Hans",
  title = {{OpenMath Technology for Interactive Mathematical Documents}},
  year = "2002",
  pages = "51-66",
  publisher = "Springer-Verlag, Berlin, Germany",
  link = "\url{http://www.win.tue.nl/~hansc/lisbon.pdf}",
  misc = "in Multimedia Tools for Communicating Mathematics",
  paper = "Capr99.pdf",
  keywords = "axiomref"
}

@article{Care06,
  author = "Carette, Jacques",
  title = {{Gaussian elimination: a case study in efficient genericity
           with MetaOCaml}},
  journal = "Sci. Comput. Program.",
  volume = "62",
  number = "1",
  pages = "3-24",
  year = "2006",
  link = "\url{http://www.cas.mcmaster.ca/~carette/publications/ge.pdf}",
  abstract =
    "The Gaussian Elimination algorithm is in fact an algorithm family –
    common implementations contain at least six (mostly independent)
    ``design choices''. A generic implementation can easily be parametrized
    by all these design choices, but this usually leads to slow and
    bloated code. Using MetaOCaml’s staging facilities, we show how we can
    produce a natural and type-safe implementation of Gaussian Elimination
    which exposes its design choices at code-generation time, so that
    these choices can effectively be specialized away, and where the
    resulting code is quite efficient.",
  paper = "Care06.pdf",
  keywords = "axiomref"
}

@article{Care11,
  author = "Carette, Jacques and Kiselyov, Oleg",
  title = {{Multi-stage programming with functors and monads: eliminating
           abstraction overhead from generic code}},
  journal = "Sci. Comput. Program",
  volume = "76",
  number = "5",
  pages = "349-375",
  year = "2011",
  link = "\url{http://www.cas.mcmaster.ca/~carette/metamonads/metamonads.pdf}",
  abstract =
    "We use multi-stage programming, monads and OCaml's advanced module
    system to demonstrate how to eliminate all abstraction overhead from
    generic programs, while avoiding any inspection of the resulting code.
    We demonstrate this clearly with Gaussian Elimination as a
    representative family of symbolic and numeric algorithms. We
    parameterize our code to a great extent -- over domain, input and
    permutation matrix representations, determinant and rank tracking,
    pivoting policies, result types, etc. -- at no run-time cost. Because
    the resulting code is generated just right and not changed afterward,
    MetoOCaml guarantees that the generated code is well-typed. We further
    demonstrate that various abstraction parameters (aspects) can be made
    orthogonal and compositional, even in the presence of name-generation
    for temporaries, and ``interleaving'' of aspects. We also show how to
    encode some domain-specific knowledge so that ``clearly wrong''
    compositions can be rejected at or before generation time, rather than
    during the compilation or running of the generated code.",
  paper = "Care11.pdf",
  keywords = "axiomref"
}

@misc{Carp04,
  author = "Carpent, Quentin and Conil, Christophe",
  title = {{Utilisation de logiciels libres pour la r\'ealisation de TP MT26}},
  year = "2004",
  link = "\url{http://axiom-wiki.newsynthesis.org/public/refs/ac20.pdf}",
  comment = "french",
  abstract = "radicalSolve(x**3+x**2-7=0,x)",
  paper = "Carp04.pdf",
  keywords = "axiomref"
}

@misc{Caru10,
  author = "Caruso, Fabrizio",
  title = {{Factorization of Non-Commutative Polynomials}},
  link = "\url{https://arxiv.org/pdf/1002.3108.pdf}",
  year = "2010",
  abstract =
    "We describe an algorithm for the factorization of non-commutative
    polynomials over a field. The first sketch of this algorithm appeared
    in an unpublished manuscript (literally hand written notes) by James
    H.  Davenport more than 20 years ago. This version of the algorithm
    contains some improvements with respect to the original sketch. An
    improved version of the algorithm has been fully implemented in the
    Axiom computer algebra system.",
  paper = "Caru10.pdf",
  keywords = "axiomref"
}

@article{Cavi85,
  author = "Caviness, Bob",
  title = {{Computer Algebra: Past and Future}},
  journal = "LNCS",
  volume = "203",
  pages = "1-18",
  year = "1985",
  paper = "Cavi85.pdf",
  keywords = "axiomref, printed"
}

@misc{Cavi03,
  author = "Caviness, Bob and Trager, Barry and Gianni, Patrizia",
  title = {{Dedicated to the Memory of Richard Dimick Jenks}},
  year = "2003",
  link = "\url{https://www.eecis.udel.edu/~caviness/jenks/issac-ded.pdf}",
  abstract =
    "On December 30, 2003, Dick Jenks died at the age of 66, after an
    extended and courageous battle with multiple system atrophy.
    
    He received his PhD in mathematics from the University of Illinois at
    Urbana-Champaign in 1966. The title of his dissertation was
    “Quadratic Differential Systems for Mathematical Models” and was
    written under the supervision of Donald Gilles. After completing his
    PhD, he was a post-doctoral fellow at Brookhaven National Laboratory
    on Long Island. In 1968 he joined IBM Research where he worked until
    his retirement in 2002.

    At IBM he was one of the principal architects of the Scratchpad
    system, one of the earliest computer algebra systems(1971). Dick
    always believed that natural user interfaces were essential and
    developed a user-friendly rule-based system for Scratchpad. Although
    this rule-based approach was easy to use, as algorithms for computer
    algebra became more complicated, he began to understand that an
    abstract data type approach would give sophisticated algorithm
    development considerably more leverage. In 1977 he began the Axiom
    development (originally called Scratchpad II) with the design of
    MODLISP, a merger of Lisp with types (modes). In 1980, with the help
    of many others, he completed an initial prototype design based on
    categories and domains that were intended to be natural for
    mathematically sophisticated users.
    
    During this period many researchers in computer algebra visited IBM
    Research in Yorktown Heights and contributed to the development of
    the Axiom system. All this activity made the computer algebra group at
    IBM one of the leading centers for research in this area and Dick was
    always there to organize the visits and provide a stimulating and
    pleasant working environment for everyone. He had a good perspective
    on the most important research directions and worked to attract
    world-renowned experts to visit and interact with his group. He was an
    ideal manager for whom to work, one who always put the project and the
    needs of the group members first. It was a joy to work in such a
    vibrant and stimulating environment.
    
    After many years of development, a decision was made to rename
    Scratchpad II to Axiom and to release it as a product. Dick and
    Robert Sutor were the primary authors of the book {\sl Axiom: The
    Scientific Computation System}.  In the foreword of the book, written
    by David and Gregory Chudnovsky, it is stated that ``The Scratchpad
    system took its time to blossom into the beautiful Axiom
    product. There is no rival to this powerful environment in its scope
    and, most importantly, in its structure and organization.'' Axiom was
    recently made available as free software. See
    http://savannah.nongnu.org/projects/axiom
    
    Dick was active in service to the computer algebra community as
    well. Here are some highlights. He served as Chair of ACM SIGSAM
    (1979-81) and Conference Co-chair (with J. A. van Hulzen) of EUROSAM
    ’84, a precursor of the ISSAC meetings. Dick also had a long period of
    service on the editorial board of the {\sl Journal of Symbolic
    Computation}. At ISSAC ’95 in Montreal, Dick was elected to the
    initial ISSAC Steering Committee and was elected as the second Chair
    of the Committee in 1997. He, along with David Chudnovsky, organized
    the highly successful meetings on Computers and Mathematics that were
    held at Stanford in 1986 and MIT in 1989. As a legacy of those
    meetings, the Jenks Prize for outstanding contributions to software 
    engineering in computer algebra has been established.
    
    Dick had many interests outside of his professional pursuits including
    reading, travel, physical fitness, and especially music. Dick was an
    accomplished pianist, organist, and vocalist. At one point he was the
    organist and choir master of the Church of the Holy Communion in
    Mahopac, NY. In the 1980s and 1990s, he sang in choral groups under
    the direction of Dr. Dennis Keene that performed at Lincoln Center in
    New York city.
    
    Personally, Dick was warm, generous, and outgoing with many
    friends. He will be missed for his technical accomplishments, his
    artist talents, and most of all for his positive, gentle, charming
    spirit.",
  paper = "Cavi03.pdf",
  keywords = "axiomref",
  beebe = "Caviness:2004:MRD"
}

@misc{Cert16,
  author = "Certik, Ondrej",
  title = {{SymPy vs. Axiom}},
  link = "\url{https://github.com/sympy/sympy/wiki/SymPy-vs.-Axiom}",
  keywords = "axiomref"
}

@inproceedings{Chan04,
  author = "Chan, L. and Cheb-Terrab, E.S.",
  title = {{Non-Liouvillian solutions for second order linear ODEs}},
  booktitle = "Proc. ISSAC 04",
  pages = "80-86",
  year = "2004",
  isbn = "1-58113-827-X",
  link = "\url{http://www.cecm.sfu.ca/CAG/papers/edgardoIS04.pdf}",
  abstract =
    "There exist sound literature and algorithms for computing Liouvillian
    solutions for the important problem of linear ODEs with rational
    coefficients. Taking as sample the 363 second order equations of that
    type found in Kamke's book, for instance, 51\% of them admit Liouvillian
    solutions and so are solvable using Kovacic's algorithm. On the other
    hand, special function solutions not admitting Liouvillian form appear
    frequently in mathematical physics, but there are not so general 
    algorithms for computing them. In this paper we present an algorithm
    for computing special function solutions which can be expressed using
    the $_2F_1$, $_1F_1$ or $_0F_1$ hypergeometric functions. They algorithm
    is easy to implement in the framework of a computer algebra system and
    systematically solves 91\% of the 363 Kamke's linear ODE examples
    mentioned.",
  paper = "Chan04.pdf",
  keywords = "axiomref"
}

@misc{Chic04,
  author = "Chicha, Yannis and Lloyd, Michael Oancea, Cosmin",
  title = {{Parametric Polymorphism for Computer Algebra Software Components}},
  booktitle = "6th Int. Symp. on Symbolic and Numeric Algorithms for 
               Scientific Computing",
  series = "SYNASC 04",
  location = "Imisoara, Romania",
  year = "2004",
  link = "\url{http://www.csd.uwo.ca/~watt/pub/reprints/2004-synasc-ppca.pdf}",
  abstract = 
    "This paper presents our experiments in providing mechanisms for
    parametric polymorphism for computer algebra software components.
    Specific interfaces between Aldor and C++ and between Aldor and Maple
    are described. We then present a general solution, Generic IDL (GIDL),
    an extension to CORBA IDL supporting generic types. We describe our
    language bindings for C++, Java 1.5 and Aldor as well as aspects of
    our implementation, consisting of a GIDL to IDL compiler and tools for
    generating interface code for the various language bindings.",
  paper = "Chic04.pdf",
  keywords = "axiomref"
}

@misc{Chouxx,
  author = "Chou, Shang-Ching and Gao, Xiao-Shan and Liu, Zhuo-Jun and
            Wang, Ding-Kang and Wang, Dongming",
  title = {{Geometric Theorem Provers and Algebraic Equations Solvers}},
  comment = "Chapter 20",
  link = "\url{http://www.mmrc.iss.ac.cn/~xgao/papers/soft1.pdf}",
  abstract =
    "The methods of mechanizing mathematics are realized by means of
    compputer software for solving scientiØc and engineering problems via
    symbolic and hybrid computation. This chapter provides a collection of
    geometric theorem provers and algebraic equations solvers that are
    pieces of mathematical software based mostly on Wu's method and were
    developed mainly by members of the extended Wu group. The early
    theorem provers, though e±cient, were written in basic programming
    languages and on primitive computers. Now there exist more powerful
    and mature geometric theorem provers of which some have already been
    published as commercial software. On the other hand, building 
    effective equations solvers is still at the experimental stage and
    remains for further research and development.",
  paper = "Chouxx.pdf"
}

@misc{Chu85,
  author = "Chudnovsky, D.V and Chudnovsky, G.V.",
  title = {{Elliptic Curve Calculations in Scratchpad II}},
  year = "1985",
  institution = "Mathematics Dept., IBM Research",
  type = "Scratchpad II Newsletter 1 (1)",
  keywords = "axiomref"
}

@proceedings{CJ86,
  editor = "Chudnovsky, David and Jenks, Richard",
  title = {{Computers in Mathematics}},
  year = "1986",
  month = "July",
  isbn = "0-8247-8341-7",
  note = "International Conference on Computers and Mathematics",
  publisher = "Marcel Dekker, Inc",
  keywords = "axiomref"
}

@InProceedings{Coli95,
  author = "Colin, Antoine",
  title = {{Formal computation of Galois groups with relative resolvants}},
  booktitle = "Proc. AAECC-11",
  series = "AAECC-11",
  year = "1995",
  publisher = "Springer-Verlag",
  location = "Paris, France",
  pages = "169-182",
  abstract = 
    "Let $k$ be a field and $f \in k[x]$ be a polynomial of degree $n$. The
    permutation action of $G$ on the roots $\{\alpha_i\}_{i=1}^n$ of $f$
    can be determined by an algorithm suggested by R. Stauduhar
    [Math. Comput. 27, 981-996 (1973; Zbl 0282.12004)] that approximates $G$
    via successive steps in a chain of subgroups 
    $S_n=H_0 > H_1 > \ldots > H_k=G$. In each step $H_{i-1} > H_i$ 
    it checks as a test for $G \le H_i$ whether a relative invariant $k_i
    \in k[x_1,\ldots,x_n]$ yields a value under the specialization
    $\varphi : g(x_1,\ldots,x_n) \mapsto g(\alpha_1,\ldots,\alpha_n)$.  In
    implementations this evaluation has been done using $p$-adic
    [H. Darmon and D. Ford, Commun. Algebra 17, No. 12, 2941-2943 (1989;
    Zbl 0693.12010)] or numerical (R. Stauduhar [ibid.]; Y. Eichenlaub and
    M. Olivier [preprint]) approximation of the roots.
    
    The paper under review presents a new approach which avoids all
    approximations: If $G \le H_i$ and $H_i$ is maximal in $H_{i-1}$ the
    invariant $h_i$ is a primitive element of the invariant field 
    $k_i=k(x_1,\ldots,x_n)^{H_i}$ as an extension of 
    $k_{i-1}=K(x_1,\ldots,x_n)^{H_{i-1}}$.
    The author develops an algorithm to express the specialized values 
    $\varphi(g)$ of elements $g \in k_i$ in terms of $k_{i-1}$ and the 
    specialization $\varphi(h_i)$.
    
    This algorithm then is applied to the relative resolvent polynomial
    \[s_i = \prod_a{(y-a(x_1,\ldots,x_n))}\]
    where $a$ runs through the images of $h_i$ under $H_{i-1}$. 
    It has $y$-coefficients which are in $k_{i-1}$. 
    The algorithm then permits to express the coefficients of the
    specialization $r_i(y)=\varphi(s_i) \in k[y]$ recursively in the 
    (already known) specializations $\varphi(h_i)$ for $j \le i-1$,
    using the coefficients of $f$ (as $S_n$-invariants in the roots) 
    as a seed. A root of $r_i(y)$ in the base field then proves that $G$
    is contained in (a conjugate of) $H_i$, and this value of the root can 
    be used as specialized $\varphi(h_{i+1})$ in the next step of the 
    algorithm. Special care is given to the case when denominators of 
    elements in $k(x_1,\ldots,x_n)$ evaluate to zero after specialization.
    
    The paper closes with a short discussion of applicability. An
    implementation using AXIOM and GAP is in process but has not yet been
    completed.",
  keywords = "axiomref"
}

@article{Coli97,
  author = "Colin, Antoine",
  title = {{Solving a system of algebraic equations with symmetries}},
  journal = "J. Pure Appl. Algebra",
  volume = "117-118",
  pages = "195-215",
  year = "1997",
  abstract =
    "Let $(F)$ be a system of $p$ polynomial equations 
    $F_i({\bf X}) \in k[{\bf X}]$, where $k$ is a commutative field and 
    ${\bf X} := (X_1,\cdots,X_n)$ are indeterminates. Let $G$ be a subgroup
    of $GL_n(k)$. A polynomial $P \in k[{\bf X}]$ (resp. rational function 
    $P \in k({\bf X})$ ) is an invariant of $G$ if and only if for all 
    $A \in G$ we have $A\cdot P = P$. We denote $k[{\bf X}]^G$ by (resp. 
    $k({\bf X})^G$) the algebra of polynomial (resp. rational function) 
    invariants of $G$. If $L$ is another subgroup of $GL_n(k)$ such that 
    $G \subset L$, $P$ is called a primary invariant of $G$ relative to $L$ if 
    and only if $Stab_L(P) = G$ (where $Stab_L(P)$ is the stabilizer of 
    $P$ in $L$).

    The paper describes the algebra of the invariants of a finite group
    and how to express these invariants in terms of a small number of
    them, from both the Cohen-Macaulay algebra and the field theory points
    of view. A method is proposed to solve $(F)$ by expressing it in terms of
    primary invariants $\Pi_1,\cdots,\Pi_n$
    (e.g. the elementary symmetric polynomials) and one
    ``primitive'' secondary invariant.
    
    The main thrust of the paper is contained in the following theorem. 
    Let $(F)$ be a set of invariants of $G$. Let $L$ be a subgroup of 
    $GL_n(k)$ such that $G \subset L$ and $k({\bf X})^L$ is a purely 
    transcendental extension of $k_i$, let $\Pi_1,\cdots,\Pi_n$ be 
    polynomials such that $k({\bf X})^L = k(\Pi_1,\cdots,\Pi_n)$,
    and let $\Theta \in k[{\bf X}]^G$ be a primitive polynomial invariant 
    of $G$ relative to $L$.
    When possible, it is convenient to choose $\Theta$ to be one of the
    polynomials in $(F)$. – An algorithm is given that allows each polynomial
    $F_i$ to be expressed as $F_i({\bf X}) = H_i(\Pi_1,\cdots,\Pi_n,\Theta)$, 
    an algebraic fraction in $\Pi_1,\cdots,\Pi_n$ and a polynomial in
    $\Theta$. Now let $L$ be the minimal polynomial of $\Theta$ over 
    $k[{\bf X}]^L$; we have 
    \[L({\bf X},T)=\prod_{\Theta^{'} \in L\cdot \Theta}(T-\Theta^{'})
    \in k[{\bf X}]^L[T]\]
    (where $L$ is called a generic Lagrange resolvent). 
    As $k(\Pi_1,\cdots,\Pi_n)=k({\bf X})^L$, we can write 
    $L({\bf X},T)=H_0(\Pi_1,\cdots,\Pi_n,T)$ where $H_0$ is some
    rational function. The question 
    $H_0(\Pi_1,\cdots,\Pi_n,\Theta)=0$ is always satisfied because 
    $\Theta$ is a root of $L$. Then, we solve the system of ($p=1$) 
    algebraic equations $H_i(\Pi_1,\cdots,\Pi_n,\Theta)=0$,
    $0 \le i \le p$ for $\Pi_1,\cdots,\Pi_n,\Theta$ as indeterminates.
    
    Theorem 1: Let $D \in k[\Pi_1,\cdots,\Pi_n]$ be the LCM of the 
    denominators of all the fractions $H_i$,$0 \le i \le p$ and let 
    $H_i^{'}=DH_i$. For every solution 
    $x:=(x_1,\cdots,x_n)$ of the system $(F)$:$F_i({\bf X})=0$,
    $1 \le i \le p$, there exists a solution ($\pi_1,\cdots,\pi_n,\Theta$)
    of the system 
    $(H^{'}):H_i^{'}(\Pi_1,\cdots,\Pi_n,\Theta)=0$, $0 \le i \le p$ such 
    that $x$ is a solution of the system 
    $(P_\pi):\Pi_i({\bf X})=\pi_i$, $1 \le i \le n$ , and of the equation 
    $\Theta({\bf X})=0$. Conversely, for any solution 
    $(\pi_1,\cdots,\pi_n,\theta)$ of the system $(H^{'})$ such that 
    $D(\pi_1,\cdots,\pi_n) \ne 0$, if $x$ is a solution of the system 
    $(P_\pi)$ relative to $(\pi_1,\cdots,\pi_n)$, then there exists 
    some $A \in L$ such that $\Theta(A\cdot x)=\theta$, and then for all 
    $B \in G$, $BA\cdot x$, is a solution of the system $(F)$.
    
    A slighly more general version of this theorem is also given. The
    paper then presents an algorithm that applies the theory and has been
    implemented in AXIOM. It is followed by several examples.",
  keywords = "axiomref"
}

@article{Coll82,
  author = "Collins, G.E. and Mignotte, M. and Winkler, F.",
  title = {{Arithmetic in Basic Algebraic Domains}},
  publisher = "Springer-Verlag",
  journal = "Computing, Supplement 4",
  pages = "189-220",
  year = "1982",
  abstract =
    "This chapter is devoted to the arithmetic operations, essentially
    addition, multiplication, exponentiation, division, gcd calculations
    and evaluation, on the basic algebraic domains. The algorithms for
    these basic domains are those most frequently used in any computer
    algebra system. Therefore the best known algorithms, from a
    computational point of view, are presented. The basic domains
    considered here are the rational integers, the rational numbers,
    integers modulo $m$, Gaussian integers, polynomials, rational
    functions, power series, finite fields and $p$-adic numbers. BOunds on
    the maximum, minimum and average computing time ($t^{+},t^{-},t^{*}$) for
    the various algorithms are given."
}

@misc{Cohe03,
  author = "Cohen, Arjeh and Cuypers, H. and Barreiro, Hans and 
            Reinaldo, Ernesto and Sterk, Hans",
  title = {{Interactive Mathematical Documents on the Web}},
  year = "2003",
  pages = "289-306",
  editor = "Joswig, M. and Takayma, N.",
  publisher = "Springer-Verlag, Berlin, Germany",
  misc = "in Algebra, Geometry and Software Systems",
  keywords = "axiomref"
}

@book{Cohe03a,
  author = "Cohen, Joel S.",
  title = {{Computer algebra and symbolic computation. Mathematical Methods}},
  year = "2003",
  link = "\url{http://eclass.uth.gr/eclass/modules/document/file.php/MHX102/Cohen\_Computer\_Algebra\_and\_Symbolic\_Computation\_\_Mathematical\_Methods.pdf}",
  publisher = "A. K. Peters",
  isbn = "1-56881-159-4",
  paper = "Cohe03a.pdf",
  keywords = "axiomref"
}

@book{Cohe03b,
  author = "Cohen, Joel S.",
  title = {{Computer algebra and symbolic computation. Elementary Algorithms}},
  year = "2003",
  publisher = "A. K. Peters",
  isbn = "1-56881-159-4",
  paper = "Cohe03b.pdf",
  keywords = "axiomref"
}

@misc{Conrxxa,
  author = "Conrad, Marc and French, Tim and Maple, Carsten and Pott, Sandra",
  title = {{Approaching Inheritance from a Natural Mathematical Perspective 
           and from a Java Driven Viewpoint: a Comparative Review}},
  link = "\url{http://axiom-wiki.newsynthesis.org/public/refs/McTfCmSp-axiom.pdf}",
  abstract = "
    It is well-known that few object-oriented programming languages allow
    objects to change their nature at run-time. There have been a number
    of reasons presented for this, but it appears that there is a real
    need for matters to change. In this paper we discuss the need for
    object-oriented programming languages to reflect the dynamic nature of
    problems, particularly those arising in a mathematical context. It is
    from this context that we present a framework that realistically
    represents the dynamic and evolving characteristic of problems and
    algorithms.",
  paper = "Conrxxa.pdf",
  keywords = "axiomref"
}

@misc{Conrxxb,
  author = "Conrad, Marc and French, Tim and Maple, Carsten and Pott, Sandra",
  title = {{Mathematical Use Cases lead naturally to non-standard Inheritance 
        Relationships: How to make them accessible in a mainstream language?}},
  abstract = "
    Conceptually there is a strong correspondence between Mathematical
    Reasoning and Object-Oriented techniques. We investigate how the ideas
    of Method Renaming, Dynamic Inheritance and Interclassing can be used
    to strengthen this relationship. A discussion is initiated concerning
    the feasibility of each of these features.",
  paper = "Conrxxb.pdf",
  keywords = "axiomref"
}

@article{Corl00,
  author = "Corless, Robert M. and Jeffrey, David J. and Watt, Stephen M. and
           Davenport, James H.",
  title = {{``According to Abramowitz and Stegun'' or 
             arccoth needn't be Uncouth}},
  journal = "SIGSAM Bulletin - Special Issue on OpenMath",
  volume = "34",
  number = "2",
  pages = "58-65",
  year = "2000",
  algebra = 
   "\newline\refto{category OM OpenMath}
    \newline\refto{domain COMPLEX Complex}
    \newline\refto{domain DFLOAT DoubleFloat}
    \newline\refto{domain FLOAT Float}
    \newline\refto{domain FRAC Fraction}
    \newline\refto{domain INT Integer}
    \newline\refto{domain LIST List}
    \newline\refto{domain SINT SingleInteger}
    \newline\refto{domain STRING String}
    \newline\refto{domain SYMBOL Symbol}
    \newline\refto{package OMEXPR ExpressionToOpenMath}
    \newline\refto{package OMSERVER OpenMathServerPackage}",
  abstract =
    "This paper addresses the definitions in OpenMath of the elementary
    functions. The original OpenMath definitions, like most other sources,
    simply cite [2] as the definition. We show that this is not adequate,
    and propose precise definitions, and explore the relationships between
    these definitions.In particular, we introduce the concept of a couth
    pair of definitions, e.g. of arcsin and arcsinh, and show that the
    pair arccot and {\sl arccoth} can be couth.",
  paper = "Corl00.pdf"
}

@book{Coxx07,
  author = "Cox, David and Little, John and O'Shea, Donald",
  title = {{Ideals, varieties and algorithms. An introduction to computational
           algebraic geometry and commutative algebra}},
  publisher = "Springer",
  isbn = "978-0-387-35650-1",
  year = "2007",
  link = "\url{http://www.dm.unipi.it/~caboara/Misc/Cox,\%20Little,\%20O'Shea\%20-\%20Ideals,\%20varieties\%20and%20algorithms.pdf}",
  algebra = "\newline\refto{package GB GroebnerPackage}
             \newline\refto{package PSEUDLIN PseudoLinearNormalForm}
             \newline\refto{package PGROEB PolyGroebner}
             \newline\refto{domain DMP DistributedMultivariatePolynomial}
    \newline\refto{domain GDMP GeneralDistributedMultivariatePolynomial}
    \newline\refto{domain HDMP HomogeneousDistributedMultivariatePolynomial}",
  abstract =
    "Around 1980 two new directions in science and technique came
    together. One was Buchberger’s algorithms in order to handle Groebner
    bases in an effective way for solving polynomial equations. The second
    one was the development of the personal computers. This was the
    starting point of a computational perspective in commutative algebra
    and algebraic geometry. In 1991 the three authors invented the first
    edition of their book as an introduction for undergraduates to some
    interesting ideas in commutative algebra and algebraic geometry with a
    strong perspective to practical and computational aspects. A second
    revised edition appeared in 1996. That means from the very beginning
    the book provides a bridge for the new, computational aspects in the
    field of commutative algebra and algebraic geometry.

    To be more precise, the book gives an introduction to Buchberger’s
    algorithm with applications to syzygies, Hilbert polynomials, primary
    decompositions. There is an introduction to classical algebraic
    geometry with applications to the ideal membership problem, solving
    polynomial equations, and elimination theory. Some more spectacular
    applications are about robotics, automatic geometric theorem proving,
    and invariants of finite groups. It seems to the reviewer to carry
    coals to Newcastle for estimating the importance and usefulness of the
    book. It should be of some interest to ask how many undergraduates
    have been introduced to algorithmic aspects of commutative algebra and
    algebraic geometry following the line of the book. The reviewer will
    be sure that this will continue in the future too.  

    What are the changes to the previous editions? There is a significant
    shorter proof of the Extension Theorem, see 3.6 in Chapter 3,
    suggested by A.H.M. Levelt. A major update has been done in Appendix C
    ``Computer Algebra Systems''. This concerns in the main the section
    about MAPLE. Some minor updated information concern the use of AXIOM,
    CoCoA, Macaulay2, Magma, Mathematica, and SINGULAR. This reflects
    about the recent developments in Computer Algebra Systems. It
    encourages an interested reader to more practical exercises. The
    authors have made changes on over 200 pages to enhance clarity and
    correctness. Many individuals have reported typographical errors and
    gave the authors feedback on the earlier editions. The book is
    well-written. The reviewer guesses that it will become more and more
    difficult to earn 1 dollar (sponsored by the authors) for every new
    typographical error as it was the case also with the first and second
    edition. The reviewer is sure that it will be a excellent guide to
    introduce further undergraduates in the algorithmic aspect of
    commutative algebra and algebraic geometry.",
  paper = "Coxx07.pdf",
  keywords = "axiomref"
}

@article{Crou95,
  author = "Crouch, Peter E. and Lamnabhi-Lagarrigue, Francoise and 
            Pinchon, Didier",
  title = {{Some realizations of algorithms for nonlinear input-output 
            systems}},
  journal = "Int. J. Control",
  volume = "62",
  number = "4",
  pages = "941-960",
  year = "1995",
  abstract = 
    "The first two authors previously developed an algorithm for
    constructing a parametrization of the observation space of a nonlinear
    control system directly from the differential equation representation
    of the input-output behaviour. This paper extends the previous
    algorithm by including settings where a set of implicit input-output
    differential equations is given as well as more general state-space
    representations in which the controls enter nonlinearly. Various
    state-space realizations, including bilinear, polynomial and nilpotent
    approximating realizations are discussed. The final section of the
    paper sketches the implementation of the algorithm using the symbolic
    manipulation package AXIOM to find the realizations mentioned above in
    feasible cases.",
  keywords = "axiomref"
}

@misc{Cuyp10,
  author = "Cuypers, Hans and Hendriks, Maxim and Knopper, Jan Willem",
  title = {{Interactive Geometry inside MathDox}},
  year = "2010",
  link = "\url{http://www.win.tue.nl/~hansc/MathDox_and_InterGeo_paper.pdf}",
  paper = "Cuyp10",
  keywords = "axiomref"
}

@inproceedings{Dalm97,
  author = {Dalmas, St\'ephane and Ga\"etano, Marc and Watt, Stephen},
  title = {{An OpenMath 1.0 Implementation}},
  booktitle = "Proc. 1997 Int. Symp. on Symbolic and Algebraic Computation",
  series = "ISSAC'97",
  year = "1997",
  isbn = "0-89791-875-4",
  location = "Kihei, Maui, Hawaii, USA",
  pages = "241-248",
  numpages = "8",
  link = "\url{http://doi.acm.org/10.1145/258726.258794}",
  doi = "10.1145/258726.258794",
  acmid = "258794",
  publisher = "ACM, New York, NY USA",
  keywords = "axiomref"
}

@inproceedings{Dalm92,
  author = "Dalmas, Stephane",
  title = {{A polymorphic functional language applied to symbolic 
            computation}},
  year = "1992",
  booktitle = "Proc. ISSAC 1992",
  series = "ISSAC 1992",
  pages = "369-375",
  isbn = "0-89791-489-9 (soft cover) 0-89791-490-2 (hard cover)",
  abstract =
    "The programming language in which to describe mathematical objects
    and algorithms is a fundamental issue in the design of a symbolic
    computation system.  XFun is a strongly typed functional programming
    language. Although it was not designed as a specialized language, its
    sophisticated type system can be successfully applied to describe
    mathematical objects and structures. After illustrating its main
    features, the author sketches how it could be applied to symbolic
    computation. A comparison with Scratchpad II is attempted. XFun seems
    to exhibit more flexibility simplicity and uniformity.",
  keywords = "axiomref",
  beebe = "Dalmas:1992:PFL"
}

@misc{Daly08,
  author = "Daly, Timothy",
  title = {{Axiom Computer Algebra System Information Sources}},
  video = "https://www.youtube.com/watch?v=CV8y3UrpadY",
  year = "2008",
  keywords = "axiomref"
}

@misc{Daly88,
  author = "Daly, Timothy",
  title = {{Axiom in an Educational Setting, Axiom course slide deck}},
  year = "1988",
  month = "January",
  keywords = "axiomref"
}

@article{Daly02,
  author = "Daly, Timothy",
  title = {{Axiom as open source}},
  journal = "SIGSAM Bulletin",
  volume = "36",
  number = "1",
  pages = "28-28",
  month = "March",
  year = "2002",
  keywords = "axiomref",
  beebe = "Daly:2002:AOS"
}

@misc{Daly05,
  author = "Daly, Timothy",
  title = {{LispNYC Presentation at Trinity}},
  year = "2005",
  month = "May",
  day = "10",
  abstract =
    "Timothy Daly presents Axiom 

    Timothy Daly, published author, academic researcher, open source
    programmer and lead developer of Axiom will be presenting about his role
    as the driving force behind Axiom.  With over 70 developers and 200
    researchers worldwide it can best be described as:

    Axiom is a general purpose Computer Algebra system. It is useful
    for research and development of mathematical algorithms providing
    a very high level way to express abstract mathematical concepts.
    The Axiom Library defines over 1,000 strongly-typed mathematical
    domains and categories.

    Axiom consists of an interpreter and compiler, a browser, a graphical
    interface, and a new online wiki that allows users to create web pages
    that inline computations.

    Axiom is built upon Common Lisp.",
  keywords = "axiomref"
}

@misc{Dalyxx,
  author = "Daly, Timothy",
  title = {{Tim Daly on Lisp in Industry}},
  link = "\url{https://news.ycombinator.com/item?id=1580904}",
  keywords = "axiomref"
}

@misc{Daly13a,
  author = "Daly, Timothy and Barnes, Nick",
  title = {{Ten reasons you must publish your code}},
  year = "2013",
  link = "\url{http://climatecode.org/blog/2013/07/ten-reasons-you-must-publish-your-code/}",
  keywords = "axiomref"
}

@article{Dave90a,
  author = "Davenport, James H.",
  title = {{Current Problems in Computer Algebra Systems Design}},
  journal = "LNCS",
  volume = "429",
  year = "1990",
  pages = "1-9",
  abstract =
    "Computer Algebra systems have been with us for over twenty years, but
    there is still no consensus on what an ``ideal'' system would look
    like.  There are all sorts of tradeoffs between portability,
    functionality, and efficiency. This paper discusses some of those issues.",
  keywords = "axiomref"
}

@techreport{Dave92e,
  author = "Davenport, James H.",
  title = {{The AXIOM System}},
  type = "technical report",
  institution = "Numerical Algorithms Group, Oxford, U.K.",
  number = "TR5/92",
  year = "1992",
  paper = "Dave92a.pdf",
  keywords = "axiomref, printed"
}

@misc{Dave99,
  author = "Davenport, James",
  title = {{A Small OpenMath Type System}},
  year = "1999",
  link = "\url{https://www.openmath.org/standard/sts.pdf}",
  paper = "Dave99.pdf",
  keywords = "printed"
}

@book{Dave05,
  author = "Davenport, James H.",
  title = {{Integration -- What do we want from the theory?}},
  booktitle = "Computer Algebra",
  publisher = "Springer",
  series = "Lecture Notes in Computer Science 162",
  pages = "2-11",
  year = "2005",
  abstract = 
    "The theory of integration has moved a long way in the last fourteen
    years, thought not far enough to satisfy the demands placed on it by
    its customers. This paper outlines what problems have yet to be solved,
    and tries to explain why they are not trivial."
}

@techreport{Dave80,
  author = "Davenport, James H. and Jenks, Richard D.",
  title = {{MODLISP: A Preliminary Design}},
  institution = "IBM Research",
  type = "Research Report",
  year = "1980",
  number = "RC 8073",
  keywords = "axiomref"
}

@techreport{Dave80a,
  author = "Davenport, James H. and Jenks, Richard D.",
  title = {{MODLISP}},
  institution = "IBM Research",
  type = "Research Report",
  year = "1980",
  number = "RC 8537 (\#37198)",
  comment = "http://www.computerhistory.org/collections/catalog/102719109",
  keywords = "axiomref"
}

@article{Dave81a,
  author = "Davenport, James H. and Jenks, Richard D.",
  title = {{MODLISP}},
  year = "1981",
  journal = "ACM SIGSAM Bulletin",
  volume = "15",
  issue = "1",
  pages = "11-20",
  publisher = "ACM",
  abstract =
    "This paper discusses the design and implementation of MODLISP, a
    LISP-like language enhanced with the idea of MODes. This extension
    permits, but does not require, the user to declare the types of
    various variables, and to compile functions with the arguments
    declared to be of a particular type. It is possible to declare several
    functions of the same name, with arguments of different type
    (e.g. PLUS could be declared for Integer arguments, or Rational, or
    Real, or even Polynomial arguments) and the system will apply the
    correct function for the types of the arguments.",
  paper = "Dave81a.pdf",
  keywords = "axiomref"
}

@manual{Dave84,
  author = "Davenport, James H. and Gianni, Patrizia and Jenks, Richard D. and
            Miller, Victor and Morrison, Scott C. and Rothstein, Michael and
            Sundaresan, Christine and Sutor, Robert S. and Trager, Barry M.",
  title = {{Scratchpad}},
  organization = "Mathematical Sciences Department",
  address = "IBM Thomas Watson Research Center, Yorktown Heights, NY",
  year = "1984",
  keywords = "axiomref",
  beebe = "Davenport:1984:S"
}

@misc{Dave84a,
  author = "Davenport, James H.",
  title = {{A New Algebra System}},
  link = "\url{http://axiom-wiki.newsynthesis.org/public/refs/Davenport-1984-a\_new\_algebra\_system.pdf}",
  abstract = "Seminal internal paper discussing Axiom design decisions.",
  paper = "Dave84a.pdf",
  keywords = "axiomref"
}

@misc{Dave84b,
  author = "Davenport, James H. and Gianni, Patrizia and Jenks, Ricard D. and
            Miller, Victor and Morrison, Scott and Rothstein, Michael and
            Sundaresan, Christine J. and Sutor, Robert S. and Trager, Barry",
  title = {{SCRATCHPAD System Programming Language Manual}},
  year = "1984",
  keywords = "axiomref"
}

@article{Dave85,
  author = "Davenport, James H.",
  title = {{The LISP/VM Foundation of Scratchpad II}},
  journal = "The Scratchpad II Newsletter",
  volume = "1",
  number = "1",
  year = "1985",
  month = "September",
  institution = "IBM Research",
  keywords = "axiomref"
}

@book{Dave88,
  author = "Davenport, James H. and Siret, Y. and Tournier, E.",
  title = {{Computer Algebra: Systems and Algorithms for Algebraic 
           Computation}}, 
  publisher = "Academic Press",
  year = "1988", 
  isbn ="0-12-204230-1",
  link = "\url{http://staff.bath.ac.uk/masjhd/masternew.pdf}",
  paper = "Dave88.pdf",
  keywords = "axiomref",
  beebe = "Davenport:1988:CA"
}

@techreport{Dave89,
  author = "Davenport, James H.",
  title = {{Looking at a set of equations}},
  institution = "University of Bath, School of Mathematical Sciences",
  year = "1989",
  type = "technical report",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.261.767}",
  abstract =
    "This working paper describes our experiences with using the
    Groebner-basis method [Buchberger, 1985] to solve some related systems
    of polynomial equations. While we have not yet been able to solve the
    system that was our primary motivation, we feel that these experiences
    may prove useful to others investigating Buchberger's algorithm in
    this context, especially when, as is the case for the system under
    investigation, the equations are highly structured. We conclude with
    some examples of the polynomials that we factored in the course of
    this investigation.",
  paper = "Dave89.pdf"
}

@misc{Dave93,
  author = "Davenport, James H.",
  title = {{The PoSSo Project}},
  paper = "Dave93.pdf",
  keywords = "axiomref"
}

@InProceedings{Dave00,
  author = "Davenport, James H.",
  title = {{Abstract data types in computer algebra}},
  booktitle = "Mathematical foundations of computer science",
  series = "MFCS 2000",
  year = "2000",
  location = "Bratislava, Slovakia",
  pages = "21-35",
  abstract =
    "The theory of abstract data types was developed in the late 1970s and
    the 1980s by several people, including the ``ADJ'' group, whose work
    influenced the design of Axiom. One practical manifestation of this
    theory was the OBJ-3 system. An area of computing that cries out for
    this approach is computer algebra, where the objects of discourse are
    mathematical, generally satisfying various algebraic rules. There have
    been various theoretical studies of this in the literature. The aim of
    this paper is to report on the practical applications of this theory
    within computer algebra, and also to outline some of the theoretical
    issues raised by this practical application. We also give a
    substantial bibliography.",
  paper = "Dave00.pdf",
  keywords = "axiomref"
}

@article{Dave02,
  author = "Davenport, James H.",
  title = {{Equality in computer algebra and beyond}},
  journal = "J. Symbolic Computing",
  volume = "34",
  number = "4",
  pages = "259-270",
  year = "2002",
  link =
    "\url{http://www.calculemus.net/meetings/siena01/Papers/Davenport.pdf}",
  abstract =
    "Equality is such a fundamental concept in mathematics that, in
    fact, we seldom explore it in detail, and tend to regard it as
    trivial. When it is shown to be non-trivial, we are often
    surprised. As is often the case, the computerization of
    mathematical computation in computer algebra systems on the one
    hand, and mathematical reasoning in theorem provers on the other
    hand, forces us to explore the issue of equality in greater
    detail.In practice, there are also several ambiguities in the
    definition of equality. For example, we refer to $\mathbb{Q}(x)$
    as ``rational functions'', even though $\frac{x^2-1}{x-1}$ and
    $x+1$ are not equal as functions from $\mathbb{R}$ to
    $\mathbb{R}$, since the former is not defined at $x=1$, even
    though they are equal as elements of $\mathbb{Q}(x)$. The aim of
    this paper is to point out some of the problems, both with
    mathematical equality and with data structure equality, and to
    explain how necessary it is to keep a clear distintion between the two.",
  paper = "Dave02.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@misc{Dave07,
  author = "Davenport, James H. and Fitch, John",
  title = {{Computer Algebra and the three 'E's: Efficiency, Elegance, and
           Expressiveness}},
  link = "\url{http://staff.bath.ac.uk/masjhd/Drafts/PLMMS2007}",
  abstract =
    "What author of a programming language would not claim that the 3 'E's
    were the goals? Nevertheless, we claim that computer algebra does lead
    to particular emphases, and constraints, in these areas.
    
    We restrict ``efficiency'' to mean machine efficiency, since the other
    'E's cover programmer efficiency. For the sake of clarity, we describe
    as ``expressiveness'', what can be expressed in the language, and
    ``elegance'' as how it can be expressed.",
  paper = "Dave07.pdf",
  keywords = "axiomref"
}

@InProceedings{Dave90,
  author = "Davenport, James H. and Trager, Barry M.",
  title = {{Scratchpad's view of algebra I: Basic commutative algebra}},
  booktitle = "Design and Implementation of Symbolic Computation Systems",
  year = "1990",
  pages = "40-54",
  series = "DISCO '90",
  location = "Capri, Italy",
  publisher = "Springer-Verlag",
  isbn = "0-387-52531-9",
  link = "\url{http://opus.bath.ac.uk/32336/1/Davenport\_DISCO\_1990.pdf}",
  comment = "AXIOM Technical Report, ATR/1, NAG Ltd., Oxford, 1992",
  keywords = "axiomref",
  abstract =
    "While computer algebra systems have dealt with polynomials and
    rational functions with integer coefficients for many years, dealing
    with more general constructs from commutative algebra is a more recent
    problem. In this paper we explain how one system solves this problem,
    what types and operators it is necessary to introduce and, in short,
    how one can construct a computational theory of commutative
    algebra. Of necessity, such a theory is rather different from the
    conventional, non-constructive, theory. It is also somewhat different
    from the theories of Seidenberg [1974] and his school, who are not
    particularly concerned with practical questions of efficiency.",
  paper = "Dave90.pdf",
  keywords = "axiomref",
  beebe = "Davenport:1990:SVA"
}

@inproceedings{Dave91,
  author = "Davenport, J. H. and Gianni, P. and Trager, B. M.",
  title = {{Scratchpad's View of Algebra II: 
           A Categorical View of Factorization}},
  booktitle = "Proc. 1991 Int. Symp. on Symbolic and Algebraic Computation",
  series = "ISSAC '91",
  year = "1991",
  isbn = "0-89791-437-6",
  location = "Bonn, West Germany",
  pages = "32--38",
  numpages = "7",
  link = "\url{http://doi.acm.org/10.1145/120694.120699}",
  doi = "10.1145/120694.120699",
  acmid = "120699",
  publisher = "ACM",
  address = "New York, NY, USA",
  abstract = "
    This paper explains how Scratchpad solves the problem of presenting a
    categorical view of factorization in unique factorization domains,
    i.e.  a view which can be propagated by functors such as
    SparseUnivariatePolynomial or Fraction. This is not easy, as the
    constructive version of the classical concept of
    UniqueFactorizationDomain cannot be so propagated. The solution
    adopted is based largely on Seidenberg's conditions (F) and (P), but
    there are several additional points that have to be borne in mind to
    produce reasonably efficient algorithms in the required generality.

    The consequence of the algorithms and interfaces presented in this
    paper is that Scratchpad can factorize in any extension of the
    integers or finite fields by any combination of polynomial, fraction
    and algebraic extensions: a capability far more general than any other
    computer algebra system possesses. The solution is not perfect: for
    example we cannot use these general constructions to factorize
    polyinmoals in $\overline{Z[\sqrt{-5}]}[x]$ since the domain
    $Z[\sqrt{-5}]$ is not a unique factorization domain, even though
    $\overline{Z[\sqrt{-5}]}$ is, since it is a field. Of course, we can
    factor polynomials in $\overline{Z}[\sqrt{-5}][x]$",
  paper = "Dave91.pdf",
  keywords = "axiomref",
  beebe = "Davenport:1991:SVA"
}

@techreport{Dave92c,
  author = "Davenport, James H. and Trager, Barry M.",
  title = {{Scratchpad's view of algebra I: Basic commutative algebra}},
  number = "TR3/92 (ATR/1) (NP2490)",
  institution = "Numerical Algorithm Group (NAG) Ltd.",
  year = "1992",
  abstract =
    "While computer algebra systems have dealt with polynomials and
    rational functions with integer coefficients for many years, dealing
    with more general constructs from commutative algebra is a more recent
    problem. In this paper we explain how one system solves this problem,
    what types and operators it is necessary to introduce and, in short,
    how one can construct a computational theory of commutative
    algebra. Of necessity, such a theory is rather different from the
    conventional, non-constructive, theory. It is also somewhat different
    from the theories of Seidenberg [1974] and his school, who are not
    particularly concerned with practical questions of efficiency.",
  paper = "Dave90c.pdf",
  keywords = "axiomref",
  beebe = "Davenport:1992:SVAa"
}

@techreport{Dave92d,
  author = "Davenport, James H. and Gianni, Patrizia and Trager, Barry M.",
  title = {{Scratchpad's view of algebra II: 
           A categorical view of factorization}},
  type = "Technical Report",
  number = "TR4/92 (ATR/2) (NP2491)",
  institution = "Numerical Algorithms Group, Inc.",
  address = "Downer's Grove, IL, USA and Oxford, UK",
  year = "1992",
  link = "\url{http://www.nag.co.uk/doc/TechRep/axiomtr.html}",
  abstract = "
    This paper explains how Scratchpad solves the problem of presenting a
    categorical view of factorization in unique factorization domains,
    i.e.  a view which can be propagated by functors such as
    SparseUnivariatePolynomial or Fraction. This is not easy, as the
    constructive version of the classical concept of
    UniqueFactorizationDomain cannot be so propagated. The solution
    adopted is based largely on Seidenberg's conditions (F) and (P), but
    there are several additional points that have to be borne in mind to
    produce reasonably efficient algorithms in the required generality.

    The consequence of the algorithms and interfaces presented in this
    paper is that Scratchpad can factorize in any extension of the
    integers or finite fields by any combination of polynomial, fraction
    and algebraic extensions: a capability far more general than any other
    computer algebra system possesses. The solution is not perfect: for
    example we cannot use these general constructions to factorize
    polyinmoals in $\overline{Z[\sqrt{-5}]}[x]$ since the domain
    $Z[\sqrt{-5}]$ is not a unique factorization domain, even though
    $\overline{Z[\sqrt{-5}]}$ is, since it is a field. Of course, we can
    factor polynomials in $\overline{Z}[\sqrt{-5}][x]$",
  paper = "Dave91.pdf",
  keywords = "axiomref",
  beebe = "Davenport:1992:SVAb"
}

@techreport{Dave92a,
  author = "Davenport, James H.",
  title = {{The AXIOM system}},
  type = "technical report",
  number = "TR5/92 (ATR/3) (NP2492)",
  institution = "Numerical Algorithms Group, Inc.",
  year = "1992",
  abstract =
    "AXIOM is a computer algebra system superficially like many others,
    but fundamentally different in its internal construction, and
    therefore in the possibilities it offers to its users. In these
    lecture notes, we will
    \begin{itemize}
    \item outline the high-level design of the AXIOM kernel and the AXIOM type
    system,
    \item explain some of the algebraic facilities implemented in AXIOM, 
    which may be more general than the reader is used to,
    \item show how the type system and the information system interact,
    \item give some references to the literature on particular aspects of 
    AXIOM and,
    \item suggest the way forward.
    \end{itemize}",
  paper = "Dave92a.pdf",
  keywords = "axiomref",
  beebe = "Davenport:1992:AS"
}

@techreport{Dave92b,
  author = "Davenport, James H.",
  title = {{How does one program in the AXIOM system?}},
  institution = "Numerical Algorithms Group, Inc.",
  year = "1992",
  type = "technical report",
  number = "TR6/92 (ATR/4)(NP2493)",
  link = "\url{http://www.nag.co.uk/doc/TechRep/axiomtr.html}",
  abstract = 
    "Axiom is a computer algebra system superficially like many others, but
    fundamentally different in its internal construction, and therefore in
    the possibilities it offers to its users and programmers. In these
    lecture notes, we will explain, by example, the methodology that the
    author uses for programming substantial bits of mathematics in Axiom.",
  paper = "Dave92b.pdf",
  keywords = "axiomref",
  beebe = "Davenport:1992:HDO"
}

@inproceedings{Dave92,
  author = "Davenport, James H.",
  title = {{Primality Testing Revisited}},
  link = "\url{http://staff.bath.ac.uk/masjhd/ISSACs/ISSAC1992.pdf}",
  booktitle = "Proc. ISSAC 1992",
  series = "ISSAC 92",
  publisher = "ACM",
  pages = "123-129",
  year = "1992",
  report = "Technical Report TR2/93 Numerical Algorithms Group, Inc",
  algebra = "\newline\refto{package PRIMES IntegerPrimesPackage}",
  abstract =
    "Rabin's algorithm is commonly used in computer algebra systems and
    elsewhere for primality testing. This paper presents an experience
    with this in the Axiom computer algebra system. As a result of this
    experience, we suggest certain strengthenings of the algorithm.",
  paper = "Dave92.pdf",
  keywords = "axiomref",
  beebe = "Davenport:1993:PTR"
}

@techreport{Faur00,
  author = "Faure, Christele and Davenport, James H. and Naciri, Hanane",
  title = {{Multi-Valued Computer Algebra}},
  year = "2000",
  type = "technical report",
  institution = "INRIA CAFE",
  number = "4001",
  abstract = 
    "One of the main strengths of computer algebra is being able to solve
    a family of problems with one computation. In order to express not
    only one problem but a family of problems, one introduces some symbols
    which are in fact the parameters common to all the problems of the
    family. The user must be able to understand in which way these
    parameters affect the result when he looks at the answer. Otherwise it
    may lead to completely wrong calculations, which when used for
    numerical applications bring nonsensical answers. This is the case in
    most current Computer Algebra Systems we know because the form of the
    answer is never explicitly conditioned by the values of the
    parameters. The user is not even informed that the given answer may be
    wrong in some cases then computer algebra systems can not be entirely
    trustworthy. We have introduced multi-valued expressions called
    conditional expressions, in which each potential value is associated
    with a condition on some parameters. This is used, in particular, to
    capture the situation in integration, where the form of the answer can
    depend on whether certain quantities are positive, negative or
    zero. We show that it is also necessary when solving modular linear
    equations or deducing congruence conditions from complex expressions.",
  paper = "Faur00.pdf"
}

@misc{Dave94,
  author = {Davenport, James and Faure, Christ\'ele},
  title = {{The Unknown in Computer Algebra}},
  link = "\url{http://axiom-wiki.newsynthesis.org/public/refs/TheUnknownInComputerAlgebra.pdf}",
  year = "1994",
  abstract = "
    Computer algebra systems have to deal with the confusion between
    ``programming variables'' and ``mathematical symbols''. We claim that
    they should also deal with ``unknowns'', i.e. elements whose values
    are unknown, but whose type is known. For examples $x^p \ne x$ if $x$
    is a symbol, but $x^p = x$ if $x \in GF(p)$. We show how we have
    extended Axiom to deal with this concept.",
  paper = "Dave94.pdf",
  keywords = "axiomref"
}

@article{Dave11,
  author = "Davenport, James H.",
  title = {{CICM 2011: Conferences on Intelligent Computer Mathematics 2011}},
  journal = "Springer Lecture Notes in Artificial Intelligence 6824",
  pages = "1-67",
  link = "\url{http://people.bath.ac.uk/masjhd/Meetings/CICM2011.pdf}",
  comment = "http://www.springerlink.com/conten/978-3-642-22672-4",
  year = "2011",
  paper = "Dave11.pdf",
  keywords = "axiomref"
}

@misc{Dave15,
  author = "Davenport, James H.",
  title = {{SIAM AAG 15 and ICIAM 2015}},
  link = "\url{http://people.bath.ac.uk/masjhd/Meetings/AAG-ICIAM15.pdf}",
  paper = "Dave15.pdf",
  keywords = "axiomref"
}

@misc{Deckxx,
  author = "Decker, Wolfram",
  title = {{Some Introductory Remarks on Computer Algebra}},
  link = "\url{https://www.math.uni-bielefeld.de/~rehmann/ECM/cdrom/3ecm/pdfs/pant3/decker.pdf}",
  abstract = 
    "Computer algebra is a relatively young but rapidly growing field. In
    this introductory note to the mini-symposium on computer algebra
    organized as part of the third European Congress of Mathematics I will
    not even attempt to adress all major streams of research and the many
    applications of computer algebra. I will concentrate on a few aspects,
    mostly from a mathematical point of view, and I will discuss a few
    typical applications in mathematics. I will present a couple of
    examples which underline the fact that computer algebra systems
    provide easy access to powerful computing tools. And, I will quote
    from and refer to a couple of survey papers, textbooks and web-pages
    which I recommend for further reading.",
  paper = "Deckxx.pdf",
  keywords = "axiomref"
}

@phdthesis{Dell99,
  author = "Delliere, Stephane",
  title = {Trangularisation de syst\`emes constructibles Application \`a
           l'\'evaluation dynamique},
  school = {L'Universit\'e de Limoges},
  year = "1999",
  link = "\url{http://www.unilim.fr/laco/theses/1999/T1999_03.pdf}",
  paper = "Dell99.pdf",
  keywords = "axiomref"
}  

@techreport{Dell00,
  author = "Delliere, Stephane and Wang, Dongming",
  title = {{simple systems and dynamic constructible closure}},
  institution = "Universite de Limoges",
  year = "2000",
  type = "technical report",
  number = "2000-16",
  link = "\url{http://www.unilim.fr/laco/rapports/2000/R2000\_16.pdf}",
  abstract =
    "Dynamic evaluation is a general method for computing with parameters
    [6, 9]. In 1994, T. Gomez-Diaz implemented the dynamic constructible
    closure in the scientific computation system Axiom [17]: by simulating
    dynamic evaluation, it offers the possibility to compute with
    parameters in a very large way [13]. The outputs of a calculs with
    T. Gomez-Diaz programs are represented by a finite collection of
    constructible triangular systems defined in [12, definition
    p.106]. Though there are numerous applications of these programs
    (notably polynomial system solving with parameters [11], automatic
    geometric theorem proving [14, 15], computation of Jordan forms with
    parameters [16]), nobody gives theorical interest to this kind of
    triangular systems. The main reason of this phenomenon is that they
    are definied in [12] within the dynamic evaluation context. On the
    opposite, most notions of triangular systems (J.F. Ritt-W.T. Wu
    characteristic sets [24, 28], M. Kalkbrener regular chains [18],
    D. Lazard triangular sets [20], M. Moreno Maza regular sets [22],
    D.M. Wang simple systems [26, 27]) are defined in terms of commutative
    algebra. This problem is at the origin of the work done in [7] where
    we give a relevant algebraic model of T. Gomez-Diaz systems within
    commutative algebra terminology. This allows us to relate them to many
    concepts of triangular systems [7]. Thus, we give interest to the
    connections with D. Lazard triangular sets in [8]. In a way, this
    paper is the continuation of this previous work. This time, we study
    relationships between T. Gomez-Diaz systems and D.M. Wang simple
    systems. The paper is structured as follows. We have collected in
    section 2 some needed notations. In section 3, we give all the
    terminology related to our algebraic model of T. Gomez-Diaz
    systems. Thus, we define the notion of weak constructible triangular
    systems and introduce the properties of normalization and
    squarefreeness. Section 4 is more detailed.  First of all, we study a
    weaker form of normalization called $L$-normalization. Then we give
    many properties of constructible triangular systems verifying this new
    notion. We obtain an algebraic and geometric framework which permits,
    in section 5, to explore the connections between T. Gomez-Diaz systems
    and D.M. Wang simple systems. In particular, this last section will
    demonstrate well the importance of our $L$-normalization
    property. Indeed, we show that simple systems and squarefree
    $L$-normalized constructible triangular systems are equivalent.",
  paper = "Dell00.pdf",
  keywords = "axiomref"
}

@techreport{Dell00a,
  author = "Delliere, Stephane",
  title = {{A first course to $D_7$ with examples}},
  institution = "Universite de Limoges",
  year = "2000",
  type = "technical report",
  number = "2000-17",
  link = "\url{http://www.unilim.fr/laco/rapports/2000/R2000_17.pdf}",
  paper = "Dell00a.pdf",
  keywords = "axiomref"
}  

@article{Dell01,
  author = "Delliere, Stephane",
  title = {{On the links between triangular sets and dynamic constructable
           closure}},
  journal = "J. Pure Appl. Algebra",
  volume = "163",
  number = "1",
  pages = "49-68",
  year = "2001",
  abstract =
    "Two kinds of triangular systems are studied: normalized triangular
    polynomial systems (a weaker form of Lazard’s triangular sets
    [D. Lazard, Discrete Appl. Math. 33, No. 1-3, 147-160 (1991; Zbl
    0753.13013)] and constructible triangular systems (involved in the
    dynamic constructible closure programs of T. Gomez-Díaz [Quelques
    applications de l'evaluation dynamique, Ph.D. Thesis, Universite de
    Limoges (1994)]. This paper shows that these notions are strongly
    related. In particular, combining the two points of view
    (constructible and polynomial) on the subject of square-free
    conditions, it allows us to effect dramatic improvements in the
    dynamic constructible closure programs.",
  keywords = "axiomref"
}

@phdthesis{Deut73,
  author = "Deutsch, L. Peter",
  title = {{An Interactive Program Verifer}},
  school = "University of California, Berkeley",
  year = "1973",
  paper = "Deut73.pdf"
}

@InProceedings{Dewa92,
  author = "Dewar, Michael C.",
  title = {{Using Computer Algebra to Select Numerical Algorithms}},
  booktitle = "Proc. ISSAC 1992",
  series = "ISSAC 1992",
  year = "1992",
  location = "Berkeley, CA",
  pages = "1-8",
  algebra =
   "\newline\refto{domain D01AJFA d01ajfAnnaType}
    \newline\refto{domain D01AKFA d01akfAnnaType}
    \newline\refto{domain D01ALFA d01alfAnnaType}
    \newline\refto{domain D01AMFA d01amfAnnaType}
    \newline\refto{domain D01ANFA d01anfAnnaType}
    \newline\refto{domain D01APFA d01apfAnnaType}
    \newline\refto{domain D01AQFA d01aqfAnnaType}
    \newline\refto{domain D01ASFA d01asfAnnaType}
    \newline\refto{domain D01FCFA d01fcfAnnaType}
    \newline\refto{domain D01GBFA d01gbfAnnaType}
    \newline\refto{domain D01TRNS d01TransformFunctionType}
    \newline\refto{domain D02BBFA d02bbfAnnaType}
    \newline\refto{domain D02BHFA d02bhfAnnaType}
    \newline\refto{domain D02CJFA d02cjfAnnaType}
    \newline\refto{domain D02EJFA d02ejfAnnaType}
    \newline\refto{domain D03EEFA d03eefAnnaType}
    \newline\refto{domain D03FAFA d03fafAnnaType}
    \newline\refto{domain E04DGFA e04dgfAnnaType}
    \newline\refto{domain E04FDFA e04fdfAnnaType}
    \newline\refto{domain E04GCFA e04gcfAnnaType}
    \newline\refto{domain E04JAFA e04jafAnnaType}
    \newline\refto{domain E04MBFA e04mbfAnnaType}
    \newline\refto{domain E04NAFA e04nafAnnaType}
    \newline\refto{domain E04UCFA e04ucfAnnaType}
    \newline\refto{domain NIPROB NumericalIntegrationProblem}
    \newline\refto{domain ODEPROB NumericalODEProblem}
    \newline\refto{domain OPTPROB NumericalOptimizationProblem}
    \newline\refto{domain PDEPROB NumericalPDEProblem}",
  abstract =
    "Many real-life problems require a compbination of both symbolic and
    numerical methods for their solution. This has led to the development
    of intgrated, interactive symbolic / numeric packages which use a
    computer algebra system for the former and a standard subroutine
    library for the latter. These systems may also be viewed as simplified
    front-ends to the numerical library. To use these packages, however, a
    user must be able to select which of the many available routines is
    the most appropriate for his or her problem, which contrsts with the
    ``black-box'' style interfaces available in computer algebra
    systems. This paper describes how a computer algebra system can be
    used to make this decision, thus providing a much-simplified and more
    orthogonal interface.",
  paper = "Dewa92.pdf"
}

@misc{Dewa95,
  author = "Dewar, Mike C.",
  title = {{AXIOM and A\#: Current Status and Future Plans}},
  year = "1995",
  link = "\url{ftp://ftp.inf.ethz.ch/org/cathode/workshops/jan95/abstracts/dewar.ps}",
  paper = "Dewa95.pdf",
  keywords = "axiomref"
}

@misc{Dewa,
 author = "Dewar, Mike",
 title = {{OpenMath: An Overview}},
 link = "\url{http://www.sigsam.org/bulletin/articles/132/paper1.pdf}",
 paper = "Dewa.pdf",
 keywords = "axiomref"
}

@phdthesis{Diaz06,
  author = "Diaz, Glauco Alfredo Lopez",
  title = {{Symbolic Methods for Factoring Linear Differential Operators}},
  school = "Johannes Kepler Universitat, Linz",
  year = "2006",
  month = "February",
  abstract = 
    "A survey of symbolic methods for factoring linear differential
    operators is given. Starting from basic notions – ring of operators,
    differential Galois theory – methods for finding rational and
    exponential solutions that can provide first order right-hand factors
    are considered. Subsequently several known algorithms for
    factorization are presented. These include Singer’s eigenring
    factorization algorithm, factorization via Newton polygons, van
    Hoeij’s methods for local factorization, and an adapted version of
    Pade approximation.
    
    In addition a procedure based on pure algebraic methods for factoring
    second order linear partial differential operators is
    developed. Splitting an operator of this kind reduces to solving a
    system of linear algebraic equations. Those solutions which satisfy a
    certain different ial condition, immediately produce linear factors of
    the operator. The method applies also to operators of third order,
    thereby resulting in a more complicated system of equations. In
    contrast to the second order case, differential equations must also be
    solved, which, in particular cases, are simplified with the aid of
    characteristic sets.
    
    Finally, complete decomposition into linear factors of ordinary
    differential operators of arbitrary order is discussed. A splitting
    formula is developed, provided that a linear basis of solutions is
    available.  This theoretical representation is valuable in
    understanding the nature of the classical Beke algorithm and its
    variants like the algorithm LODEF by Schwarz and the Beke-Bronstein
    algorithm.",
  paper = "Diaz06.pdf",
  keywords = "axiomref"
}

@article{DiBl95,
  author = "DiBlasio, Paolo and Temperini, Marco",
  title = {{Subtyping Inheritance and Its Application in Languages for
           Symbolic Computation Systems}},
  journal = "J. Symbolic Computation",
  volume = "19",
  pages = "39-63",
  year = "1995",
  abstract = 
    "Application of object-oriented programming techniques to design and
    implementation of symbolic computation is investigated. We show the
    significance of certain correctness problems, occurring in programming
    environments based on specialization inheritance, due to use of method
    redefinition and polymorphism. We propose a solution to these
    problems, by defining a mechanism of subtyping inheritance and the
    prototype of an object-oriented programming language for a symbolic
    computation system. We devise the subtyping inheritance {\sl ESI
    (Enhanced String Inheritance)} by lifting to programming language
    constructs a given model of subtyping, which is established by a
    monotonic (covariant) subtyping rule. Type safeness of language
    instructions is proved.
    
    The adoption of {\sl ESI} allows to model method and class
    specialization in a natural way. The {\sl ESI} mechanism verifies the
    type correctness of language statements by means of type checking
    rules and preserves their correctness at run-time by a suitable method
    lookup algorithm.",
  paper = "DiBl95.pdf",
  keywords = "axiomref"
}

@InProceedings{DiBl97,
  author = "DiBlasio, Paolo and Temperini, Marco",
  title = {{On subtyping in languages for symbolic computation systems}},
  booktitle = "Advances in the design of symbolic computation systems",
  series = "Monographs in Symbolic Computation",
  year = "1997",
  publisher = "Springer",
  pages = "164-178",
  abstract =
    "We want to define a strongly typed OOP language suitable as the
    software development tool of a symbolic computation system, which
    provides class structure to manage ADTs and supports multiple
    inheritance to model specialization hierarchies. In this paper, we
    provide the theoretical background for such a task.",
  keywords = "axiomref"
}

@InProceedings{Dicr88,
  author = "Dicrescenzo, C. and Duval, D.",
  title = {{Algebraic extensions and algebraic closure in Scratchpad II}},
  booktitle = "Proc. ISSAC 1988",
  series = "ISSAC 1998",
  year = "1998",
  pages = "440-446",
  isbn = "3-540-51084-2",
  abstract =
    "Many problems in computer algebra, as well as in high-school
    exercises, are such that their statement only involves integers but
    their solution involves complex numbers. For example, the complex
    numbers $\sqrt{2}$ and $-\sqrt{2}$ appear in the solutions of
    elementary problems in various domains. 
    \begin{itemize}
    \item in {\bf integration}:
    \[\int{\frac{dx}{x^2-2}} = \frac{Log(x-\sqrt{2})}{2\sqrt{2}}
    +\frac{Log(x-(-\sqrt{2}))}{2(-\sqrt{2})}\]
    \item in {\bf linear algebra}: the eigenvalues of the matrix
    \[\left(\begin{array}{cc}
    1 & 1\\
    1 & -1
    \end{array}\right) = \sqrt{2} {\rm\ and\ }-\sqrt{2}\]
    \item in {\bf geometry}: the line $y=x$ intersects the circle
    $y^2+x^2=1$ at the points
    \[(\sqrt{2},\sqrt{2}) {\rm\ and\ }(-\sqrt{2},-\sqrt{2})\]
    \end{itemize}
    Of course, more ``complicated'' complex numbers appear in more
    complicated examples.

    But two facts have to be emphasized:
    \begin{itemize}
    \item in general, if a problem is stated over the integers (or over
    the field $\mathbb{Q}$ of rational numbers), the complex numbers that
    appear are {\sl algebraic} complex numbers, which means that they are
    roots of some polynomial with rational coefficients, like $\sqrt{2}$
    and $-\sqrt{2}$ are roots of $T^2-2$.
    \item Similar problems appear with base fields different from
    $mathbb{Q}$. For example finite fields, or fields of rational
    functions over $\mathbb{Q}$ or over a finite field. The general
    situation is that a given problem is stated over some ``small field''
    $K$, and its solution is expressed in an {\sl algebraic closure}
    $\overline{K}$ of $K$, which means that this solution involves numbers
    which are roots of polynomials with coefficients in $K$.
    \end{itemize}

    The aim of this paper is to describe an implementation of an algebraic
    closure domain constructor in the language Scratchpad II, simply
    called Scratchpad below. In the first part we analyze the problem, and
    in the second part we describe a solution based on the D5 system.",
  keywords = "axiomref",
  beebe = "Dicrescenzo:1989:AEA"
}

@misc{Dicr95,
  author = "Dicrescenzo, C. and Jung, Francoise",
  title = {{COMPASS package}},
  year = "1995",
  link = "\url{ftp://ftp.inf.ethz.ch/org/cathode/workshops/jan95/abstracts/bronstein.ps}",
  paper = "Dicr95.pdf",
  keywords = "axiomref"
}

@book{Dicr05,
  author = "Dicrescenzo, C. and Duval, D.",
  title = {{Algebraic extensions and algebraic closure in Scratchpad II}},
  booktitle = "Symbolic and Algebraic Computation",
  series = "Lecture Notes in Computer Science 358",
  year = "2005",
  publisher = "Springer",
  pages = "440-446",
  keywords = "axiomref"
}

@InProceedings{Ding94,
  author = "Dingle, Adam and Fateman, Richard",
  title = {{Branch Cuts in Computer Algebra}},
  year = "1994",
  booktitle = "Proc. ISSAC 1994",
  series = "ISSAC 94",
  link = "\url{http://www.cs.berkeley.edu/~fateman/papers/ding.ps}",
  abstract = 
    "Many standard functions, such as the logarithms and square root
    functions, cannot be defined continuously on the complex
    plane. Mistaken assumptions about the properties of these functions
    lead computer algebra systems into various conundrums. We discuss how
    they can manipulate such functions in a useful fashion.",
  paper = "Ding94.pdf",
  keywords = "axiomref"
}

@InProceedings{Dool98,
  author = "Dooley, Samuel S.",
  title = {{Coordinating mathematical content and presentation markup in
           interactive mathematical documents}},
  booktitle = "Proc. ISSAC 1998",
  series = "ISSAC 98",
  year = "1998",
  publisher = "ACM Press",
  location = "Rostock, Germany",
  pages = "13-15",
  abstract =
    "This paper presents a method for representing mathematical content
    and presentation markup in interactive mathematical documents that
    treats each view of the information on a separate and equal
    footing. By providing extensible, overridable, default mappings from
    content to presentation in a way that supports efficient mappings back
    from the presentation to the underlying content, a user interface for
    an interactive textbook has been implemented where the user interacts
    with high-quality presentation markup that supports user operations
    defined in terms of the mathematical content. In addition, the user
    interface can be insulated from content-specific information, while
    still being enabled to transfer that information to other programs for
    computation. This method has been employed to embed interactive
    mathematical content into the IBM techexplorer Interactive Textbook
    for Linear Algebra. The issues involved in the implementation of the
    interactive textbook also shed some light on the problems faced by the
    MathML working group in representing both presentation and content for
    mathematics for interactive web documents.",
  keywords = "axiomref"
}

@article{Reis06,
  author = "Dos Reis, Gabriel and Stroustrup, Bjarne",
  title = {{Specifying C++ Concepts}},
  journal = "POPL",
  publisher = "ACM",
  year = "2006",
  link = "\url{http://www.stroustrup.com/popl06.pdf}",
  abstract = 
    "C++ templates are key to the design of current successful mainstream
    libraries and systems. They are the basis of programming techniques in
    diverse areas ranging from conventional general-purpose programming to
    software for safety-critical embedded systems. Current work on improving
    templates focuses on the notion of {\sl concepts} (a type system for
    templates), which promises significantly improved error diagnostics and
    increased expressive power such as concept-based overloading and function
    template partial specialization. This paper presents C++ templates with
    an emphasis on problems related to separate compilation. We consider the
    problem of how to express concepts in a precise way that is simple enough
    to be usable by ordinary programmers. In doing so, we expose a few
    weaknesses of the current specification of the C++ standard library and
    suggest a far more precise and complete specification. We also present
    a systematic way of translating our proposed concept definitions, based
    on use-patterns rather than function signatures, into constraint sets
    that can serve as convenient basis for concept checking in a compiler.",
  paper = "Reis06.pdf",
  keywords = "axiomref"
}

@article{Reis12,
  author = "Dos Reis, Gabriel",
  title = {{A System for Axiomatic Programming}},
  journal = "Proc. Conf. on Intelligent Computer Mathematics",
  publisher = "Springer",
  year = "2012",
  link = "\url{http://www.axiomatics.org/~gdr/liz/cicm-2012.pdf}",
  abstract = "
    We present the design and implementation of a system for axiomatic
    programming, and its application to mathematical software
    construction. Key novelties include a direct support for user-defined
    axioms establishing local equality between types, and overload
    resolution based on equational theories and user-defined local
    axioms. We illustrate uses of axioms, and their organization into
    concepts, in structured generic programming as practiced in
    computational mathematical systems.",
  paper = "Reis12.pdf",
  keywords = "axiomref"
}

@phdthesis{Doye97,
  author = "Doye, Nicolas James",
  title = {{Order Sorted Computer Algebra and Coercions}},
  school = "University of Bath",
  year = "1997",
  abstract = 
    "Computer algebra systems are large collections of routines for solving
    mathematical problems algorithmically, efficiently and above all,
    symbolically. The more advanced and rigorous computer algebra systems
    (for example, Axiom) use the concept of strong types based on
    order-sorted algebra and category theory to ensure that operations are
    only applied to expressions when they ``make sense''.

    In cases where Axiom uses notions which are not covered by current
    mathematics we shall present new mathematics which will allow us to
    prove that all such cases are reducible to cases covered by the
    current theory. On the other hand, we shall also point out all the
    cases where Axiom deviates undesirably from the mathematical ideal.
    Furthermore we shall propose solutions to these deviations.

    Strongly typed systems (especially of mathematics) become unusable
    unless the system can change the type in a way a user expects. We wish
    any change expected by a user to be automated, ``natural'', and
    unique. ``Coercions'' are normally viewed as ``natural type changing
    maps''. This thesis shall rigorously define the word ``coercion'' in
    the context of computer algebra systems.

    We shall list some assumptions so that we may prove new results so
    that all coercions are unique. This concept is called ``coherence''.

    We shall give an algorithm for automatically creating all coercions in
    type system which adheres to a set of assumptions. We shall prove that
    this is an algorithm and that it always returns a coercion when one
    exists. Finally, we present a demonstration implementation of this
    automated coerion algorithm in Axiom.",
  paper = "Doye97.pdf",
  keywords = "axiomref, printed"
}

@inproceedings{Doye99,
  author = "Doye, Nicolas James",
  title = {{Automated coercion for Axiom}},
  booktitle = "Proc. ISSAC 1999",
  pages = "229-235",
  year = "1999",
  isbn = "1-58113-073-2",
  link = "\url{http://www.acm.org/citation.cfm?id=309944}",
  paper = "Doye99.pdf",
  keywords = "axiomref, printed",
  beebe = "Doye:1999:ACA"
}

@InProceedings{Domi01,
  author = {Dom\'inguez, C\'esar; Rubio, Julio},
  title = {{Modeling Inheritance as Coercion in a Symbolic Computation 
           System}},
  booktitle = "Proc. ISSAC 2001",
  series = "ISSAC 2001",
  year = "2001",
  abstract = "
    In this paper the analysis of the data structures used in a symbolic
    computation system, called Kenzo, is undertaken. We deal with the
    specification of the inheritance relationship since Kenzo is an
    object-oriented system, written in CLOS, the Common Lisp Object
    System. We focus on a particular case, namely the relationship between
    simplicial sets and chain complexes, showing how the order-sorted
    algebraic specifications formalisms can be adapted, through the
    ``inheritance as coercion'' metaphor, in order to model this Kenzo
    fragment.",
  paper = "Domi01.pdf",
  keywords = "axiomref"
}

@InProceedings{Drag10,
  author = "Dragan, Laurentiu and Watt, Stephen",
  title = {{Type Specialization in Aldor}},
  booktitle = "Computer algebra in scientific computing",
  series = "CASC 2010",
  year = "2010",
  location = "Tsakhadzor, Armenia",
  pages = "73-84",
  link = "\url{http://www.csd.uwo.ca/~watt/pub/reprints/2010-casc-specdom.pdf}",
  abstract =
    "Computer algebra in scientific computation squarely faces the dilemma
    of natural mathematical expression versus efficiency. While
    higher-order programming constructs and parametric polymorphism
    provide a natural and expressive language for mathematical
    abstractions, they can come at a considerable cost. We investigate how
    deeply nested type constructions may be optimized to achieve
    performance similar to that of hand-tuned code written in lower-level
    languages.",
  paper = "Drag10.pdf",
  keywords = "axiomref"
}

@misc{Duns99b,
  author = "Dunstan, Martin",
  title = {{An Introduction to Aldor and its Type System}},
  year = "1999",
  link = "\url{http://www.aldor.org/docs/reports/cfc99/aldor-cfc99.pdf}",
  comment = "slides",
  paper = "Duns99b.pdf"
}

@misc{Dupe95,
  author = "Dupee, Brian J. and Davenport, James H.",
  title = {{Using Computer Algebra to Choose and Apply Numerical Routines}},
  year = "1995",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.5645}",
  algebra =
   "\newline\refto{domain D01AJFA d01ajfAnnaType}
    \newline\refto{domain D01AKFA d01akfAnnaType}
    \newline\refto{domain D01ALFA d01alfAnnaType}
    \newline\refto{domain D01AMFA d01amfAnnaType}
    \newline\refto{domain D01ANFA d01anfAnnaType}
    \newline\refto{domain D01APFA d01apfAnnaType}
    \newline\refto{domain D01AQFA d01aqfAnnaType}
    \newline\refto{domain D01ASFA d01asfAnnaType}
    \newline\refto{domain D01FCFA d01fcfAnnaType}
    \newline\refto{domain D01GBFA d01gbfAnnaType}
    \newline\refto{domain D01TRNS d01TransformFunctionType}
    \newline\refto{domain D02BBFA d02bbfAnnaType}
    \newline\refto{domain D02BHFA d02bhfAnnaType}
    \newline\refto{domain D02CJFA d02cjfAnnaType}
    \newline\refto{domain D02EJFA d02ejfAnnaType}
    \newline\refto{domain D03EEFA d03eefAnnaType}
    \newline\refto{domain D03FAFA d03fafAnnaType}
    \newline\refto{domain E04DGFA e04dgfAnnaType}
    \newline\refto{domain E04FDFA e04fdfAnnaType}
    \newline\refto{domain E04GCFA e04gcfAnnaType}
    \newline\refto{domain E04JAFA e04jafAnnaType}
    \newline\refto{domain E04MBFA e04mbfAnnaType}
    \newline\refto{domain E04NAFA e04nafAnnaType}
    \newline\refto{domain E04UCFA e04ucfAnnaType}
    \newline\refto{domain NIPROB NumericalIntegrationProblem}
    \newline\refto{domain ODEPROB NumericalODEProblem}
    \newline\refto{domain OPTPROB NumericalOptimizationProblem}
    \newline\refto{domain PDEPROB NumericalPDEProblem}",
  abstract =
    "In applied mathematics, electronic and chemical engineering, the
    modelling process can produce a number of mathematical problems which
    require numerical solutions for which symbolic methods are either not
    possible or not obvious. With the plethora of numerical library
    routines for the solution of these problems often the numerical
    analyst has to answer the question {\sl Which routine to choose?} and
    {\sl How do I use it?}. Some analysis needs to be carried out before
    the appropriate routine can be identifed, i.e. {\sl How stiff is this
    ODE?} and {\sl Is this function continuous?}. It may well be the case
    that more than one routine is applicable to the problem. So the
    question may become {\sl Which is likely to be the best?}. Such a
    choice may be critical for both accuracy and efficiency.
    
    An expert system is thus required to make this choice based on the
    results of its own analysis of the problem, call the routine and act
    on the outcome. This may be to put the answer in a relevant form or
    react to an apparent failure of the chosen routine and thus choose and
    call an alternative. It should also have sufficient explanation
    mechanisms to inform on the choice of routine and the reasons for that
    choice. Much of this work can be achieved using computer algebra and
    symbolic algebra packages.
    
    This paper describes an expert system currently in prototype in terms
    of both its object-based structure and its computational agents. Some
    of these agents are described in detail, paying particular attention
    to the practical aspects of their algorithms and the use of computer
    algebra.
    
    The {\bf axiom2} Symbolic Algebra System is used as a user interface
    as well as the link to the NAG Foundation Library for the numerical
    routines and the inference mechanisms for the expert system.",
  paper = "Dupe95.pdf",
  keywords = "axiomref"
}

@inproceedings{Dupe99,
  author = "Dupee, Brian J. and Davenport, James H.",
  title = {{An Automatic Symbolic-Numeric Taylor Series ODE Solver}},
  booktitle = "Computer Algebra in Scientific Computing, CASC'99",
  isbn = "978-3-540-66047-7",
  pages = "37-50",
  year = "1999",
  link = "\url{http://people.eecs.berkeley.edu/~fateman/papers/casc99-34.pdf}",
  comment = "Contains FORTRAN Code of Taylor Series",
  algebra = "\newline\refto{package EXPRODE ExpressionSpaceODESolver}",
  abstract = 
    "One of the basic techniques in every mathematician's toolkit is the
    Taylor series representation of functions. It is of such fundamental
    importance and it is so well understood that its use is often a first
    choice in numerical analysis. This faith has not, unfortunately, been
    transferred to the design of computer algorithms.
    
    Approximation by use of Taylor series methods is inherently partly a
    symbolic process and partly numeric> This aspect has often, with
    reason, been regared as a major hindrance in algorithm design. Whilst
    attempts have been made in the past to build a consistent set of
    programs for the symbolic and numeric paradigms, these have been
    necessarily multi-stage processes.
    
    Using current technology it has at last become possible to integrate
    these two concepts and build an automatic adaptive symbolic-numeric
    algorithm within a uniform framework which can hide the internal
    workings behind a modern interface.",
  paper = "Dupe99.pdf",
  keywords = "axiomref"
}

@inproceedings{Dupe05,
  author = "Dupee, Brian J. and Davenport, James H.",
  title = {{An Intelligent Interface to Numerical Routines}},
  booktitle = "Design and Implementation of Symbolic Computation Systems",
  series = "Lecture Notes in Computer Science 1128",
  pages = "252-262",
  publisher = "Springer",
  year = "2005",
  abstract =
    "Links from Computer Algebra Systems to Numerical Libraries have been
    increasingly made available. However, the remain, like the numerical
    routines which comprise those libraries, difficult to use by a novice
    and there is little help in choosing the appropriate routine for any
    given problem, should there be a choice.
    
    Compuer Algebra Systems use generic names for each problem area. For
    examples, 'integrate' (or 'int') is used for integration of a
    function, whatever method the code may use. Numeric interfaces still
    use different names for each method together with a variety of extra
    parameters, some of which may be optional. Ideally, we should extend
    the generic name structure to cover numerical routines. This would
    then, necessarily, require algorithms for making an assessment of the
    efficacy of different methods where such a choice exists.
    
    This paper considers the link to the NAG Fortran Library from version
    2.0 of Axiom and shows how we can build on this to extend and simplify
    the interface using an expert system for choosing and using the
    numerical routines."
}

@article{Duva92,
  author = "Duval, Anne and Loday-Richaud, Michele",
  title = {{Kovacic's Algorithm and Its Application to Some Families 
           of Special Functions}},
  journal = "Applicable Algebra in Engineering, Communication, and Computing",
  series = "AAECC 3",
  pages = "211-246",
  year = "1992",
  publisher = "Springer-Verlag",
  abstract =
    "We apply the Kovacic algorithm to some families of special functions,
    mainly the hypergeometric one and that of Heun, in order to discuss
    the existence of closed-form solutions. We begin by giving a slightly
    modified version of the kovacic algorithm and a sketch proof.",
  keywords = "axiomref"
}

@inproceedings{Duva92a,
  author = "Duval, Dominique and Jung, F.",
  title = {{Examples of problem solving using computer algebra}},
  booktitle = "Programming environments for high-level scientific problem
               solving",
  series = "IFIP Transactions",
  editor = "Gaffney, Patrick W. and Houstis, Elias N.",
  publisher = "North-Holland",
  pages = "133-143",
  year = "1992",
  keywords = "axiomref",
  beebe = "Duval:1992:EPS"
}

@misc{Duva94e,
  author = "Duval, Dominique",
  title = {{Symbolic or algebraic computation?}},
  booktitle = "Publication du LACO",
  year = "1995",
  location = "Madrid Spain",
  comment = "NAG conference",
  keywords = "axiomref"
}

@article{Duva94d,
  author = "Duval, Dominique and Senechaud, Pascale",
  title = {{Sketches and parametrization}},
  journal = "Theor. Comput. Sci.",
  volume = "123",
  number = "1",
  pages = "117-130",
  year = "1994",
  abstract =
    "The paper deals with problems about conception and design of
    high-level computer algebra systems. Here we use a categorical
    approach given by the notion of sketches. Sketches allow to describe
    computation mechanisms in a syntactic way, well adapted to
    implementation.
    
    A computer algebra system must allow the manipulation of algebraic
    structures, in particular, the construction of new structures from
    known ones. In the paper we give a definition, at the sketch level, of
    parametrization of a structure by another one.",
  keywords = "axiomref"
}

@article{Duva95,
  author = "Duval, Dominique",
  title = {{Dynamic evaluation and algebraic closure in Axiom}},
  comment = "Evaluation dynamique et cl\^oture alg\'ebrique en Axiom",
  journal = "Journal of Pure and Applied Algebra",
  volume = "99",
  year = "1995",
  pages = "267--295",
  abstract =
    "Dynamic evaluation allows to compute with algebraic numbers without
    factorizing polynomials.  It also allows to manipulate parameters in a
    flexible and user-friendly way.  The aim of this paper is the
    following: Explain what is dynamic evaluation, with its basic notions
    of dynamic set and splitting.  Present its application to computations
    involving algebraic numbers, which amounts to defining the dynamic
    algebraic closure of a field.  Describe the Axiom program which
    implements this, and give a user guide for it (only this last point
    assumes some knowledge of Axiom) Dynamic evaluation is described here
    without any reference to sketch theory, however our presentation, less
    rigourous, may be considered as more accessible.",
  paper = "Duva95.pdf",
  keywords = "axiomref",
  beebe = "Duval:1995:DEA"
}

@mastersthesis{ElAl01,
  author = "El-Alfy, Hazem Mohamed",
  title = {{Computer Algebra and its Applications}},
  school = "Alexandria University, Department of Engineering, Mathematics, 
            and Physics",
  year = "2001",
  link = "\url{http://www.umiacs.umd.edu/~helalfy/pub/mscthesis01.pdf}",
  file = "ElAl01.pdf",
  abstract = 
    "In the recent decades, it has been more and more realized that
    computers are of enormous importance for numerical
    computations. However, these powerful general-purpose machines can
    also be used for transforming, combining and computing symbolic
    algebraic expressions. In other words, computers can not only deal
    with numbers, but also with abstract symbols representing mathematical
    formulas. This fact has been realized much later and is only now
    gaining acceptance among mathematicians and engineers. [Franz Winkler,
    1996].
    
    Computer Algebra is that field of computer science and mathematics,
    where computation is performed on symbols representing mathematical
    objects rather than their numeric values.
    
    This thesis attempts to present a definition of computer algebra by
    means of a survey of its main topics, together with its major
    application areas. The survey includes necessary algebraic basics and
    fundamental algorithms, essential in most computer algebra problems,
    together with some problems that rely heavily on these algorithms. The
    set of applications, presented from a range of fields of engineering
    and science, although very short, indicates the applied nature of
    computer algebra systems.
    
    A recent research area, central in most computer algebra software
    packages and in geometric modeling, is the implicitization
    problem. Curves and surfaces are naturally reperesented either
    parametrically or implicitly. Both forms are important and have their
    uses, but many design systems start from parametric
    representations. Implicitization is the process of converting curevs
    and surfaces from parametric form into implicit form.
    
    We have surveyed the problem of implicitization and investigated its
    currently available methods. Algorithms for such methods have been
    devised, implemented and tested for practical examples. In addition, a
    new method has been devised for curves for which a direct method is
    not available. The new method has been called {\sl near implicitization}
    since it relies on an approximation of the input problem. Several
    variants of the method try to compromise between accuracy and
    complexity of the designed algorithms.
    
    The problem of implicitization is an active topic where research is
    still taking place.  Examples of further research points are included
    in the conclusion",
  paper = "ElAl01.pdf",
  keywords = "axiomref"
}

@misc{Ency16,
  author = "Unknown",
  title = {{Encyclopedia of Mathematics}},
  link = "\url{https://www.encyclopediaofmath.org/index.php/Computer\_algebra\_package}",
  keywords = "axiomref"
}

@article{Fakl97,
  author = "Fakler, Winfried",
  title = {{On second order homogeneous linear differential equations with
           Liouvillian solutions}},
  journal = "Theor. Comput. Sci.",
  volume = "187",
  number = "1-2",
  pages = "27-48",
  year = "1997",
  abstract =
    "We determine all minimal polynomials for second order homogeneous
    linear differential equations with algebraic solutions decomposed into
    invariants and we show how easily one can recover the known conditions
    on differential Galois groups [J. Kovacic, J. Symb. Comput. 2, 3-43
    (1986; Zbl 0603.68035), M. F. Singer and F. Ulmer,
    J. Symb. Comput. 16, 9-36, 37-73 (1993; Zbl 0802.12004, Zbl
    0802.12005), F.Ulmer and J. A. Weil, J. Symb. Comput. 22, 179-200
    (1996; Zbl 0871.12008)] using invariant theory. Applying these
    conditions and the differential invariants of a differential equation
    we deduce an alternative method to the algorithms given in (loc. cit.)
    for computing Liouvillian solutions. For irreducible second order
    equations our method determines solutions by formulas in all but three
    cases.",
  paper = "Fakl97.pdf",
  keywords = "axiomref"
}

@article{Farm03,
  author = "Farmer, William M. and von Mohrenschildt, Martin",
  title = {{An overview of A Formal Framework For Managing Mathematics}},
  journal = "Ann. Math. Artif. Intell.",
  volume = "38",
  number = "1-3",
  pages = "165-191",
  year = "2003",
  link = "\url{https://www.emis.de/proceedings/MKM2001/farmer.ps}",
  abstract = 
    "Mathematics is a process of creating, exploring, and connecting
    mathematical models. This paper presents an overview of a formal
    framework for managing the mathematics process as well as the
    mathematical knowledge produced by the process. The central idea of
    the framework is the notion of a biform theory which is simultaneously
    an axiomatic theory and an algorithmic theory. Representing a
    collection of mathematical models, a biform theory provides a formal
    context for both deduction and computation. The framework includes
    facilities for deriving theorems via a mixture of deduction and
    computation, constructing sound deduction and computation rules, and
    developing networks of biform theories linked by interpretations. The
    framework is not tied to a specific underlying logic; indeed, it is
    intended to be used with several background logics
    simultaneously. Many of the ideas and mechanisms used in the framework
    are inspired by the IMPS Interactive Mathematical Proof System and the
    Axiom computer algebra system.",
  paper = "Farm03.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@inproceedings{Fate79,
  author = "Fateman, Richard J.",
  title = {{MACSYMA's General Simplifier: Philosophy and Operation}},
  booktitle = "Proc. Macsyma Users' Conference 1979",
  year = "1979",
  link = "\url{http://people.eecs.berkeley.edu/~fateman/papers/simplifier.txt}",
  abstract =
    "Ideally the transformations performed by MACSYMA's simplification
    program on algebraic expressions correspond to those simplifications
    desired by each user and each program.  Since it is impossible for a
    program to intuit all users' requirements simultaneously, explicit
    control of the simplifier is necessary to override default
    transformations.  A model of the simplification process is helpful in
    controlling this large and complex program.
    
    Having examined several algebraic simplification programs, it appears
    that to date no program has been written which combines a conceptually
    simple and useful view of simplification with a program nearly as
    powerful as MACSYMA's.  {note, 1979. not clear this would be different
    in 2001.  RJF} Rule-directed transformation schemes struggle to
    approach the power of the varied control structures in more usual
    program schemes [Fenichel, 68].  {note, Mathematica pushes rules
    further. RJF}
    
    It is our belief that a thorough grasp of the decision and data
    structures of the MACSYMA simplifier program itself is the most direct
    way of understanding its potential for algebraic expression
    transformation.  This is an unfortunate admission to have to make, but
    it appears to reflect the state of the art in dealing with
    formalizations of complex programs.  Simplification is a perplexing
    task.  Because of this, we feel it behooves the ``guardians of the
    simplifier'' to try to meet the concerned MACSYMA users part-way by
    documenting the program as it has evolved.  We hope this paper
    continues to grow to reflect a reasonably accurate, complete, and
    current description.
    
    Of course Lisp program details are available to the curious, but even
    for those without a working knowledge of the Lisp language (in which
    the simplifier is written) we expect this paper to be of some help in
    answering questions which arise perennially as to why MACSYMA deals
    with some particular class of expressions in some unanticipated
    fashion, or is inefficient in performing some set of transformations.
    Most often difficulties such as these are accounted for by implicit
    design decisions which are not evident from mere descriptions of what
    is done in the anticipated and usual cases.  We also hope that
    improvements or revisions of the simplifier will benefit from the more
    centralized treatment of issues given here.  We also provide
    additional commentary which reflects our current outlook on how
    simplification programs should be written, and what capabilities they
    should have.",
  paper = "Fate79.txt",
  keywords = "axiomref"
}

@inproceedings{Fate90,
  author = "Fateman, Richard J.",
  title = {{Advances and trends in the design and construction of algebraic 
           manipulation systems}},
  booktitle = "Proc. ISSAC 1990",
  publisher = "ACM",
  pages = "60-67",
  isbn = "0-89791-401-5",
  year = "1990",
  link = "\url{http://people.eecs.berkeley.edu/~fateman/papers/advances.pdf}",
  abstract =
    "We compare and contrast several techniques for the implementation of
    components of an algebraic manipulation system. On one hand is the
    mathematical-algebraic approach which chaaracterizes (for example)
    IBM's Axiom. On the other hand is the more {\sl ad hoc} approach which
    characterizes many other popular systems (for example, Macsyma,
    Reduce, Maple, and Mathematica). While the algebraic approach has
    generally positive results, careful examination suggests that there
    are significant remaining problems, expecially in the representation
    and manipulation of analytical, as opposed to algebraic,
    mathematics. We describe some of these problems and some general
    approaches for solutions.",
  paper = "Fate90.pdf",
  keywords = "axiomref",
  beebe = "Fateman:1990:ATD"
}

@misc{Fate94,
  author = "Fateman, Richard J.",
  title = {{On the Design and Construction of Algebraic Manipulation Systems}},
  link = "\url{http://www.cs.berkeley.edu/~fateman/papers/asmerev94.ps}",
  abstract =
    "We compare and contrast several techniques for the implementation of
    components of an algebraic manipulation system. On one hand is the
    mathematical-algebraic approach which characterizes (for example)
    IBM's Axiom. On the other hand is the more {\sl ad-hoc} approach which
    characterizes many other popular systems (for example, Macsyma,
    Reduce, Maple, and Mathematica). While the algebraic approach has
    generally positive results, careful examination suggests that there
    are significant remaining problems, especially in the representation
    and manipulation of analytical, as opposed to algebraic,
    mathematics. We describe some of these problems and some general
    approaches for solutions.",
  paper = "Fate94.pdf",
  keywords = "axiomref"
}

@InProceedings{Fate96,
  author = "Fateman, Richard J.",
  title = {{A Review of Symbolic Solvers}},
  booktitle = "Proc 1996 ISSAC",
  series = "ISSAC 96",
  year = "1996",
  pages = "86-94",
  link = "\url{http://http.cs.berkeley.edu/~fateman/papers/eval.ps}",
  abstract =
    "``Evaluation'' of expressions and programs in a computer algebra
    system is central to every system, but inevitably fails to provide
    complete satisfaction. Here we explain the conflicting requirements,
    describe some solutions from current systems, and propose alternatives
    that might be preferable sometimes. We give examples primarily from
    Axiom, Macsyma, Maple, Mathematica, with passing metion of a few other
    systems.",
  paper = "Fate96.pdf",
  keywords = "axiomref"
}

@inproceedings{Fate97,
  author = "Fateman, Richard J.",
  title = {{Network Servers for Symbolic Mathematics}},
  booktitle = "Proc. ISSAC 1997",
  pages = "249-256",
  year = "1997",
  isbn = "0-89791-875-4",
  link = "\url{http://http.cs.berkeley.edu/~fateman/papers/cas-serve.ps}",
  abstract =
    "We describe advantages to using network socket facilities for
    communication and distributed computing from the perspective of
    symbolic mathematics systems. For some applications, an easily
    constructed Lisp server model provides a flexible portal between
    computer algebra and other programs, and one need not use new
    languages or new systems or write new stand-alone web-specific cgi-bin
    applications. Such socket programs can use, if necessary, HTML as a
    common transport encoding, but more efficient means are possible. We
    show that, rather than distributing all information to each computer
    algebra user's system, it makes sense to consider networking for
    accessing tables of information maintained at one or a few
    sites. Finally, we mention some consequences of the economic value of
    computation.",
  paper = "Fate97.pdf",
  keywords = "axiomref"
}

@inproceedings{Fate99,
  author = "Fateman, Richard J.",
  title = {{Symbolic mathematics system evaluators}},
  booktitle = "Proc. ISSAC 1996",
  pages = "86-94",
  year = "1999",
  link = "\url{http://people.eecs.berkeley.edu/~fateman/papers/evalnew.pdf}",
  abstract =
    "``Evaluation'' of expressions and programs in a computer algebra
    system is central to every system, but inevitably fails to provide
    complete satisfaction. Here we explain the conflicting requirements,
    describe some solutions from current systems, and propose alternatives
    that might be preferable sometimes. We give examples primarily from
    Axiom, Macsyma, Maple, Mathematica, with passing metion of a few other
    systems.",
  paper = "Fate99.pdf",
  keywords = "axiomref"
}

@misc{Fate99a,
  author = "Fateman, Richard J. and Caspi, Eylon",
  title = {{Parsing TeX into Mathematics}},
  year = "1999",
  link = "\url{http://lib.org.by/\_djvu/\_Papers/Computer\_algebra/CAS%20systems/}",
  abstract =
    "Communication, storage, transmission, and searching of complex
    material has become increasingly important. Mathematical computing in
    a distributed environment is also becoming more plausible as libraries
    and computing facilities are connected with each other and with user
    facilites. TeX is a well-known mathematical typesetting language, and
    from the display perspective it might seem that it could be used for
    communication between computer systems as well as an intermediate form
    for the results of OCR (optical character recognition) of mathematical
    expressions. There are flaws in this reasoning, since exchanging
    mathematical informaiton requires a system to parse and semantically
    ``understand'' the TeX, even if it is ``ambiguous'' notationally. A
    program we developed can handle 43\% of 10,740 TeX formulas in a
    well-known table of integrals. We expect that a higher success rte can
    be achieved easily.",
  paper = "Fate99a.pdf",
  keywords = "axiomref"
}

@InProceedings{Fate00,
  author = "Fateman, Richard J.",
  title = {{Problem solving environments and symbolic computing}},
  booktitle = "Enabling technologies for computational science",
  publisher = "Kluwer Academic Publishers",
  year = "2000",
  pages = "91-102",
  link = "\url{http://people.eecs.berkeley.edu/~fateman/papers/pse-kluwer.pdf}",
  abstract =
    "What role should be played by symbolic mathematical computation
    facilities in scientific and engineering ``problem solving
    environments''? Drawing upon standard facilities such as numerical and
    graphical libraries, symbolic computation should be useful for: The
    creation and manipulation of mathematical models; The production of
    custom optimized numerical software; The solution of delicate classes
    of mathematical problems that require handling beyond that available
    in traditional machine-supported floating-point computation. Symbolic
    representation and manipulation can potentially play a central
    organizing role in PSEs since their more general object representation
    allows a program to deal with a wider range of computational
    issues. In particular numerical, graphical, and other processing can
    be viewed as special cases of symbolic manipulation with interactive
    symbolic computing providing both an organizing backbone and the
    communication ``glue'' among otherwise dissimilar components",
  paper = "Fate00.pdf",
  keywords = "axiomref"
}

@misc{Fate91,
  author = "Fateman, Richard J.",
  title = {{A Review of Macsyma}},
  year = "1991",
  link = "\url{http://people.eecs.berkeley.edu/~fateman/papers/mma.pdf}",
  abstract =
    "The Mathematica computer system is reviewed from the perspective of
    its contributions to symbolic and algebraic computation, as well as
    its stated goals. Design and implementation issues are discussed.",
  paper = "Fate91.pdf",
  keywords = "axiomref"
}

@article{Fate01,
  author = "Fateman, Richard J.",
  title = {{A Review of Macsyma}},
  journal = "IEEE Trans. Knowl. Eng.",
  volume = "1",
  number = "1",
  year = "2001",
  link = "\url{http://people.eecs.berkeley.edu/~fateman/papers/mac82b.pdf}",
  abstract =
    "We review the successes and failures of the Macsyma algebraic 
    manipulation system from the point of view of one of the original
    contributors. We provide a retrospective examination of some of the
    controversial ideas that worked, and some that did not. We consider
    input/output, language semantics, data types, pattern matching,
    knowledge-adjunction, mathematical semantics, the user community,
    and software engineering. We also comment on the porting of this
    system to a variety of computing systems, and possible future
    directions for algebraic manipulation system-building.",
  paper = "Fate01.pdf",
  keywords = "axiomref"
}

@misc{Fate02,
  author = "Fateman, Richard J.",
  title = {{Comparing the speed of programs for sparse polynomial 
           multiplication}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/fastmult.pdf}",
  paper = "Fate02.pdf",
  keywords = "axiomref"
}

@misc{Fate05,
  author = "Fateman, Richard J.",
  title = {{An incremental approach to building a mathematical 
           expert out of software}},
  conference = "Axiom Computer Algebra Conference",
  location = "City College of New York, CAISS project",
  year = "2005",
  month = "April",
  day = "19",
  link = "\url{http://www.cs.berkeley.edu/~fateman/papers/axiom.pdf}",
  paper = "Fat05.pdf",
  keywords = "axiomref"
}

@misc{Fate05a,
  author = "Fateman, Richard J.",
  title = {{Haddock's eyes and computer algebra systems: Some essays}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/haddock.pdf}",
  year = "2005",
  abstract =
    "(From {\sl Through the Looking Glass} by Lewis Caroll)

    The White Knight proposes to comfort Alice by singing her a song:
  
    ``Is it very long?'' Alice asked, for she had heard a good deal of
    poetry that day.

    ``It's long,'' said the Knight, ``but it's very, very beautiful. 
    Everybody that hears me sing it--either it brings tears into the
    eyes, or else --

    ``Or else what?'' said Alice, for the Knight had made a sudden pause.

    ``Or else it doesn't, you know. The name of the song is called
    `Haddock's Eyes'.''

    ``Oh, that' the name of the song, is it?'' Alice said, trying to feel
    interested.

    ``No, you don't understand,'' the Knight said, looking a little vexed.

    ``That's what the name is called. The name really is 'The Aged Aged Man'''

    ``Then I ought to have said 'That's what the song is called?'' Alice
    corrected herself.

    ``No, you oughtn't: that's quite another thing! The song is called
    `Ways and Means': but that's only what it's called, you know!''

    ``Well, what is the song, then?'' said Alice, who was by this time
    completely bewildered.

    ``I was coming to that,'' the Knight said. ``The song really is
    `A-sitting on a Gate': and the tune's my own invention.''",
  paper = "Fate05a.pdf"
}

@misc{Fate06a,
  author = "Fateman, Richard J.",
  title = {{Comments on Extending Macsyma with New Data Types}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/addformat.pdf}",
  abstract = 
    "Any design for a computer algebra system (CAS) naturally includes a
    set of data layouts for symbolic or mathematical algebraic expressions
    intended for use by built-in or user-written programs. The CAS cannot
    build in all plausible data designs, but supports those of most
    interest to the programmers. In such a situation it is almost
    inevitable that some new data encoding idea will come to mind and with
    it an interest in adding additional data forms. The motivation may be
    for compact representation, or efficient (fast) manipulation, or for
    other reasons such as interchange with other programs. Most CAS
    therefore include at least one way to extend the base set of
    operations. We comment on the kinds of extensions possible, using
    Macsyma as an example CAS. The particular interest in Macsyma and its
    open-source soruceforge variant ``Maxima'' is that a substantial group
    of very-losely coupled independent researchers are approaching this
    problem and may benefit from some guidance. Some the observations
    apply to other CAS, even though they are not open-source.",
  paper = "Fate06a.pdf",
  keywords = "axiomref"
}

@misc{Fate06b,
  author = "Fateman, Richard J.",
  title = {{Building Algebra Systems by Overloading Lisp: Automatic
           Differentiation}},
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/overload-AD.pdf}",
  year = "2006",
  abstract =
    "In an earlier paper we began a discussion of the use of overloaded
    languages for support of computer algebra systems. Here we extend that
    notion to provide a more detailed approach to Automatic
    Differentiation or Algorithm Differention (AD).
    
    This paper makes three points. 1. It is extremely easy to do express
    AD by overloading in Common Lisp. 2. While the resulting program is
    not the most efficient approach in terms of run-time, it is quite
    small and very general. It also interacts nicely with some other kinds
    of generic arithmetic. 3. A more efficient AD compile-time program
    generation approach is described as well.",
  paper = "Fate06b.pdf",
  keywords = "axiomref"
}

@article{Faug94,
 author = "Faug\'ere, J.C. and Gianni, P. and Lazard, D. and Mora, T.",
 title = {{Efficient Computation of Zero-dimensional Grobner Bases by
          Change of Ordering}},
 journal = "J. Symbolic Computation",
 issue_date = "February 1994",
 volume = "11",
 number = "2",
 month = "February",
 year = "1984",
 link = "\url{http://www-polsys.lip6.fr/~jcf/Papers/FGLM.pdf}",
 publisher = "Academic Press Limited",
 algebra = "\newline\refto{package LEXTRIPK LexTriangularPackage}",
 abstract = "
    We present an efficient algorithm for the transformation of a
    Grobner basis of a zero-dimensional ideal with respect to any given
    ordering into a Grobner basis with respect to any other
    ordering. This algorithm is polynomial in the degree of the idea. In
    particular, the lexicographical Grobner basis can be obtained by
    applying this algorithm after a total degree Grobner basis
    computation: it is usually much faster to compute the basis this way
    than with a direct application of Buchberger's algorithm.",
 paper = "Faug94.pdf",
 keywords = "axiomref"
}

@misc{Faurxx,
  author = {Davenport, James and Faure, Christ\'ele},
  title = {{Parameters in Computer Algebra}},
  abstract =
    "One of the main strengths of computer algebra is being able to solve
    a family of problems with one computation. In order to express not
    only one problem but a family of problems, one introduces some symbols
    which are in fact the parameters common to all the problems of the family.

    The user must be able to understand in which way these parameters
    affect the result when he looks at the answer. This is not the case in
    most current Computer Algebra Systems we know because the form of the
    answer is never explicitly conditioned by the values of the
    parameters. We have introduced multi-valued expressions called 
    {\sl conditional expressions}, in which each potential value is associated
    with a condition on some parameters. This is used, in particular, to
    capture the situation in integration, where the form of the answer can
    depend on whether certain quantities are positive, negative, or zero.",
  keywords = "axiomref, provisos"
}

@InProceedings{Fitc93,
  author = "Fitch (ed), John P.",
  title = {{Design and Implementation of Symbolic Computation Systems}},
  year = "1992",
  booktitle = "Int. Symp. DISCO '92 Proceedings",
  series = "DISCO 92",
  publisher = "Springer-Verlag, Berlin",
  isbn = "0-387-57272-4",
  paper = "Fitc93.tex",
  keywords = "axiomref"
}

@book{Flei94,
  author = "Fleischer, J. and Grabmeier, J. and Hehl, F.W. and 
            Kuchlin, W. (eds)",
  title = {{Proc. Conf. Computer Algebra in Science and Engineering}},
  booktitle = "Computer Algebra in Science and Engineering",
  year = "1994",
  location = "Bielefeld, Germany",
  publisher = "World Scientific, River Edge, NJ",
  abstract =
    "Systems and tools of computer algebra (Like AXIOM, Derive, FORM,
    Mathematica, Maple, Mupad, REDUCE, Macsyma…) let us manipulate
    extremely complex algebraic formulae symbolically on a
    computer. Contrary to numerics these computations are exact and there
    is no loss of accuracy. After decades of research and development,
    these tools are now becoming as indispensable in Science and
    Engineering as traditional number crunching already is.
    
    The ZiF'94 workshop is amongst the first devoted specifically to
    applications of computer algebra (CA) in Science and Engineering. The
    book documents the state of the art in this area and serves as an
    important reference for future work."
}

@techreport{Fort85,
  author = "Fortenbacher, A. and Jenks, Richard and Lucks, Michael and
            Sutor, Robert and Trager, Barry and Watt, Stephen",
  title = {{An Overview of the Scratchpad II Language and System}},
  institution = "IBM",
  year = "1985",
  type = "Research Report",
  publisher = "IBM Research Computer Algebra Group",
  keywords = "axiomref"
}

@inproceedings{Fort90,
  author = "Fortenbacher, Albrecht",
  title = {{Efficient type inference and coercion in computer algebra}},
  booktitle = "Design and Implementation of Symbolic Computation Systems",
  series = "Lecture Notes in Computer Science 429",
  pages = "56-60",
  isbn = "0-387-52531-9",
  year = "1990",
  abstract =
    "Computer algebra systesm of the new generation, like SCRATCHPAD, are
    characterized by a very rich type concept, which models the
    relationship between mathematical domains of computation. To use these
    systems interactively, however, the user should be freed of type
    information. A type inference mechanism determines the appropriate
    function to call. All known models which allow to define a semantics
    for type inference cannot express the rich ``mathematical'' type
    structure, so presently type inference is done heuristically. The
    following paper defines a semantics for a subproblem therof, namely
    coercion, which is based on rewrite rules. From this definition, an
    efficient coercion algorithm for SCRATCHPAD is constructed using graph
    techniques.",
  keywords = "axiomref",
  beebe = "Fortenbacher:1990:ETI"
}

@article{Fort05,
  author = "Fortuna, E. and Gianni, P. and Luminati, D. and Parenti, P.",
  title = {{The adjacency graph of a real algebraic surface}},
  journal = "Appl. Algebra Eng. Commun. Comput.",
  volume = "16",
  number = "5",
  pages = "271-292",
  year = "2005",
  link = "\url{http://eprints.biblio.unitn.it/788/1/UTM671.pdf}",
  abstract =
    "The paper deals with the question of recognizing the mutual positions
    of the connected components of a non-singular real projective surface
    $S$ in the real projective 3-space. We present an algorithm that
    answers this question through the computation of the adjacency graph
    of the surface; it also allows to decide whether each connected
    component is contractible or not. The algorithm, combined with a
    previous one returning as an output the topology of the surface,
    computes a set of data invariant up to ambient-homeomorphism which,
    though not sufficient to determine the pair $(\mathbb{R}\mathbb{P}^3,S)$, 
    give information about the nature of the surface as an embedded object.",
  paper = "Fort05.pdf",
  keywords = "axiomref"
}

@techreport{Fouc90,
  author = "Fouche, Francois",
  title = {{Une implantation de l'algorithme de Kovacic en Scratchpad}},
  type = "technical report",
  number = "ULP-IRMA-447-P-254",
  year = "1990",
  institution = {Institut de Recherche Math{\'{e}}matique Avanc{\'{e}}e''},
  location = "Strasbourg, France",
  keywords = "axiomref",
  beebe = "Fouche:1990:ILK"
}

@article{Frit94,
  author = "Fritzson, D. and Fritzson, P. and Viklund, L. and Herber, J.",
  title = {{Object-oriented mathematical modelling - applied to machine
           elements}},
  journal = "Comput. Struct.",
  volume = "51",
  number = "3",
  pages = "241-253",
  year = "1994",
  abstract =
    "Machine element analysis has a goal of describing function and other
    aspects of machine elements in a theoretical form. This paper shows
    how ideas from object-oriented modelling can be applied to machine
    elment analysis. The models thus obtained are both easier to
    understand, better structured, and allow a higher degree of re-use
    than conventional models. An object-oriented model description is
    natural and suitable for machine element analysis. As a realistic
    example an equational model of rolling bearings is presented. The
    structure of the model is general, and applies to many types of
    rolling bearings. The model and one solution require approximately
    200+200 equations. The model is extensible, e.g. simple submodels of
    detailed properties can be made more complex without altering the
    overall structure. The example model has been implemented in a
    language of our own design. ObjectMath (Object-oriented Mathematical
    language for scientific computing). Using ObjectMath, it is possible
    to model classes of equation objects, to support multiple and single
    inheritance of equations, to support composition of equations, and to
    solve systems of equations. Algebraic transformations can conveniently
    be done since ObjectMath models are translated into the Mathematica
    computer algebra language. When necessary, equations can be
    transformed int C++ code for efficient numerical solution. The re-use
    of equations through inheritance reduced the size of the model by a
    factor of two, compared to a direct representation of the model in the
    Mathematica computer algebra language.",
  paper = "Frit94.pdf",
  keywords = "axiomref"
}

@article{Gall90,
  author = "Galligo, A. and Grimm, J. and Pottier, L.",
  title = {{The Design of SISYPHE: A System for doing Symbolic and
           Algebraic Computations}},
  journal = "LNCS",
  volume = "429",
  pages = "30-39",
  year = "1990",
  keywords = "axiomref"
}

@techreport{Gall92,
  author = "Gallopoulos, Stratis and Houstis, Elias and Rice, John",
  title = {{Future Research Directions in Problem Solving Environments for
           Computational Science}},
  institution = "Purdue University",
  year = "1992",
  type = "technical report",
  number = "CSD-TR-92-032",
  link = "\url{http://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=1953\&context=cstech}",
  abstract =
    "During the early 19605 some were visualizing that computers could
    provide a powerful problem solving environment (PSE) which would
    interact with scientists on their own terms.  By the mid 1960s there
    were many attempts underway to create these PSEs, but the early
    1970s almost all of these attempts had been abandoned, because the
    technological infrastructure could not yet support PSEs in
    computational science.  The dream of the 1960s can be the reality of
    the 1990s; high performance computers combined with better
    understanding of computing and computational science have put PSEs
    well within our reach.",
  paper = "Gall92.pdf",
  keywords = "axiomref"
}

@book{Ganz00,
  author = "Ganzha, Victor G. and Vorozhtsov, Evgenii V. and Wester, Michael",
  title = {{An Assessment of the Efficiency of Computer Algebra Systems in 
           the Solution of Scientific Computing Problems}},
  booktitle = "Computer Algebra in Scientific Computing",
  year = "2000",
  isbn = "978-3-540-41040-9",
  publisher = "Springer",
  pages = "145-166",
  abstract =
    "Computer algebra systems (CASs) have become an important tool for the
    solution of scientific computing problems. With the increasing number
    of general purpose CASs, there is now a need for an assessment of the
    efficiency of these systems. We discuss some peculiarities associated
    with the analysis of CPU time efficiency in CASs, and then present
    results from three specific systems (Maple Vr5, Mathematics 4.0 and
    MuPAD 1.4) on a sample of intermediate size problems. These results
    show that Maple Vr5 is generally the speediest on our
    examples. Finally, we formulate some requirements for developing a
    comprehensive suite for analyzing the efficiency of CASs.",
  keywords = "axiomref"
}

@inproceedings{Gebu86,
  author = "Gebauer, R{\"u}diger and M{\"o}ller, H. Michael",
  title = {{Buchberger's algorithm and staggered linear bases}},
  booktitle = "Proc. 1986 Symposium on Symbolic and Algebraic Computation",
  series = "SYMSAC '86",
  year = "1986",
  pages = "218-221",
  publisher = "ACM Press",
  isbn = "0-89791-199-7",
  doi = "http://dx.doi.org/10.1145/32439.32482",
  keywords = "axiomref",
  beebe = "Gebauer:1986:BAS"
}

@article{Geba88,
  author = "Gebauer, Rudiger and Moller, H. Michael",
  title = {{On an installation of Buchberger's algorithm}},
  journal = "Journal of Symbolic Computation", 
  volume = "6",
  number = "2-3",
  pages = "275-286",
  year = "1988",
  abstract = 
    "Buchberger's algorithm calculates Groebner bases of polynomial
    ideals. Its efficiency depends strongly on practical criteria for
    detecting superfluous reductions. Buchberger recommends two
    criteria. The more important one is interpreted in this paper as a
    criterion for detecting redundant elements in a basis of a module of
    syzygies. We present a method for obtaining a reduced, nearly minimal
    basis of that module. The simple procedure for detecting (redundant
    syzygies and )superfluous reductions is incorporated now in our
    installation of Buchberger's algorithm in SCRATCHPAD II and REDUCE
    3.3. The paper concludes with statistics stressing the good
    computational properties of these installations.",
  paper = "GM88.pdf",
  keywords = "axiomref",
  beebe = "Gebauer:1988:IBA"
}

@book{Gedd92,
  author = "Geddes, Keith and Czapor, O. and Stephen R. and Labahn, George",
  title = {{Algorithms For Computer Algebra}},
  year = "1992",
  publisher = "Kluwer Academic Publishers",
  isbn = "0-7923-9259-0",
  month = "September",
  abstract =
    "Computer Algebra (CA) is the name given to the discipline of
    algebraic, rather than numerical, computation. There are a number of
    computer programs – Computer Algebra Systems (CASs) – available for
    doing this. The most widely used general-purpose systems that are
    currently available commercially are Axiom, Derive, Macsyma, Maple,
    Mathematica and REDUCE. The discipline of computer algebra began in
    the early 1960s and the first version of REDUCE appeared in 1968.
    
    A large class of mathematical problems can be solved by using a CAS
    purely interactively, guided only by the user documentation. However,
    sophisticated use requires an understanding of the considerable amount
    of theory behind computer algebra, which in itself is an interesting
    area of constructive mathematics. For example, most systems provide
    some kind of programming language that allows the user to expand or
    modify the capabilities of the system.
    
    This book is probably the most general introduction to the theory of
    computer algebra that is written as a textbook that develops the
    subject through a smooth progression of topics. It describes not only
    the algorithms but also the mathematics that underlies them. The book
    provides an excellent starting point for the reader new to the
    subject, and would make an excellent text for a postgraduate or
    advanced undergraduate course. It is probably desirable for the reader
    to have some background in abstract algebra, algorithms and
    programming at about second-year undergraduate level.
    
    The book introduces the necessary mathematical background as it is
    required for the algorithms. The authors have avoided the temptation
    to pursue mathematics for its own sake, and it is all sharply focused
    on the task of performing algebraic computation. The algorithms are
    presented in a pseudo-language that resembles a cross between Maple
    and C. They provide a good basis for actual implementations although
    quite a lot of work would still be required in most cases. There are
    no code examples in any actual programming language except in the
    introduction.
    
    The authors are all associated with the group that began the
    development of Maple. Hence, the book reflects the approach taken by
    Maple, but the majority of the discussion is completely independent of
    any actual system. The authors’ experience in implementing a practical
    CAS comes across clearly.
    
    The book focuses on the core of computer algebra. The first chapter
    introduces the general concept and provides a very nice historical
    survey. The next three chapters discuss the fundamental topics – data
    structures, representations and the basic arithmetic of integers,
    rational numbers, multivariate polynomials and rational functions – on
    which the rest of the book is built.
    
    A major technique in CA involves projection onto one or more
    homomorphic images, for which the ground ring is usually chosen to be
    a finite field. The image solution is lifted back to the original
    problem domain by means of the Chinese Remainder Theorem in the case
    of multiple homomorphic images, or the Hensel (-adic or ideal-adic)
    construction in the case of a single image. The next two chapters are
    devoted to these techniques in a fairly general setting. The two
    subsequent chapters specialise them to GCD computation and
    factorisation for multivariate polynomials; the first of these
    chapters also discusses the important but difficult topic of
    subresultants.
    
    The next two chapters describe the use of fraction-free Gaussian
    elimination, resultants and Gröbner Bases for manipulation and exact
    solution of linear and nonlinear polynomial equations. The two final
    chapters describe ``classical'' algorithms and the more recent Risch
    algorithm for symbolic indefinite integration, and provide an
    introduction to differential algebra.
    
    The book does not consider more specialised problem areas such as
    symbolic summation, definite integration, differential equations,
    group theory or number theory. Nor does it consider more applied
    problem areas such as vectors, tensors, differential forms, special
    functions, geometry or statistics, even though Maple and other CASs
    provide facilities in all or many of these areas. It does not consider
    questions of CA programming language design, nor any of the important
    but non-algebraic facilities provided by current CASs such as their
    user interfaces, numerical and graphical facilities.
    
    This is a long book (nearly 600 pages); it is generally very well
    presented and the three authors have merged their contributions
    seamlessly. I noticed very few typographical errors, and none of any
    consequence. I have only two complaints about the book. The typeface
    is too small, particularly for the relatively large line spacing used,
    and it is much too expensive, particularly for a book that would
    otherwise be an excellent student text. I recommend it highly to
    anyone who can afford it.",
  keywords = "axiomref"
}

@inproceedings{Gian89,
  author = "Gianni, Patrizia and Mora, T.",
  title = {{Algebraic solution of systems of polynomial equations 
           using Groebner bases.}},
  booktitle = "Applied Algebra, Algebraic Algorithms and Error-Correcting
               Codes",
  series = "AAECC-5",
  pages = "247-257",
  year = "1989",
  isbn = "3-540-51082-6",
  abstract =
    "One of the most important applications of Buchberger's algorithm for
    Groebner basis computation is the solution of systems of polynomial
    equations (having finitely many roots), i.e. the computation of zeros
    of 0-dimensional polynomial ideals. It is based on a relation between
    Groebner bases w.r.t. a lexicographical ordering and elimination
    ideals, which was discovered by Trinks.

    Packages for isolation of real roots of systems of polynomial
    equations using Groebner basis computation are currently available in
    different computer algebra systems, including SAC-2, Reduce,
    Scratchpad II, Maple.
    
    In principle, Buchberger-Trinks algorithm should allow to compute
    solutions of such systems in the algebraic closure of the coefficient
    field $k$ (usually the rational numbers), in the sense that it is
    possible to represent explicitly a finite extension of $k$ containing
    all solutions and to express the roots in this field.
    
    However, this requires several factorisations of polynomials over a
    tower of algebraic extensions of $k$, which is usually very costly, so
    that the resulting algorithm is not very feasible and, as far as we
    know, no implementation is available.
    
    The results of [GT2] on primary decomposition of ideals include a
    thorough study on the structure of Groebner bases for 0-dimensional
    ideals; in particular, the paper shows, that after a ``generic''
    linear change of coordinates, the roots of a system of polynomial
    equations can be expressed in a simple extension of $k$. Therefore, in
    this case, no factorisation of polynomials over towers of algebraic
    extensions is needed.
    
    However performing a change of coordinates has the undesirable effects
    of introducing dense polynomials and of increasing the size of
    coefficients.
    
    The problem then arises of producing strategies to compute Groebner
    bases for (0-dimensional) ideals, which at least are able to control
    the influence of these side-effects: two such strategies are presented
    in this paper, together with the application to the present problem of
    an algorithm by Gianni that computes the radical of a 0-dimensional
    ideal after a ``generic'' change of coordinates.
    
    A different approach, based on her ``splitting algorithm'', to compute
    solutions of systems of polynomial equations without the need of
    polynomial factorisations has been proposed by D. Duval; also her
    algorithm should be simplified by a ``generic'' change of coordinates.
    
    The algorithms discussed in this paper are implemented in SCRATCHPAD II.
    
    In the first section we recall some well-known properties of Groebner
    bases and properties on the structure of Groebner bases of
    zero-dimensional ideals from [GT2]; in the second section we recall
    the Groebner basis algorithm for solving systems of algebraic
    equations.
    
    The original results are contained in Sections 3 to 5; in Section 3 we
    take advantage of the obvious fact that density can be controlled by
    performing ``small'' changes of coordinates: we show that such
    approach is possible during a Groebner basis computation, in such a
    way that computations done before a change of coordinates are valid
    also after it; in Section 4 we propose a ``linear algebra'' approach
    to obtain the Groebner basis w.r.t the lexicographical ordering from
    the one w.r.t the total-degree ordering; in Section 5, we present a
    zero-dimensional radical algorithm and show how to apply it to the
    present problem.",
  paper = "Gian89.pdf",
  keywords = "axiomref",
  beebe = "Gianni:1989:ASS"
}

@inproceedings{Gilx92,
  author = "Gil, Isabelle",
  title = {{Computation of the Jordan canonical form of a square matrix 
           (using the Axiom programming language)}},
  booktitle = "Proc ISSAC 1992",
  series = "ISSAC '92",
  year = "1992",
  publisher = "ACM",
  pages = "138-145",
  isbn = "0-89791-489-9 (soft cover), 0-89791-490-2 (hard cover)",
  abstract =
    "Presents an algorithm for computing: the Jordan form of a square
    matrix with coefficients in a field K using the computer algebra
    system Axiom. This system presents the advantage of allowing generic
    programming. That is to say, the algorithm can first be implemented
    for matrices with rational coefficients and then generalized to
    matrices with coefficients in any field.  Therefore the author
    presents the general method which is essentially based on the use of
    the Frobenius form of a matrix in order to compute its Jordan form;
    and then restricts attention to matrices with rational
    coefficients. On the one hand the author streamlines the algorithm
    froben which computes the Frobenius form of a matrix, and on the other
    she examines in some detail the transformation from the Frobenius form
    to the Jordan form, and gives the so called algorithm Jordform. The
    author studies in particular, the complexity of this algorithm and
    proves that it is polynomial when the coefficients of the matrix are
    rational. Finally the author gives some experiments and a conclusion.",
  keywords = "axiomref",
  beebe = "Gil:1992:CJC"
}

@phdthesis{Gome92,
  author = "Gomez-Dias, Teresa",
  title = {Quelques applications de l`\'evaluation dynamique},
  school = "L'Universite de Limoges",
  year = "1992",
  month = "March",
  paper = "Gome92.pdf"
}

@misc{Gome94,
  author = "Gomez-Diaz, Teresa",
  title = {{The Possible Solutions to the Control Problem}},
  year = "1994"
}

@article{Gome96,
  author = "Gomez-Diaz, Theresa",
  title = {{Examples of using dynamic constructible closure}},
  journal = "Math. Comput. Simul.",
  volume = "42",
  number = "4-6",
  pages = "375-383",
  year = "1996",
  abstract =
    "We present here some examples of using the ``Dynamic Constructible
    Closure'' program, which performs automatic case distinctions in
    computations involving parameters over a base field ``K''. This
    program is an application of the ``Dynamic Evaluation'' principle
    which generalizes tradional evaluation and was first used to deal with
    algebraic numbers.",
  keywords = "axiomref"
}

@misc{Gonn05,
  author = "Gonnet, Gaston and Haigh, Thomas",
  title = {{An Interview with Gaston Gonnet}},
  year = "2005",
  publisher = "SIAM",
  link = "\url{http://history.siam.org/pdfs2/Gonnet_final.pdf}",
  abstract =
    "Born in Uruguay, Gonnet was first exposed to computers while working
    for IBM in Montevideo as a young man. This led him to a position at
    the university computer center, and in turn to an undergraduate degree
    in computer science in 1973. In 1974, following a military coup, he
    left for graduate studies in computer science at the University of
    Waterloo. Gonnet earned an M.Sc. and a Ph.D. in just two and a half
    years, writing a thesis on the analysis of search algorithms under the
    supervision of Alan George. After one year teaching in Rio de Janeiro
    he returned to Waterloo, as a faculty member.
    
    In 1980, Gonnet began work with a group including Morven Gentleman and
    Keith Geddes to produce an efficient interactive computer algebra
    system able to work well on smaller computers: Maple. Gonnet discusses
    in great detail the goals and organization of the Maple project, its
    technical characteristics, the Maple language and kernel, the Maple
    library, sources of funding, the contributions of the various team
    members, and the evolution of the system over time. He compares the
    resulting system to MACSYMA, Mathematica, Reduce, Scratchpad and other
    systems. Gonnet also examines the licensing and distribution of Maple
    and the project’s relations to its users. Maple was initially used for
    teaching purposes within the university, but soon found users in other
    institutions. From 1984, distribution was handled by Watcom, a company
    associated with the university, and 1988, Gonnet and Geddes created a
    new company, Waterloo Maple Software, Inc. to further commercialize
    Maple, which established itself as the leading commercial computer
    algebra system. However, during the mid-1990s the company ran into
    trouble and disagreements with his colleagues caused Gonnet to
    withdraw from managerial involvement. Since then, he feels that Maple
    has lost its battle with Mathematica. Gonnet also discusses Maple’s
    relation to Matlab and its creator, Cleve Moler.
    
    From 1984 onward with Frank Tompa, Tim Bray, and other Waterloo
    colleagues, Gonnet worked on the production of computer software to
    support the creation of the second edition of the Oxford English
    Dictionary. This led to the creation of another startup company, Open
    Text, producing software for the searching and indexing of textual
    information within large corporations. Gonnet explains his role in the
    firm, including his departure and his feeling that it made a strategic
    blunder by not exploiting its early lead in Internet search.
    
    Gonnet continued to work in a number of areas of computer science,
    including analysis of algorithms. In 1990, Gonnet moved from Waterloo
    to ETH in Switzerland. Among his projects since then have been Darwin,
    a bioinformatics system for the manipulation of genetic data, and
    leadership of the OpenMath project to produce a standard
    representation for mathematical objects. He has been involved in
    several further startup companies, including Aruna, a relational
    database company focused on business intelligence applications.",
  keywords = "axiomref",
  paper = "Gonn05.pdf"
}

@inproceedings{Good91,
  author = "Goodwin, B. M. and Buonopane, R. A. and Lee, A.",
  title = {{Using MathCAD in teaching material and energy balance concepts}},
  booktitle = "Challenges of a Changing World",
  comment = "Proc. 1991 Ann. Conf., Amer. Soc. for Engineering Education",
  pages = "345-349",
  year = "1991",
  abstract =
    "We show how PC-based applications software, specifically MathCAD, is
    used in the teaching of material and energy balance concepts. MathCAD
    is a microcomputer software package which acts as a mathematical
    scratchpad. It has proven to be a very useful instructional tool in
    introductory chemical engineering courses. MathCAD solutions to
    typical course problems are presented.",
  keywords = "axiomref",
  beebe = "Goodwin:1991:UMT"
}

@inproceedings{Good93,
  author = "Goodloe, A. and Loustaunau, Philippe",
  title = {{An abstract data type development of graded rings}},
  booktitle = "Design and Implementation of Symbolic Computation Systems",
  series = "Lecture Notes in Computer Science 721",
  pages = "193-202",
  isbn = "0-387-57272-4 (New York), 3-540-57272-4 (Berlin)",
  year = "1993",
  abstract =
    "Recently new computer algebra systems such as Scratchpad and Weyl have
    been developed with built in mechanisms for expressing abstract data types.
    These systems are object oriented in that they incorporate multiple
    inheritance and polymorphic types. Davenport and Trager have build much
    of the framework for basic commutative algebra in Scratchpad II 
    utilizing its rich set of abstraction mechanisms. Davenport and Trager
    concentrated on developing factorization algorithms on domains which
    were abstract data types.

    We are taking a similar approach to the development of algorithms for
    computing in graded rings. The purpose of this paper is to develop the
    tools required to compute with polynomials with coefficients in a graded
    ring $R$. We focus on graded rings $R$ which are polynomial rings graded
    by a monoid, and we allow partial orders on the monomials. The ideas
    presented here can be applied to more general graded rings $R$, such as
    associated graded rings to filtered rings, as long as certain computational
    ``requirements'' are satisfied",
  keywords = "axiomref",
  beebe = "Goodloe:1993:ADT"
}

@misc{Grab98,
  author = "Grabe, Hans-Gert",
  title = {{About the Polynomial System Solve Facility of Axiom, Macsyma, 
           Maple Mathematica, MuPAD, and Reduce}},
  link = "\url{https://www.informatik.uni-leipzig.de/~graebe/ComputerAlgebra/Publications/WesterBook.pdf}",
  abstract = 
    "We report on some experiences with the general purpose Computer
    Algebra Systems (CAS) Axiom, Macsyma, Maple, Mathematica, MuPAD, and
    Reduce solving systems of polynomial equations and the way they
    present their solutions. This snapshot (taken in the spring of 1996)
    of the current power of the different systems in a special area
    concentrates on both CPU-times and the quality of the output.",
  paper = "Grab98.pdf",
  keywords = "axiomref"
}

@InProceedings{Grab02,
  author = "Grabe, Hans-Gert",
  title = {{The SymbolicData Benchmark Problems Collection of Polynomial 
           Systems}},
  booktitle = "Workshop on Under- and Overdetermined Systems of Algebraic or
               Differential Equations",
  location = "Karlsruhe, Germany",
  pages = "57-76",
  year = "2002",
  link = "\url{http://symbolicdata.org/Papers/karlsruhe-02.pdf}",
  paper = "Grab02.pdf",
  keywords = "axiomref"
}

@misc{Grab06,
  author = "Grabe, Hans-Gert",
  title = {{The Groebner Factorizer and Polynomial System Solving}},
  year = "2006",
  report = "Special Semester on Groebner Bases",
  location = "Linz",
  link = "\url{https://www.ricam.oeaw.ac.at/specsem/srs/groeb/download/06\_02\_Solver.pdf}",
  abstract =
    "Let $S := k[x_1,\ldots, x_n]$ be the polynomial ring in the
    variables $x_1,\ldots,x_n$ over the field $k$ and 
    $B := \{f_1,\ldots,f_m\} \subset S$
    be a finite system of polynomials. Denote by $I(B)$ the
    ideal generated by these polynomials. One of the major tasks of
    constructive commutative algebra is the derivation of information
    about the structure of 
    \[V(B):=\{a \in K^n : \forall f \in B{\rm\ such\ that\ }f(a)=0\}\]
    the set of common zeroes of the system $B$ over an
    algebraically closed extension $K$ of $k$.  Splitting the system into
    smaller ones, solving them separately, and patching all solutions
    together is often a good guess for a quick solution of even highly
    nontrivial problems. This can be done by several techniques, e.g.,
    characteristic sets, resultants, the Groebner factorizer or some ad
    hoc methods. Of course, such a strategy makes sense only for problems
    that really will split, i.e., for reducible varieties of
    solutions. Surprisingly often, problems coming from 11real life''
    fulfill this condition.  
    
    Among the methods to split polynomial systems into smaller pieces
    probably the Groebner factorizer method attracted the most
    theoretical attention, see Czapor ([4, 5]), Davenport ([6]), Melenk, M
    ̈oller and Neun ([16, 17]) and Gr ̈abe ([13, 14]). General purpose
    Computer Algebra Systems (CAS) are well suited for such an approach,
    since they make available both a (more or less) well tuned
    implementation of the classical Groebner algorithm and an effective
    multivariate polynomial factorizer.
    
    Furthermore it turned out that the Groebner factorizer is not only a
    good heuristic approach for splitting, but its output is also usually
    a collection of almost prime components. Their description allows a
    much deeper understanding of the structure of the set of zeroes
    compared to the result of a sole Groebner basis computation.  
    
    Of course, for special purposes a general CAS as a multipurpose
    mathematical assistant can’t offer the same power as specialized
    software with efficiently implemented and well adapted algorithms and
    data types. For polynomial system solving, such specialized software
    has to implement two algorithmically complex tasks, solving and
    splitting, and until recently none of the specialized systems (as
    e.g., GB, Macaulay, Singular, CoCoA, etc.) did both
    efficiently. Meanwhile, being very efficient computing (classical)
    Groebner bases, development efforts are also directed, not only 
    for performance reasons, towards a better inclusion of factorization
    into such specialized systems.  Needless to remark that it needs some
    skill to force a special system to answer questions and the user will
    probably first try his ``home system'' for an answer. Thus the
    polynomial systems solving facility of the different CAS should behave
    especially well on such polynomial systems that are hard enough not to
    be done by hand, but not really hard to require special efforts. It
    should invoke a convenient interface to get the solutions in a form
    that is (correct and) well suited for further analysis in the familiar
    environment of the given CAS as the personal mathematical assistant.",
  paper = "Grab06.pdf",
  keywords = "axiomref"
}

@misc{Grab91,
  author = "Grabmeier, Johannes and Huber, K. and Krieger, U.",
  title = {{Das ComputeralgebraSystem AXIOM bei kryptologischen und 
           verkehrstheoretischen Untersuchungen des Forschunginstituts 
           der Deutschen Bundespost TELEKOM'}},
  type = "technical report",
  number = "TR 75.91.20",
  location = "Heidelberg, Germany", 
  year = "1991",
  keywords = "axiomref",
  beebe = "Grabmeier:1991:CSA"
}

@article{Grab91b,
  author = "Grabmeier, Johannes",
  title = {{Axiom, ein Computeralgebrasystem mit abstrakten Datentypen}},
  journal = "mathPAD",
  volume = "1",
  number = "3",
  pages = "13-15",
  year = "1991",
  paper = "Grab91b.pdf",
  keywords = "axiomref"
}

@book{Gree01,
  author = "Green, Edward L.",
  title = {{Symbolic Computation: Solving Equations in Algebra, Geometry, and
           Engineering}},
  booktitle = "Proc. AMS-IMS-SIAM Joint Summer Research Conference on Symbolic
               Computation",
  volume = "232",
  publisher = "American Mathematical Society",
  year = "2001",
  abstract =
    "This volume contains papers related to the research conference,
    ``Symbolic Computation: Solving Equations in Algebra, Analysis, and
    Engineering,'' held at Mount Holyoke College (MA). It provides a broad
    range of active research areas in symbolic computation as it applies
    to the solution of polynomial systems. The conference brought together
    pure and applied mathematicians, computer scientists, and engineers,
    who use symbolic computation to solve systems of equations or who
    develop the theoretical background and tools needed for this
    purpose. Within this general framework, the conference focused on
    several themes: systems of polynomials, systems of differential
    equations, noncommutative systems, and applications.",
  keywords = "axiomref"
}

@InProceedings{Grie71,
  author = "Griesmer, James H. and Jenks, Richard D.",
  title = {{SCRATCHPAD/1 -- an interactive facility for symbolic mathematics}},
  booktitle = "Proc. second ACM Symposium on Symbolic and Algebraic
               Manipulation",
  series = "SYMSAC 71",
  year = "1971",
  pages = "42--58",
  doi = "http://dx.doi.org/10.1145806266",
  link = "\url{http://delivery.acm.org/10.1145/810000/806266/p42-griesmer.pdf}",
  abstract = "
    The SCRATCHPAD/1 system is designed to provide an interactive symbolic
    computational facility for the mathematician user. The system features
    a user language designed to capture the style and succinctness of
    mathematical notation, together with a facility for conveniently
    introducing new notations into the language. A comprehensive system
    library incorporates symbolic capabilities provided by such systems as
    SIN, MATHLAB, and REDUCE.",
  paper = "Grie71.pdf",
  keywords = "axiomref",
  beebe = "Griesmer:1971:SIF"
}

@techreport{Grie72a,
  author = "Griesmer, James H. and Jenks, Richard D.",
  title = {{Experience with an online symbolic math system SCRATCHPAD}},
  institution = "IBM",
  year = "1972",
  isbn = "0-903796-02-3",
  keywords = "axiomref",
  beebe = "Griesmer:1972:EOSb"
}

@article{Grie72,
  author = "Griesmer, James H. and Jenks, Richard D.",
  title = {{SCRATCHPAD: A capsule view}},
  journal = "ACM SIGPLAN Notices",
  volume = "7",
  number = "10",
  pages = "93-102",
  year = "1972",
  comment = "Proc. Symp. Two-dimensional man-machine communications",
  doi = "http://dx.doi.org/10.1145807019",
  abstract =
    "SCRATCHPAD is an interactive system for algebraic manipulation
    available under the CP/CMS time-sharing system at Yorktown Heights. It
    features an extensible declarative language for the interactive
    formulation of symbolic computations. The system is a large and
    complex body of LISP programs incorporating significant portions of
    other symbolic systems. Here we present a capsule view of SCRATCHPAD,
    its language and its capabilities. This is followed by an example
    which illustrates its use in an application involving the solution of
    an integral equation.",
  keywords = "axiomref",
  beebe = "Griesmer:1972:SCV"
}

@article{Grie74,
  author = "Griesmer, James H. and Jenks, Richard D.",
  title = {{A solution to problem \#4: the lie transform}},
  journal = "SIGSAM Bulletin",
  volume = "8",
  number = "4",
  pages = "12-13",
  year = "1974",
  abstract =
    "The following SCRATCHPAD conversation for carrying out the Lie
    Transform computation represents a slight modification of one written
    by Dr. David Barton, when he was a summer visitor during 1972 at the
    Watson Research Center.",
  keywords = "axiomref"
}  

@techreport{Grie75,
  author = "Griesmer, James H. and Jenks, Richard D. and Yun, David Y.Y",
  title = {{SCRATCHPAD User's Manual}},
  institution = "IBM",
  year = "1975",
  type = "Research Report",
  number = "RA70",
  keywords = "axiomref"
}

@article{Grie75a,
  author = "Griesmer, James H. and Jenks, Richard D. and Yun, David Y.Y.",
  title = {{A SCRATCHPAD solution to problem \#7}},
  journal = "SIGSAM",
  volume = "9",
  number = "3",
  pages = "13-17",
  year = "1975"
}  

@article{Grie75b,
  author = "Griesmer, James H. and Jenks, Richard D. and Yun, David Y.Y.",
  title = {{A FORMAT statement in SCRATCHPAD}},
  journal = "SIGSAM",
  volume = "9",
  number = "3",
  pages = "24-25",
  year = "1975",
  abstract =
    "Algebraic manipulation covers branches of software, particularly list
    processing, mathematics, notably logic and number theory, and
    applications largely in physics. The lectures will deal with all of these
    to a varying extent.",
  keywords = "axiomref"
}  

@article{Grie79,
  author = "Griesmer, James H.",
  title = {{The state of symbolic computation}},
  journal = "SIGSAM Bulletin",
  volume = "13",
  number = "3",
  pages = "25-28",
  year = "1979",
  abstract =
    "When I was first asked to give this banquet talk, I was somewhat
    hesitant to accept. I have not been very heavily involved in the field
    of symbolic and algebraic manipulation for the past three years. I
    have contented myself with such behind-the-scenes activities as
    serving as an associate editor for the ACM Transactions on
    Mathematical Software and as a member of the SIGSAM Nominating
    Committees two years ago and again this year.",
  paper = "Grie79.pdf",
  keywwords = "axiomref"
}

@article{Grun94,
  author = "Gruntz, Dominik and Monagan, Michael B.",
  title = {{Introduction to Gauss}},
  journal = "SIGSAM Bulletin",
  volume = "28",
  number = "3",
  pages = "3-19",
  year = "1994",
  link = "\url{http://ftp.cecm.sfu.ca/personal/monaganm/papers/Gauss.pdf}",
  abstract =
    "The Gauss package offers Maple users a new approach to programming
    based on the idea of parameterized types (domains) which is central to
    the AXIOM system. This approach to programming is now regarded by many
    as the right way to go in computer algebra systems design. In this
    article, we describe how Gauss is designed and show examples of usage.
    We end with some comments about how Gauss is being used in Maple.",
  paper = "Grun94.pdf",
  keywords = "axiomref",
  beebe = "Gruntz:1994:IG"
}

@phdthesis{Grun96,
  author = "Gruntz, Dominik",
  title = {{On Computing Limits in a Symbolic Manipulation System}},
  school = "Swiss Federal Institute of Technology Zurich",
  year = "1996",
  link = "\url{http://www.cybertester.com/data/gruntz.pdf}",
  abstract = "
    This thesis presents an algorithm for computing (one-sided) limits
    within a symbolic manipulation system. Computing limtis is an
    important facility, as limits are used both by other functions such as
    the definite integrator and to get directly some qualitative
    information about a given function.

    The algorithm we present is very compact, easy to understand and easy
    to implement. It overcomes the cancellation problem other algorithms
    suffer from. These goals were achieved using a uniform method, namely
    by expanding the whole function into a series in terms of its most
    rapidly varying subexpression instead of a recursive bottom up
    expansion of the function. In the latter approach exact error terms
    have to be kept with each approximation in order to resolve the
    cancellation problem, and this may lead to an intermediate expression
    swell. Our algorithm avoids this problem and is thus suited to be
    implemented in a symbolic manipulation system.",
  paper = "Grun96.pdf",
  keywords = "axiomref"
}

@misc{Gute16,
  author = "Gutenberg Self-Publishing Press",
  title = {{OpenAxiom}},
  link = "\url{http://self.gutenberg.org/articles/openaxiom}",
  year = "2016",
  keywords = "axiomref"
}

@article{Hall96,
  author = "Hall, Cordelia V. and Hammond, Kevin and Jones, Simon L. Peyton
            and Wadler, Philip L.",
  title = {{Type Classes in Haskell}},
  journal = "Trans. on Programming Langues and Systems",
  volume = "18",
  number = "2",
  pages = "109-138",
  year = "1996",
  abstract =
    "This article de nes a set of type inference rules for resolving
    overloading introduced by type classes, as used in the functional
    programming language Haskell.  Programs including type classes are
    transformed into ones which may be typed by standard Hindley-Milner
    inference rules. In contrast to other work on type classes, the rules
    presented here relate directly to Haskell programs.  An innovative
    aspect of this work is the use of second-order lambda calculus to
    record type information in the transformed program.",
  paper = "Hall96.pdf",
  keywords = "printed"
}  

@article{Harr98,
  author = "Harrison, J. and Thery, L.",
  title = {{A Skeptic's approach to combining HOL and Maple}},
  journal = "J. Autom. Reasoning",
  volume = "21",
  number = "3",
  pages = "279-294",
  year = "1998",
  link = "\url{http://www.cl.cam.ac.uk/~jrh13/papers/cas.ps.gz}",
  abstract =
    "We contrast theorem provers and computer algebra systems, pointing
    out the advantages and disadvantages of each, and suggest a simple way
    to achieve a synthesis of some of the best features of both. Our
    method is based on the systematic separation of search for a solution
    and checking the solution, using a physical connection between
    systems. We describe the separation of proof search and checking in
    some detail, relating it to proof planning and to the complexity class
    NP, and discuss different ways of exploiting a physical link between
    systems. Finally, the method is illustrated by some concrete examples
    of computer algebra results proved formally in the HOL theorem prover
    with the aid of Maple.",
  paper = "Harr98.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@misc{Harr12,
  author = "Harriss, Edmund and Daly, Timothy",
  title = {{Have we ever lost mathematics?}},
  link = "\url{https://maxwelldemon.com/2012/05/09/have-we-ever-lost-mathematics/}",
  year = "2012",
  keywords = "axiomref"
}

@inproceedings{Kead93a,
  author = "Keady, G. and Richardson, M.G.",
  title = {{An application of IRENA to systems of nonlinear equations arising
           in equilibrium flows in networks}},
  booktitle = "Proc. ISSAC 1993",
  series = "ISSAC '93",
  year = "1993",
  abstract =
    "IRENA - an $I$nterface from $RE$DUCE to $NA$G - runs under the REDUCE
    Computer Algebra (CA) system and provides an interactive front end to
    the NAG Fortran Library.
    
    Here IRENA is tested on a problem closer to an engineering problem
    than previously publised examples. We also illustrate the use of the
    {\tt codeonly} switch, which is relevant to larger scale problems. We
    describe progress on an issue raised in the 'Future Developments'
    section in our {\sl SIGSAM Bulletin} article [2]: the progress improves
    the practical effectiveness of IRENA.",
  paper = "Kead93a.pdf",
  keywords = "axiomref"
}

@inproceedings{Hawk95,
  author = "Hawkes, Evatt and Keady, Grant",
  title = {{Two more links to NAG numerics involving CA systems}},
  booktitle = "IMACS Applied Computer Algebra Conference",
  location = "University of New Mexico",
  year = "1995",
  algebra = 
   "\newline\refto{domain ASP1 Asp1}
    \newline\refto{domain ASP10 Asp10}
    \newline\refto{domain ASP12 Asp12}
    \newline\refto{domain ASP19 Asp19}
    \newline\refto{domain ASP20 Asp20}
    \newline\refto{domain ASP24 Asp24}
    \newline\refto{domain ASP27 Asp27}
    \newline\refto{domain ASP28 Asp28}
    \newline\refto{domain ASP29 Asp29}
    \newline\refto{domain ASP30 Asp30}
    \newline\refto{domain ASP31 Asp31}
    \newline\refto{domain ASP33 Asp33}
    \newline\refto{domain ASP34 Asp34}
    \newline\refto{domain ASP35 Asp35}
    \newline\refto{domain ASP4 Asp4}
    \newline\refto{domain ASP41 Asp41}
    \newline\refto{domain ASP42 Asp42}
    \newline\refto{domain ASP49 Asp49}
    \newline\refto{domain ASP50 Asp50}
    \newline\refto{domain ASP55 Asp55}
    \newline\refto{domain ASP6 Asp6}
    \newline\refto{domain ASP7 Asp7}
    \newline\refto{domain ASP73 Asp73}
    \newline\refto{domain ASP74 Asp74}
    \newline\refto{domain ASP77 Asp77}
    \newline\refto{domain ASP78 Asp78}
    \newline\refto{domain ASP8 Asp8}
    \newline\refto{domain ASP80 Asp80}
    \newline\refto{domain ASP9 Asp9}",
  abstract =
    "The 'more' in the title is because this paper is a sequel to papers
    by Keving Broughan, [BKRRD,BK]. For some years GK has had interests in
    (i) interactive front-ends to numeric computation, such as the
    NAG/IMSL library computation, and (ii) Fortran code generation for
    Argument SubPrograms (ASPs), such as those neede by some NAG/IMSL
    routines. Demonstrations of three links to the NAG library are
    described in [BKRRD]. A description of a link to NAG from Macsyma
    which was mentioned, but not in a sufficiently advanced state to
    demonstrate in early 1991, is given in [BK]. The situation at the end
    of 1991 was that there were links to NAG involving each of Macsyma,
    REDUCE and Mathematica. The links are called Naglink, IRENA and
    InterCall, respectively. The principal authors of IRENA are Mike Dewar
    and Mike Richardson. InterCall is not specific to the NAG library;
    indeed InterCall is used with calls to IMSL and to elsewhere at the
    conference venue, the University of New Mexico.
    
    The two futher links to NAG library treated in this paper are AXIOM2.0
    and genmex/ESC, genmex allows calls to NAG from Matlab. genmex can be
    regarded as similar to InterCall: genmes uses Matlab's mex files in a
    similar way to InterCall's use of Mathematica's MathLink. Again genmex
    is not specific to the NAG library. Mike Dewar is an author both of
    IRENA and the AXIOM2.0 link to the NAG library: see [D] foe discussion
    of the differences between the IRENA project and the AXIOM-NAG link
    project.",
  paper = "Hawk95.pdf",
  keywords = "axiomref"
} 

@inproceedings{Hear80,
  author = "Hearn, Anthony C.",
  title = {{Symbolic Computation and its Application to High Energy Physics}},
  booktitle = "Proc. 1980 CERN School of Computing",
  pages = "390-406",
  year = "1980",
  link = "\url{http://www.iaea.org/inis/collection/NCLCollectionStore/\_Public/12/631/12631585.pdf}",
  abstract =
    "It is clear that we are in the middle of an electronic revolution
    whose effect will be as profound as the in dustrial revolution.  The
    continuing advances in computing technology will provide us with
    devices which will make present day computers appear primitive.  In
    this environment, the algebraic and other non-numerical capabilities
    of such devices will become increasingly important. These lectures
    will review the present state of the field of algebraic computation
    and its potential for problem solving in high energy physics and
    related areas.  We shall begin with a brief description of the
    available systems and examine the data objects which they consider.
    As an example of the facilities which these systems can offer, we
    shall then consider the problem of analytic integration, since this •
    is so fundamental to many of the calculational techniques used by high
    energy physicists.  Finally, we shall study the implications which the
    current developments in hardware technology hold for scientific
    problem solving.",
  paper = "Hear80.pdf"
}

@article{Hear82,
  author = "Hearn, Anthony C.",
  title = {{REDUCE - A Case Study in Algebra System Development}},
  journal = "Lecture Notes in Computer Science",
  volume = "144",
  pages = "263-272",
  year = "1982",
  paper = "Hear82.pdf",
  keywords = "axiomref"
}

@article{Hear95,
  author = "Hearn, Anthony C. and Eberhard, Schrufer",
  title = {{A computer algebra system based on order-sorted algebra}},
  journal = "J. Symbolic Computing",
  volume = "19",
  number = "1-3",
  pages = "65-77",
  year = "1995",
  abstract =
    "This paper presents the prototype design of an algebraic computation
    system that manipulates algebraic quantities as generic objects using
    order-sorted algebra as the underlying model. The resulting programs
    have a form that is closely related to the algorithmic description of
    a problem, but with the security of full type checking in a compact,
    natural style.",
  paper = "Hear95.pdf",
  keywords = "axiomref"
}

@book{Heck93,
  author = "Heck, Andre",
  title = {{Introduction to Maple}},
  year = "1993",
  publisher = "Springer-Verlag",
  abstract =
    "This is an introductory book on one of the most powerful computer
    algebra systems, viz, Maple: The primary emphasis in this book is on
    learning those things that can be done with Maple and how it can be
    used to solve mathematical problems. In this book usage of Maple as a
    programming language is not discussed at a higher level than that of
    defining simple procedures and using simple language constructs. 
    However, the Maple data structures are discussed in detail.

    This book is divided into eighteen chapters spanning a variety of
    topics. Starting with an introduction to symbolic computation and
    other similar computer algebra systems, this book covers several
    topics like polynomials and rational functions, series,
    differentiation and integration, differential equations, linear
    algebra, 2-D and 3-D graphics, etc. The applications covered include
    kinematics of the Stanford manipulator, a 3-component model for
    cadmium transfer through the human body, molecular-orbital Hückel
    theory, prolate spheroidal coordinates and Moore-Penrose inverses.
    
    At the end of each chapter, a good number of excercises is given. A
    list of relevant references is also given at the end of the book.
    This book is very useful to all users of Maple package.",
  keywords = "axiomref"
}

@phdthesis{Hemm03,
  author = "Hemmecke, Ralf",
  title = {{Involutive Bases for Polynomial Ideals}},
  school = "Johannes Kepler University, RISC",
  year = "2003",
  abstract =
    "This thesis contributes to the theory of polynomial involutive
    bases. Firstly, we present the two existing theories of involutive
    divisions, compare them, and come up with a generalised approach of
    {\sl suitable partial divisions}. The thesis is built on this
    generalized approach. Secondly, we treat the question of choosing a
    ``good'' suitable partial division in each iteration of the involutive
    basis algorithm. We devise an efficient and flexible algorithm for
    this purpose, the {\sl Sliced Division} algorithm. During the
    involutive basis algorithm, the Sliced Division algorithm contributes
    to an early detection of the involutive basis property and a
    minimisation of the number of critical elements. Thirdly, we give new
    criteria to avoid unnecessary reductions in an involutive basis
    algorithm. We show that the termination property of an involutive
    basis algorithm which applies our criteria is independent of the
    prolongation selection strategy used during its run. Finally, we
    present an implementation of the algorithm and results of this thesis
    in our software package CALIX."
}

@misc{RISC06,
  author = "Hemmecke, Ralf and Rubey, Martin",
  title = {{AXIOM Workshop 2006}},
  link = "\url{http://axiom-wiki.newsynthesis.org/WorkShopRISC2006}",
  year = "2006",
  location = "Hagenberg, Austria",
  abstract =
    "Axiom is a computer algebra system with a long tradition. It recently
    became free software.

    The workshop aims at a cooperation of Axiom developers with developers
    of packages written for other Computer Algebra Systems or developers
    of stand-alone packages. Furthermore, the workshop wants to make the
    potential of Axiom and Aldor more widely known in order to attract new
    users and new developers.",
  keywords = "axiomref"
}

@misc{RISC07,
  author = "Hemmecke, Ralf and Rubey, Martin",
  title = {{AXIOM Workshop 2007}},
  link = "\url{http://axiom-wiki.newsynthesis.org/WorkShopRISC2007}",
  year = "2007",
  location = "Hagenberg, Austria",
  abstract =
    "The workshop aims at a cooperation of Axiom developers with deelopers
    of packages written for other Computer Algebra Systems, and
    mathematicians that would like to use a computer algebra system to
    perform experiments.

    One goal of the workshop is to learn about the mathematical theory,
    the design of packages written for other CAS and to make those
    functionalities available in Axiom." ,
  keywords = "axiomref"
}

@misc{Hera16,
  author = "Heras, Jonathan and Martin-Mateos, Franciso Jesus and 
            Pascual, Vico",
  title = {{A Hierarchy of Mathematical Structures in ACL2}},
  link = "\url{http://staff.computing.dundee.ac.uk/jheras/papers/ahomsia.pdf}",
  abstract =
    "In this paper, we present a methodology which allows one to deal with
    {\sl mathematical structures} in the ACL2 theorem prover. Namely, we
    cope with the representation of mathematical structures, the
    certification that an object fulfills the axioms characterizing an
    algebraic structure and the generation of generic theories about
    concrete structures. As a by-product, an {\sl ACL2 algebraic
    hierarchy} has been obtained. Our framework has been tested with the
    definition of {\sl homology groups}, an example coming from
    Homological Algebra which involves several notions related to
    Universal Algebra. The method presented here, when compared to a
    from-scratch approach, is preferred when working with complex
    mathematical structures; for instance, the ones coming from Algebraic
    Topology. The final aim of this work is the verification of Computer
    Algebra systems, a field where our hierarchy fits better than the ones
    developed in other systems.",
  paper = "Hera16.pdf",
  keywords = "axiomref"
}

@article{Hera15,
  author = "Heras, Jonathan and Martin-Mateos, Franciso Jesus and 
            Pascual, Vico",
  title = {{Modelling algebraic structures and morphisms in ACL2}},
  journal = "Appl. Algebra Eng. Commun. Comput.",
  volume = "26",
  number = "3",
  pages = "277-303",
  year = "2015",
  abstract =
    "In this paper, we present how algebraic structures and morphisms can
    be modelled in the ACL2 theorem prover. Namely, we illustrate a
    methodology for implementing a set of tools that facilitates the
    formalisations related to algebraic structures -- as a result, an
    algebraic hierarchy ranging from setoids to vector spaces has been
    developed. The resultant tools can be used to simplify the development
    of generic theories about algebraic structures. In particular, the
    benefits of using the tools presented in this paper, compared to a
    from-scratch approach, are especially relevant when working with
    complex mathematical structures; for example, the structures employed
    in Algebraic Topology. This work shows that ACL2 can be a suitable
    tool for formalising algebraic concepts coming, for instance, from
    computer algebra systems.",
  keywords = "axiomref"
}

@misc{Here96,
  author = "Hereman, Willy",
  title = {{The Incredible World of Symbolic Mathematics 
            A Review of Computer Algebra Systems}},
  year = "1996",
  link = "\url{https://inside.mines.edu/~whereman/papers/Hereman-PhysicsWorld-9-March1996.pdf}",
  paper = "Here96.pdf",
  keywords = "axiomref"
}

@article{Here97,
  author = "Hereman, Willy",
  title = {{Review of Symbolic Software for Lie Symmetry Analysis}},
  journal = "Math. Comput. Modelling",
  volume = "25",
  number = "8/9",
  pages = "115-132",
  year = "1997",
  abstract =
    "Sophus Lie (1842-1899) pioneered the study of continuous
    transformation groups that leave systems of differential equations
    invariant.  Lie’s work [l-3] brought diverse and ad hoc integration
    methods for solving special classes of differential equations under a
    common conceptual umbrella.  Indeed, Lie’s infinitesimal
    transformation method provides a widely applicable technique to find
    closed form solutions of ordinary differential equations (ODES).
    Standard solution methods for first-order or linear ODES can be
    characterized in terms of symmetries.  Through the group
    classification of ODES, Lie succeeded in identifying all ODES that can
    either be reduced to lower-order ones or be completely integrated via
    group theoretic techniques.  
    
    Applied to partial differential equations (PDEs), Lie’s method [2]
    leads to group-invariant solutions and conservation laws.  Exploiting
    the symmetries of PDEs, new solutions can be derived from known ones,
    and PDEs can be classified into equivalence classes.  Furthermore,
    group-invariant solutions obtained via Lie’s approach may provide
    insight into the physical models themselves, and explicit solutions
    can serve as benchmarks in the design, accuracy testing, and
    comparison of numerical algorithms.
    
    Nowadays, the concept of symmetry plays a key role in the study and
    development of mathematics and physics.  Indeed, the theory of Lie
    groups and Lie algebras is applied to diverse fields of mathematics
    including differential geometry, algebraic topology, bifurcation
    theory, to name a few.  Lie’s original ideas greatly influenced the
    study of physically important systems of differential equations in
    classical and quantum mechanics, fluid dynamics, elasticity, and many
    other applied areas [4-81].
    
    The application of Lie group methods to concrete physical systems
    involves tedious computations.  Even the calculation of the
    continuous symmetry group of a modest system of differential equations
    is prone to errors, if done with pencil and paper.  Computer algebra
    systems (CAS) such as Mathematica, MACSYMA, Maple, REDUCE, AXIOM and
    MuPAD are extremely useful for such computations.  Symbolic packages
    [9-11], written in the language of these GAS, can find the determining
    equations of the Lie symmetry group.  The most sophisticated packages
    then reduce these into an equivalent but more suitable system,
    subsequently solve that system in closed form, and go on to calculate
    the infinitesimal generators that span the Lie algebra of symmetries.
    
    In Section 2, we discuss methods and algorithms used in the
    computation of Lie symmetries.  We address the computation of
    determining systems, their reduction to standard form, solution
    techniques, and the computation of the size of the symmetry group.
    In Section 3, we look beyond Lie-point symmetries, addressing contact
    and generalized symmetries, as well as nonclassical or conditional
    symmetries.  
    
    Section 4 is devoted to a review of modern Lie symmetry
    programs, classified according to the underlying CAS.  The review
    focuses on Lie symmetry software for classical Lie-point symmetries,
    contact (or dynamical), generalized (or Lie-Backlund) symmetries,
    nonclassical (or conditional) symmetries.  Most of these packages were
    written in the last decade.  Researchers interested in details about
    pioneering work should consult [9,10,12].  In Section 5, two examples
    illustrate results that can be obtained with Lie symmetry software.
    In Section 6 we draw some conclusions.  
    
    Lack of space forces us to give only a few key references for the Lie
    symmetry packages.  A comprehensive survey of the literature devoted
    to theoretical as well as computational aspects of Lie symmetries,
    with over 300 references, can be found elsewhere [11].",
  paper = "Here97.pdf",
  keywords = "axiomref"
}

@article{Hive04,
  author = "Hivert, Florent and Thiery, Nicolas M.",
  title = {{MuPAD-Combinat, an open-source package for research in 
           algebraic combinatorics}},
  journal = "Seminaire Lotharingien de Combinatoire",
  volume = "51",
  number = "B51z",
  year = "2004",
  link = "\url{http://www.emis.de/journals/SLC/wpapers/s51thiery.pdf}",
  abstract = 
    "In this article we give an overview of the MuPAD-Combinat open-source
    algebraic combinatorics package for the computer algebra system MuPAD
    2.0.0 and higher. This includes our motivations for developing yet
    another combinatorial software, a tutorial introduction with lots of
    examples, as well as notes on the general design. The material
    presented here is also available as a part of the MuPAD-Combinat
    handbook; further details and references on the algorithms used can be
    found there. The package and the handbook are available from the web
    page, together with download and installation instructions, mailing
    lists, etc.",
  paper = "Hive04.pdf",
  keywords = "axiomref"
}

@article{Hoan00,
  author = "Hoang, Ngoc Minh and Petitot, Michel and Van er Hoeven, Joris",
  title = {{Shuffle algebra and polylogarithms}},
  journal = "Discrete Math.",
  volume = "225",
  number = "1-3",
  pages = "217-230",
  year = "2000",
  abstract =
    "Generalized polylogarithms are defined as iterated integrals with
    respect to the two differential forms $\omega_0=\frac{dz}{z}$ and
    $\omega_1=\frac{dz}{(1-z)}$. We give an algorithm which computes the
    monodromy of these special functions. This algorithm, implemented in
    AXIOM, is based on the computation of the associator $\Phi_{KZ}$ of
    Drinfel’d, in factorized form. The monodromy formulae involve special
    constants, called multiple zeta values. We prove that the algebra of
    polylogarithms is isomorphic to a shuffle algebra.",
  paper = "Hoan00.pdf",
  keywords = "axiomref"
}

@article{Hoar08,
  author = "Hoarau, Emma and David, Claire",
  title = {{Lie group computation of finite difference schemes}},
  year = "2008",
  journal = "math.NA",
  link = "\url{http://arxiv.org/pdf/math/0611895.pdf}",
  abstract =
    "A Mathematica based program has been elaborated in order to determine
    the symmetry group of a finite difference equation. The package
    provides functions which enabel use to solve the determining equations
    of the related Lie group.",
  paper = "Hoar08.pdf",
  keywords = "axiomref"
}

@phdthesis{Hodo11,
  author = "Hodorog, Madalina",
  title = {{Symbolic-Numeric Algorithms for Plane Algebraic Curves}},
  year = "2011",
  school = "RISC Research Institute for Symbolic Computation",
  abstract =
    "In computer algebra, the problem of computing topological invariants
    (i.e. delta-invariant, genus) of a plan complex algebraic curve is
    well-understood if the coefficients of the defining polynomial of the
    curve are exact data (i.e. integer numbers or rational numbers). The
    challenge is to handle this problem if the coefficients are inexact
    (i.e. numerical values).

    In this thesis, we approach the algebraic problem of computing
    invariants of a plane complex algebraic curve defined by a polynomial
    with both exact and inexact data. For the inexact data, we associate a
    positive real number called {\sl tolerance} or {\sl noise}, which
    measures the error level in the coefficients. We deal with an {\sl
    ill-posed} problem in the sense that, tiny changes in the input data
    lead to dramatic modifications in the output solution.

    For handling the ill-posedness of the problem we present a {\sl
    regularization} method, which estimates the invariants of a plane
    complex algebraic curve. Our regularization method consists of a set
    of {\sl symbolic-numeric algorithms} that extract structural
    information on the input curve, and of a {\sl parameter choice rule},
    i.e. a function in the noise level. We first design the following
    symbolic-numeric algorithms for computing the invariants of a plane
    complex algebraic curve:
    \begin{itemize}
    \item we compute the link of each singularity of the curve by numerical
    equation solving
    \item we compute the Alexander polynomial of each link by using
    algorithms from computational geometry (i.e. an adapted version of
    the Bentley-Ottmann algorithm) and combinatorial objects from knot
    theory.
    \item we derive a formula for the delta-invariant and for the genus
    \end{itemize}

    We then prove that the symbolic-numeric algorithms together with the
    parameter choice rule compute approximate solutions, which satisfy the
    {\sl convergence for noisy data property}. Moreover, we perform
    several numerical experiments, which support the validity for the
    convergence statement.

    We implement the designed symbolic-numeric algorithms in a new
    software package called {\sl Genom3ck}, developed using the {\sl Axel}
    free algebraic modeler and the {\sl Mathemagix} free computer algebra
    system. For our purpose, both of these systems provide modern
    graphical capabilities, and algebraic and geometric tools for
    manipulating algebraic curves and surfaces defined by polynomials with
    both exact and inexact data. Together with its main functionality to
    compute the genus, the package {\sl Genom3ck} computes also other type
    of information on a plane complex algebraic curve, such as the
    singularities of the curve in the projective plane and the topological
    type of each singularity.",
  paper = "Hodo11.pdf"
}

@misc{Hoep95,
  author = "Hoeppner, Sabine",
  title = {{Linear differential equations of second order in fields of
           positive characteristic}},
  comment = "Essen: Univ. Essen, FB Math 71 S.",
  year = "1995",
  abstract =
    "Let be a differential field of characteristic $p>2$ of the type
    $(\mathbb{F}_p(x),\frac{d}{dx})$. The equation studied is
    $y^{\prime\prime}+ay^{\prime}+by=0$ with $a,b \in K$.  The goal is to
    produce a Liouvillian extension $L \supset K$ which contains two
    independent solutions. This is done by solving the associated Riccati
    equation $u^{\prime}+u^2+au+b=0$. Unlike the characteristic zero
    situation, the Riccati equation has one or two solutions in an
    algebraic extension of degree $\le 2$ of $K$.  Full solutions are
    given for the various cases that do occur. The paper ends with a
    program in AXIOM which computes the solutions of the Riccati equation
    and the Liouvillian extensions.",
  keywords = "axiomref"
}

@misc{Hoev13,
  author = "Hoeven, Joris van der and Lecerf, Gregoire",
  title = {{Interfacing Mathemagix with C++}},
  link = "\url{http://www.texmacs.org/joris/mmxcpp/mmxcpp.pdf}",
  abstract =
    "In this paper, we give a detailed description of the interface 
    between the Mathemagix language and C++. In particular, we describe
    the mechanism which allows us to import a C++ template library
    (which only permits static instantiation) as a fully generic
    Mathemagix template library.",
  paper = "Hoev13.pdf",
  keywords = "axiomref"
}

@misc{Hoev15,
  author = "Hoeven, Joris van der and Lecerf, Gregoire",
  title = {{Interfacing Mathemagix with C++}},
  link = "\url{http://www.texmacs.org/joris/mmxcpp/mmxcpp.html}",
  abstract =
    "In this paper, we give a detailed description of the interface 
    between the Mathemagix language and C++. In particular, we describe
    the mechanism which allows us to import a C++ template library
    (which only permits static instantiation) as a fully generic
    Mathemagix template library.",
  keywords = "axiomref"
}

@book{Hold11,
  author = "Hohold, Tom and van Lint, Jacobus H. and Pellikaan, Ruud",
  title = {{Algebraic Geometry Codes}},
  year = "2011",
  pages = "871-961",
  booktitle = "Handbook of Coding Theory",
  publisher = "V.S Pless, W.C. Huffman and R.A. Brualdi (eds)",
  volume = "I",
  link = "\url{http://www.win.tue.nl/~ruudp/paper/31.pdf}",
  algebra = "\newline\refto{category PRSPCAT ProjectiveSpaceCategory}",
  isbn = "9780444814722",
  paper = "Hold11.pdf"
}  

@book{Hous92,
  author = "Houstis, E.N. and Gaffney, P.W.",
  title = {{Programming environments for high-level scientific problem 
           solving}},
  year = "1992",
  publisher = "Elsevier",
  isbn = "978-0444891761",
  abstract =
    "Programming environments, as the name suggests, are intended to
    provide a unified, extensive range of capabilities for a person
    wishing to solve a problem using a computer. In this particular
    proceedings volume, the problem considered is a high-level scientific
    computation. In other words, a scientific problem whose solution
    usually requires sophisticated computing techniques and a large
    allocation of computing resources.",
  keywords = "axiomref"
}

@article{Hous00,
  author = "Houstis, Elias N. and Rice, John R.",
  title = {{Future problem solving environments for computational science}},
  journal = "Math. Comput. Simul.",
  volume = "54",
  number = "4-5",
  pages = "243-257",
  year = "2000",
  abstract =
    "We review the current state of the Problem Solving Environment (PSE)
    field and make projections for the future. First, we describe the
    computing context, the definition of a PSE and the goals of a PSE. The
    state-of-the-art is summarized along with the principal components and
    paradigms for building PSEs. The discussion of the future is given in
    three parts: future trends, scenarios for 2010/2025, and research
    issues to be addressed.",
  keywords = "axiomref"
}

@article{Igar75,
  author = "Igarashi, Shigeru and London, Ralph L. and Luckham, David C.",
  title = {{Automatic Program Verification I: A Logical Basis and Its
             Implementation}},
  journal = "Acta Informatica",
  volume = "4",
  number = "2",
  pages = "145-182",
  year = "1975",
  abstract =
    "Defining the semantics of programming languages by axioms and rules
    of inference yields a deduction system within which proofs may be
    given that programs satisfy specifications. The deduction system
    herein is shown to be consistent and also deduction complete with
    respect to Hoare's system. A subgoaler for the deduction system is
    described whose input is a significant subset of Pascal programs plus
    inductive assertions. The output is a set of verification conditions
    or lemmas to be proved. Several non-trivial arithmetic and sorting
    programs have been shown to satisfy specifications by using an
    interactive theorem prover to automatically generate proofs of the
    verification conditions. Additional components for a more powerful
    verification system are under construction.",
  paper = "Igar75.pdf"
}

@article{Jacq97,
  author = "Jacquemard, Alain and Khechichine-Mourtada, F.Z. and Mourtada, A.",
  title = {{Formal algorithms applied to the study of the cyclicity of a
           generic algebraic polycycle with four hyperbolic crests}},
  journal = "Nonlinearity",
  volume = "10",
  number = "1",
  pages = "19-53",
  year = "1997",
  comment = "french",
  abstract =
    "Drawing on the work of Mourtada, we show that a family of vector
    fields with a generic algebraic polycycle of four hyperbolic apices
    possesses a maximum capacity of four limit cycles. This cyclicity is
    attained in an opening connecting the parameters which the edge
    contains, in particular a generic line of singularities of dovetail
    type. We also give an asymptotic estimation of the volume of this
    opening, as well as an explicit example of a family of polynomial
    vector fields replicating the above-described conditions and
    possessing five limit cycles. The methods employed are very diverse:
    geometrical arguments (Thom’s theory of catastrophes and the theory of
    algebraic singularities), developments from Puiseux, the number of
    major roots by Descartes’ law and calculated exactly by Sturm series,
    and other specific methods for formal calculus, such as for example
    the cylindrical algebraic decomposition and the resolution of
    algebraic systems via the construction of Gröbner bases. The
    calculations have been executed formally, that is to say without
    making the least appeal to numerical approximation, in using the
    formal calculus system AXIOM.",
  keywords = "axiomref"
}

@article{Jacq02,
  author = "Jacquemard, Alain and Teixeira, M.A.",
  title = {{Effective algebraic geometry and normal forms of reversible 
           mappings}},
  journal = "Rev. Mat. Complut.",
  volume = "15",
  number = "1",
  pages = "31-55",
  year = "2002",
  abstract = 
    "The authors consider a problem coming from the theory of discrete
    dynamical systems (iterations of diffeomorphisms in
    $\mathbb{R}^3$). Namely, how to characterize the behaviour of the
    trajectories near the fixed points and the stability of this behaviour
    under perturbations. The authors use the computer in the context of
    reversible mappings. When such a mapping has some degree of degeneracy
    they apply Gröbner bases techniques and the theory of (formal) normal
    forms in the space of the coefficients of the jets of the
    mapping. They show that a language with typed objects like AXIOM is
    very convenient to solve such problems.",
  keywords = "axiomref"
}

@article{Jaes93,
  author = "Jaeschke, Gerhard",
  title = {{On Strong Pseudoprimes to Several Bases}},
  journal = "Mathematics of Computation",
  volume = "61",
  number = "204",
  year = "1993",
  pages = "915-926",
  algebra = "\newline\refto{package PRIMES IntegerPrimesPackage}",
  abstract = 
    "With $\psi_k$ denoting the smallest strong pseudoprime to all of the
    first $k$ primes taken as bases we determine the exact values for
    $\psi_5$, $\psi_6$, $\psi_7$, $\psi_8$, and give upper bounds for
    $\psi_9$, $\psi_{10}$, $\psi_{11}$. We discuss the methods and
    underlying facts for obtaining these results",
  paper = "Jaes93.pdf",
  keywords = "axiomref"
}

@article{Jage96,
  author = "Jager, Bram De and van Asch, Bram",
  title = {{Symbolic Solutions for a Class of Partial Differential Equations}},
  journal = "J. Symbolic Computation",
  volume = "22",
  year = "1996",
  pages = "459-468",
  link = "\url{http://www.mate.tue.nl/mate/pdfs/1610.pdf}",
  abstract =
    "An algorithm to generate solutions for members of a class of
    completely integrable partial differential equations has been derived
    from a constructive proof of Frobenius' Theorem. The algorithm is
    implemented as a procedure in the computer algebra system
    Maple. Because the implementation uses the facilities of Maple for
    solving sets of ordinary differential equations and for sets of
    nonlinear equations, and those facilities are limited, the problems
    that actually can be solved are restricted in size and
    complexity. Several examples, some derived from industrial practice,
    are presented to illustrate the use of the algorithm and to
    demonstrate the advantages and shortcomings of the implementation.",
  paper = "Jage96.pdf",
  keywords = "axiomref"
}

@techReport{Jenk71,
  author = "Jenks, Richard D.",
  title = {{META/PLUS: The syntax extension facility for SCRATCHPAD}},
  type = "Research Report",
  number = "RC 3259",
  institution = "IBM Research",
  year = "1971",
  keywords = "axiomref",
  beebe = "Jenks:1971:MPS"
}

@misc{Jenk72,
  author = "Jenks, Richard D.",
  title = {{SCRATCHPAD}},
  volume = "??",
  number = "24",
  pages = "16-17",
  month = "October",
  year = "1972",
  abstract =
    "The following SCRATCHPAD solution of Problem \#2 was run on a 1280K
    virtual machine under CP/CMS time sharing system on a System/360
    model 67. The conversation below is a modification of a program
    originally written by Yngve Sundblad, August 1972. The program uses
    symmetrised formulae, saves certain intermediate results, but does not
    eliminate numerical factors in denominators",
  keywords = "axiomref"
}

@article{Jenk74,
  author = "Jenks, Richard D.",
  title = {{The SCRATCHPAD language}},
  journal = "ACM SIGPLAN Notices",
  comment = "reprinted in SIGSAM Bulletin, Vol 8, No. 2, pp 20-30 May 1974",
  volume = "9",
  number = "4",
  pages = "101-111",
  year = "1974",
  doi = "http://dx.doi.org/10.1145807051",
  abstract =
    "SCRATCHPAD is an interactive system for symbolic mathematical
    computation. Its user language, originally intended as a
    special-purpose non-procedural language, was designed to capture the
    style and succinctness of common mathematical notations, and to serve
    as a useful, effective tool for on-line problem solving. This paper
    describes extensions to the language which enable it to serve also as
    a high-level programming language, both for the formal description of
    mathematical algorithms and their efficient implementation.",
  paper = "Jenk74.pdf",
  keywords = "axiomref, printed",
  beebe = "Jenks:1974:SL"
}

@article{Jenk76a,
  author = "Jenks, Richard D.",
  title = {{Problem \#11: generation of Runge-Kutta equations}},
  journal = "SIGSAM Bulletin",
  volume = "10",
  number = "1",
  year = "1976",
  abstract =
    "Generate a set of equations for an explicit k-th order, m stage,
    Runge-Kutta method for integrating an autonomous system of ordinary
    differential equations, k and m as large as possible. The number of
    conditions and variables for various k and m are given in Table
    1. Tabulate the costs C(m,k)."
}

@inproceedings{Jenk76,
  author = "Jenks, Richard D.",
  title = {{A pattern compiler}},
  booktitle = "Proc. 1976 ACM Symposium on Symbolic and Algebraic Computation",
  series = "SYMSAC '76",
  year = "1976",
  publisher = "ACM Press",
  doi = "http://dx.doi.org/10.1145806324",
  abstract =
    "A pattern compiler for the SCRATCHPAD system provides an efficient
    implementation of sets of user-defined pattern-replacement rules for
    symbolic mathematical computation such as tables of integrals or
    summation identities. Rules are compiled together, with common search
    paths merged and factored out and with the resulting code optimized
    for efficient recognition over all patterns. Matching principally
    involves structural comparison of expression trees and evaluation of
    predicates. Pattern recognizers are ``fully compiled''; if values of 
    match variables can be determined by solving equations at compile time. 
    Recognition times for several pattern matchers are compared.",
  keywords = "axiomref",
  beebe = "Jenks:1976:PC"
}

@article{Jenk77,
  author = "Jenks, Richard D.",
  title = {{On the design of a mode-based symbolic system}},
  journal = "SIGGAM Bulletin",
  volume = "11",
  number = "1",
  pages = "16-19",
  year = "1977",
  abstract =
    "This paper is a preliminary report on the design and implementation
    of a mode-based symbolic programming system and compiler which allows
    programming with rewrite rules and LET and IS pattern-match constructs. 
    An important feature of this design is the provision for mode-valued 
    variables which allow algebraic domains to be run-time parameters.",
  keywords = "axiomref"
}

@inproceedings{Jenk79,
  author = "Jenks, Richard D.",
  title = {{MODLISP: An Introduction}},
  booktitle = "Proc. ISSAC 1979",
  series = "EUROSAM 79", 
  pages = "466-480",
  year = "1979",
  publisher = "Springer-Verlag",
  isbn = "3-540-09519-5",
  comment = "IBM Research Report RC 8073 Jan 1980",
  keywords = "axiomref"
}

@article{Jenk79a,
  author = "Jenks, Richard D.",
  title = {{SCRATCHPAD/360: reflections on a language design}},
  journal = "SIGSAM",
  volume = "13",
  number = "1",
  pages = "16-26",
  year = "1979",
  comment = "IBM Research RC 7405",
  abstract =
    "The key concepts of the SCRATCHPAD language are described, assessed,
    and illustrated by an example. The language was originally intended as
    an interactive problem solving language for symbolic mathematics. 
    Nevertheless, as this paper intends to show, it can be used as a 
    programming language as well.",
  paper = "Jenk79a.pdf",
  keywords = "axiomref"
}

@InProceedings{Jenk81,
  author = "Jenks, Richard D. and Trager, Barry M.",
  title = {{A Language for Computational Algebra}},
  year = "1981",
  booktitle = "Proc. Symp. on Symbolic and Algebraic Manipulation",
  series = "SYMSAC 1981",
  location = "Snowbird, Utah",
  comment = "IBM Research Report 8930",
  abstract =
    "This paper reports ongoing research at the IBM Research Center on the
    development of a language with extensible parameterized types and
    generic operators for computational algebra. The language provides an
    abstract data type mechanism for defining algorithms which work in as
    general a setting as possible. The language is based on the notions of
    domains and categories. Domains represent algebraic
    structures. Categories designate collections of domains having common
    operations with stated mathematical properties. Domains and categories
    are computed objects which may be dynamically assigned to variables,
    passed as arguments, and returned by functions. Although the language
    has been carefully tailored for the application of algebraic
    computation, it actually provides a very general abstract data type
    mechanism. Our notion of a category to group domains with common
    properties appears novel among programming languages (cf. image
    functor of RUSSELL) and leads to a very powerful notion of abstract
    algorithms missing from other work on data types known to the authors.",
  paper = "Jenk81.pdf",
  keywords = "axiomref, printed"
}

@inproceedings{Jenk84a,
  author = "Jenks, Richard D.",
  title = {{The New SCRATCHPAD Language and System for Computer Algebra}},
  booktitle = "Proc. 1984 MACSYMA Users Conference",
  year = "1984",
  pages = "409-??",
  keywords = "axiomref",
  beebe = "Jenks:1984:NSL"
}

@inproceedings{Jenk84b,
  author = "Jenks, Richard D.",
  title = {{A primer: 11 keys to New Scratchpad}},
  booktitle = "Proc. EUROSAM ISSAC 1984",
  year = "1984",
  publisher = "Springer-Verlag",
  pages = "123-147",
  isbn = "0-387-13350-X",
  abstract =
    "This paper is an abbreviated primer for the language of new
    SCRATCHPAD, a new implementation of SCRATCHPAD which has been under
    design and development by the Computer Algebra Group at the IBM
    Research Center during the past 6 years. The basic design goals of the
    new SCRATCHPAD language and interface to the user are to provde:
    \begin{itemize}
    \item a ``typeless'' interactive language suitable for on-line solution
    of mathematical problems by novice users with little or no programming
    required, and
    \item a programming language suitable for the formal description of
    algorithms and algebraic structures which can be compiled into run-time
    efficient object code.
    \end{itemize}
    
    The new SCRATCHPAD language is introduced by 11 keys with each
    successive key introducing an additional capability of the
    language. The language is thus described as a ``concentric'' language
    with each of the 11 levels corresponding to a language subset. These
    levels are more than just a pedagogic device, since they correspond to
    levels at which the system can be effectively used. Level 1 is
    sufficient for naive interactive use; levels 2-8 progressively
    introduce interactive users to capabilities of the language; levels
    9-11 are for system programmers and advanced users. Levesl 2, 4, 6,
    and 7 give users the full power of LISP with a high-level language;
    level 8 introduces ``type declarations;'' level 9 allows polymorphic
    functions to be defined and compiled; levels 10-11 give users an
    Ada-like facility for defining types and packages (those of new
    SCRATCHPAD are dynamically constructable, however). One language is
    used for both interactive and system programming language use,
    although several freedomes such as abbreviation and optional
    type-declarations allowed at top-level are not permitted in system
    code. The interactive language (levels 1-8) is a blend of original
    SCRATCHPAD [GRJY75], some proposed extensions [JENK74], work by Loos
    [LOOS74], SETL [DEWA79], SMP [COWO81], and new ideas; the system
    programming language (levels 1-11) superficially resembles Ada but is
    more similar to CLU [LISK74] in its semantic design.
    
    This presentation of the language in this paper omits many details to
    be covered in the SCRATCHPAD System Programming Manual [SCRA84] and an
    expanded version of this paper will serve as a primer for SCRATCHPAD
    users [JESU84].",
  keywords = "axiomref, printed",
  beebe = "Jenks:1984:PKN"
}

@misc{Jenk84c,
  author = "Jenks, Richard D. and Sundaresan, Christine J.",
  title = {{The 11 Keys to SCRATCHPAD: A Primer}},
  year = "1984",
  keywords = "axiomref"
}

@article{Jenk87a,
  author = "Jenks, Richard D.",
  title = {{1962-1992: The First 30 Years of Symbolic Mathematical 
           Programming Systems}},
  journal = "Lecture Notes in Computer Science",
  volume = "296",
  year = "1987",
  pages = "1-1",
  abstract = 
    "This talk examines the history and future of symbolic mathematical
    computer systems. This talk will trace the development of three
    generations of computer algebra systems as typified by an early system
    of 60's: FORMAC, the standalone systems of the 70's: REDUCE and
    MACSYMA, and those developed in the 80's: muMATH, MAPLE, SMP, with
    particular emphasis on Scratchpad II, a system of revolutionary design
    currently under development by IBM Research.
    
    This talk will trace the progress of algebraic algorithm research in
    the past 25 years, advances in hardware and software technology over
    the same period, and the impact of such progress on the design issues
    of such systems. The talk will conclude with a description of the
    workstation of the future and its anticipated impact on the research
    and educational communities.",
  paper = "Jenk87a.pdf",
  keywords = "axiomref"
}

@misc{Acad16,
  author = "Academic Search",
  title = {{A Primer: 11 Keys to New Scratchpad}},
  link = "\url{http://libra.msra.cn/publication/645035/a-primer-11-keys-to-new-scratchpad}",
  year = "2016",
  keywords = "axiomref"
}

@misc{Jenk86a,
  author = "Jenks, Richard D.",
  title = {{Basic Algebraic Facilities of the Scratchpad II Computer 
           Algebra System}},
  institution = "IBM Research",
  year = "1986",
  keywords = "axiomref"
}

@misc{Jenk86b,
  author = "Jenks, Richard D.",
  title = {{Scratchpad II Examples from INPUT files}},
  institution = "IBM Research",
  year = "1986",
  keywords = "axiomref"
}

@techreport{Jenk86,
  author = "Jenks, Richard D. and Sutor, Robert S. and Watt, Stephen M.",
  title = {{Scratchpad II: An Abstract Datatype System for Mathematical 
           Computation}},
  institution = "IBM Research",
  year = "1986",
  type = "Research Report",
  number = "RC 12327 (\#55257)",
  link = "\url{http://www.csd.uwo.ca/~watt/pub/reprints/1987-ima-spadadt.pdf}",
  abstract = "
    Scratchpad II is an abstract datatype language and system that is
    under development in the Computer Algebra Group, Mathematical Sciences
    Department, at the IBM Thomas J. Watson Research Center. Some features
    of APL that made computation particularly elegant have been borrowed.
    Many different kinds of computational objects and data structures are
    provided. Facilities for computation include symbolic integration,
    differentiation, factorization, solution of equations and linear
    algebra.  Code economy and modularity is achieved by having
    polymorphic packages of functions that may create datatypes. The use
    of categories makes these facilities as general as possible.",
  paper = "Jenk86.pdf",
  keywords = "axiomref, printed",
  beebe = "Jenks:1986:SIA"
}

@inproceedings{Jenk87,
  author = "Jenks, Richard D. and Sutor, Robert S. and Watt, Stephen M.",
  title = {{Scratchpad II: an Abstract Datatype System for 
           Mathematical Computation'}},
  booktitle = "Proceedings Trends in Computer Algebra", 
  series = "Lecture Notes in Computer Science 296",
  pages = "157-182",
  publisher = "Springer-Verlag",
  isbn = "0-387-18928-9",
  year = "1987",
  comment = "IBM Research Report RC 12327 (\#55257) See Jenks86.pdf",
  abstract =
    "Scratchpad II is an abstract datatype language and system that is
    under development in the Computer Algebra Group, Mathematical Sciences
    Department, at the IBM Thomas J. Watson Research Center. Many
    different kinds of computational objects and data structures are
    provided. Facilities for computation include symbolic integration,
    differentation, factorization, solution of equations and linear
    algebra. Code economy and modularity is achieved by having polymorphic
    packages of functions that may create datatypes. The use of categories
    makes these facilities as general as possible.",
  keywords = "axiomref"
}

@inproceedings{Jenk88,
  author = "Jenks, Richard D. and Sutor, Robert S. and Watt, Stephen M.",
  title = {{Scratchpad II: An Abstract Datatype System for Mathematical 
           Computation}},
  booktitle = "Mathematical Aspects of Scientific Software",
  year = "1988",
  pages = "157-182",
  publisher = "Springer",
  isbn = "0-387-18928-9",
  abstract =
    "Scratchpad II is an abstract datatype language and system that is
    under development in the Computer Algebra Group, Mathematical Sciences
    Department, at the IBM Thomas J. Watson Research Center. Many
    different kinds of computational objects and data structures are
    provided. Facilities for computation include symbolic integration,
    differentation, factorization, solution of equations and linear
    algebra. Code economy and modularity is achieved by having polymorphic
    packages of functions that may create datatypes. The use of categories
    makes these facilities as general as possible.",
  keywords = "axiomref",
  beebe = "Jenks:1988:SIA"
}

@article{Jenk88d,
  author = "Jenks, Richard D.",
  title = {{Scratchpad II: A computer algebra language and system}},
  journal = "The Journal of the Acoustical Society of America",
  year = "1988",
  volume = "83",
  number = "S1",
  pages = "S106",
  abstract =
    "The Scratchpad II system represents a new generation of systems for
    doing symbolic mathematics, based on modern algebra and abstract data
    types. A large number of facilities are provided, for example:
    symbolic integration, ``infinite'' power series, differential operators,
    Cartesian tensors, and solution of nonlinear systems. Scratchpad II
    has been designed from the outset to be extendible. The system
    introduces a new data abstraction notion, the ``category,'' to express
    intricate interrelationships between data types. The result design
    permits the compilation of algorithms described in their most natural
    mathematical setting. The use of categories guarantees user defined
    types and packages are compatible with each other and with built in
    facilities. This system provides a single high‐level language with an
    intepreter and compiler. The language can be used by the naive user
    for convenient interactive mathematics calculations and by the
    advanced user for the efficient implementation of
    algorithms. Scratchpad II is built on Lisp/VM and runs on IBM/370
    class mainframes. An implementation of the system on the RT/PC is
    expected soon.",
  keywords = "axiomref"
}  

@book{Jenk92,
  author = "Jenks, Richard D. and Sutor, Robert S.",
  title = {{AXIOM: The Scientific Computation System}},
  publisher = "Springer-Verlag, Berlin, Germany",
  year = "1992",
  isbn = "0-387-97855-0",
  keywords = "axiomref",
  beebe = "Jenks:1992:ASC"
}

@InProceedings{Jenk94,
  author = "Jenks, Richard D. and Trager, Barry M.",
  title = {{How to make AXIOM into a Scratchpad}},
  booktitle = "Proceedings of the ACM-SIGSAM 1989 International
           Symposium on Symbolic and Algebraic Computation, ISSAC '94",
  series = "ISSAC 94",
  year = "1994",
  pages = "32-40",
  isbn = "0-89791-638-7",
  publisher = "ACM Press",
  address = "New York, NY, USA",
  abstract =
    "Scratchpad [GrJe71] was a computer algebra system developed in the
    early 1970s.  Like M\&M (Maple [CGG91ab] and Mathematical [W01S92]) and
    other systems today, Scratchpad had one principal representation for
    mathematical formulae based on ``expression trees''.  Its user interface
    design was based on a pattern-matching paradigm with infinite rewrite-
    rule semantics, providing what we believe to be the most natural
    paradigm for interactive symbolic problem solving.  Like M\&M, however,
    user programs were interpreted, often resulting in poor performance
    relative to similar facilities coded in standard programming languages
    such as FORTRAN and C.  
    
    Scratchpad development stopped in 1976 giving way to a new system
    design ([JenR79], [JeTr81]) that evolved into AXIOM [JeSu92].
    AXIOM has a strongly-typed programming language for building a library
    of parameterized types and algorithms, and a type-inferencing
    interpreter that accesses the library and can build any of an infinite
    number of types for interactive use.
    
    We suggest that the addition of an expression tree type to AXIOM can
    allow users to operate with the same freedom and convenience of
    untyped systems without giving up the expressive power and run-time
    efficiency provided by the type system.  We also present a design that
    supports a multiplicity of programming styles, from the Scratchpad
    pattern-matching paradigm to functional programming to more
    conventional procedural programming.  The resulting design seems to us
    to combine the best features of Scratchpad with current AXIOM and to
    offer a most attractive, flexible, and user-friendly environment for
    interactive problem solving.
    
    Section 2 is a discussion of design issues contrasting AXIOM with
    other symbolic systems.  Sections 3 and 4 is an assessment of AXIOM’s
    current design for building libraries and interactive use.  Section 5
    describes a new interface design for AXIOM, its resulting paradigms,
    and its underlying semantic model.  Section 6 compares this work with
    others.",
  paper = "Jenk94.pdf",
  keywords = "axiomref",
  beebe = "Jenks:1994:HMA"
}

@phdthesis{Joha14,
  author = "Johansson, Fredrik",
  title = {{Fast and Rigorous Computation of Special Functions to High
           Precision}},
  school = "Johannes Kepler University, Linz, Austria RISC",
  year = "2014",
  abstract = 
    "The problem of efficiently evaluating special functions to high
    precision has been considered by numerous authors. Important tools
    used for this purpose include algorithms for evaluation of linearly
    recurrent sequences, and algorithms for power series arithmetic.

    In this work, we give new baby-step, giant-step algorithms for
    evaluation of linearly recurrent sequences involving an expensive
    parameter (such as a high-precision real number) and for computing
    compositional inverses of power series. Our algorithms do not have the
    best asymptotic complexity, but they are faster than previous
    algorithms in practice over a large input range.

    Using a combination of techniques, we also obtain efficient new
    algorithms for numerically evaluating the gamma function $\Gamma(z)$
    and the Hurwitz zeta function $\zeta(s,a)$, or Taylor series
    expansions of those functions, with rigorous error bounds. Our methods
    achieve softly optimal complexity when computing a large number of
    derivatives to proportionally high precision.

    Finally, we show that isolated values of the integer partition
    function $p(n)$ can be computed rigorously with softly optimal
    complexity by means of the Hardy-Ramanujan-Rademacher formula and
    careful numerical evaluation.

    We provide open source implementations which run significantly faster
    than previous published software. The implementations are used for
    record computations of the partition function, including the
    tabulation of several billion Ramanujan-type congruences, and of
    Taylor series associated with the Reimann zeta function.",
  paper = "Joha14.pdf"
}

@article{Joha02,
  author = "Johansson, Leif and Lambe, Larry and Skoldberg, Emil",
  title = {{On Constructing Resolutions over the Polynomial Algebra}},
  journal = "Homology, Homotopy and Applications",
  volume = "4",
  number = "2",
  year = "2002",
  pages = "315-336",
  link = "\url{http://projecteuclid.org/download/pdf\_1/euclid.hha/1139852468}",
  abstract =
    "Let $k$ be a field, and $A$ be a polynomial algebra over $k$. 
    Let $I \subseteq A$ be an ideal. We present a novel method for
    computing resolutions of $A/I$ over $A$. The method is a synthesis
    of Groebner basis techniques and homological perturbation theory.
    The examples in this paper were computed using computer algebra.",
  paper = "Joha02.pdf",
  keywords = "axiomref"
}

@article{John94,
  author = "Johnson, M.E. and Rogers, C. and Schief, W.K. and Seiler, W.M.",
  title = {{On moving pseudospherical surfaces: a generalised Weingarten
           system and its formal analysis}},
  journal = "Lie Groups Appl.",
  volume = "1",
  pages = "124-136",
  year = "1994",
  abstract =
    "The connection between the motion of certain curves in $\mathbb{R}^3$ 
    and $1+1$-dimensional soliton equations is by now well-established. On the
    other hand, the sine-Gordon and other integrable equations may be
    readily derived via the classical geometry of stationary
    pseudospherical surfaces. Here, the motion of pseudospherical surfaces
    $S$ is considered in a natural orthonormal triad formulation. In one case,
    in a motion in which the Gaussian curvature of $S$ remains constant in
    time, an integrable nonlinear evolution equation is derived which has
    its origin in the description of wave propagation in an anharmonic
    crystal. In a second case, wherein the Gaussian curvature is allowed
    to vary in time, a classical generalised Weingarten system is derived
    in connection with the purely normal propagation of a pseudospherical
    surface. This is linked to triply orthogonal coordinate systems of
    Bianchi type. The generalised Weingarten system incorporates an
    integrable $2+1$-dimensional sine-Gordon equation. The arbitrariness of the
    solutions of the generalised Weingarten system is determined via a
    completion procedure.",
  keywords = "axiomref"
}

@article{Joll13,
  author = "Jolly, Raphael",
  title = {{Category as Type Classes in the Scala Algebra System}},
  journal = "LNCS",
  pages = "209-218",
  year = "2013",
  abstract =
    "A characterization of the categorical view of computer algebra is
    proposed. Some requirements on the ability for abstraction that
    programming languages must have in order to allow a categorical
    approach is given. Object-oriented inheritance is presented as a
    suitable abstraction scheme and exemplified by the Java Algebra
    System. Type classes are then introduced as an alternative abstraction
    scheme and shown to be eventually better suited for modeling
    categories. Pro and cons of the two approaches are discussed and a
    hybrid solution is exhibited.",
  paper = "Joll13.pdf",
  keywords = "axiomref"
}

@misc{Joyn08,
  author = "Joyner, David and Stein, William",
  title = {{Open Source Mathematical Software: A White Paper}},
  year = "2008",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.124.7499}",
  abstract =
    "Open source software has had a profound effect on computing during
    the last decade, especially on web servers (Apache), web browsers 
    (Firefox), operating systems (Linux and OS X), and programming
    languages (GC C, Java, Python, Perl, etc.). The purpose of this paper
    is to put forward the case that open source development methodologies
    might also have a positive effect on mathematical software,
    especially if the National Science Foundation (NSF) increases their
    support of open source mathematical software de velopment.  We argue
    that careful funding of open source mathematical software may lead to
    a lower total cost of ownership in the research and education 
    community, and to more efficient and trustworthy mathematical software.",
  paper = "Joyn08.pdf",
  keywords = "axiomref"
}

@misc{Joyn16,
  author = "Joyner, David",
  title = {{Links to some open source mathematical programs}},
  link = "\url{http://www.opensourcemath.org/opensource\_math.html}",
  keywords = "axiomref"
}

@article{Joyn08a,
  author = "Joyner, David",
  title = {{Open source computer algebra systems: Axiom}},
  journal = "ACM Commun. Comput. Algebra",
  volume = "42",
  number = "1-2",
  pages = "39-47",
  year = "2008",
  abstract =
    "This survey will look at Axiom, a free and very powerful computer
    algebra system available. It is a general purpose CAS useful for
    symbolic computation, research, and the development of new
    mathematical algorithms. Axiom is similar in some ways to Maxima,
    covered in the survey, but different in many ways as well. Axiom,
    Maxima, and SAGE, are the largest of the general-purpose open-source
    CASs.",
  keywords = "axiomref",
  beebe = "Joyner:2008:OSC"
}

@inproceedings{Kajl92,
  author = "Kajler, Norbert",
  title = {{CAS/PI: a Portable and Extensible Interface for Computer
           Algebra Systems}},
  year = "1992",
  booktitle = "Proc. ISSAC 1992",
  series = "ISSAC 1992",
  pages = "376-386",
  isbn = "0-89791-489-9 (soft cover) 0-89791-490-2 (hard cover)",
  abstract = 
    "CAS/$\pi$ is a Computer Algebra System graphic user interface
    designed to be highly portable and extensible. It has been developed
    by composition of pre-existing software tools such as Maple, Sisyphe,
    or Ulysse systems, ZicVis 3-D plotting library, etc, using control
    integration technology and a set of high level graphic toolkits to
    build the formula editor and the dialog manager. The main aim of
    CAS/$\pi$ is to allow a wide range of runtime recon gurations and
    extensions. For instance, it is possible to add new tools to a running
    system, to modify connections between working tools, to extend the set
    of graphic symbols managed by the formula editor, to design new high
    level editing commands based on the syntax or semantics of
    mathematical formulas, to customize and extend the menu-button based
    user interface, etc. More generally, CAS/$\pi$ can be seen equally as
    a powerful system-independent graphic user interface enabling
    inter-systems communications, a toolkit to allow fast development of
    custom-made scientific software environments, or a very convenient
    framework for experimenting with computer algebra systems protocols
    and man-machine interfaces.",
  paper = "Kajl92.pdf",
  keywords = "axiomref"
}

@article{Kajl94,
  author = "Kajler, Norbert and Soiffer, Neil",
  title = {{Some human interaction issues in computer algebra}},
  journal = "SIGSAM Bulletin",
  volume = "28",
  number = "1",
  pages = "18-28",
  year = "1994",
  abstract =
    "This paper addresses some of the current issues concerning the
    improvement of user interfaces for computer algebra systems. Some
    state of the art commercial software as well as research prototypes
    are presented, followed by a description of present research
    directions.",
  keywords = "axiomref"
}

@misc{Kani16,
  author = "Kanigel, Robert",
  title = {{OldQuotes}},
  link = "\url{http://www.oldquotes.com}",
  year = "2016",
  abstract =
    "Sometimes in studying Ramanujan's work, George Andrews said at
    another time, ``I have wondered how much Ramanujan could have done if
    he had had MACSYMA or SCRATCHPAD or some other symbolic algebra package''"
}

@book{Kapl05,
  author = "Kaplan, Michael",
  title = {{Computer Algebra}},
  publisher = "Springer, Berlin, Germany",
  year = "2005",
  isbn = "3-540-21379-1",
  comment = "German Language",
  keywords = "axiomref"
}

@misc{Kapu81,
  author = "Kapur, D. and Musser, D.R. and Stepanov, A.A.",
  title = {{Operators and Algebraic Structures}},
  link = "\url{http://www.stepanovpapers.com/p59-kapur.pdf}",
  year = "1981",
  abstract =
    "Operators in functional languages such as APL and FFP are a useful
    programming concept.  However, this concept cannot be fully
    exploited in these languages because of certain constraints.  It is
    proposed that an operator should be associated with a structure having
    the algebraic properties on which the operator's behavior depends.
    This is illustrated by introducing a language that provides mechanisms
    for defining structures and operators on them.  Using this language,
    it is possible to describe algorithms abstractly, thus emphasizing
    the algebraic properties on which the algorithms depend.  The role
    that formal representation of mathematical knowledge can play in the
    development of programs is illustrated through an example.  An
    approach for associating complexity measures with a structure and
    operators is also suggested.  This approach is useful in analyzing
    the complexity of algorithms in an abstract setting.",
  paper = "Kapu81.pdf"
}

@inproceedings{Kaue08,
  author = "Kauers, Manuel",
  title = {{Integration of Algebraic Functions: A Simple Heuristic for 
           Finding the Logarithmic Part}},
  booktitle = "Proc ISSAC 2008",
  series = "ISSAC '08",
  year = "2008",
  pages = "133-140",
  isbn = "978-1-59593-904",
  link =
     "\url{http://www.risc.jku.at/publications/download/risc_3427/Ka01.pdf}",
  abstract = 
    "A new method is proposed for finding the logarithmic part of an
    integral over an algebraic function. The method uses Groebner bases
    and is easy to implement. It does not have the feature of finding a
    closed form of an integral whenever there is one. But it very often
    does, as we will show by a comparison with the built-in integrators of
    some computer algebra systems.",
  paper = "Kaue08.pdf",
  keywords = "axiomref"
}

@inproceedings{Kead93,
  author = "Keady, G. and Nolan, G.",
  title = {{Production of Argument SubPrograms in the AXIOM -- NAG link: 
           examples involving nonlinear systems}},
  booktitle = "Proc. Workshop on Symbolic and Numeric Computation",
  location = "Helsinki",
  year = "1993",
  pages = "13-32",
  comment = "NAG Technical Report TR1/94",
  link = "\url{school.maths.uwa.edu.au/%7Ekeady/KeadyPapers/93Helsinki.ps}",
  algebra = 
   "\newline\refto{domain ASP1 Asp1}
    \newline\refto{domain ASP10 Asp10}
    \newline\refto{domain ASP12 Asp12}
    \newline\refto{domain ASP19 Asp19}
    \newline\refto{domain ASP20 Asp20}
    \newline\refto{domain ASP24 Asp24}
    \newline\refto{domain ASP27 Asp27}
    \newline\refto{domain ASP28 Asp28}
    \newline\refto{domain ASP29 Asp29}
    \newline\refto{domain ASP30 Asp30}
    \newline\refto{domain ASP31 Asp31}
    \newline\refto{domain ASP33 Asp33}
    \newline\refto{domain ASP34 Asp34}
    \newline\refto{domain ASP35 Asp35}
    \newline\refto{domain ASP4 Asp4}
    \newline\refto{domain ASP41 Asp41}
    \newline\refto{domain ASP42 Asp42}
    \newline\refto{domain ASP49 Asp49}
    \newline\refto{domain ASP50 Asp50}
    \newline\refto{domain ASP55 Asp55}
    \newline\refto{domain ASP6 Asp6}
    \newline\refto{domain ASP7 Asp7}
    \newline\refto{domain ASP73 Asp73}
    \newline\refto{domain ASP74 Asp74}
    \newline\refto{domain ASP77 Asp77}
    \newline\refto{domain ASP78 Asp78}
    \newline\refto{domain ASP8 Asp8}
    \newline\refto{domain ASP80 Asp80}
    \newline\refto{domain ASP9 Asp9}",
  abstract =
    "Dewar's paper [6] earlier in this Proceedings 'sketches out the
    design of the AXIOM-NAG link' and gives a general account of new tools
    for generating Fortran. This paper is a sequel to [6]. Here we present
    'examples' of some of the items discussed in [6]. We have attempted to
    achieve some coherence by selecting our 'examples' from just the one
    application area - solving nonlinear systems.",
  paper = "Kead93.pdf",
  keywords = "axiomref",
  beebe = "Keady:1994:PAS"
}

@phdthesis{Kels99,
  author = "Kelsey, Tom",
  title = {{Formal Methods and Computer Algebra: A Larch Specification of 
          AXIOM Categories and Functors}},
  school = "University of St Andrews",
  year = "1999",
  keywords = "axiomref"
}

@misc{Kels00a,
  author = "Kelsey, Tom",
  title = {{Formal specification of computer algebra}},
  abstract = 
    "We investigate the use of formal methods languages and tools in the
    design and development of computer algebra systems (henceforth CAS).
    We demonstrate that errors in CAS design can be identified and
    corrected by the use of (i) abstract specifications of types and
    procedures, (ii) automated proofs of properties of the specifications,
    and (iii) interface specifications which assist the verification of
    pre- and post conditions of implemented code.",
  paper = "Kels00a.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@article{Kend01,
  author = "Kendall, Wilfrid S.",
  title = {{Symbolic It\^o calculus in AXIOM: an ongoing story}},
  journal = "Statistics and Computing",
  volume = "11",
  pages = "25-35",
  year = "2001",
  link = "\url{http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/kendall/personal/ppt/327.ps.gz}",
  abstract =
    "Symbolic It\^o calculus refers both to the implementation of the
    It\^o calculus algebra package and to its application. This article
    reports on progress in the implementation of It\^o calculus in the
    powerful and innovative computer algebra package AXIOM, in the context
    of a decade of previous implementations and applications. It is shown
    how the elegant algebraic structure underylying the expressive and
    effective formalism of It\^o calculus can be implemented directly in
    AXIOM using the package's programmable facilities for ``strong
    typing'' of computational objects.  An application is given of the use
    of the implementation to provide calculations for a new proof, based
    on stochastic differentials, of the Mardia-Dryden distribution from
    statistical shape theory.",
  paper = "Kend01.pdf",
  keywords = "axiomref, CAS-Proof, printed",
  beebe = "Kendall:2001:SIC"
}

@article{Kend07,
  author = "Kendall, Wilfrid S.",
  title = {{Coupling all the Levy Stochastic Areas of Multidimensional
           Brownian Motion}},
  journal = "The Annals of Probability",
  volume = "35",
  number = "3",
  pages = "935-953",
  year = "2007",
  comment = "Author used Axiom for computation but says missed citation",
  link = "\url{http://arxiv.org/pdf/math/0512336v2.pdf}",
  abstract =
    "It is shown how to construct a successful co-adapted coupling of two
    copies of an $n$-dimensional Brownian motion ($B_1,\ldots,B_n$) while
    simultaneously coupling all corresponding copies of the L{\'e}vy
    stochastic areas $\int B_idB_j - \int B_j dB_i$. It is conjectured
    that successful co-adapted couplings still exist when the L{\'e}vy
    stochastic areas are replaced by a finite set of multiply iterated
    path- and time-integrals, subject to algebraic compatibility of the
    initial conditions.",
  paper = "Kend07.pdf",
  keywords = "axiomref"
}

@misc{Kenn01,
  author = "Kennedy, A.D.",
  title = {{Semantics of Categories in Aldor}},
  year = "2001",
  abstract =
    "We consider some questions about the semantics of Aldor regarding the
    way that types can be considered on an equal footing with any other
    objects in the language. After a digression into the relationship
    between Aldor categories and mathematical categories, we shall discuss
    the more practical issue of the limitations of the {\tt define}
    keyword and what the compiler should do about them.",
  paper = "Kenn01.pdf"
}

@article{Kerb98,
  author = "Kerber, Manfred and Kohlhase, Michael and Volker, Sorge",
  title = {{Integrating computer algebra into proof planning}},
  journal = "J. Autom. Reasoning",
  volume = "21",
  number = "3",
  year = "1998",
  pages = "327-355",
  link =
   "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.40.3914}",
  abstract =
    "Mechanized reasoning systems and computer algebra systems have
    different objectives. Their integration is highly desirable, since
    formal proofs often involve both of the two different tasks proving
    and calculating. Even more important, proof and computation are often
    interwoven and not easily separable.

    In this article, we advocate an integration of computer algebra into
    mechanized reasoning systems at the proof plan level. This approach
    allows us to view the computer algebra algorithms as methods, that is,
    declarative representations of the problem-solving knowledge specific
    to a certain mathematical domain. Automation can be achieved in many
    cases by searching for a hierarchic proof plan at the method level by
    using suitable domain-specific control knowledge about the
    mathematical algorithms. In other words, the uniform framework of
    proof planning allows us to solve a large class of problems that are
    not automatically solvable by separate systems.

    Our approach also gives an answer to the correctness problems inherent
    in such an integration. We advocate an approach where the computer
    algebra system produces high-level protocol information that can be
    processed by an interface to derive proof plans. Such a proof plan in
    turn can be expanded to proofs at different levels of abstraction, so
    the approach is well suited for producing a high-level verbalized
    explication as well as for a low-level, machine-checkable,
    calculus-level proof. We present an implementation of our ideas and
    exemplify them using an automatically solved example.",
  paper = "Kerb98.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@phdthesis{King70,
  author = "King, James Cornelius",
  title = {{A Program Verifier}},
  school = "Carnegie Mellon University",
  year = "1970"
}

@article{Kocb96,
  author = "Kocbach, Ladislav and Liska, Richard",
  title = {{Generation and Verification of Algorithms for Symbolic\_Numeric
           Processing}},
  journal = "J. Symbolic Computation",
  volume = "11",
  pages = "1-16",
  year = "1996",
  abstract =
    "Some large scale physical computations require algorithms performing
    symbolic computations with a particular class of algebraic formulas in
    a numerical code. Developing and implementing such algorithms in a
    numerical programming language is a tedious and error prone task. The
    algorithms can be developed in a computer algebra system and their
    correctness can be checked by comparison with built-in facilities of
    the system so that the system is used as an advanced debugging
    tool. After that a numerical code for the algorithms is automatically
    generated from the same source code. The proposed methodolgy is
    explained in detail on a simple example. Real applications to
    calculation of matrix elements of Coulomb interaction and two-centre
    exchange integrals needed in atomic collision codes, are
    described. The method makes the developing and debugging of such
    algorithms easier and faster.",
  paper = "Kocb96.pdf",
  keywords = "axiomref"
}

@article{Koep96,
  author = "Koepf, Wolfram",
  title = {{Closed form Laurent-Puiseux series of algebraic functions}},
  journal = "Appl. Algebra Eng. Commun. Comput.",
  volume = "7",
  number = "1",
  pages = "21-26",
  year = "1996",
  abstract =
    "Let $y$ be an algebraic function defined by a polynomial equation
    $P(x,y)=0$ whose coefficients are polynomials in $x$ over a field $K$
    which may be one of the fields $\mathbb{Q}$, $\mathbb{R}$, or
    $\mathbb{C}$.  D. V. and G. V. Chudnovsky [J. Complexity 2, 271-294
    (1986; Zbl 0629.68038); ibid. 3, 1-25 (1987; Zbl 0656.34003)] describe
    a pair of algorithms to calculate the coefficients in the
    Laurent-Puiseux developments of the branches of $y$: The first
    algorithm returns a linear differential equation
    \[q_n(x)y^{(n)} + y_{n-1}(x)y^{(n-1)}+\cdots+q_1(x)y^{'}+q_0(x)y=0\]
    which is satisfied by all branches of $y$ and whose coefficients are
    polynomials in $x$ over $K$, the other uses this differential equation
    to get a linear recurrence relation for the Puiseux coefficients. The
    author used this algorithms (the second in a simpler version) to
    calculate the recurrence relation; if this relation contains only two
    terms, an algorithm found by the author returns an explicit formula
    for the Puiseux coefficients [J. Symb. Comp. 13, 581-603 (1992; Zbl
    0758.30026)].  In this paper, the author gives examples to illustrate
    his algorithms and to show that for many algebraic functions defined
    by polynomials of low degree a closed form of their Puiseux
    coefficients may be calculated. He points out that on the other side
    the complexity of the resulting recurrence equation may be extremely
    high even for an algebraic function defined by a sparse polynomial of
    low degree.",
  keywords = "axiomref"
}

@InProceedings{Koep99,
  author = "Koepf, Wolfram",
  title = {{Orthogonal polnomials and computer algebra}},
  booktitle = "Recent developments in complex analysis and computer algebra",
  series = "ISSAC 97",
  year = "1999",
  publisher = "Kluwer Adademic Publishers",
  location = "Newark, DE",
  pages = "205-234",
  abstract =
    "Orthogonal polynomials have a long history, and are still important
    objects of consideration in mathematical research as well as in
    applications in Mathematical Physics, Chemistry, and
    Engineering. Quite a lot is known about them. Particularly well-known
    are differential equations, recurrence equations, Rodrigues formulas,
    generating functions and hypergeometric representations for the
    classical systems of Jacobi, Laguerre and Hermite which can be found
    in mathematical dictionaries. Less well-known are the corresponding
    representations for the classical discrete systems of Hahn,
    Krawtchouk, Meixner and Charlier, as well as addition theorems,
    connection relations between different systems and other identities
    for these and other systems of orthogonal polynomials. The ongoing
    research in this still very active subject of mathematics expands the
    knowledge database about orthogonal polynomials continuously. In the
    last few decades the classical families have been extended to a rather
    large collection of polynomial systems, the so-called Askey-Wilson
    scheme, and they have been generalized in other ways as well.

    Recently new algorithmic approaches have been discovered to compute
    differential, recurrence and similar equations from series or integral
    representations. These methods turn out to be quite useful to prove or
    detect identities for orthogonal polynomial systems. Further
    algorithms to detect connection coefficients or to identify polynomial
    systems from given recurrence equations have been developed. Although
    some algorithmic methods had been known already in the last century,
    their use was rather limited due to the immense amount of
    calculations. Only the existence and distribution of computer algebra
    systems makes their use simple and useful for everybody.

    In this plenary lecture an overview is given of how algorithmic
    methods implemented in computer algebra systems can be used to prove
    identities about and to detect new knowledge for orthogonal
    polynomials and other hypergeometric type special functions.
    Implementations for this type of algorithms exist in Maple,
    Mathematica and REDUCE, and maybe also in other computer algebra
    systems. Online demonstrations will be given using Maple V.5.",
  paper = "Koep99.pdf",
  keywords = "axiomref"
}

@article{Koep99a,
  author = "Koepf, Wolfram",
  title = {{Software for the algorithmic work with orthogonal polynomials
           and special functions}},
  journal = "Electron. Trans. Numer. Anal.",
  volume = "9",
  year = "1999",
  link = "\url{http://arxiv.org/pdf/math/9809125v1.pdf}",
  abstract =
    "An overview of the MAPLE routines that can be used for hypergeometric
    and basic hypergeometric series with some discussion of how and why
    they work.",
  paper = "Koep99a.pdf",
  keywords = "axiomref"
}

@misc{Koep14,
  author = "Koepf, Wolfram",
  title = {{Methods of Computer Algebra for Orthogonal Polynomials}},
  year = "2014",
  location = "Rutgers, NJ, USA",
  link = "\url{http://www.mathematik.uni-kassel.de/~koepf/Vortrag/2014-Zeilberger-Vortrag.pdf}",
  video1 = "https://vimeo.com/85573338",
  video2 = "https://vimeo.com/85573712",
  website = "http://www.caop.org",
  paper = "Koep14.pdf"
}

@article{Kosl91,
  author = "Koseleff, P.-V.",
  title = {{Word games in free Lie algebras: several bases and formulas}},
  journal = "Theoretical Computer Science",
  volume = "79",
  number = "1",
  pages = "241-256",
  year = "1991",
  abstract =
    "The author compares the efficiency of many methods which allow
    calculations in Lie algebras. Many construction methods exist for the
    base of free Lie algebras developed from finite sets. They use two
    algorithms for calculation of several Campbell-Hausdorf formulas.
    Diverse implementations are realised in LISP on Scratchpad II",
  keywords = "axiomref",
  beebe = "Koseleff:1991:WGF"
}

@article{Kred90,
  author = "Kredel, Heinz",
  title = {{MAS Modula-2 Algebra System}},
  journal = "LNCS",
  volume = "429",
  pages = "270-271",
  year = "1990",
  keywords = "axiomref"
}

@article{Kred08,
  author = "Kredel, Heinz",
  title = {{On a Java computer algebra system, 
           its performance and applications}},
  journal = "Sci. Comput. Program.",
  volume = "70",
  number = "2-3",
  pages = "185-207",
  year = "2008",
  link = "\url{http://ac.els-cdn.com/S0167642307001736/1-s2.0-S0167642307001736-main.pdf}",
  abstract =
    "This paper considers Java as an implementation language for a
    starting part of a computer algebra library. It describes a design of
    basic arithmetic and multivariate polynomial interfaces and classes
    which are then employed in advanced parallel and distributed Groebner
    base algorithms and applications. The library is type-safe due to its
    design with Java's generic type parameters and thread-safe using
    Java's concurrent programming facilities. We report on the performance
    of the polynomial arithmetic and on applications built upon the core
    library.",
  paper = "Kred08.pdf",
  keywords = "axiomref"
}

@InProceedings{Kred07,
  author = "Kredel, Heinz",
  title = {{Evaluation of a Java computer algebra system}},
  booktitle = "Computer Mathematics, 8th Asian symposium",
  series = "ASCM 2007",
  year = "2007",
  isbn = "978-3-540-87826-1",
  location = "Singapore",
  pages = "121-138",
  link = "\url{http://krum.rz.uni-mannheim.de/kredel/oocas-casc2010-slides.pdf}",
  abstract =
    "This paper evaluates the suitability of Java as an implementation
    language for the foundations of a computer algebra library. The design
    of basic arithmetic and multivariate polynomial interfaces and classes
    have been presented. The library is type-safe due to its design with
    Java's generic type parameters and thread-safe using Java's concurrent
    programming facilities. We evaluate some key points of our library and
    differences to other computer algebra systems.",
  paper = "Kred07.pdf",
  keywords = "axiomref"
} 

@InProceedings{Kred10,
  author = "Kredel, Heinz and Jolly, Raphael",
  title = {{Generic, type-safe and object oriented computer algebra software}},
  booktitle = "Proc. 12th International Workshop",
  series = "CASC 2010",
  year = "2010",
  isbn = "978-3-642-15273-3",
  location = "Berlin",
  pages = "162-177",
  link = "\url{http://krum.rz.uni-mannheim.de/kredel/oocas-casc2010-slides.pdf}",
  abstract = 
    "Advances in computer science, in particular object oriented
    programming, and software engineering have had little practical impact
    on computer algebra systems in the last 30 years. The software design
    of existing systems is still dominated by ad-hoc memory management,
    weakly typed algorithm libraries and proprietary domain specific
    interactive expression interpreters. We discuss a modular approach to
    computer algebra software: usage of state-of-the-art memory management
    and run-time systems (e.g. JVM) usage of strongly typed, generic,
    object oriented programming languages (e.g. Java) and usage of general
    purpose, dynamic interactive expression interpreters (e.g. Python). To
    illuatrate the workability of this approach, we have implemented and
    studied computer algebra systems in Java and Scala. In this paper we
    report on the current state of this work by presenting new examples.",
  paper = "Kred10.pdf",
  keywords = "axiomref"
}

@book{Kred11,
  author = "Kredel, Heinz",
  title = {{Unique factorization domains in the Java computer algebra system}},
  year = "2011",
  publisher = "Springer",
  booktitle = "Automated deduction in geometry (ADG 2008)",
  pages = "86-115",
  isbn = "978-3-642-21045-7",
  abstract =
    "This paper describes the implementation of recursive algorithms in
    unique factorization domains, namely multivariate polynomial greatest
    common divisors (gcd) and factorization into irreducible parts in the
    Java computer algebra library (JAS). The implementation of gcds,
    resultants and factorization is part of the essential building blocks
    for any computation in algebraic geometry, in particular in automated
    deduction in geometry. There are various implementations of these
    algorithms in procedural programming languages. Our aim is an
    implementation in a modern object oriented language with generic data
    types, as it is provided by Java programming language. We exemplify
    that the type design and implementation of JAS is suitable for the
    implementation of several greatest common divisor algorithms and
    factorization of multivariate polynomials. Due to the design we can
    employ this package in very general settings not commonly seen in other
    computer algebra systems. As for example, in the coefficient
    arithmetic for advanced Groebner basis computations like in polynomial
    rings over rational function fields or (finite, commutative) regular
    rings. The new package provides factory methods for the selection of
    one of the several implementations for non experts. Further we
    introduce a parallel proxy for gcd implementations which runs
    different implementations concurrently.",
  keywords = "axiomref"
}

@InProceedings{Kred11a,
  author = "Kredel, Heinz and Jolly, Raphael",
  title = {{Algebraic structures as typed objects}},
  booktitle = "Proc. 13th International Workshop",
  series = "CASC 2011",
  year = "2011",
  isbn = "978-3-642-23567-2",
  location = "Berlin",
  pages = "294-308",
  link = "\url{http://krum.rz.uni-mannheim.de/kredel/to-cas-casc2011-slides.pdf}",
  abstract = 
    "Following the research direction of strongly typed, generic, object
    oriented computer algebra software, we examine the modeling of
    algebraic structures as typed objects in this paper. We discuss the
    design and implementation of algebraic and transcendental extension
    fields together with the modeling of real algebraic and complex
    algebraic extension fields. We will show that the modeling of the
    relation between algebraic and real algebraic extension fields using
    the delegation design concept has advantages over the modeling as
    sub-types using sub-class implementation. We further present a summary
    of design problems, which we have encountered so far with our
    implementation in Java and present possbile solutions in Scala.",
  paper = "Kred11a.pdf",
  keywords = "axiomref"
}

@book{Kreu14,
  author = "Kreuzer, Edwin",
  title = {{Computerized Symbolic Manipulation in Mechanics}},
  publisher = "Springer",
  year = "2014",
  abstract =
    "The aim of this book is to present important software tools, basic
    concepts, methods, and highly sophisticated applications of
    computerized symbolic manipulation to mechanics problems. An overview
    about general-purpose symbolic software is followed by general
    guidelines how to develop and implement high-quality computer algebra
    code. The theoretical background including modeling techniques for
    mechanical systems is provided which allows for the computer aided
    generation of the symbolic equation of motion for multibody
    systems. It is shown how the governing equations for different types
    of problems in structural mechanics can be automatically derived and
    how to implement finite element techniques via computer algebra
    software. Perturbation methods as a very powerful approach for
    nonlinear problems are discussed in detail and are demonstrated for a
    number of applications. The applications covered in this book
    represent some of the most advanced topics in the rapidly growing
    field of research on symbolic computation."
}

@misc{Krip94,
  author = "Kripfganz, Jochen and Perlt, Holger",
  title = {{Working with Mathematica. An Introduction with examples}},
  comment = "Arbeiten mit Mathematica. Eine Einfuhrung mit Beispielen",
  book = "Hander",
  year = "1994",
  keywords = "axiomref"
}

@article{Kuma00,
  author = "Kumar, P. and Pellegrino, S.",
  title = {{Computation of kinematic paths and bifurcation points}},
  journal = "Int. J. Solids Struct.",
  volume = "37",
  number = "46-47",
  pages = "7003-7027",
  year = "2000",
  abstract = 
    "This article deals with the kinematic simulation of movable
    structures that go through special configurations of kinematic
    bifurcation, as they move. A series of algorithms are developed for
    structures that can be modelled using pin-jointed bars and that admit
    a single-parameter motion. These algorithms are able to detect and
    locate any bifurcation points that exist along the path of the
    structure and, at each bifurcation point, can determine all possible
    motions of the structure. The theory behind the algorithms is
    explained, and the analysis of a simple example is discussed in
    detail. Then, a simplified version of the particular problem that had
    motivated this work, the simulation of the folding and deployment of a
    thin membrane structure forming a solar sail, is analysed. For the
    particular cases that are considered, it is found that the entire
    process is inextensional, but a detailed study of the simulation
    results shows that in more general cases, it is likely that stretching
    or wrinkling will occur.",
  keywords = "axiomref"
}

@inproceedings{Kusc89,
  author = "Kusche, K. and Kutzler, B. and Mayr, H.",
  title = {{Implementation of a geometry theorem proving package 
           in SCRATCHPAD II}},
  booktitle = "Proc. of Eurocal '87",
  series = "Lecture Notes in Computer Science 378",
  pages = "246-257",
  isbn = "3-540-51517-8",
  year = "1987",
  abstract = 
    "The problem of automatically proving geometric theorems has gained a
    lot of attention in the last two years. Following the general approach
    of translating a given geometric theorem into an algebraic one,
    various powerful provers based on characteristic sets and Groebner
    bases have been implemented by groups at Academia Sinica Bejing
    (China), U. Texas at Austin (USA), General Electric Schenectady (USA),
    and Research Institute for Symbolic Computation Linz (Austria). So ar,
    fair comparisons of the various provers were not possible, because the
    underlying hardware and the underlying algebra systems differed
    greatly. This paper reports on the first uniform implementation of all
    of these provers in the computer algebra system and language
    SCRATCHPAD II. We summarize the recent achievements in the area of
    automated geometry theorem proving, shortly review the SCRATCHPAD II
    system, describe the implementation of the geometry theorem proving
    package, and finally give a computing time statistics of 24 examples.",
  keywords = "axiomref",
  beebe = "Kusche:1989:IGT"
}

@article{Lamb03a,
  author = "Lamban, Laureano and Pascual, Vico and Rubio, Julio",
  title = {{An object-oriented interpretation of the EAT system}},
  journal = "Appl. Algebra Eng. Commun. Comput.",
  volume = "14",
  number = "3",
  year = "2003",
  pages = "187-215",
  abstract =
    "In a previous paper we characterized, in the category theory setting,
    a class of implementations of abstract data types, which has been
    suggested by the way of programming in the EAT system. (EAT, Effective
    Algebraic Topology, is one of Sergeraert’s systems for effective
    homology and homotopy computation.) This characterization was
    established using classical tools, in an unrelated way to the current
    mainstream topics in the field of algebraic specifications. Looking
    for a connection with these topics, we have found, rather
    unexpectedly, that our approach is related to some object-oriented
    formalisms, namely hidden specifications and the coalgebraic view. In
    this paper, we explore these relations making explicit the implicit
    object-oriented features of the EAT system and generalizing the data
    structure analysis we had previously done.",
  keywords = "axiomref"
}

@article{Lamb89,
  author = "Lambe, Larry A.",
  title = {{Scratchpad II as a tool for mathematical research}},
  journal = "Notices of the AMS",
  year = "1989",
  pages = "143-147",
  keywords = "axiomref"
}

@article{Lamb91,
  author = "Lambe, Larry A.",
  title = {{Resolutions via homological perturbation}},
  journal = "Journal of Symbolic Computation",
  volume = "12",
  number = "1",
  pages = "71-87",
  year = "1991",
  abstract =
    "The purpose of this paper is to review an algorithm for computing
    ``small'' resolutions in homological algebra, to provide examples of
    its use as promised in [L1], [LS], and to illustrate the use of
    computer algebra in an area not usually associated with that
    subject. Comparison of the complexes produced by the method discussed
    here with those produced by other methods shows that the algorithm
    generalizes several other approaches, [GL], [GLS1], [GLS2], [BL], [BL2].  
    
    This is an expository note which is intended to help make homological
    perturbation theory more accesible and to encourage wider use of
    Computer Algebra in mathematical research.
    
    The class of objects presented here -- Finitely generated torsion-free
    nilpotent groups (of arbitrary nilpotency class) -- are given because
    of their simplicity. The examples point to the general phenomena that
    are to be expected when trying to derive complexes smaller than
    ``standard complexes'' in other homological contexts. The complexes
    produced are generally {\sl much} smaller than the bar construction, but
    larger than a {\sl minimal resolution}.",
  paper = "Lamb91.pdf",
  keywords = "axiomref",
  beebe = "Lambe:1991:RHP"
}

@article{Lamb92,
  author = "Lambe, Larry",
  title = {{Next Generation Computer Algebra Systems AXIOM and the 
           Scratchpad Concept: Applications to Research in Algebra}},
  publisher = "21st Nordic Congress of Mathematicians",
  journal = "unknown",
  year = "1992",
  link = "\url{http://axiom-wiki.newsynthesis.org/public/refs/axiom-21cong.pdf}",
  algebra = "\newline\refto{category FMCAT FreeModuleCat}",
  abstract = 
    "One way in which mathematicians deal with infinite amounts of data is
    symbolic representation. A simple example is the quadratic equation
    \[x = \frac{-b\pm\sqrt{b^2-4ac}}{2a}\]
    a formula which uses symbolic representation to describe the solutions
    to an infinite class of equations. Most computer algebra systems can
    deal with polynomials with symbolic coefficients, but what if symbolic
    exponents are called for (e.g. $1+t^i$)? What if symbolic limits on
    summations are also called for, for example
    \[1+t+\ldots+t^i=\sum_j{t^j}\]

    The ``Scratchpad Concept'' is a theoretical ideal which allows the
    implementation of objects at this level of abstraction and beyond in a
    mathematically consistent way. The Axiom computer algebra system is an
    implementation of a major part of the Scratchpad Concept.  Axiom
    (formerly called Scratchpad) is a language with extensible
    parameterized types and generic operators which is based on the
    notions of domains and categories. By examining some aspects of the
    Axiom system, the Scratchpad Concept will be illustrated. It will be
    shown how some complex problems in homologicial algebra were solved
    through the use of this system.",
  paper = "Lamb92.pdf",
  keywords = "axiomref, printed",
  beebe = "Lambe:1994:NGC"
}

@article{Lamb93a,
  author = "Lambe, Larry and Luczak, Richard",
  title = {{Object-Oriented Mathematical Programming and 
           Symbolic/Numeric Interface}},
  journal = "3rd Int. Conf. on Expert Systems in Numerical Computing",
  year = "1993",
  link = "\url{http://axiom-wiki.newsynthesis.org/public/refs/axiom-fem.pdf}",
  abstract = 
    "The Axiom language is based on the notions of ``categories'',
    ``domains'', and ``packages''. These concepts are used to build an
    interface between symbolic and numeric calculations. In particular, an
    interface to the NAG Fortran Library and Axiom's algebra and graphics
    facilities is presented. Some examples of numerical calculations in a
    symbolic computational environment are also included using the finite
    element method. While the examples are elementary, we believe that
    they point to very powerful methods for combining numeric and symbolic
    computational techniques.",
  paper = "Lamb93a.pdf",
  keywords = "axiomref"
}

@article{Lamb93b,
  author = "Lambe, Larry A. and Radford, David E.",
  title = {{Algebraic Aspects of the Quantum Yang-Baxter Equation}},
  journal = "Journal of Algebra",
  volume = "154",
  pages = "228-288",
  year = "1993",
  link = "\url{pages.bangor.ac.uk/~mas019/papers/lamrad.pdf}",
  abstract =
    "In this paper we examine a variety of algebraic contexts in which the
    quantum Yang-Baxter equation arises, and derive methods for generating
    new solutions from given ones. The solutions we describe are encoded
    in objects which have a module and a comodule structure over a
    bialgebra. Our work here is based in part on the ideas of [DR1,DR2].",
  paper = "Lamb93b.pdf",
  keywords = "axiomref"
}

@book{Lamb97,
  author = "Lambe, Larry A. and Radford, David E.",
  title = {{Introduction to the quantum Yang-Baxter equation and quantum
           groups: an algebraic approach}},
  booktitle = "Mathematics and its Applications",
  publisher = "Kluwer Adademic Publishers",
  year = "1997",
  abstract =
    "The quantum Yang-Baxter equation (QYBE) has roots in statistical
    mechanics and the inverse scattering method and leads to a natural
    construction of a bialgebra. It turns out to have important
    connections with knot theory and invariants of 3-manifolds. There are
    now available many reference books to quantum groups and these various
    applications. The book under review develops the algebraic
    underpinning and theory of the QYBE, including the constant form and
    the one and two parameter forms.
    
    We give a brief description of the chapters. Chapter 1 (together with
    an Appendix) gives the algebraic preliminaries involving coalgebras,
    bialgebras, Hopf algebras, modules and comodules. Chapter 2 introduces
    the various forms of the QYBE, and the basic algebraic structures
    associated to them, including Faddeev-Reshetikhin-Takhtadzhan (FRT)
    construction. Chapter 3 explores various categorical settings for the
    constant form of the QYBE, the most basic being the category of left
    QYB modules over a bialgebra and the notion of algebras, coalgebras,
    etc. in this category. Chapter 4 develops universal mapping properties
    of the FRT construction and its reduced version, and the authors
    investigate when the reduced FRT construction leads to a pointed
    bialgebra or a pointed Hopf algebra. Chapter 5 develops the quantum
    groups associated to $SL(2)$, i.e., the quantum universal enveloping
    algebra, and the quantum function algebra. Chapter 6 introduces
    quasitriangular Hopf algebras, and discusses how the
    finite-dimensional ones give rise to solutions of the QYBE through
    their representation theory. The most important example is the
    Drinfeld double of a finite-dimensional Hopf algebra. The authors note
    (through an exercise!) that every finite-dimensional Hopf algebra is
    the reduced FRT construction of some solution to the QYBE. Chapter 7
    introduces coquasitriangular bialgebras, the most important being the
    FRT and the reduced FRT constructions. There are some generalizations
    here to the one-parameter form of the QYBE. Chapter 8 uses all the
    previously developed techniques to find solutions of the QYBE in
    certain cases, including the one-parameter form. Some of these were
    discovered by computer algebra methods. The final chapter 9 gives a
    brief discussion of certain categorical constructions and the QYBE is
    certain fairly abstract categories, motivated by the fact that the FRT
    construction is a coend.
    
    This book fills an important niche in the literature involving the
    QYBE by highlighting the algebraic aspects and applications. Although
    this is basically a reference book, it includes so many important
    parts of the study of Hopf algebras that it could be used as a
    textbook for a certain type of course on Hopf algebras and quantum
    groups, and certainly as supplementary reading material for such a
    course. There are frequent exercises which would be useful for such
    purposes. Besides being a basic source book, the authors include some
    new results and some novel approaches to earlier results. All this
    makes this book a most welcome addition to the quantum group
    literature.",
  keywords = "axiomref"
}

@article{Lamb02,
  author = "Lambe, Larry A. and Seiler, Werner M.",
  title = {{Differential equations, Spencer cohomology, and computing 
           resolutions}},
  journal = "Georgian Mathematical Journal",
  volume = "9",
  number = "4",
  pages = "723-774",
  year = "2002",
  link = "\url{https://www.emis.de/journals/GMJ/vol9/v9n4-11.pdf}",
  abstract =
    "Spencer cohomology is one of the classical tools in the study of
    overdetermined systems of partial differential equations. The paper
    proposes a new point of view of the Spencer cohomology based on a dual
    approach via comodules that allows to relate the Spencer cohomology to
    standard constructions in homological algebra, and discuss concrete
    methods for its construction based on homological perturbation
    theory. It also gives a detailed introduction to the subject, and
    proposes a new algorithm of constructing the Spencer resolution by
    using symbolic computation systems.",
  paper = "Lamb02.pdf",
  keywords = "axiomref"
}

@article{Lamb03,
  author = "Lambe, Larry A. and Luczak, Richard and Nehrbass, John W.",
  title = {{A New Finite Difference Method for the Helmholtz Equation
           Using Symbolic Computation}},
  journal = "Int. J. Comp. Eng. Sci.",
  volume = "4",
  year = "2003",
  link = "\url{http://pages.bangor.ac.uk.~mas019/papers/lln.pdf}",
  abstract = 
    "A new finite difference method for the Helmholtz equation is
    presented. The method involves replacing the standard ``weights'' in the
    central difference quotients (Secs. 2.1, 2.2, and 2.3) by weights that
    are optimal in a sense that will be explained in the sections just
    mentioned. The calculation of the optimal weights involves some
    complicated and error-prone manipulations of integral formulas that is
    best done using computer-aided symbolic computation (SC). In addition,
    we discuss the important problem of interpolation involving meshes
    that have been refined in certain subregions. Analytic formulae are
    derived using SC for these interpolation schemes. Our results are
    discussed in Sec. 5. Some hints about the computer methods we used to
    accomplish these results are given in the Appendix. More information
    is available and access to that information is referenced.
    
    While we do not want to make SC the focus of this work, we also do not
    want to underestimate its value. Armed with robust and efficient SC
    libraries, a researcher can {\sl comfortably} and {\sl conveniently}
    experiment with ideas that he or she might not examine otherwise.",
  paper = "Lamb03.pdf",
  keywords = "axiomref"
}

@InProceedings{Laza93,
  author = "Lazard, Daniel",
  title = {{On the representation of rigid-body motions and its application
           to generalized platform manipulators}},
  booktitle = "Proc. Workshop Computational Kinematics",
  year = "1993",
  location = "Dagstuhl Castle, Germany",
  publisher = "Kluwer Academic Publishers",
  pages = "175-181",
  abstract =
    "Different ways for representing rigid body motions (direct isometries)
    by a computer are presented. It turns out that the choice between them
    may have a dramatic effect on the difficulty of a computation or of a
    proof. As an application, a computational proof is given of the fact
    that the direct kinematic problem for the generalized Stewart platform
    has at most 40 complex solutions.",
  keywords = "axiomref"
}

@inproceedings{LeBl91,
  author = "LeBlanc, S.E.",
  title = {{The use of MathCAD and Theorist in the ChE classroom}},
  booktitle = "Proc. ASEE Annual Meeting",
  year = "1991",
  pages = "287-299",
  abstract =
    "MathCAD and Theorist are two powerful mathematical packages available
    for instruction in the ChE classroom. MathCAD is advertised as an
    `electronic scratchpad' and it certainly lives up to its billing.  It
    is an extremely user-friendly collection of numerical routines that
    eliminates the drudgery of solving many of the types of problems
    encountered by undergraduate ChE's (and engineers in general). MathCAD
    is available for both the Macintosh and IBM PC compatibles. The PC
    version is available as a full-functioned student version for around
    US\$40 (less than many textbooks). Theorist is a symbolic mathematical
    package for the Macintosh. Many interesting and instructive things can
    be done with it in the ChE curriculum. One of its many attractive
    features includes the ability to generate high quality three
    dimensional plots that can be very instructive in examining the
    behavior of an engineering system. The author discusses the
    application and use of these packages in chemical engineering and give
    example problems and their solutions for a number of courses including
    stoichiometry, unit operations, thermodynamics and design.",
  keywords = "axiomref",
  beebe = "LeBlanc:1991:UMT"
}

@article{Lece02,
  author = "Lecerf, Gregoire",
  title = {{Quadratic Newton iteration for systems with multiplicity}},
  journal = "Found. Comput. Math.",
  volume = "2",
  number = "3",
  pages = "247-293",
  year = "2002",
  abstract =
    "The author proposes an efficient iterator with quadratic convergence
    that generalizes Newton iterator for multiple roots. It is based on an
    $m$-adic topology where the ideal $m$ can be chosen generic
    enough. Compared to the Newton iterator the proposed iterator
    introduces a small overhead that grows with the square of the
    multiplicity of the root.",
  paper = "Lece02.pdf",
  keywords = "axiomref"
}

@InProceedings{Lece10,
  author = "Lecerf, Gregoire",
  title = {{Mathemagix: toward large scale programming for symbolic and
           certified numeric computations}},
  booktitle = "Mathematical software",
  series = "ICMS 2010",
  year = "2010",
  isbn = "978-3-642-15581-9",
  location = "Berlin",
  pages = "329-332",
  abstract = 
    "Coordinated by Joris van der Hoeven from the 90's, the Mathemagix
    project aims at the design of a scientific programming language for
    symbolic and certified numeric algorithms. This language can be
    compiled and interpreted, and it features a strong type system with
    classes and categories. Several C++ libraries are also being
    developed, mainly with Bernard Mourrain and Philippe Trebuchet, for
    the elementary operations with polynomials, power series and matrices,
    with a special care towards efficiency and numeric stability.

    In my talk I will give an overview of the language, of the design and
    the contents of the C++ libraries, and I will illustrate possibilities
    offered for certified numeric computations with balls and intervals.",
  keywords = "axiomref"
}

@misc{Lece96,
  author = "Lecerf, Gregoire",
  title = {{Dynamic Evaluation and Real Closure Implementation in Axiom}},
  year = "1996",
  link = "\url{http://lecerf.perso.math.cnrs.fr/software/drc/drc.ps}",
  paper = "Lece96.pdf",
  keywords = "axiomref, printed"
}

@article{Leeu94,
  author = "van Leeuwen, Andre M.A.",
  title = {{LiE, A software package for Lie group computations}},
  journal = "Euromath Bulletin",
  volume = "1",
  number = "2",
  year = "1994",
  abstract =
    "A description is given of LiE, a specialized computer algebra package
    for computations concerning Lie groups and algebras, and their
    representations. The functionality of the package is demonstrated by
    sample computations, and the structure of the program and the 
    algorithms implemented in it are discussed.",
  paper = "Leeu94.pdf",
  keywords = "axiomref"
}

@article{Leti97,
  author = "Letichevskij, A. Alexander and Marinchenko, V. G.",
  title = {{Objects in algebraic programming system}},
  journal = "Cybern. Syst. Anal.",
  volume = "33",
  number = "2",
  pages = "283-299",
  year = "1997",
  comment = "translated from Russian",
  abstract =
    "The algebraic programming system (APS) developed at the
    V. M. Glushkov Institute of Cybernetics of the Academy of Sciences of
    the Ukrainian SSR integrates the basic programming paradigms,
    including procedural, functional, algebraic, and logic programming.
    
    Algebraic programming in APS relies on special data structures, the
    so-called graph terms, which permit using diverse data and knowledge
    representations in relevant application domains. In the language
    APLAN, graph terms are described by expressions or systems of
    expressions of a many-sorted algebra of data. They may represent both
    objects of the application domain and reasoning about these
    objects. The option of setting an arbitrary interpretation of the
    operations in the algebra of data makes it possible to use APS as a
    basis for various extensions.
    
    Symbolic computation systems such as Scratchpad/AXIOM have acquired
    special importance. They provide various possibilities of manipulating
    typed mathematical objects, including objects of complex hierarchical
    structure. This is a natural requirement when working with algebraic
    objects. In particular, the properties of many algebraic structures
    (such as groups, rings, fields, etc.) are naturally
    hierarchical-modular.
    
    The Institute of Cybernetics and the Kherson Teachers’ College have
    developed an instruction-oriented computer algebra system AIST. The
    AIST kernel is a hierarchical structure of mathematical concepts
    described in the APS language. However, construction of new
    applications on the basis of this hierarchical structure has proved
    difficult. The system kernel can be made more flexible by providing
    tools for flexible description of hierarchical structures of
    mathematical concepts.
    
    In this article, we describe an extension of the language APLAN, which
    provides tools for the object-oriented style of programming. This is
    one of the possible ways of introducing types in APS. The
    object-oriented technology also can be used to develop a hierarchical
    system of mathematical objects.",
  keywords = "axiomref"
}

@article{Lewi99,
  author = "Lewis, Robert H. and Wester, Michael",
  title = {{Comparison of polynomial-orienged computer algebra systems}},
  journal = "SIGSAM Bulletin",
  volume = "33",
  number = "4",
  pages = "5-13",
  year = "1999",
  link = "\url{https://home.bway.net/lewis/cacomp.ps}",
  abstract =
    "Exact symbolic computation with polynomials and matrices over
    polynomial rings has wide applicability to many fields [Hereman96,
    Lewis99]. By ``exact symbolic'', we mean computation with polynomials
    whose coefficients are integers (of any size), rational numbers, or
    from finite fields, as opposed to coefficients that are ``floats'' of a
    certain precision. Such computation is part of most computer algebra
    (CA) systems. Over the last dozen years, several large CA systems have
    become widely available, such as Axiom, Derive, Macsyma, Maple,
    Mathematica and Reduce. They tend to have great breadth, be produced
    by profit-making companies, and be relatively expensive, at least for
    a full blown non-student version. However, most if not all of these
    systems have difficulty computing with the polynomials and matrices
    that arise in actual research. Real problems tend to produce large
    polynomials and large matrices that the general CA systems cannot
    handle [Lewis99].
    
    In the last few years, several smaller CA systems focused on
    polynomials have been produced at universities by individual
    researchers or small teams. They run on Macs, PCs and workstations.
    They are freeware or shareware. Several claim to be much more
    efficient than the large systems at exact polynomial computations. The
    list of these systems includes CoCoA, Fermat, MuPAD, Pari-Gp and
    Singular [CoCoA, Fermat, MuPAD, Pari-Gp, Singular].
    
    In this paper, we compare these small systems to each other and to two
    of the large systems (Magma and Maple) on a set of problems involving
    exact symbolic computation with polynomials and matrices. The problems
    here involve:
    \begin{itemize}
    \item the ground rings $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{Z}/p$ 
    and other finite fields
    \item basic arithmetic of polynomials over the ground ring
    \item basic arithmetic of rational functions over the ground ring
    \item polynomial evaluation (substitution)
    \item matrix normal form
    \item determinants and characteristic polynomial
    \item GCDs of multivariate polynomial
    \item resultants
    \end{itemize}",
  paper = "Lewi99.pdf",
  keywords = "axiomref"
}

@article{Lewi17,
  author = "Lewis, Robert Y.",
  title = {{An Extensible Ad Hoc Interface between Lean and Mathematica}},
  journal = "EPTCS",
  volume = "262",
  pages = "23-37",
  year = "2017",
  abstract =
    "We implement a user-extensible ad hoc connection between the Lean
    proof assistant and the computer algebra system Mathematica. By
    reflecting the syntax of each system in the other and providing a
    flexible interface for extending translation, our connection allows
    for the exchange of arbitrary information between the two systems. We
    show how to make use of the Lean metaprogramming framework to verify
    certain Mathematica computations, so that the rigor of the proof
    assistant is not compromised.",
  paper = "Lewi17.pdf",
  keywords = "axiomref, printed"
}

@inproceedings{Liao95,
  author = "Liao, Hsin-Chao and Fateman, Richard J.",
  title = {{Evaluation of the heuristic polynomial GCD}},
  booktitle = "Proc. ISSAC 1995",
  year = "1995",
  pages = "240-247",
  link = "\url{http://www.cs.berkeley.edu/~/fateman/papers/phil8.ps}",
  abstract =
    "The Heuristic Polynomial GCD procedure (GCDHEU) is used by the Maple
    computer algebra system, but not other. Because Maple has an
    especially efficient kernel that provides fast integer arithmetic, but
    a relatively slower interpreter for non-kernel code, the GCDHEU
    routine is especially effective in that it moves much of the
    computation into ``bignum'' arithmetic and hence executes primarily in
    the kernel.
    
    We speculated that in other computer algebra systems an implementation
    GCDHEU would not be advantageous. In particular, if {\sl all} the
    system code is compiled to run at ``full speed'' in a (presumably more
    bulky) kernel that is entirely written in C or compiled Lisp, then
    there would seem to be no point in recasting the polynomial gcd
    problem into a bignum GCD problem. Manipulating polynomials that are
    vectors of coefficients would seem to be equivalent computationally to
    manipulating vectors of big digits.
    
    Yet our evidence suggests that one can take advantage of the GCDHEU in
    a Lisp system as well. Given a good implementation of bignums, for
    most small problems and many large ones, a substantial speedup can be
    obtained by the appropriate choice of GCD algorithm, including often
    enought, the GCDHEU approach. Another major winner seem to be the
    subresultant polynomial remainder sequence algorithm. Because more
    sophisticated sparse algorithms are relatively slow on small problems
    and only occasionally emerge as superior (on larger problems) it seems
    the choice of a fast GCD algorithm is tricky.",
  paper = "Liao95.pdf"
}

@phdthesis{Lixx05,
  author = "Li, Xin",
  title = {{Efficient Management of Symbolic Computation with Polynomials}},
  school = "University of Western Ontario",
  year = "2005",
  link = "\url{http://www.csd.uwo.ca/~moreno//Publications/XinLi-MSThesis-2005.pdf.gz}",
  abstract = 
    "Symbolic polynomial computation is widely used to solve many applied
    or abstract mathematical problems. Some of them, such as solving
    systems of polynomial equations, have exponential complexity. Their
    implementation is, therefore, a challenging task.

    By using adapted data structures, asymptotically fast algorithms and
    effective code optimization techniques, we show how to reduce the
    practical and theoretical complexity of these computations. Our effort
    is divided into three categories: integrting the best known techniques
    into our implementation, investigating new directions, and measuring
    the interactions between numerous techniques.

    We chose AXIOM and ALDOR as our implementation and experimentation
    environment, since they are both strongly typed and highly efficient
    Computer Algebra Systems (CAS). Our implementation results show that
    our methods have great potential to improve the efficiency of exact
    polynomial computations with the selected CASs. The performance of our
    implementation is comparable to that of (often outperforming) the best
    available packages for polynomial computations.",
  paper = "Lixx05.pdf",
  keywords = "axiomref"
}

@InProceedings{Lixx06,
  author = "Li, Xin and Moreno Maza, Marc",
  title = {{Efficient implementation of polynomial arithmetic in a 
           multiple-level programming environment}},
  booktitle = "Mathematical Software",
  series = "ICMS 2006",
  year = "2006",
  isbn = "978-3-540-38084-9",
  location = "Spain",
  pages = "12-23",
  link = "\url{http://www.math.kobe-u.ac.jp/icms2006/icms2006-video/slides/046.pdf}",
  abstract =
    "The purpose of this study is to investigate implementation techniques
    for polynomial arithmetic in a multiple-level programming
    environment. Indeed, certain polynomial data types and algorithms can
    further take advantage of the features of lower level languages, such
    as their specialized data structures or direct access to machine
    arithmetic. Whereas, other polynomial operations, like Groebner basis
    over an arbitrary field, are suitable for generic programming in a
    high-level language.

    We are interested in the integration of polynomial data type
    implementations realized at different language levels, such as Lisp, C
    and Assembly. In particular, we consider situations for which code
    from different levels can be combined together within the same
    application in order to achieve high-performance.  We have developed
    implementation techniques in the multiple-level programming
    environment provided by the computer algebra system AXIOM. For a given
    algorithm realizing a polynomial operation, available at the user
    level, we combine the strengths of each language level and the
    features of a specific machine architecture. Our experimentations show
    that this allows us to improve performances of this operation in a
    significant manner.",
  paper = "Lixx06.pdf",
  keywords = "axiomref",
  beebe = "Li:2006:EIP"
}

@inproceedings{Lixx07,
  author = "Li, Xin and Maza, Marc Moreno and Schost, Eric",
  title = {{On the Virtues of Generic Programming for Symbolic Computation}},
  booktitle = "Computational Science -- ICCS 2007",
  series = "Lecture Notes in Computer Science",
  isbn = "3-540-72585-7",
  pages = "251-258",
  year = "2007",
  abstract = 
    "The purpose of this study is to measure the impact of C level code
    polynomial arithmetic on the performances of Axiom high-level algorithms,
    such as polynomial factorization. More precisely, given a high-level
    Axiom package P parameterized by a univariate polynomial domain U, we
    have compared the performances of P when applied to different U's,
    including an Axiom wrapper for our C level code.
    
    Our experiments show that when P relies on U for its univariate
    polynomial computations, our specialized C level code can provide a
    significant speed-up. For instance, the improved implementation of
    square-free factorization in Axiom is 7 times faster than the one in
    Maple and very close to the one in Magma. On the contrary, when P does
    not rely much on the operations of U and implements its private univariate
    polynomial operation, the P cannot benefit from our highly optimized C
    level code. Consequently, code which is poorly generic reduces the 
    speed-up opportunities when applied to highly efficient and specialized.",
  keywords = "axiomref",
  paper = "Lixx07.pdf",
  beebe = "Li:2007:VGP"
}

@phdthesis{Lixx09a,
  author = "Li, Xin",
  title = {{Toward High-Performance Polynomial System Solvers Based On
           Triangluar Decompositions}},
  school = "University of Western Ontario",
  year = "2009",
  link = "\url{http://www.csd.uwo.ca/~moreno/Publications/XinLiPhDThesis-2008.pdf}",
  abstract = 
    "This thesis is devoted to the design and implementation of polynomial
    system solvers based on symbolic computation. Solving systems of
    non-linear, algebraic or differential equations, is a fundamental
    problem in mathematical sciences. It has been studied for centuries
    and still stimulates many research developments, in particular on the
    front of high-performance computing.

    Triangular decompositions are a highly promising technique with the
    potential to produce high-performance polynomial system solvers. This
    thesis makes several contributions to this effort.

    We propose asymptotically fast algorithms for the core operati ons on
    which triangular decompositions rely. Complexity results and comparati
    ve implementation show that these new algorithms provide substantial
    performance improvements.

    We present a fundamental software library for polynomial arithmetic in
    order to support the implementation of high-performance solvers based
    on triangular decompositions. We investigate strategies for the
    integration of this library in high-level programming environments
    where triangular decompositions are usually implemented.

    We obtain a high performance library combining highly optimized C
    routines and solving procedures written in the Maple computer algebra
    system. The experimental result shows that our approaches are very
    effective, since our code often outperforms pre-existing solvers in a
    significant manner.",
  paper = "Lixx09a.pdf",
  keywords = "axiomref"
}

@InProceedings{Lixx09,
  author = "Li, Xin and Moreno Maza, Marc and Schost, Eric",
  title = {{Fast arithmetic for triangular sets: from theory to practice}},
  booktitle = "Proc 32nd ISSAC",
  series = "ISSAC 2007",
  year = "2007",
  isbn = "978-1-59593-743-8",
  location = "Canada",
  pages = "269-276",
  link = "\url{http://www.csd.uwo.ca/~moreno/Publications/LiMorenoSchost-ISSAC-2007.pdf}",
  abstract =
    "We study arithmetic operations for triangular families of
    polynomials, concentrating on multiplication in dimension zero. By a
    suitable extension of fast univariate Euclidean division, we obtain
    theoretical and practical improvements over a direct recursive
    approach; for a family of special cases, we reach quasi-linear
    complexity. The main outcome we have in mind is the acceleration of
    higher-level algorithms, by interfacing our low-level implemention
    with languages such as AXIOM or Maple. We show the potential for hugh
    speed-ups, by comparing two AXIOM implementations of vanHoeij and
    Monagan's modular GCD algorithm.",
  paper = "Lixx09.pdf",
  keywords = "axiomref"
}

@inproceedings{Lixx11,
  author = "Li, Yue and Dos Reis, Gabriel",
  title = {{An Automatic Parallelization Framework for Algebraic 
           Computation Systems}},
  booktitle = "Proc. ISSAC 2011",
  pages = "233-240",
  isbn = "978-1-4503-0675-1",
  year = "2011",
  link = "\url{http://www.axiomatics.org/~gdr/concurrency/oa-conc-issac11.pdf}",
  abstract = 
    "This paper proposes a non-intrusive automatic parallelization
    framework for typeful and property-aware computer algebra systems.
    Automatic parallelization remains a promising computer program
    transformation for exploiting ubiquitous concurrency facilities
    available in modern computers. The framework uses semantics-based
    static analysis to extract reductions in library components based on
    algebraic properties. An early implementation shows up to 5 times
    speed-up for library functions and homotopy-based polynomial system
    solver. The general framework is applicable to algebraic computation
    systems and programming languages with advanced type systems that
    support user-defined axioms or annotation systems.",
  paper = "Lixx11.pdf",
  keywords = "axiomref"
}

@misc{Lint98,
  author = "Linton, Stephen",
  title = {{The GAP 4 Type System Organising Algebraic Algorithms}},
  link = "\url{http://www.gap-system.org/Doc/Talks/kobe.ps}",
  paper = "Lint98.pdf",
  keywords = "axiomref"
}

@misc{Linu15,
  author = "Linux Links",
  title = {{Axiom}},
  link = "\url{http://www.linuxlinks.com/article/20080810064748970/Axiom.html}",
  year = "2015",
  keywords = "axiomref"
}

@book{Lisk99,
  author = "Liska, Richard and Drska, Ladislav and Limpouch, Jiri and
            Sinor, Milan and Wester, Michael and Winkler, Franz",
  title = {{Computer Algebra - Algorithms, Systems and Applications}},
  year = "1999",
  publisher = "web",
  link =
    "\url{https://people.eecs.berkeley.edu/~fateman/282/readings/liska.pdf}",
  paper = "Lisk99.pdf",
  keywords = "axiomref"
}

@InProceedings{Lomo02,
  author = "Lomonaco, Samual J. and Kauffman, Louis H.",
  title = {{Quantum hidden subgroup algorithms: a mathematical perspective}},
  booktitle = "Quantum computation and information",
  series = "AMS special session",
  year = "2002",
  isbn = "0-8218-2140-7",
  location = "Washington",
  pages = "139-202",
  link = "\url{https://arxiv.org/pdf/quant-ph/0201095v3.pdf}",
  abstract =
    "The ultimate objective of this paper is to create a stepping stone to
    the development of new quantum algorithms. The strategy chosen is to
    begin by focusing on the class of abelian quantum hidden subgroup
    algorithms, i.e., the class of abelian algorithms of the Shor/Simon
    genre. Our strategy is to make this class of algorithms as
    mathematically transparent as possible. By the phrase ``mathematically
    transparent'' we mean to expose, to bring to the surface, and to make
    explicit the concealed mathematical structures that are inherently and
    fundamentally a part of such algorithms. In so doing, we create
    symbolic abelian quantum hidden subgroup algorithms that are analogous
    to the those symbolic algorithms found within such software packages
    as Axiom, Cayley, Maple, Mathematica, and Magma.

    As a spin-off of this effort, we create three different
    generalizations of Shor’s quantum factoring algorithm to free abelian
    groups of finite rank. We refer to these algorithms as wandering (or
    vintage $\mathbb{Z}_Q$) Shor algorithms. They are essentially quantum
    algorithms on free abelian groups of finite rank $n$ which, with each
    iteration, first select a random cyclic direct summand $\mathbb{Z}$ of
    the group and then apply one iteration of the standard Shor algorithm
    to produce a random character of the “approximating” finite group
    $\widetilde{A} = \mathbb{Z}_Q$, called the group probe. These
    characters are then in turn used to find either the order $P$ of a
    maximal cyclic subgroup $\mathbb{Z}_P$ of the hidden quotient group
    $H_{\varphi}$, or the entire hidden quotient group $H_{\varphi}$. An
    integral part of these wandering quantum algorithms is the selection
    of a very special random transversal $t_{\mu}:\widetilde{A}\rightarrow A$,
    which we refer to as a Shor transversal. The algorithmic time
    complexity of the first of these wandering Shor algorithms is found to
    be $O(n^2({rm lg\ } Q)^3 ({\rm lg\ }{\rm lg\ } Q)^{n+1})$.",
  paper = "Lomo02.pdf",
  keywords = "axiomref"
}

@inproceedings{Luck86,
  author = "Lucks, Michael",
  title = {{A fast implementation of polynomial factorization}},
  booktitle = "Proc. 1986 Symposium on Symbolic and Algebraic Computation",
  series = "SYMSAC '86",
  year = "1986", 
  location = "Waterloo, Ontario",
  pages = "228-232",
  publisher = "ACM Press",
  isbn = "0-89791-199-7",
  doi = "http://dx.doi.org/10.1145/32439.32485",
  abstract =
    "A new package for factoring polynomials with integer coefficients is
    described which yields significant improvements over previous
    implementations in both time and space requirements. For multivariate
    problems, the package features an inexpensive method for early
    detection and correction of spurious factors. This essentially solves
    the multivariate extraneous factor problem and eliminates the need to
    factor more than one univariate image, except in rare cases. Also
    included is an improved technique for coefficient prediction which is
    successful more frequently than prior versions at short-circuiting the
    expensive multivariate Hensel lifting stage. In addition some new
    approaches are discussed for the univariate case as well as for the
    problem of finding good integer substitution values. The package has
    been implemented both in Scratchpad II and in an experimental version
    of muMATH.",
  keywords = "axiomref",
  beebe = "Lucks:1986:FIP"
}

@mastersthesis{Luek77,
  author = "Lueken, E.",
  title = {{Ueberlegungen zur Implementierung eines 
            Formelmanipulationssystems}},
  school = {Technischen Universit{\"{a}}t Carolo-Wilhelmina zu Braunschweig}, 
  address = "Braunschweig, Germany",
  year = "1977",
  keywords = "axiomref",
  beebe = "Lueken:1977:UIF"
}

@article{Lync91,
  author = "Lynch, R. and Mavromatis, H. A.",
  title = {{New quantum mechanical perturbation technique using an 
           'electronic scratchpad' on an inexpensive computer}},
  journal = "American Journal of Pyhsics",
  volume = "59",
  number = "3",
  pages = "270-273",
  year = "1991",
  abstract = 
    "The authors have developed a new method for doing numerical quantum
    mechanical perturbation theory. It has the flavor of
    Rayleigh–Schrödinger perturbation theory (division of the Hamiltonian
    into an unperturbed Hamiltonian and a perturbing term, use of the
    basis formed by the eigenfunctions of the unperturbed Hamiltonian)
    while turning out to be a variational technique. Furthermore, it is
    easily implemented by means of the widely used ‘‘electronic
    scratchpad,’’ MathCAD 2.0, using an inexpensive computer. As an
    example of the method, the problem of a harmonic oscillator with a
    quartic perturbing term is examined.",
  keywords = "axiomref",
  beebe = "Lynch:1991:NQM"
}

@inproceedings{Mahb05,
  author = "Mahboubi, Assia",
  title = {{Programming and certifying the CAD algorithm inside the 
            coq system}},
  year = "2005",
  booktitle = "Mathematics, Algorithms, Proofs, volume 05021 of Dagstuhl
               Seminar Proceedings, Schloss Dagstuhl (2005)",
  abstract =
    "A. Tarski has shown in 1975 that one can perform quantifier
    elimination in the theory of real closed fields. The introduction of
    the Cylindrical Algebraic Decomposition (CAD) method has later allowed
    to design rather feasible algorithms. Our aim is to program a
    reflectional decision procedure for the Coq system, using the CAD, to
    decide whether a (possibly multivariate) system of polynomial
    inequalities with rational coefficients has a solution or not. We have
    therefore implemented various computer algebra tools like gcd
    computations, subresultant polynomial or Bernstein polynomials.",
  paper = "Mahb05.pdf",
  keywords = "axiomref"
}

@article{Math89,
  author = "Mathews, J.",
  title = {{Symbolic computational algebra applied to Picard iteration}},
  journal = "Mathematics and computer education",
  volume = "23",
  number = "2",
  pages = "117-122",
  year = "1989",
  link = "\url{http://mathfaculty.fullerton.edu/mathews/articles/1989PicardIteration.pdf}",
  abstract =
    "The term ``Picard iteration'' occurs two places in undergraduate
    mathematics. In numerical analysis it is used when discussing fixed
    point iteration for finding a numerical approximation to the equation
    $s=g(x)$. In differential equations, Picard iteration is a
    constructive procedure for establishing the existence of a solution to
    a differential equation $y^{\prime} = f(x,y)$.
    
    The first type of Picard iteration uses computations to generate a
    sequence of numbers which converges to a solution. We will not present
    this application, but mention that it involves the traditional role of
    the computer as a ``number cruncher.''
    
    The second application of Picard iteration illustrates how to use a
    computer to generate a sequence of functions which converges to a
    solution. The purpose of this article is to show the step by step
    process in translating mathematical theory into the symbolic
    manipulation setting. Systems such as MACSYM, ALTRAN, REDUCE, SMP,
    MAPLE, SCRATCHPAD and muMATH are being introduced in undergraduate
    mathematics courses to assist in keeping trace of equations during
    complicated manipulations.",
  paper = "Math89.pdf",
  keywords = "axiomref",
  beebe = "Mathews:1989:SCA"
}

@misc{Maxi16,
  author = "Maxima",
  title = {{Other Free Computer Algebra Systems}},
  link = "\url{http://maxima.sourceforge.net/compalg.html}",
  year = "2016",
  abstract = 
    "Axiom is a general purpose Computer Algebra system. It is useful for
    doing mathematics by computer and for research and development of
    mathematical algorithms. It defines a strongly typed, mathematically
    correct type hierarchy. It has a programming language and a built-in
    compiler.
    
    There is also an interesting Rosetta Stone which offers translations
    of many basic operations for several computer algebra systems,
    including Maxima.",
  keywords = "axiomref"
}

@techreport{Maza00,
  author = "Maza, Marc Moreno",
  title = {{On Triangular Decompositions of Algebraic Varieties}},
  institution = "Numerical Algorithms Group",
  year = "2000",
  month = "June",
  type = "technical report",
  number = "TR 4/99",
  link = "\url{http://www.csd.uwo.ca/~moreno//Publications}",
  algebra = 
   "\newline\refto{category TSETCAT TriangularSetCategory}
    \newline\refto{category RSETCAT RegularTriangularSetCategory}
    \newline\refto{category NTSCAT NormalizedTriangularSetCategory}
    \newline\refto{category SFRTCAT SquareFreeRegularTriangularSetCategory}
    \newline\refto{package RSDCMPK RegularSetDecompositionPackage}",
  abstract =
    "Different kinds of triangular decompositions of algebraic varieties
    are presented. The main result is an efficient method for obtaining
    them. Our strategy is based on a lifting theorem for polynomial
    computations module regular chains.",
  paper = "Maza00.pdf",
  keywords = "axiomref"
}

@misc{Maza06,
  author = "Maza, Marc Moreno",
  title = {{Axiom: Generic, open and powerful}},
  link = "\url{http://www.csd.uwo.ca/~moreno//Publications/ICMS-06-1.pdf}",
  year = "2006",
  paper = "Maza06.pdf",
  keywords = "axiomref"
}

@article{Mcge10,
  author = "McGettrick, Michael",
  title = {{One dimensional quantum walks with memory}},
  journal = "Quantum Inf. Comput.",
  volume = "10",
  number = "5-6",
  pages = "509-524",
  year = "2010",
  link = "\url{https://arxiv.org/pdf/0911.1653v2.pdf}",
  abstract = 
    "We investigate the quantum versions of a one-dimensional random walk,
    whose corresponding Markov chain is of order 2. This corresponds to
    the walk having a memory of one previous step. We derive the
    amplitudes and probabilities for these walks, and point out how they
    differ from bot classical random walks, and quantum walks without
    memory.",
  paper = "Mcge10.pdf",
  keywords = "axiomref"
}

@article{Mela90,
  author = "Melachrinoudis, E.; Rumpf, D. L.",
  title = {{Teaching advantages of transparent computer software -- MathCAD}},
  journal = "CoED",
  volume = "10",
  number = "1",
  pages = "71-76",
  year = "1990",
  abstract =
    "The case is presented for using mathematical scratchpad software,
    such as MathCAD, in undergraduate and graduate engineering
    courses. The pedagogical benefits, especially relative to the usual
    black box engineering software, are described. Several examples of
    student written projects are presented. The projects solve problems in
    operations research, control theory and statistical regression
    analysis.",
  keywords = "axiomref",
  beebe = "Melachrinoudis:1990:TAT"
}

@misc{Mele88,
  author = "Melenk, H. and Moller, H. M. and Neun, W.",
  title = {{On Groebner Bases Computation on a Supercomputer Using REDUCE}},
  link = "\url{https://opus4.kobv.de/opus4-zib/files/10/SC-88-02.pdf}",
  abstract = 
    "Groebner bases are the main tool for solving systems of algebraic
    equations and some other problems in connection with polynomial ideals
    using Computer Algebra Systems. The procedure for the computation of
    Groebner bases in REDUCE 3.3 has been modified in order to solve more
    complicated algebraic systems of equations by some general
    improvements and by some tools based on the specific resources of the
    CRAY X-MP. We present this modification and illustrate it by examples.",
  paper = "Mele88.pdf",
  keywords = "axiomref"
}

@article{Mich01,
  author = "Michler, Gerhard O.",
  title = {{The character values of multiplicity-free irreducible constituents
           of a transitive permutation representation}},
  journal = "Kyushu J. Math.",
  volume = "55",
  number = "1",
  pages = "75-106",
  year = "2001",
  link = "\url{https://www.jstage.jst.go.jp/article/kyushujm/55/1/55\_1\_75/\_pdf}",
  abstract =
    "Let $G$ be a finite group and $M$ a subgroup of $G$. Let the
    permutation character of $G$ on the set of right cosets of $M$ be
    denoted by $(1_M)^G$. Any ordinary irreducible constituent of
    $(1_M)^G$ with multiplicity one is called a multiplicity-free
    constituent of $(1_M)^G$. In the paper under review the author gives
    an efficient algorithm to compute the values of any multiplicity-free
    constituent of $(1_M)^G$. The character value is in terms of the
    double coset decomposition of $M$ and related concepts concerning
    intersection matrices. The author uses this algorithm to determine the
    values of the multiplicity-free constituents of $(1_C)^{J_1}$, where
    $J_1$ is the smallest Janko group of order 175560 and $C$ is the
    centralizer of an involution in $J_1$. Using these, he then is able to
    compute the character table of $J_1$ which is already known.",
  paper = "Mich01.pdf",
  keywords = "axiomref"
}

@misc{Mill95,
  author = "Miller, Bruce R.",
  title = {{An expression formatter for MACSYMA}},
  year = "1995",
  abstract =
    "A package for formatting algebraic expressions in MACSYA is described.
    It provides facilities for user-directed hierarchical structuring of
    expressions, as well as for directing simplifications to selected
    subexpressions. It emphasizes a semantic rther than syntactic description
    of the desired form. The package also provides utilities for obtaining
    efficiently the coefficients of polynomials, trigonometric sums and
    power series. Similar capabilities would be useful in other computer
    algebra systems.",
  paper = "Mill95.pdf",
  keywords = "axiomref"
}

@misc{Mino07,
  author = "Minoiu, N. and Netto, M and Mammar, S",
  title = {{Assistance control based on a composite Lyapunov function for
           lane departure avoidance}},
  booktitle = "Proc. 15 Med. Conf. on Control \& Automation",
  year = "2007",
  abstract =
    "This paper presents a vehicle steering assistance designed to avoid
    lane departure during driver inattention periods. Activated for a
    driver loss of concentration during a lane keeping maneuver the
    steering assistance drives the vehicle back to the center of the
    lane. In order to ensure a vehicle trajectory as close as possible to
    the centerline, the control law has been developed based on invariant
    sets theory and on composite Lyapunov functions. The computation has
    been performed using LMI methods, which allow in addition imposing a
    maximum bound for the control steering angle.",
  keywords = "axiomref"
}

@misc{Mish87,
  author = "Mishra, Bhubaneswar and Yap, Chee",
  title = {{Notes on Groebner Basis}},
  year = "1987",
  link = "\url{https://people.eecs.berkeley.edu/~fateman/282/readings/mishra87note.pdf}",
  abstract =
    "We present a self-contained exposition of the theory of Groebner
     basis and its applications",  
  paper = "Mish87.pdf"
}

@book{Mish93,
  author = "Mishra, Bhubaneswar",
  title = {{Algorithmic Algebra}},
  publisher = "Springer-Verlag",
  series = "Texts and Monographs in Computer Sciences",
  year = "1993",
  abstract =
    "This book is based on a graduate course in computer science taught in
    1987. The following topics are covered: computational ideal theory,
    solving systems of polynomial equations, elimination theory, real
    algebra, as well as an introduction chapter and two chapters with the
    needed algebraic background. The book is self-contained and the proofs
    are given with many details.
    
    It is clear that this book is only an introduction to the topic and
    does not cover the many improvements that appeared in the last 7 years
    about for example the computation of Groebner basis, polynomial
    solving, multivariate resultants and algorithms in real
    algebra. Choices had to be made to keep the content of a reasonable
    size and the complexity issues are not considered.
    
    The choice of topics is excellent, there are many exercises and
    examples. It is a very useful book.",
  paer = "Mish93.pdf",
  keywords = "axiomref"
}

@InProceedings{Miss05,
  author = "Missura, Stephan A.",
  title = {{Theories = Signatures + Propositions Used as Types}},
  booktitle = "Integrating Symbolic Mathematics and Artificial Intelligence",
  volume = "958",
  pages = "144-155",
  year = "2005",
  abstract =
    "Languages that distinguish between types and structures use explicit
    components for the carrier type(s) in structures. Examples are the
    function language Standard ML and most algebraic specification
    systems.  Hence, they have to use general sum types or signatures to
    give types to structures and be able to build, for instance, the
    algebraic hierarchy.",
  paper = "Miss05",
  keywords = "axiomref"
}

@misc{Miss94,
  author = "Missura, Stephan A. and Weber, Andreas",
  title = {{Using Commutativity Properties for Controlling Coercions}},
  link = "\url{http://cg.cs.uni-bonn.de/personal-pages/weber/publications/pdf/WeberA/MissuraWeber94a.pdf}",
  abstract = 
    "This paper investigates some soundness conditions which have to be
    fulfilled in systems with coercions and generic operators. A result of
    Reynolds on unrestricted generic operators is extended to generic
    operators which obey certain constraints. We get natural conditions
    for such operators, which are expressed within the theoretic framework
    of category theory. However, in the context of computer algebra, there
    arise examples of coercions and generic operators which do not fulfil
    these conditions. We describe a framework -- relaxing the above
    conditions -- that allows distinguishing between cases of ambiguities
    which can be resolved in a quite natural sense and those which
    cannot. An algorithm is presented that detects such unresolvable
    ambiguities in expressions.",
  paper = "Miss94.pdf",
  keywords = "axiomref, printed"
}

@inproceedings{Mona93,
  author = "Monagan, Michael B.",
  title = {{Gauss: a parameterized domain of computation system with 
           support for signature functions}},
  booktitle = "Design and Implementation of Symbolic Computation Systems",
  series = "Lecture Notes in Computer Science 722",
  pages = "81-94",
  isbn = "3-540-57235-X",
  year = "1993",
  link = "\url{http://www.cecm.sfu.ca/~monaganm/papers/DISCO93.pdf}",
  abstract =
    "The fastest known algorithms in classical algebra make use of signature
    functions. That is, reducing computation with formulae to computing with
    the integers module $p$, by substituting random numbers for variables,
    and mapping constants module $p$. This isea is exploited in specific
    algorithms in computer algebra systems, e.g. algorithms for polynomial
    greatest common divisors. It is also used as a heuristic to speed up
    other calculations. But none exploit it in a systematic manner. The
    goal of this work was twofold. First, to design an AXIOM like system
    in which these signature functions can be constructed automatically,
    hence better exploited, and secondly, to exploit them in new ways. In
    this paper we report on the design of such a system, Gauss.",
  paper = "Mona93.pdf",
  keywords = "axiomref",
  beebe = "Monagan:1993:GPD"
}

@misc{Mona94,
  author = "Monagan, Michael B. and Gonnet, Gaston H.",
  title = {{Signature Functions for Algebraic Numbers}},
  link = "\url{http://lib.org/by/\_djvu\_Papers/Computer\_algebra/Algebraic\%20numbers}",
  abstract = 
    "In 1980 Schwartz gave a fast {\sl probabilistic} method which tests
    if a matrix of polynomials of $\mathbb{Z}$ is singular or not. The
    method is based on the idea of {\sl signature functions} which are
    mappings of mathematical expressions into finite rings. In Schwartz's
    paper, they were polynomials over $\mathbb{Z}$ into GF($p$). Because
    computation in GF($p$) is very fast compared with computing with
    polynomials, Schwartz's method yields an enormous speedup both in
    theory and in practice. Therefore it is desirable to extend the class
    of expressions for which we can find effective signature functions. In
    the mid 80's Gonnet extended the class of expressions, for which
    signature functions can be found, to include a restricted class of
    elementary functions and integer roots. In this paper we present and
    compare methods for constructing signature functions for expressions
    containing {\sl algebraic numbers}. Some experimental results are
    given.",
  paper = "Mona94.djvu",
  keywords = "axiomref"
}  

@InProceedings{Mona07,
  author = "Monagan, Michael and Pearce, Roman",
  title = {{Polynomial division using dynamic arrays, heaps, and packed 
           exponent vectors}},
  booktitle = "Computer algebra in scientific computing",
  series = "CASC 2007",
  year = "2007",
  isbn = "978-3-540-75186-1",
  location = "Bonn",
  pages = "295-315",
  link = "\url{http://www.cecm.sfu.ca/~rpearcea/sdmp/sdmp\_paper.pdf}",
  abstract =
    "A common way of implementing multivariate polynomial multiplication
    and division is to represent polynomials as linked lists of terms
    sorted in a term ordering and to use repeated merging. This results in
    poor performance on large sparse polynomials.

    In this paper we use an auxiliary heap of pointers to reduce the
    number of monomial comparisons in the worst case while keeping the
    overall storage linear. We give two variations. In the first, the size
    of the heap is bounded by the number of terms in the quotient(s). In
    the second, which is new, the size is bounded by the number of terms
    in the divisor(s).

    We use dynamic arrays of terms rather than linked lists to reduce
    storage allocations and indirect memory references. We pack monomials
    in the array to reduce storage and to speed up monomial
    comparisons. We give a new packing for the graded reverse
    lexicographical ordering.

    We have implemented the heap algorithms in C with an interface to
    Maple. For comparison we have also implemented Yan’s ``geobuckets'' data
    structure. Our timings demonstrate that heaps of pointers are
    comparable in speed with geobuckets but use significantly less
    storage.",
  paper = "Mona07.pdf",
  keywords = "axiomref"
}

@inproceedings{Mona00,
  author = "Monagan, Michael and Wittkopf, Allan D.",
  title = {{On the Design and Implementation of Brown's Algorithm over the
           Integers and Number Fields}},
  booktitle = "ISSAC 2000",
  pages = "225-233",
  year = "2000",
  isbn = "1-58113-218-2",
  abstract =
    "We study the design and implementation of the dense modular GCD
    algorithm of Brown applied to bivariate polynomial GCDs over the
    integers and number fields. We present an improved design of Brown's
    algorithm and compare it asymptotically with Brown's original
    algorithm, with GCDHEU, the heuristic GCD algorithm, and with the
    EEZGCD algorithm. We also make an empirical comparison based on Maple
    implementations of the algorithms. Our findings show that a careful
    implementation of our improved version of Brown's algorithm is much
    better than the other algorithms in theory and in practice.",
  paper = "Mona00.pdf"
}

@misc{Mont07,
  author = "Montes, Antonio",
  title = {{On the canonical discussion of polynomial systems with 
            parameters}},
  year = "2007",
  link = "\url{http://arxiv.org/pdf/math/0601674.pdf}",
  abstract =
    "Given a parametric polynomial ideal $I$, the algorithm DISPGB,
    introduced by the author in 2002, builds up a binary tree describing a
    dichotomic discussion of the different reduced Groebner bases
    depending on the values of the parameters, whose set of terminal
    vertices form a Comprehensive Groebner System (CGS). It is relevant
    to obtain CGS’s having further properties in order to make them more
    useful for the applications. In this paper the interest is focused on
    obtaining a canonical CGS. We define the objective, show the
    difficulties and formulate a natural conjecture. If the conjecture is
    true then such a canonical CGS will exist and can be computed. We also
    give an algorithm to transform our original CGS in this direction and
    show its utility in applications.",
  paper = "Mont07.pdf",
  keywords = "axiomref"
}

@misc{Muld95,
  author = "Mulders, Thom",
  title = {{Primitives: Orepoly and Lodo}},
  year = "1995",
  link = "\url{ftp://ftp.inf.ethz.ch/org/cathode/workshops/jan95/abstracts/mulders.ps}",
  algebra =
  "\newline\refto{category OREPCAT UnivariateSkewPolynomialCategory}
   \newline\refto{category LODOCAT LinearOrdinaryDifferentialOperatorCategory}
   \newline\refto{domain AUTOMOR Automorphism}
   \newline\refto{domain ORESUP SparseUnivariateSkewPolynomial}
   \newline\refto{domain OREUP UnivariateSkewPolynomial}
   \newline\refto{domain LODO LinearOrdinaryDifferentialOperator}
   \newline\refto{domain LODO1 LinearOrdinaryDifferentialOperator1}
   \newline\refto{domain LODO2 LinearOrdinaryDifferentialOperator2}
   \newline\refto{package APPLYORE ApplyUnivariateSkewPolynomial}
   \newline\refto{package OREPCTO UnivariateSkewPolynomialCategoryOps}
   \newline\refto{package LODOF LinearOrdinaryDifferentialOperatorFactorizer}
   \newline\refto{package LODOOPS LinearOrdinaryDifferentialOperatorsOps}",
  paper = "Muld95.pdf",
  keywords = "axiomref"

}

@article{Muss82,
  author = "Musser, David R. and Kapur, Deepak",
  title = {{Rewrite Rule Theory and Abstract Data Type Analysis}},
  journal = "Lecture Notes in Computer Science",
  volume = "144",
  pages = "77-90",
  year = "1982",
  paper = "Muss82.pdf",
  keywords = "axiomref"
}

@InProceedings{Nayl06,
  author = "Naylor, William and Padget, Julian",
  title = {{From Untyped to Polymorphically Typed Objects in Mathematical 
           Web Services}},
  booktitle = "Lecture Notes in Computer Science",
  volume = "4108",
  pages = "222-236",
  year = "2006",
  abstract = 
    "OpenMath is a widely recognized approach to the semantic markup of
    mathematics that is often used for communication between OpenMath
    compliant systems. The Aldor language has a sophisticated
    category-based type system that was specifically developed for the
    purpose of modelling mathematical structures, while the system itself
    supports the creation of small-footprint applications suitable for
    deployment as web services. In this paper we present our first results
    of how one may perform translations from generic OpenMath objects into
    values in specific Aldor domains, describing how the Aldor interfae
    domain ExpresstionTree is used to achieve this. We outline our Aldor
    implementation of an OpenMath translator, and describe an efficient
    extention of this to the Parser category. In addition, the Aldor
    service creation and invocation mechanism are explained. Thus we are
    in a position to develop and deploy mathematical web services whose
    descriptions may be directly derived from Aldor's rich type language.",
  paper = "NPxx.pdf",
  keywords = "axiomref"
}

@techreport{Ngxx80,
  author = "Ng, Edward W.",
  title = {{Symbolic-Numeric Interface: A Review}},
  type = "technical report",
  year = "1980",
  number = "NASA-CR-162690 HC A02/MF A01",
  institution = "NASA Jet Propulsion Lab",
  link = "\url{http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19800008508.pdf}",
  abstract =
    "This is a survey of recent activities that either used or encouraged the
    potential use of a combination of symbolic and numerical calculations.
    Symbolic calculations here primarily refer to the computer processing of
    procedures from classical algebra, analysis and calculus. Numerical 
    calculations refer to both numerical mathematics research and scientific
    computation. This survey is inteded to point out a large number of problem
    areas where a co-operation of symbolic and numeric methods is likely to
    bear many fruits. These areas include such classical operations as
    differentiation and integration, such diverse activities as function
    approximations andqualitative analysis, and such contemporary topics as
    finite element calculations and computational complexity. It is contended
    that other less obvious topics such as the fast Fourier transform, linear
    algebra, nonlinear analysis and error analysis would also benefti from a
    synergistic approach advocated here.",
  paper = "Ngxx80.pdf",
  keywords = "axiomref"
}

@article{Norm75,
  author = "Norman, Arthur C.",
  title = {{Computing with Formal Power Series}},
  journal = "ACM Transactions on Mathematical Software",
  volume = "1",
  number = "4",
  pages = "346-356",
  year = "1975",
  doi = "10.1145/355656.355660",
  keywords = "axiomref",
  beebe = "Norman:1975:CFP"
}

@misc{Normxx,
  author = "Norman, Arthur C.",
  title = {{Notes 13: How to Compute a Groebner Basis}},
  link = "\url{http://people.math.umass.edu/~norman/462\_11/notes/m462notes13.pdf}",
  algebra = 
  "\newline\refto{package AFALGGRO AffineAlgebraicSetComputeWithGroebnerBasis}
   \newline\refto{package GBEUCLID EuclideanGroebnerBasisPackage}
   \newline\refto{package GBF GroebnerFactorizationPackage}
   \newline\refto{package GBINTERN GroebnerInternalPackage}
   \newline\refto{package GB GroebnerPackage}
   \newline\refto{package GROEBSOL GroebnerSolve}
   \newline\refto{package INTERGB InterfaceGroebnerPackage}
   \newline\refto{package LGROBP LinGroebnerPackage}
   \newline\refto{package PGROEB PolyGroebner}",
  paper = "Normxx.pdf"
}

@article{Norm75a,
  author = "Norman, Arthur C.",
  title = {{The SCRATCHPAD Power Series Package}},
  journal = "SIGSAM",
  volume = "9",
  number = "1",
  pages = "12-20",
  year = "1975",
  comment = "IBM T.J. Watson Research RC4998",
  keywords = "axiomref, printed"
}

@misc{Norm94,
  author = "Norman, Arthur C.",
  title = {{Algebraic Manipulation}},
  paper = "Norm94.pdf",
  keywords = "axiomref"
}

@InProceedings{Norm96,
  author = "Norman, Arthur C.",
  title = {{Memory tracing of algebraic calculations}},
  booktitle = "Proc. 1996 ISSAC",
  series = "ISSAC 1996",
  year = "1996",
  publisher = "ACM Press",
  location = "New York, NY",
  pages = "113-119",
  link = "\url{http://opus.bath.ac.uk/16452/1/NormanFitch96a.ps}",
  abstract =
    "We present a software tool which allows us to visualize details of the
    use of memory during the execution of an algebra system. We apply this
    to gain a better understanding of the behaviour of REDUCE, and hence
    to make proposals for ways in which the execution can be improved. The
    same tool will soon be used in the performance engineering of a
    version of axiom.",
  paper = "Norm96.pdf",
  keywords = "axiomref"
}

@phdthesis{Nguy09,
  author = "Nguyen, Minh Van",
  title = {{Exploring Cryptography Using the Sage Computer Algebra System}},
  school = "Victoria University",
  year = "2009",
  abstract = 
    "Cryptography has become indispensable in areas such as e-commerce,
    the legal safe-guarding of medical records, and secure electronic
    communication. Hence, it is incumbent upon software engineers to
    understand the concepts and techniques underlying the cryptosystems
    that they implement. An educator needs to consider which topics to
    cover in a course on cryptography as well as how to present the
    concepts and techniques to be covered in the course. This thesis
    contributes to the field of cryptography pedagogy by discussing and
    implementing small-scale cryptosystems whose encryption and
    decryption processes can be stepped through by hand. Our
    implementation has been accepted and integrated into the code base of
    the computer algebra system Sage. As Sage is free and open source,
    students and educators of cryptology need not worry about paying
    license fees in order to use Sage, but can instead concentrate on
    exploring cryptography using Sage’s built-in support for cryptography.",
  paper = "Nguy09.pdf",
  keywords = "axiomref"
}

@InProceedings{Oanc05,
  author = "Oancea, Cosmin E. and Watt, Stephen M.",
  title = {{Domains and expressions: an interface between two approaches to
           computer algebra}},
  booktitle = "Proc. 2005 ISSAC",
  series = "ISSAC 2005",
  year = "2005",
  isbn = "1-59593-095-7",
  location = "Beijing, China",
  pages = "261-268",
  link = "\url{http://www.csd.uwo.ca/~watt/pub/reprints/2005-issac-alma.pdf}",
  abstract =
    "This paper describes a method to use compiled, strongly typed Aldor
    domains in the interpreted, expression-oriented Maple
    environment. This represents a non-traditional approach to structuring
    computer algebra software: using an efficient, compiled language,
    designed for writing large complex mathematical libraries, together
    with a top-level system based on user-interface priorities and ease of
    scripting.

    We examine what is required to use Aldor libraries to extend Maple in
    an effective and natural way. Since the computational models of Maple
    and Aldor differ significantly, new run-time code must implement a
    non-trivial semantic correspondence. Our solution allows Aldor
    functions to run tightly coupled to the Maple environment, able to
    directly and efficiently manipulate Maple data objects. We call the
    overall system Alma.",
  paper = "Oanc05.pdf",
  keywords = "axiomref"
}

@misc{ORMS,
  author = "Unknown",
  title = {{Oberwolfach References on Mathematical Software}},
  link = "\url{http://orms.mfo.de/project?id=234}",
  keywords = "axiomref"
}

@article{OKee96,
  author = "O'Keefe, Christine M. and Storme, Leo",
  title = {{Arcs in PG(n,q) fixed by {A\_5} and {A\_6}}},
  journal = "J. Geom.",
  volume = "55",
  number = "1-2",
  pages = "123-138",
  year = "1996",
  abstract =
    "A $k$-arc in a projective space is a set of points, no three of which
    are collinear. The author determines the $k$ arcs in $PG(n,q)$ which
    are fixed by primitive groups isomorphic to $A_5$ or $A_6$. The best
    known examples are $q+1$ arcs: in general these are normal rational
    curves and are conics in $PG(2,q)$ and twisted cubics in $PG(3,q)$.
    The other cases turn out to be 6-arcs or 10-arcs.",
  keywords = "axiomref"
}

@inproceedings{Olli89,
  author = "Ollivier, F.",
  title = {{Inversibility of rational mappings and structural 
           identifiablility in automatics}},
  booktitle = "Proc. SIGSAM 1989",
  series = "SIGSAM '89",
  pages = "43-54",
  isbn = "0-89791-325-6",
  year = "1989",
  abstract =
    "We investigate different methods for testing whether a rational
    mapping $f$ from $k^n$ to $k^m$ admits a rational inverse, or whether
    a polynomial mapping admits a polynomial one. We give a new solution,
    which seems much more efficient in practice than previously known ones
    using ``tag'' variables and standard basis, and a majoration for the
    degree of the standard basis calculations which is valid for both
    methods in the case of a polynomial map which is birational. We
    further show that a better bound can be given for our method, under
    some assumptions on the form of $f$. Our method can also extend to
    check whether a given polynomial belong to the subfield generated by a
    finite set of fractions.
    
    We then illustrate our algorithm, with a application to structural
    identifiability. The implantation has been done in the IBM computer
    algebra system Scratchpad II.",
  paper = "Olli89.pdf",
  keywords = "axiomref",
  beebe = "Ollivier:1989:IRM"
}

@InProceedings{Page07,
  author = "Page, William S.",
  title = {{Axiom - Open Source Computer Algebra System}},
  booktitle = "Poster ISSAC 2007 Proceedings",
  series = "ISSAC 2007",
  year = "2007",
  volume = "41",
  pages = "114",
  abstract =
    "Axiom has been in development since 1971. Originally called
    Scratchpad II, it was developed by IBM under the direction of Richard
    Jenks[1]. The project evolved over a period of 20 years as a research
    platform for developing new ideas in computational mathematics.
    ScratchPad also attracted the interest and contributions of a large
    number of mathematicians and computer scientists outside of IBM. In
    the 1990s, the Scratchpad project was renamed to Axiom, and sold to
    the Numerical Algorithms Group (NAG) in England who marketed it as a
    commercial system. NAG withdrew Axiom from the market in October 2001
    and agreed to release Axiom as free software, under an open source
    license.
    
    Tim Daly (a former ScratchPad developer at IBM) setup a pubic open
    source Axiom project[2] in October 2002 with a primary goal to improve
    the documentation of Axiom through the extensive use of literate
    programming[3].  The first free open source version of Axiom was
    released in 2003. Since that time the project has attracted a small
    but very active group of developers and a growing number of users.
    
    This exhibit includes a laptop computer running a recent version
    of Axiom, Internet access (if available) to the Axiom Wiki website[4], 
    and CDs containing Axiom software for free distribution[5].",
  keywords = "axiomref",
  beebe = "Page:2007:AOS"
}

@misc{Page08,
  author = "Page, Bill",
  title = {{Algebraist Network}},
  link = "\url{http://lambda-the-ultimate.org/node/2737}",
  year = "2008",
  keywords = "axiomref"
}

@misc{Paulxx,
  author = "Paule, Peter and Kartashova, Lena and Kauers, Manuel and
            Schneider, Carsten and Winkler, Franz",
  title = {{Hot Topics in Symbolic Computation}},
  publisher = "Springer",
  link = "\url{http://www.risc.jku.at/publications/download/risc_3845/chapter1.pdf}",
  paper = "Paulxx.pdf"
}

@article{Peti99,
  author = "Petitjean, S.",
  title = {{Algebraic Geometry and Computer Vision: Polynomial Systems, Real
           and Complex Roots}},
  journal = "J. of Mathematical Imaging and Vision",
  volume = "10",
  number = "1",
  year = "1999",
  link = "\url{http://www.loria.fr/~petitjea/papers/jmiv99.pdf}",
  abstract = 
    "We review the different techniques known for doing exact computations
    on polynomial systems. Some are based on the use of Groebner bases and
    linear algebra, others on the more classical resultants and its modern
    counterparts. Many theoretical examples of the use of these techniques
    are given. Furthermore, a full set of examples of applications in the
    domain of artificial vision, where many constraints boil down to
    polynomial systems, are presented. Emphasis is also put on very recent
    methods for determining the number of (isolated) real and complex
    roots of such systems.",
  paper = "Peti99.pdf",
  keywords = "axiomref"
}

@inproceedings{Peti93,
  author = "Petitot, Michel",
  title = {{Experience with Axiom}},
  booktitle = "Proc. Int. IMACS Symposium on Symbolic Computation",
  pages = "240",
  year = "1993",
  keywords = "axiomref",
  beebe = "Petitot:1993:EA"
}

@InProceedings{Prit06,
  author = "Pritchard, F. Leon and Sit, William Y.",
  title = {{On initial value problems for ordinary differential-algebraic 
           equations}},
  booktitle = "Radon Series on Computational and Applied Mathematics",
  year = "2006",
  pages = "283-340",
  isbn = "978-3-11-019323-7",
  abstract =
    "This paper addresses polynomial implicit ODEs in an autonomous
    context. These ODEs are defined by a system of the form
    \[f_i(z_1,\cdots,z_n,\dot{z_1},\cdots,\dot{z_n})=0,\quad i=1,\cdots,m\]
    where $f_i$ is (for $i=1,\ldots,m$) a polynomial in $2n$ variables
    $(X,P)=(X_1,\ldots,X_n,P_1,\ldots,P_n)$. This covers in particular
    quasilinear systems, often encountered in applications and defined
    by polynomials $f_i$ in which the total degree in the variables $P$
    is at most one. The approach is close to the geometrical framework
    of Rabier and Rheinboldt, profitting from the polynomial form of
    the system.

    The authors develop an algorithm for the symbolic computation of
    the set of consistent initial values via ideal-theoretic results; 
    this is based on a stationary algebraic process of ``prolongation'',
    together with the notions of the completion of a given ideal and the
    algebraic index of the system, defined as the number of steps taken
    by the process to stabilize. Over- and under-determined systems are
    also accommodated in their framework.",
  keywords = "axiomref"
}

@inproceedings{Purt86,
  author = "Purtilo, J.",
  title = {{Applications of a software interconnection system in 
           mathematical problem solving environments}},
  booktitle = "Proc.1986 Symposium on Symbolic and Algebraic Computation",
  series = "SYMSAC '86",
  pages = "16-23",
  year = "1986",
  publisher = "ACM Press",
  isbn = "0-89791-199-7",
  doi = "http://dx.doi.org/10.1145/32439.32443",
  keywords = "axiomref",
  beebe = "Purtilo:1986:ASI"
}

@book{Rals78,
  author = "Ralston, Anthony and Rabinowitz, Philip",
  title = {{A First Course in Numerical Analysis}},
  year = "1978",
  publisher = "McGraw-Hill",
  isbn = "0-07-051158-6",
}

@article{Riga99,
  author = "Rigal, Alain",
  title = {{High-order compact schemes: Application to bidimensional unsteady
           diffusion-convection problems.}},
  journal = "C. R. Acad. Sci.",
  volume = "328",
  number = "6",
  pages = "535-538",
  year = "1999",
  abstract =
    "For unsteady 2D diffusion-convection problems, we present two classes
    of compact difference schemes of order 2 in time and 4 in space. These
    finite difference schemes are essentially derived from 1D schemes,
    extensively analyzed in our previous paper [J. Comput. Phys. 114,
    No. 1, 59-76 (1994; Zbl 0807.65056)]. We propose two approaches:
    construction of 2D schemes as product of 1D schemes and global
    formulation of 2D schemes. Part II by M. Fournié [C. R. Acad. Sci.,
    Paris, Sér. I, Math. 328, No. 6, 539-542 (1999; reviewed below)]
    focuses on the development and analysis of global schemes with the
    assistance of symbolic computation software (AXIOM).",
  keywords = "axiomref"
}

@article{Riob09,
  author = "Rioboo, Renaud",
  title = {{Invariants for the FoCaL language}},
  journal = "Ann. Math. Artif. Intell.",
  volume = "56",
  number = "3-4",
  pages = "273-296",
  year = "2009",
  abstract =
    "We present a FoCaL formalization for quotient structures which are
    common in mathematics. We first present a framework for stating
    invariant properties of the data manipulated by running programs.  A
    notion of equivalence relation is then encoded for the FoCaL library.
    It is implemented through projections functions, this enables us to
    provide canonical representations which are commonly used in Computer
    Algebra but seldom formally described. We further provide a FoCaL
    formalization for the code used inside the library for modular
    arithmetic through the certification of quotient groups and quotient
    rings which are involved in the model. We finally instantiate our
    framework to provide a trusted replacement of the existing FoCaL
    library.",
  keywords = "axiomref"
}

@article{Roan03,
  author = "Roanes-Lozano, Eugenio and Roanes-Macias, Eugenio and
            Villar-Mena, M.",
  title = {{A bridge between dynamic geometry and computer algebra}},
  journal = "Math. Comput. Modelling",
  volume = "37",
  number = "9-10",
  pages = "1005-1028",
  year = "2003",
  link = "\url{http://ac.els-cdn.com/S0895717703001158/1-s2.0-S0895717703001158-main.pdf}",
  abstract =
    "Both Computer Algebra Systems (CASs) and dynamic geometry systems
    (DGSs) have reached a high level of development. Some CASs (like Maple
    or Derive) include specific and powerful packages devoted to Euclidean
    geometry, but CASs have incorporated neither mouse drawing
    capabilities nor dynamic capabilities. Meanwhile, the well-known DGSs
    do not provide algebraic facilities.

    Maple’s and Derive’s paramGeo packages and the DGS-CAS translator (all
    freely available from the authors) make it possible to draw a
    geometric configuration with the mouse (using The Geometer’s Sketchpad
    3 or 4) and to obtain the coordinates, equations, etc., of the drawn
    configuration in Maple’s or Derive’s syntax. To obtain complicated
    formulae, coordinates of points or equations of loci, to perform
    automatic theorem proving and to perform automatic discovery directly
    from sketches are examples of straightforward applications. Moreover,
    this strategy could be adapted to other CASs and DGSs.

    This work clearly has a didactic application in geometric problems
    exploration. Nevertheless, its main interest is to provide a
    convenient time-saving way to introduce data when dealing with rule
    and compass geometry, which has a wider scope than only educational
    purposes.",
  paper = "Roan03.pdf",
  keywords = "axiomref"
}

@article{Roan10,
  author = "Roanes-Lozano, Eugenio and val Labeke, Nicolas and 
            Roanes-Macias, Eugenio",
  title = {{Connecting the 3D DGS Calques3D with the CAS Maple}},
  journal = "Math. Comput. Simul.",
  volume = "80",
  number = "6",
  pages = "1153-1176",
  year = "2010",
  link = "\url{http://nvl.calques3d.org/publications/2010.MatCom.Connecting.pdf}",
  abstract =
    "Many (2D) Dynamic Geometry Systems (DGSs) are able to export numeric
    coordinates and equations with numeric coefficients to Computer
    Algebra Systems (CASs). Moreoever, different approaches and systems
    that linke (2D) DGSs with CASs, so that symbolic coordinates and
    equations with symbolic coefficients can be exported from the DGS to
    the CAS, already exist. Although the 2D DGS Calques3D can export
    numeric coordinates and equations with numeric coefficients to Maple
    and Mathematica, it cannot export symbolic coordinates and equations
    with symbolic coefficients. A connetion between the 3D DGS Calques3D
    and the CAS Maple, that can handle symbolic coordinates and equations
    with symbolic coefficients, is presented here. Its main interest is to
    provide a convenient time-saving way to explore problems and directly
    obtain both algebraic and numeric data when dealing with a 3D
    extension of ``ruler and compass geometry''. This link has not only
    educational purposes but mathematical ones, like mechanical theorem
    proving in geometry, geometry discovery (hypothesis completion),
    geometric loci finding -- As far as we know, there is no comparable
    ``symbolic'' link in the 3D case, except the prototype 3D-LD
    (restricted to determining algebraic surfaces as geometric loci).",
  paper = "Roan10.pdf",
  keywords = "axiomref"
}

@article{Roes99,
  author = "Roesner, K. G.",
  title = {{Supersonic flow around accelerated and decelerated bodies,
           analysed by analytical methods}},
  journal = "Z. Angew. Math. Mech.",
  volume = "79",
  number = "3",
  pages = "815-816",
  year = "1999",
  abstract =
    "By an extensive use of the computer algebra system AXIOM, a power
    series expansion with respect to the radial variable $r$ is used to
    describe the accelerated or decelerated supersonic flow field around
    the tip of slender conical bodies. The set of coupled nonlinear
    differential equations for the coefficient functions, depending on
    $\theta$ and $t$, is derived in closed form, and the first and second
    approximation of the coefficient functions are determined numerically.",
  keywords = "axiomref"
}

\article{Roja13,
  author = "Rojas-Bruna, Carlos",
  title = {{Trace forms and ideals on commutative algebras satisfying an 
           identity of degree four}},
  journal = "Rocky Mt. J. Math.",
  year = "2013",
  volume = "43",
  number = "4",
  pages = "1325-1336",
  abstract =
    "This paper deals with the variety of commutative algebras satsifying
    the identity
    \[((xy)z)t - ((xy)t)z + ((yt)x)z - ((yt)z)x + ((yz)t)x - ((yz)x)t = 0\]
    These algebras appeared in the classification of the degree four
    identities in Carini et al. We prove the existence of a trace
    form. Moreover, if we assume the existence of degenerate trace form,
    the $A$ satisfies the identity $((yx)x)x=y((xx)x)$, a generalization
    of right-alternativity.  Finally we prove that $Ass[A]$ and $N(A)$ are
    ideals in these algebras.",
  keywords = "axiomref"
}

@misc{Robi93,
  author = "Robidoux, Nicolas",
  title = {{Does Axiom Solve Systems of O.D.E's Like Mathematica?}},
  year = "1993",
  link = "\url{http://axiom-wiki.newsynthesis.org/public/refs/Robidoux.pdf}",
  abstract = "
    If I were demonstrating Axiom and were asked this question, my reply
    would be ``No, but I am not sure that this is a bad thing''. And I
    would illustrate this with the following example.

    Consider the following system of O.D.E.'s
    \[
    \begin{array}{rcl}
    \frac{dx_1}{dt} & = & \left(1+\frac{cos t}{2+sin t}\right)x_1\\
    \frac{dx_2}{dt} & = & x_1 - x_2
    \end{array}
    \]
    This is a very simple system: $x_1$ is actually uncoupled from $x_2$",
  paper = "Robi93.pdf",
  keywords = "axiomref"
}

@article{Roes95,
  author = "Roesner, K. G.",
  title = {{Verified solutions for parameters of an exact solution for 
           non-Newtonian liquids using computer algebra}},
  journal = "Zeitschrift fur Angewandte Mathematik und Physik", 
  volume = "75",
  number = "suppl. 2",
  pages = "S435-S438", 
  year = "1995",
  abstract =
    "An exact solution of the time independent velocity field for the
    Taylor--Couette flow of a polymer solution is derived solving the
    resulting first order ordinary differential equation of fifth degree
    analytically. Intensive use is made of computer algebra systems AXIOM
    and MACSYMA to find the exact solution.  The coaxial cylinders in the
    Taylor--Couette flow problem are assumed to rotate at different
    angular velocities. The geometrical and kinematic parameters can be
    chosen arbitrarily. The model equation for the material law of the
    viscoelastic liquid is based on the thermodynamic model for dilute
    solutions due to Lhuillier and Ouibrahim (1980) which is an analogy to
    the earlier paper of Frankel and Acrivos (1970). In the present
    investigation the influence of the parameters of the viscoelastic
    model on the velocity profile in the cylindrical gap is studied and
    the range of validity of the constitutive equation is investigated.",
  keywords = "axiomref",
  beebe = "Roesner:1995:VSP"
}

@article{Safo00,
  author = "Safouhi, Hassan",
  title = {{The $HD$ and $H\overline{D}$ methods for accelerating the 
          convergence of three-center nuclear attraction and four-center
          two-electron Coulomb integrals over $B$ functions and their
          convergence properties.}},
  journal = "J. Comput. Phys.",
  volume = "165",
  number = "2",
  pages = "473-495",
  year = "2000",
  abstract = 
    "Three-center nuclear attraction and four-center two-electron
    Coulomb integrals over Slater-type orbitals are required for $ab$
    initio and density functional theory molecular structure
    calculations. They occur in many millions of terms, even for small
    molecules and require rapid and accurate evaluation. The $B$
    functions are used as a basis set of atomic orbitals. These
    functions are well adapted to the Fourier transform method that
    allowed analytical expressions for the integrals of interest to be
    developed. Rapid and accurate evaluation of these analytical
    expressions is now made possible by applying the $HD$ and
    $H\overline{D}$ methods for accelerating the convergence of the
    semi-infinite oscillatory integrals. The convergence properties of
    the new methods are analyzed. The numerical results section shows
    the high predetermined accuracy and the substantial gain in the
    calculation times obtained using the new methods.",
  keywords = "axiomref"
}

@article{Safo01,
  author = "Safouhi, Hassan",
  title = {{Numerical evaluation of three-center two-electron Coulomb and
           hybrid integrals over $B$ functions using the $HD$ and 
           $H\overline{D}$ methods and convergence properties}},
  journal = "J. Math. Chem.",
  volume = "29",
  number = "3",
  pages = "213-232",
  year = "2001",
  abstract = 
    "The $B$ function is a product of a spherical $K$ Bessel function and
    a spherical harmonic. The integrals to be evaluated contain
    complicated expressions of finite hypergeometric functions and
    spherical $J$ Bessel functions. Sequence transformation techniques are
    used for the numerical evaluation of the integrals. Numerical examples
    illustrate the accuracy and the efficiency of the method.",
  keywords = "axiomref"
}

@misc{SALSA,
  author = "SALSA",
  title = {{Solvers for Algebraic Systems and Applications}},
  link = "\url{http://www.ens-lyon.fr/LIP/Arenaire/SYMB/teams/salsa/proposal-salsa.pdf}",
  algebra =
    "\newline\refto{category TSETCAT TriangularSetCategory}
     \newline\refto{category RSETCAT RegularTriangularSetCategory}
     \newline\refto{category NTSCAT NormalizedTriangularSetCategory}
     \newline\refto{category SFRTCAT SquareFreeRegularTriangularSetCategory}
     \newline\refto{package RSDCMPK RegularSetDecompositionPackage}",
  paper = "SALSA.pdf"
}

@techreport{Salv89,
  author = "Salvy, Bruno",
  title = {{Examples of automatic asymptotic expansions}},
  institution = "Inst. Nat. Recherche Inf. Autom.",
  type = "technical report",
  number = "114",
  year = "1989",
  comment = "SIGSAM Bulletin Vol 25 No 2 1991 pp4-17",
  abstract =
    "We describe the current state of a Maple library, gdev, designed to
    perform asymptotic expansions for a large class of expressions. Many
    examples are provided, along with a short sketch of the underlying
    principles. At the time when this report is written, a striking
    feature of these examples is that none of them can be computed
    directly with any of today's most widespread symbolic computation
    systems (Macsyma, Mathematica, Maple or Scratchpad II).",
  paper = "Salv89.pdf",
  keywords = "axiomref",
  beebe = "Salvy:1989:EAA"
}

@article{Salv91,
  author = "Salvy, Bruno",
  title = {{Examples of automatic asymptotic expansions}},
  journal = "SIGSAM Bulletin",
  volume = "25",
  number = "2",
  pages = "4-17",
  year = "1991",
  abstract =
    "We describe the current state of a Maple library, gdev, designed to
    perform asymptotic expansions for a large class of expressions. Many
    examples are provided, along with a short sketch of the underlying
    principles. At the time when this report is written, a striking
    feature of these examples is that none of them can be computed
    directly with any of today's most widespread symbolic computation
    systems (Macsyma, Mathematica, Maple or Scratchpad II).",
  keywords = "axiomref",
  beebe = "Salvy:1991:EAA"
}

@article{Sant95,
  author = "Santas, Philip S.",
  title = {{A type system for computer algebra}},
  journal = "J. Symbolic Computation",
  volume = "19",
  number = "1-3",
  pages = "79-109",
  year = "1995",
  abstract =
    "This paper presents a type system for support of subtypes,
    parameterized types with sharing and categories in a computer algebra
    environment. By modeling representation of instances in terms of
    existential types, we obtain a simplified model, and build a basis for
    defining subtyping among algebraic domains. The inheritance at
    category level has been formalized; this allows the automatic
    inference of type classes. By means of type classes and existential
    types we construct subtype relations without involving coercions. A
    type sharing mechanism works in parallel and allows the consistent
    extension and combination of domains. The expressiveness of the system
    is further increased by viewing domain types as special case of
    package types, forming weak and strong sums respectively. The
    introduced system, although awkward at first sight, is simpler than
    other proposed systems for computer algebra without including some of
    their problems. The system can be further extended in other to support
    more constructs and increase its flexibility.",
  paper = "Sant95.pdf",
  keywords = "axiomref, printed"
}

@inproceedings{Sant96,
  author = "Santas, Philip S.",
  title = {{Conditional Categories and Domains}},
  booktitle = "Proc. DISCO 1996",
  year = "1996",
  pages = "112-125",
  isbn = "3-540-61697-7",
  abstract = 
    "We extend the Type System defined in [San95] with Axiom-like
    Conditional Categories with the additional property of Static Typing
    and Checking.  Categories and Domains may contain conditionals in
    their bodies, which are elaborated by our compiler by techniques used
    in standard typing.  We define an appropriate calozlus and discuss
    its properties.  Examples inspired by the Axiom library illustrate the
    power of our approach and its application in constructing algebraic
    concepts.  The full calculus has been implemented and tested with our
    LA compiler which generates executable files.",
  paper = "Sant96.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@book{Sant05,
  author = "Santas, Philip S.",
  title = {{Conditional Categories and Domains}},
  booktitle = "Design and Implementation of Symbolic Computation Systems",
  year = "2005",
  series = "Lecture Notes in Computer Science",
  volume = "1128",
  publisher = "Springer",
  abstract =
    "We extend the Type system defined in [Sant95] with Axiom-like
    Conditional Categories with the additional property of Static Typing
    and Checking. Categories and Domains may contain conditionals in their
    bodies, which are elaborated by our compiler by techniques used in
    standard typing. We define an appropriate calculus and discuss its
    properties. Examples inspired by the Axiom library illustrate the
    power of our apprach and its application in constructing algebraic
    concepts. The full calculus has been implemented and tested with our
    LA compiler which generated executable files.",
  keywords = "axiomref"
}

@article{Saun80,
 author = "Saunders, B. David",
 title = {{A Survey of Available Systems}},
 journal = "SIGSAM Bull.",
 issue_date = "November 1980",
 volume = "14",
 number = "4",
 month = "November",
 year = "1980",
 issn = "0163-5824",
 pages = "12--28",
 numpages = "17",
 link = "\url{http://doi.acm.org/10.1145/1089235.1089237}",
 doi = "10.1145/1089235.1089237",
 acmid = "1089237",
 publisher = "ACM",
 address = "New York, NY, USA",
 paper = "Saun80.pdf",
 keywords = "axiomref,survey"
}

@book{Schw95,
  author = "Schwardmann, Ulrich",
  title = {{Computer algebra systems}},
  comment = "Computeralgebra-Systeme, German",
  publisher = "Addison-Wesley",
  year = "1995",
  keywords = "axiomref"
}

@inproceedings{Schw87,
  author = "Schwarz, Fritz",
  title = {{Programming with abstract data types: the symmetry package 
          SPDE in Scratchpad'}},
  booktitle = "Trends in Computer Algebra",
  series = "Lecture Notes in Computer Science 296",
  year = "1987",
  pages = "167-176",
  isbn = "3-540-18928-9",
  abstract =
    "The main problem which occurs in developing Computer Algebra packages
    for special areas in mathematics is the complexity. The unique concept
    which is advocated to cope with that problem is the introduction of
    suitable abstract data types. The corresponding decomposition into
    modules makes it much easier to develop, maintain and change the
    program. After introducing the relevant concepts from software
    engineering they are elaborated by means of the symmetry analysis of
    differential equations and the Scratchpad package SPDE which
    abbreviates Symmetries of Partial Differential Equations.",
  paper = "Schw87.pdf",
  keywords = "axiomref, printed",
  beebe = "Schwarz:1988:PAD"
}

@inproceedings{Schw89,
  author = "Schwarz, Fritz",
  title = {{A factorization algorithm for linear ordinary 
           differential equations}},
  booktitle = "Proc. SYMSAC 1989",
  series = "SYMSAC '89",
  isbn = "0-89791-325-6",
  year = "1989",
  pages = "17-25",
  abstract = 
    "The reducibility and factorization of linear homogeneous differential
    equations are of great theoretical and practical importance in
    mathematics.  Although it has been known for a long time that
    factorization is in principle a decision procedure, its use in an
    automatic differential equation solver requires a more detailed
    analysis of the various steps involved. Especially important are
    certain auxiliary equations, the so-called associated equations. An
    upper bound for the degree of its coefficients is derived.  Another
    important ingredient is the computation of optimal estimates for the
    size of polynomial and rational solutions of certain differential
    equations with rotational coefficients. Applying these results, the
    design of the factorization algorithm LODEF and its implementation in
    the Scratchpad II Computer Algebra System is described.",
  paper = "Schw89.pdf",
  keywords = "axiomref",
  beebe = "Schwarz:1989:FAL"
}

@article{Schw91,
  author = "Schwarz, Fritz",
  title = {{Monomial orderings and Groebner bases}},
  journal = "SIGSAM Bulletin",
  volume = "25",
  number = "1",
  year = "1991", 
  pages = "10-23",
  abstract =
    "Let there be given a set of monomials in n variables and some order
    relations between them. The following {\sl fundamental problem of
    monomial ordering} is considered. Is it possible to decide whether
    these ordering relations are consistent and if so to extend them to an
    {\sl admissible} ordering for all monomials? The answer is given in
    terms of the algorithm {\sl MACOT} which constructs a matrix of so
    called {\sl cotes} which establishes the desired ordering
    relations. The main area of application of this algorithm, i.e. the
    construction of Groebner bases for different orderings and of
    universal Groebner bases is treated in the last section.",
  keywords = "axiomref",
  beebe = "Schwarz:1991:MOG"
}

@InProceedings{Schw02,
  author = "Schwarz, Fritz",
  title = {{ALLTYPES: An algebraic language and type system}},
  booktitle = "1st Int. Congress on Mathematical Software",
  year = "2002",
  isbn = "981-238-048-5",
  location = "Beijing China",
  pages = "486-500",
  link = "\url{http://www.scai.fraunhofer.de/content/dam/scai/de/documents/Mitarbeiterinnen-und-Mitarbeiter/ICMS2002.pdf}",
  abstract =
    "The software system ALLTYPES provides an environment that is
    particularly designed for developing software in differential
    algebra. Its most important features may be described as follows: A
    set of about thirty parametrized algebraic types is defined. Data
    objects represented by these types may be manipulated by more than one
    hundred polymorphic functions. Reusability of code is achieved by
    genericity and multiple inheritance. The user may extend the system by
    defining new types and polymorphic functions. A language comprising
    seven basic language constructs is defined for implementing
    mathematical algorithms. The easy manipulation of types is
    particularly supported due to a special portion of the language
    dedicated to manipulating typed objects, i.e. for performing
    user-defined or automatic type coercions. Type inquiries are also
    included in the language.",
  paper = "Schw02.pdf",
  keywords = "axiomref"
}

@InProceedings{Schw94,
  author = "Schwarz, Fritz",
  title = {{Computer algebra software for scientific applications}},
  booktitle = "Computerized symbolic manipulation in mechanics",
  year = "1994",
  publisher = "Springer-Verlag",
  pages = "67-117",
  series = "CISM Courses Lecture 343",
  abstract =
    "The central subject of this article are two basic questions: How to
    make the process of developing computer algebra software on a large
    scale ($10^4$ to $10^5$) lines of code or more) more efficient and
    how to improve the quality of the result. Taking procedures from well
    established engineering sciences as a guide, two fundamental
    principles turned out to be of overwhelming importance: Modularization
    and limitation of growth through reuse. Important means for achieving
    these goals turned out to be concept of an abstract data type and the
    principles of object-oriented design. It is advocated to install an
    additional abstraction level between the mathematics and the machine
    in order to render it possible to develop (computer algebra) system
    independent mathematical software. Basic constituents of this level
    are a type system and a high-level language.",
  keywords = "axiomref"
}

@phdthesis{Seil94,
  author = "Seiler, Werner Markus",
  title = {{Analysis and Application of the Formal Theory of 
           Partial Differential Equations}},
  school = "Universitat Karlsruhe",
  year = "1994",
  link = 
  "\url{http://www.mathematik.uni-kassel.de/~seiler/Papers/Diss/diss.ps.gz}",
  abstract = "
    An introduction to the formal theory of partial differential equations
    is given emphasizing the properties of involutive symbols and
    equations.  An algorithm to complete any differential equation to an
    involutive one is presented. For an involutive equation possible
    values for the number of arbitrary functions in its general solution
    are determined. The existence and uniqueness of solutions for analytic
    equations is proven.  Applications of these results include an
    analysis of symmetry and reduction methods and a study of gauge
    systems. It is show that the Dirac algorithm for systems with
    constraints is closely related to the completion of the equation of
    motion to an involutive equation. Specific examples treated comprise
    the Yang-Mills Equations, Einstein Equations, complete and Jacobian
    systems, and some special models in two and three dimensions. To
    facilitate the involved tedious computations an environment for
    geometric approaches to differential equations has been developed in
    the computer algebra system Axiom. The appendices contain among others
    brief introductions into Carten-K{\"a}hler Theory and Janet-Riquier
    Theory.",
  paper = "Seil94.pdf",
  keywords = "axiomref"
}

@inproceedings{Seil94a,
  author = "Seiler, Werner Markus",
  title = {{Completion to involution in AXIOM}},
  booktitle = "Proc. Rhine Workshop on Computer Algebra",
  year = "1994",
  pages = "103-104",
  abstract =
    "We have implemented an algorithm to complete a given system of
    partial differential equations to an involutive one in the computer
    algebra system AXIOM. An earlier version of this program has been
    described in Schu, Seiler, and Calmet (1992). The new version is much
    more efficient due to better simplification and many new features. It
    also provides procedures for the analysis of the arbitrariness of the
    general solution.  The goal of the implementation was not to transform
    simply an algorithm into a program but to start with the construction
    of an environment for symbolic computations within the geometric
    theory of differential equations. The modular structure allows an easy
    extension e.g. by a package for the symmetry analysis.",
  keywords = "axiomref",
  beebe = "Seiler:1994:CIA"
}

@article{Seil94b,
  author = "Seiler, Werner Markus",
  title = {{Pseudo Differential Operators and Integrable Systems in AXIOM}},
  journal = "Computer Physics Communications",
  volume = "79",
  number = "2",
  pages = "329-340",
  year = "1994",
  abstract = 
    "An implementation of the algebra of pseudo differential operators in
    the computer algebra system Axiom is described. In several exmaples
    the application of the package to typical computations in the theory
    of integrable systems is demonstrated.",
  paper = "Seil94b.pdf",
  keywords = "axiomref",
  beebe = "Seiler:1994:PDO"
}

@misc{Seil95,
  author = "Seiler, Werner Markus",
  title = {{Applying AXIOM to partial differential equations}},
  institution = {Universit\"at Karlsruhe, Fakult\"at f\"ur Informatik},
  year = "1995",
  type = "Internal Report",
  number = "95-17",
  link = "\url{http://axiom-wiki.newsynthesis.org/public/refs/Axiom-pdf.pdf}",
  abstract = 
    "We present an Axiom environment called JET for geometric computations
    with partial differential equations within the framework of the jet
    bundle formalism. This comprises expecially the completion of a given
    differential equation to an involutive one according to the
    Cartan-Kuranishi Theorem and the setting up of the determining system
    for the generators of classical and non-classical Lie
    symmetries. Details of the implementations are described and
    applications are given. An appendix contains tables of all exported
    functions.",
  paper = "Seil95.pdf",
  keywords = "axiomref"
}

@misc{Seil95a,
  author = "Seiler, Werner Markus and Calmet, J.",
  title = {{JET -- An Axiom Environment for Geometric Computations with 
           Differential Equations}},
  link = "\url{http://axiom-wiki.newsynthesis.org/public/refs/axiom-jet95.pdf}",
  abstract = 
    "JET is an environment within the computer algebra system Axiom to
    perform such computations. The current implementation emphasises the
    two key concepts involution and symmetry. It provides some packages
    for the completion of a given system of differential equations to an
    equivalent involutive one based on the Cartan-Kuranishi theorem and
    for setting up the determining equations for classical and
    non-classical point symmetries.",
  paper = "Seil95a.pdf",
  keywords = "axiomref"
}

@article{Seil97,
  author = "Seiler, Werner M.",
  title = {{Computer Algebra and Differential Equations: An Overview}},
  journal = "mathPAD7",
  volume = "7",
  pages = "34-49",
  year = "1997",
  link = "\url{http://www.mathematik.uni-kassel.di/~seiler/Papers/Postscript/CADERep.ps.gz}",
  abstract = 
    "We present an informal overview of a number of approaches to
    differential equations which are popular in computer algebra. This
    includes symmetry and completion theory, local analysis, differential
    ideal and Galois theory, dynamical systems and numerical analysis.  A
    large bibliography is provided.",
  paper = "Seil97.pdf",
  keywords = "axiomref"
}

@misc{Seil99,
  author = "Seiler, Werner Markus",
  title = {{DETools: A Library for Differential Equations}},
  year = "1999",
  abstract =
    "This article tries to give at least a brief introduction. The MuPAD
    library is extended on two levels. The first one consists of a new
    library detools containing a number of routines for treating
    differential equations. This includes support for the graphical
    presentation of the output of the numerical routines in MuPAD, some
    methods for analysing or generating differential equations and also
    routines for solving some classes of partial differential
    equations. The use of this new library will be described in this
    article. The second level is somewhat more advanced and requires a
    certain familiarity with the object-oriented domains.",
  paper = "Seil99.pdf",
  keywords = "axiomref"
}

@misc{Seil01,
  author = "Seiler, Werner Markus",
  title = {{Involution - The Formal Theory of Differential Equations and
           its Applications in Computer Algebra and Numerical Analysis}},
  year = "2001",
  link =
  "\url{http://www.mathematik.uni-kassel.de/~seiler/Papers/Habil/habil.ps.gz}",
  paper = "Seil01.pdf",
  keywords = "axiomref"
}

@techreport{Sene87,
  author = "Senechaud, P. and Siebert, F. and Villard, Gilles",
  title = {{Scratchpad II: Pr{\'e}sentation d'un nouveau langage de 
           calcul formel}},
  type = "Technical Report",
  number = "640-M", 
  institution = "TIM 3 (IMAG)", 
  address = "Grenoble, France", 
  year = "1987",
  keywords = "axiomref",
  beebe = "Senechaud:1987:SIP"
}

@techreport{Sene87a,
  author = "Senechaud, Pascale and Siebert, F.",
  title = {{Etude dl l'algorithme de Kovacic et son implantation sur
           Scratchpad II}},
  type = "Technical Report",
  number = "639", 
  institution = "Institut IMAG, Informatique et Mathematiques Appliquees
                 de Grenoble",
  address = "Grenoble, France", 
  year = "1987",
  keywords = "axiomref"
}

@article{Shan88,
  author = "Shannon, D. and Sweedler, M.",
  title = {{Using Gr\"obner bases to determine algebra membership, 
           split surjective algebra homomorphisms determine birational 
           equivalence}},
  journal = "Journal of Symbolic Computation",
  volume = "6",
  number = "2-3",
  pages = "267-273",
  year = "1988",
  abstract =
    "This paper presents a simple algorithm, based on Groebner bases, to
    test if a given polynomial $g$ of $k(X_1,\ldots,X_n)$ lies in 
    $k(f_1,\ldots,f_m)$ where $k$ is a field, $X_1,\ldots,X_n$ are
    indeterminates over $k$ and $f_1,\ldots,f_m$ in $k(X_1,\ldots,X_n)$.
    If so, the algorithm produces a polynomial $P$ of $m$ variables
    where $g=P(f_1,\ldots,f_m)$. Say $\Omega(B)$ to $k(X_1,\ldots,X_n)$ is a 
    homomorphism where $\Omega(b_i)=f_i$, for algebra generators ($b_i$)
    contained in / implied by $B$. If $\Omega$ is onto, the algorithm
    gives a homomorphism $\lambda k(X_1,\ldots,X_n)$ to $B$, where the
    composite $\Omega \lambda$ is the identity map. In particular, the
    algorithm computes the inverse of algebra automorphisms of the 
    polynomial ring. A variation of the test if 
    $k(f_1,\ldots,f_m)=k(X_1,\ldots,X_n)$ tells if 
    $k(f_1,\ldots,f_m)=k(X_1,\ldots,X_n)$. Existing computer algebra
    systems, such as IBM's SCRATCHPAD II, have Groebner basis packages
    which allow the user to specify a term ordering sufficient to carry
    out the algorithm.",
  keywords = "axiomref",
  beebe = "Shannon:1988:UGB"
}

@misc{SIGS16,
  author = "SIGSAM, ACM",
  title = {{Axiom}},
  link = "\url{http://www.sigsam.org/software/axiom.html}",
  year = "2016",
  contact = "Infodir\_SIGSAM\@acm.org",
  abstract =
    "Axiom is a free, open source, general-purpose computer algebra
    system. It features a strongly typed language. The system has an
    interactive interpreter and a compiler. It includes over 1100
    supported categories, domains, and packages covering large areas of
    Mathematics."
}

@article{Sing93,
  author = "Singer, Michael F. and Ulmer, Felix",
  title = {{Galois groups of second and third order linear differential 
           equations}},
  journal = "J. Symb. Comput.",
  volume = "16",
  number = "1",
  pages = "9-36",
  year = "1993",
  abstract = 
    "The authors discuss the first problem of Galois theory of differential
    equations. Let $F$ be an ordinary (for simplicity) differential field
    and $L(y)=0$ be an ordinary linear differential equation over $F$. How
    can one calculate the Galois group of $L$ over $F$? The authors
    suppose a new approach to the problem. They reduce it to the problem
    of finding solutions of linear differential equations in $F$ and to
    the factorization problem of such equations over $F$. These allow them
    to give simple necessary and sufficient conditions for a second order
    linear differential equation to have Liouvillian solutions and for a
    third order linear differential equation to have Liouvillian solutions
    or to be solvable in terms of second order equations.",
  paper = "Sing93.pdf",
  keywords = "axiomref"
}

@article{Sing93a,
  author = "Singer, Michael F. and Ulmer, Felix",
  title = {{Liouvillian and algebraic solutions of second and third order 
           linear differential equations}},
  journal = "J. Symb. Comput.",
  volume = "16",
  number = "1",
  pages = "37-73",
  year = "1993",
  abstract =
    "Let $F$ be an ordinary differential field of characteristic 0 and 
    $L \in F <y>$ be a linear homogeneous polynomial. How can one find the
    Liouvillian solutions of $L(y)=0$? In the paper this problem is
    reduced to the problems of (1) factorization and (2) finding $u$
    solutions such that $\frac{u^{\prime}}{y} \in F$ of $L$ and some
    polynomials associated with it (symmetric powers of $L$).
    
    Now there are the algorithms for the solution of the last problems for
    $F=\mathbb{Q}(x)$ [see D. Yu. Grigor’ev, J. Symb. Comput. 10, 7-37
    (1990; Zbl 0728.68067) and M. F. Singer, Am. J. Math. 103, 661-682
    (1981; Zbl 0477.12026)].
    
    For polynomials $L$ of the second and third order the authors provide
    full investigation of the most difficult case when the solution $u$ of
    $L(y)$ is algebraic. They show that one can compute the minimal
    polynomial $P(y) \in F[y]$ of $u$. We note that the authors
    essentially used the tools of representation theory, invariant theory
    and computer algebra.",
  paper = "Sing93a.pdf",
  keywords = "axiomref"
}

@inproceedings{Sitx89,
  author = "Sit, William Y.",
  title = {{On Goldman's algorithm for solving first-order multinomial 
           autonomous systems}},
  booktitle = "Proc. Algebraic Algorithms and Error-Correcting Codes, AAECC-6",
  series = "Lecture Notes in Computer Science 357",
  location = "Rome, Italy",
  year = "1988",
  isbn = "3-540-51083-4",
  pages = "386-395",
  abstract =
    "In this article, a brief exposition of a method for finding first
    integrals for first order multinomial autonomous systems (FOMAS) of
    ordinary differential equations with constant coefficients will be
    given. The method is a simplified as well as a redesigned version
    based on a paper of Goldman (1987). We shall see how it can be applied
    to FOMAS with parametric coefficients. The algorithm is currently
    being implemented by the author, using the SCRATCHPAD II computer
    algebra language and system at the IBM T.J. Watson Research Center.
    
    FOMAS occur and are of interest in many disciplines and their first
    integrals (or trajectories of motion) are generally difficult to
    find. Examples of FOMAS are too numerous to list, some well-known ones
    are the Riccati equation, the Lotka-Volterra equations for competing
    populations, Selkov's model for chemical reactions, the Lorenz system
    of the Rayleigh-Bernard problem, and Hamiltonian systems (where the
    Hamiltonian is a sum of monomial terms with constant coefficients).
    
    Let $Y=(y_1,\ldots,y_n)$ be $n$ functions depending on the variable
    $\tau$. A monomial in $Y$ is a product of the form $y_1^{k_1}\cdots
    y_n^{k_n}$, where $k_1,\ldots,k_n$ are constants. If
    $K=(k_1,\ldots,k_n)$, we shall denote the monomial in $Y$ by $Y^K$,
    and $K$ is called the exponent vector for the monomial. By convention,
    exponent vectors are column vectors, but whenever convenient, we shall
    write exponent vectors as row vectors. We say that $Y$ satisfies a
    first-order multinomial autonomous system (FOMAS) if for each $i$, $1
    \le i \le n$, $y_i$ satisfies a first order differential equation of
    the form:
    \[y_i^{\prime} = f_i(Y)\quad\quad\quad(1)\]
    where $f_i$ is a linear combination of monomials in $Y$ with coefficients
    which may be either constants or parametric constants. For example, the
    Lotka-Volterra equations for three competing species considered by
    Schwarz and Steeb (1984), form a FOMAS:
    \[x_1^{\prime}=x_1(1+ax_2+bx_3)\]
    \[x_2^{\prime}=x_2(1-ax_1+bx_3)\]
    \[x_3^{\prime}=x_3(1-bx_1-cx_2)\]
    When the exponent vectors occuring in $f_i$ are all non-negative integers,
    as in the example above, a FOMAS reduces to a polynomial autonomous
    system (FOPAS).
    
    A computer program was developed by Schwarz (1986) to compute the
    first integrals of FOPAS's which are themselves polynomials in
    $y_1,\ldots,y_n$. Schwarz's algorithm literally takes a general
    polynomial of a fixed degree $d$ in $n$ variables and substitutes it
    into (1). This method does not work well on a FOMAS, because in a
    FOMAS, the exponent vectors need not have integral components. Also,
    it wll not find integrals with exponent vectors that involve
    fractional or irrational numbers.
    
    Goldman (1987) proved a theorem which gives necessary and sufficient
    conditions for the existence of a multinomial first integral for
    FOMAS. The proof also contained the outline of an algorithm for
    finding such integrals. In Goldman's paper, he introduced the notion
    of an integral array, which is a certain matrix satisfying some 10
    conditions. He gave a few hints and several examples but did not
    elaborate on how such an integral array can be found in general
    (except in the case $q=2$). Assuming such an array is found, he can
    compute the integral, in most cases, by solving systems of linear
    equations, or at worse in certain cases, by solving a system of
    algebraic equations. It was not clear when algebraic conditions are
    necessary.
    
    In this brief exposition, Goldman's method will be expanded to a
    complete algorithm with a new simplified notation. The integral arrays
    are replaced by addition schemes (which is equivalent to integral
    arrays with some conditions removed). The generation of addition
    schemes is a combinatorial problem unrelated, in a sense, to
    FOMAS. When the first integral is a polynomial, the additioin scheme
    is trivial to compute. We shall now begin by explaining some details
    of this theory.",
  keywords = "axiomref",
  beebe = "Sit:1989:GAS"
}

@article{Sitx92,
  author = "Sit, William Y.",
  title = {{An algorithm for solving parametric linear systems}},
  journal = "Journal of Symbolic Computations",
  volume = "13",
  number = "4",
  pages = "353-394",
  year = "1992",
  abstract = 
    "We present a theoretical foundation for studying parametric systesm of
    linear equations and prove an efficient algorithm for identifying all
    parametric values (including degnerate cases) for which the system is
    consistent. The algorithm gives a small set of regimes where for each
    regime, the solutions of the specialized systems may be given
    uniformly. For homogeneous linear systems, or for systems were the
    right hand side is arbitrary, this small set is irredunant. We discuss
    in detail practical issues concerning implementations, with particular
    emphasis on simplification of results. Examples are given based on a
    close implementation of the algorithm in SCRATCHPAD II. We also give a
    complexity analysis of the Gaussian elimination method and compare
    that with our algorithm.",
  paper = "Sitx92.pdf",
  keywords = "axiomref",
  beebe = "Sit:1992:ASP"
}

@inproceedings{Smed92,
  author = "Smedley, Trevor J.",
  title = {{Using pictorial and object oriented programming for computer 
           algebra}},
  booktitle = "Applied computing --- technological challenges of the
              1990's: proceedings of the 1992 ACM\slash SIGAPP
              Symposium on Applied Computing, Kansas City Convention
              Center, March 1--3, 1992",
  publisher = "ACM Press",
  pages = "1243-1247",
  year = "1992",
  isbn = "0-89791-502-X",
  keywords = "axiomref",
  beebe = "Smedley:1992:UPO"
}

@article{Smit93,
  author = "Smith, Geoff C.",
  title = {{Group theory results with machine generated proofs}},
  journal = "An. Univ. Timis., Ser. Mat.-Inform.",
  volume = "31",
  number = "2",
  pages = "273-280",
  year = "1993",
  abstract =
    "There are a variety of theorems in group theory which admit of proofs
    by machine. This talk illustrates these techniques in action. Examples
    are given of this phenomenon, drawn from the theory of group
    presentations, and from the theory of $p$-groups. The systems involved
    include AXIOM, CAYLEY and QUOTPIC",
  keywords = "axiomref"
}

@InProceedings{Smit07,
  author = "Smith, Jacob and Dos Reis, Gabriel and Jarvi, Jaakko",
  title = {{Algorithmic differentiation in Axiom}},
  booktitle = "ACM SIGSAM Proceedings",
  series = "ISSAC 2007",
  year = "2007",
  pages = "347-354",
  isbn = "978-1-59593-743-8",
  abstract = "
    This paper describes the design and implementation of an algorithmic
    differentiation framework in the Axiom computer algebra system. Our
    implementation works by transformations on Spad programs at the level
    of the typed abstract syntax tree -- Spad is the language for extending
    Axiom with libraries. The framework illustrates an algebraic theory
    of algorithmic differentiation, here only for Spad programs, but
    we suggest that the theory is general. In particular, if it is
    possible to define a compositional semantics for programs, we define
    the exact requirements for when a program can be algorithmically
    differentiated. This leads to a general algorithmic differentiation
    system, and is not confined to functions which compute with basic
    data types, such as floating point numbers.",
  paper = "Smit07.pdf",
  keywords = "axiomref",
  beebe = "Smith:2007:ADA"
}

@phdthesis{Smit10,
  author = "Smith, Jacob Nyffeler",
  title = {{Techniques in Active and Generic Software Libraries}},
  school = "Texas A and M University",
  year = "2010",
  link = "\url{oaktrust.library.tamu.edu/bitstream/handle/1969.1/ETD-TAMU-2010-05-7823/SMITH-DISSERTATION.pdf}",
  abstract = 
    "Reusing code from software libraries can reduce the time and effort
    to construct software systems and also enable the development of
    larger systems. However, the benefits that come from the use of
    software libraries may not be realized due to limitations in the way
    that traditional software libraries are constructed. Libraries come
    equipped with application programming interfaces (API) that help
    enforce the correct use of the abstraction in those libraries. Writing
    new components and adapting existing ones to conform to library APIs
    may require substantial amounts of ``glue'' code that potentially
    affects software's efficiency, robustness, and ease-of-maintenance. 
    If, as a result, the idea of reusing functionality from a software 
    library is rejected, no benefits of reuse will be realized.
    
    This dissertation explores and develops techniques that support the
    construction of software libraries with abstraction layers that do not
    impede efficiency. In many situations, glue code can be expected to
    have very low (or zero) performance overhead. In particular, we
    describe advances in the design and development of active libraries --
    software libraries that take an active role in the compilation of the
    user's code. Common to the presented techniques is that they may
    ``break'' a library API (in a controlled manner) to adapt the
    functionality of the library for a particular use case.
    
    The concrete contributions of this dissertation are: a library API
    that supports iterator selection in the Standard Template Library,
    allowing generic algorithms to find the most suitable traversal
    through a container, allowing (in one case) a 30-fold improvement in
    performance; the development of techniques, idioms, and best practices
    for {\tt concepts} and {\tt concept\_maps} in C++, allowing the
    construction of algorithms for one domain entirely in terms of
    formalisms from a second domain; the construction of generic
    algorithms for algorithmic differentiation, implemented as an active
    library in Spad, language of the Open Axiom computer algebra system,
    allowing algorithmic differentiation to be applied to the appropriate
    mathematical object and not just concrete data-types; and the
    description of a static analysis framework to describe the generic
    programming notion of local specialization with Spad, allowing more
    sophisticated (value-based) control over algorithm selection and
    specialization in categories and domains.
    
    We will find that active libraries simultaneously increase the
    expressivity of the underlying language and the performance of
    software using those libraries",
  paper = "Smit10.pdf",
  keywords = "axiomref",
}

@misc{Smit12,
  author = "Smith, Robert",
  title = {{Excursions in Mathematics Using Lisp}},
  link = "\url{https://www.youtube.com/watch?v=nTI_d-jS6dI}",
  year = "2012",
  keywords = "axiomref"
}

@article{Spit11,
  author = "Spitters, Bas and van der Weegen, Eelis",
  title = {{Type classes for mathematics in type theory}},
  journal = "Math. Struct. Comput. Sci.",
  volume = "21",
  number = "4",
  pages = "795-825",
  year = "2011",
  link = "\url{https://arxiv.org/pdf/1102.1323.pdf}",
  abstract =
    "The introduction of first-class type classes in the Coq system calls
    for a re-examination of the basic interfaces used for mathematical
    formalisation in type theory. We present a new set of type classes for
    mathematics and take full advantage of their unique features to make
    practical a particularly flexible approach that was formerly thought
    to be infeasible. Thus, we address traditional proof engineering
    challenges as well as new ones resulting from our ambition to build
    upon this development a library of constructive analysis in which any
    abstraction penalties inhibiting efficient computation are reduced to
    a minimum.

    The basis of our development consists of type classes representing a
    standard algebraic hierarchy, as well as portions of category theory
    and universal algebra. On this foundation, we build a set of
    mathematically sound abstract interfaces for different kinds of
    numbers, succinctly expressed using categorical language and universal
    algebra constructions.

    Strategic use of type classes lets us support these high-level
    theory-friendly definitions, while still enabling efficient
    implementations unhindered by gratuitous indirection, conversion or
    projection.

    Algebra thrives on the interplay between syntax and semantics. The
    Prolog-like abilities of type class instance resolution allow us to
    conveniently define a quote function, thus facilitating the use of
    reflective techniques.",
  paper = "Spit11.pdf",
  keywords = "axiomref, printed"
}

@InProceedings{Schu92,
  author = "Schu, J. and Seiler, Werner Markus and Calmet, Jacques",
  title = {{Algorithmic Methods For Lie Pseudogroups'}},
  booktitle = "Proc. Modern Group Analysis: Advanced Analytical and 
               Computational Methods in Mathematical Physics",
  pages = "337-344",
  location =  "Acireale (Italy)",
  year = "1992",
  publisher = "Kluwer",
  link = "\url{http://www.iks.kti.edu/fileadmin/User/calmet/papers/Acireale-93.ps.gz}",
  abstract =
    "The authors recall the concepts of involutive non-linear systems of
    partial differential equations, the classical Cartan-Riquier-Janet
    criterion for involutiveness, and the Cartan-Kuranishi prolongation
    theorem. Then the algorithm of prolongation into involutive case is
    clearly outlined, the arising computational problems are discussed,
    experience with implementation in the computer algebra system AXIOM
    (1992) is described, and comparison with Maple and REDUCE algorithms
    is made.",
  paper = "Schu92.pdf",
  keywords = "axiomref"
}

@book{Stau95,
  author = "Stauffer, Deitrich",
  title = {{Annual Review of Computational Physics I}},
  publisher = "unknown",
  section = "5.2.2",
  year = "1995",
  abstract = 
    "Based on our experiences with IRENA, we decided to use generic
    inter-process communication tools for the link to AXIOM. This has the
    added advantage that we can operate across a network. The main
    technique we use is the {\sl Remote Procedure Call} (RPC) [Sun
    Microsystems Inc., 1988] which allows us to interact with a server on
    another machine (or on the local machine). RPC takes care of
    differences in data representation (e.g. the byte-order of floating
    point numbers) on different architectures.
    
    AXIOM is a multi-process package. Normally when a user starts up the
    system they start up the various components which then interact via
    standard socket operations. If they are using the line, they start up
    a new process: the NAG Manager (NAGMAN for short). Additionally, there
    will be a NAG daemon (NAGD) running on any machine on which the user
    may wish to execute NAG routines (which could include the local
    host). NAGMAN commnicates with the running AXIOM system via a socket
    down which is transmitted the details of and data for the particular
    routine to be called. NAGMAN calls a NAGD on another machine via RPC
    and eventually returns the results to AXIOM.
    
    NAGD consists of the server program, and a set of stub codes designed
    to call individual NAG routines. It is, in effect, a remotely-callable
    version of the NAG library. There is no reason why AXIOM should be the
    only system to use it, and indeed there are plans to incorporate the
    ability to call NAGD into other systems.
    
    An ASP is treated just like any other piece of data by the AXIOM-NAG
    link. The source code is passed to NAGD and compiled. (There are
    various optimisations to prevent the same code being compiled multiple
    times, but the details needn't concern us here.) This compiled code is
    linked with the NAG Library to make the executable. Thus if a user
    calls the same NAG routine with different ASPs the routine will be
    relinked each time.
    
    It would be nice if this were not necessary. The authors of the link
    considered two other possibilities:
    \begin{itemize}
    \item Have AXIOM simulate the ASPs, so that the NAG Library would call
    back to AXIOM when it wanted to call an ASP. This was rejected as
    being far too slow across a network.
    \item Give NAGD the ability to interpret AXIOM or Fortran code. Thus
    the NAG routine would call a function which would evaluate a
    representation of an ASP to get the required values. This may happen
    in the future if data interchange mechanisms between systems are
    stanardised, but was rejected for the time being since such a system
    would have to be tailored to match each Fortran compiler that NAGD used.
    \end{itemize}
    
    By transmitting source code for ASPs we allow the remote Fortran
    compiler to take care of low-level portability problems.
    
    Both IRENA and the AXIOM-NAG Link make using the NAG Library easier in
    a functional kind of way. They present a more natural, interactive
    interface and, what is more, they cut out some of the tedium of
    writing code by generating values for some parameters (array
    dimensions etc.) and computing derivatives and
    jacobians. Unfortunately the user still has to choose which algorithm
    to use, and tuen the control parameters to give a result of the
    required accuracy.
    
    This is not a trivial task. The NAG Library has some thirty quadrature
    routines, and maby forty top-level ODE-solving routines. The criteria
    for selecting a routine can include features of the problem such as
    whether a function is continuous, or they can include requrements from
    the user such as whether speed or accuracy is important. The user
    should also think carefully about whether the problem is being
    presentin in the best way: perhaps an integral can be solved more
    easily if the range of integration is split into several pieces for
    example.
    
    Many users of numerical software would like to be presented with a
    ``black box'' for solving a given class of problems. In practice many
    users will always try solving a problem with the ``simplest'' method
    first, in the hope that they can escape having to think seriously
    about their problem! This is not always a bad idea anyway, since the
    feedback from the routine can help determine a better technique
    (e.g. by identifying the regions where problems occur).
    
    We contend that computer algebra has an important role to play in this
    area. The ability of algebra systems to analyse the symbolic features
    of a problem make them idea {\sl agents} for knowledge-based (or
    expert) systems to use in selecting routines. Their ability (via IRENA
    or AXIOM-NAG) to drive the numerical codes is a bonus.",

}

@misc{Stei16,
  author = "Stein, William",
  title = {{Axiom as an OSCAS}},
  link = "\url{http://wiki.sagemath.org/Axiom\_as\_an\_OSCAS}",
  keywords = "axiomref"
}

@article{Stor95,
  author = "Storme, L. and van Maldeghem, H.",
  title = {{Cyclic caps in PG(3,q)}},
  journal = "Geom. Dedicata",
  volume = "56",
  number = "3",
  pages = "271-284",
  year = "1995",
  link = "\url{https://cage.ugent.be/~hvm/artikels/44.pdf}",
  abstract =
    "A $k$-cap $K$ on $PG(n,q)$ is a set of $k$ points, no three of which
    are collinear. $K$ is complete if it cannot be extended to a $k+1$
    cap. If $K$ is invariant under a cyclic subgroup (which acts regularly
    on $K$) of $PGL(n+1,q)$, the $K$ is cyclic.

    This article investigates cyclic complete $k$-caps in
    $PG(3,q)$. Namely, the different types of complete $k$-caps $K$ in
    $PG(3,q)$ stabilized by a cyclic projective group $G$ of order $k$,
    acting regularly on the points of $K$, are determined. We show that in
    $PG(3,q)$, $q$ even, the elliptic quadric is the only cyclic complete
    $k$-cap. For $q$ odd, it is shown that besides the elliptic quadric,
    there also exist cyclic $k$-caps containing $k/2$ points of two
    disjoint elliptic quadrics or two disjoint hyperbolic quadrics and
    that there exist cyclic $k$-caps stabilized by a transitive cyclic
    group $G$ fixed precisely one point and one plane of
    $PG(3,q)$. Concrete examples of such caps, found using AXIOM and
    CAYLEY, are presented.",
  paper = "Stor95.pdf",
  keywords = "axiomref"
}

@book{Stro99,
  author = "Stroeker, Roelof J. and Kaashoek, Johan F.",
  title = {{Discovering mathematics with Maple. An interactive exploration for
           mathematicians, engineers and econometricians}},
  year = "1999",
  publisher = "Birkhauser",
  abstract =
    "During the past decade, the mathematical computer software packages
    such as Mathematica, Maple, MATLAB (Axiom, Derive, Macsyma, MuPad are
    some further examples of such software) [see Macsyma 2.3. Lite – the
    student edition (1998; Zbl 0911.68089); B. W. Char, K. O. Geddes,
    G. H. Gonnet, B. L. Leong, M. B. Monagan, and S. M. Watt, Maple V
    Library reference manual (1991; Zbl 0763.68046); J. L. Zachary,
    Introduction to scientific programming. Computational problem solving
    using Mathematica and C (1997; Zbl 0891.68053); The student edition of
    MATLAB. Student user guide. The problem-solving tool for engineers,
    mathematicians, and scientists (1992; Zbl 0782.65001); H. Benker,
    Ingenieurmathematik mit Computeralgebra-Systemen. AXIOM, DERIVE,
    MACSYMA, MAPLE, MATHCAD, MATHEMATICA, MATLAB und MuPAD in der
    Anwendung (1998; Zbl 0909.68109); W. Koepf, Hohere Analysis mit DERIVE
    (1994; Zbl 0819.26003)] have greatly faciliated mathematical
    experiments and have thus become popular tools for the modern
    mathematician. It is a pity that most of these packages are quite
    expensive, and that the frequently upgraded versions are not free for
    the owners of the earlier versions (fortunately, there are inexpensive
    student versions of some of these packages). There is a constant
    demand of instructional textbooks by users of these packages. This
    demand is reflected in the growing number of such textbooks. Many of
    these books provide software support (diskette, CD-ROM, access by
    ftp). Such a textbook should meet, in my opinion, the following
    criteria: (1) The size should be small, not bulky like the complete
    technical descriptions of the software. (2) There should be a lot of
    examples of the use of the software covering a wide range of
    mathematical topics. Electronic versions of these examples should be
    made available for free to the users of the textbook
    (e.g. diskette/CD-ROM, access by ftp). (3) There should be a good
    supply of exercises covering the basic mathematical applications. (4)
    The book should be visually pleasing, easy to read, have good indexes
    and provide pointers to other books and electronic sources of
    information. The book under review provides, in addition to the actual
    text, an interactive exploratorium of its topics, based on the
    mechanism of Maple worksheets. These worksheets can be ``opened'' by
    the Maple program and they form a mixture of usual text, hypertext,
    and Maple commands and have a nice style appearance. They also can be
    ``exported'' in a file and included in a file for further treatment.
    The book meets all the aforementioned criteria (1)-(4) with elegance.
    There are many exercises which cover all the usual mathematical topics
    from linear algebra to differential equations and statistics. A
    valuable feature is an appendix with hints and answers for all
    exercises.  One of the highlights of the book is the examination of
    Riemann's non-differentiable function
    \[x \mapsto \sum_{k=1}^\infty{k^{-2}} sin(\pi kx)\]
    which is differentiable only at the rational points $p/q$ with $p$
    and $q$ odd and relatively prime, where its derivative is $-1/2$.

    The book is intended for students of mathematics, engineering
    sciences, and econometry. This book is an ideal guide for this purpose
    and it could probably be used along, without the bulky technical
    documentation of the Maple language. Note that Maple has a
    comprehensive on-line help program, which contains large parts of the
    original documentation.",
  keywords = "axiomref"
}

@inproceedings{Suto85,
  author = "Sutor, Robert S.",
  title = {{The Scratchpad II computer algebra language and system}},
  booktitle = "Research Contributions from the Euro. Conf. on Comp. Alg.",
  series = "Lecture Notes in Computer Science Volume 204",
  volume = "2",
  pages = "32-33",
  year = "1985",
  isbn = "0-387-15983-5 (vol. 1),0-387-15984-3 (vol. 2)",
  keywords = "axiomref",
  beebe = "Sutor:1985:SIC"
}

@article{Suto87,
  author = "Sutor, Robert S. and Jenks, Richard D.",
  title = {{The type inference and coercion facilities in the Scratchpad II 
           interpreter}},
  journal = "SIGPLAN Notices",
  volume = "22",
  number = "7",
  pages = "56-63",
  year = "1987",
  isbn = "0-89791-235-7",
  comment = "IBM Research Report RC 12595 (\#56575)",
  abstract = 
    "The Scratchpad II system is an abstract datatype programming
    language, a compiler for the language, a library of packages of
    polymorphic functions and parametrized abstract datatypes, and an
    interpreter that provides sophisticated type inference and coercion
    facilities. Although originally designed for the implementation of
    symbolic mathematical algorithms, Scratchpad II is a general purpose
    programming language . This paper discusses aspects of the
    implementation of the interpreter and how it attempts to provide a
    user friendly and relatively weakly typed front end for the strongly
    typed programming language.",
  paper = "Suto87.pdf",
  keywords = "axiomref, printed",
  beebe = "Sutor:1987:TICb"
}

@misc{Suto87b,
  author = "Sutor, Robert S.",
  title = {{The Scratchpad II Computer Algebra System. 
           Using and Programming the Interpreter}},
  type = "IBM Course presentation slide deck",
  year = "1987",
  keywords = "axiomref"
}

@techreport{Suto87c,
  author = "Sutor, Robert S. and Jenks, Richard D.",
  title = {{The type inference and coercion facilities in the 
           Scratchpad II interpreter'}},
  type = "Research Report",
  number = "RC 12595 (\#56575)",
  institution = "Mathematical Sciences Department",
  address = "IBM Thomas J. Watson Research Center, Yorktown Heights, NY",
  year = "1987",
  abstract = 
    "The Scratchpad II system is an abstract datatype programming language,
    a compiler for the language, a library of packages of polymorphic
    functions and parameterized abstract datatypes, and an interpreter
    that provides sophisticated type inference and coercion facilities.
    Although originally designed for the implementation of symbolic
    mathematical algorithms, Scratchpad II is a general purpose
    programming language. This paper discusses aspects of the
    implementation of the intepreter and how it attempts to provide a user
    friendly and relatively weakly typed front end for the strongly typed
    programming language.",
  paper = "Suto87c.pdf",
  keywords = "axiomref",
  beebe = "Sutor:1987:TICa"
}

  author = "R. D. Jenks R. S. Sutor and S. M. Watt",
  title = {{Scratchpad II: An abstract Datatype system for
            mathematical computation}},
  pages = "12-37",
  year = "1988"
}
@misc{Swmath,
  author = "Unknown",
  title = {{Axiom}},
  link = "\url{https://www.swmath.org/software/63}",
  abstract =
    "Axiom is a general purpose Computer Algebra System (CAS). It is 
    useful for research and developement of mathematical algorithms.
    It defines a strongly typed, mathematically correct type hierarchy.
    It has a programming language and a built-in compiler.",
  keywords = "axiomref"
}

@misc{Sympy,
  author = "Certik, Ondrej",
  link = "\url{https://github.com/sympy/sympy/wiki/SymPy-vs.-Axiom}",
  keywords = "axiomref"
}

@InProceedings{Thom01,
  author = "Thompson, Simon",
  title = {{Logic and dependent types in the Aldor Computer Algebra System}},
  booktitle = "Symbolic computation and automated reasoning",
  series = "CALCULEMUS 2000",
  year = "2001",
  location = "St. Andrews, Scotland",
  pages = "205-233",
  link = 
    "\url{http://axiom-wiki.newsynthesis.org/public/refs/aldor-calc2000.pdf}",
  abstract = 
    "We show how the Aldor type system can represent propositions of
    first-order logic, by means of the 'propositions as types'
    correspondence. The representation relies on type casts (using
    pretend) but can be viewed as a prototype implementation of a modified
    type system with {\sl type evaluation} reported elsewhere. The logic
    is used to provide an axiomatisation of a number of familiar Aldor
    categories as well as a type of vectors.",
  paper = "Thom01.pdf",
  keywords = "axiomref"
}

@misc{Thomxx,
  author = "Thompson, Simon and  Timochouk, Leonid",
  title = {{The Aldor\-\- language}},
  abstract = "
    This paper introduces the \verb|Aldor--| language, which is a
    functional programming language with dependent types and a powerful,
    type-based, overloading mechanism. The language is built on a subset
    of Aldor, the 'library compiler' language for the Axiom computer
    algebra system. \verb|Aldor--| is designed with the intention of
    incorporating logical reasoning into computer algebra computations.
    
    The paper contains a formal account of the semantics and type system
    of \verb|Aldor--|; a general discussion of overloading and how the
    overloading in \verb|Aldor--| fits into the general scheme; examples
    of logic within \verb|Aldor--| and notes on the implementation of the
    system.",
  paper = "Thomxx.pdf",
  keywords = "axiomref"
}

@misc{Tour98,
  author = "Touratier, Emmanuel",
  title = {{Etude du typage dans le syst\`eme de calcul scientifique Aldor}},
  comment = "Study of types in the Aldor scientific computation system",
  year = "1998",
  link =
    "\url{http://axiom-wiki.newsynthesis.org/public/refs/Aldor-T1998_04.pdf}",
  paper = "Tour98.pdf",
  keywords = "axiomref"
}

@misc{Tour95,
  author = "Tournier, Evelyne",
  title = {{Summary of organisation, history and possible future}},
  year = "1995",
  link = "\url{ftp://ftp.inf.ethz.ch/org/cathode/workshops/jan95/abstracts/tournier.ps}",
  paper = "Tour95.pdf",
  keywords = "axiomref"
}

@article{Tuom96,
  author = "Tuomela, Jukka",
  title = {{On the construction of arbitrary order schemes for the many
           dimensional wave equation}},
  journal = "BIT",
  volume = "36",
  number = "1",
  pages = "158-165",
  year = "1996",
  abstract =
    "The paper is devoted to a problem which was of an interest in the
    beginning of the theory of difference methods. The elementary
    constructed explicit high-order approximations for the wave equation
    (on the simplest cubic grid in space) assume that the solution is very
    smooth and that no boundary conditions are given. Stability is also
    understood in the simplest way (in $L_2$).",
  keywords = "axiomref"
}

@article{Uebe94,
  author = "Ueberberg, Johannes",
  title = {{Interactive Theorem Proving and Computer Algebra}},
  journal = "Lecture Notes in Computer Science",
  volume = "958",
  year = "1994",
  abstract =
    "Interactive Theorem Proving, ITP for short, is a new approach for the
    use of current computer algebra systems to support mathematicians in
    proving theorems. ITP grew out of a more general project -- called
    Symbolic Incidence Geometry -- which is concerned with the problem of
    the systematic use of the computer in incidence geometry.",
  paper = "Uebe94.pdf",
  keywords = "printed"
}

@misc{Unkn13,
  author = "Unknown",
  title = {{Hindley-Milner Type Inference}},
  link = "\url{https://www7.in.tum.de/um/courses/seminar/sove/SS2013/final/hindley-milner.slides.pdf}",
  paper = "Unkn13.pdf",
  keywords = "printed"

}

@misc{Unkn16,
  author = "Unknown",
  title = {{Computer Algebra Systems}},
  link = "\url{http://www.mhtlab.uwaterloo.ca/courses/me755/web\_intro.pdf}",
  paper = "Unkn16.pdf"
}

@article{Hoev12,
  author = "van der Hoeven, Joris",
  title = {{Overview of the Mathemagix type system}},
  journal = "ASCM 2102",
  year = "2012",
  link = "\url{http://www.texmacs.org/joris/mmxtyping/mmxtyping.pdf}",
  abstract =
    "The goal of the {\tt MATHEMAGIX} project is to develop a new and free
    software for computer algebra and computer analysis, base on a
    strongly typed and compiled language. In this paper, we focus on the
    nderlying type system of this language, which allows for heavy
    overloading, including parameterized overloading with parameters in so
    called ``categories''. The exposition is informal and aims at giving
    the reader an overview of the main concepts, ideas and differences
    with existing languages. In a forthcoming paer, we intend to describe
    the formal semantics of the type system in more detail.",
  paper = "Hoev12.pdf",
  keywords = "axiomref"
}

@article{Hoei94,
  author = "{van Hoeij}, M.",
  title = {{An algorithm for computing an integral basis in an algebraic 
           function field}},
  journal = "Journal of Symbolic Computation",
  volume = "18",
  number = "4",
  year = "1994",
  pages = "353-363",
  issn = "0747-7171",
  abstract = "
    Algorithms for computing integral bases of an algebraic function field
    are implemented in some computer algebra systems. They are used e.g.
    for the integration of algebraic functions. The method used by Maple
    5.2 and AXIOM is given by Trager in [Trag84]. He adapted an algorithm
    of Ford and Zassenhaus [Ford, 1978], that computes the ring of
    integers in an algebraic number field, to the case of a function field.

    It turns out that using algebraic geometry one can write a faster
    algorithm. The method we will give is based on Puiseux expansions.
    One cas see this as a variant on the Coates' algorithm as it is
    described in [Davenport, 1981]. Some difficulties in computing with
    Puiseux expansions can be avoided using a sharp bound for the number
    of terms required which will be given in Section 3. In Section 5 we
    derive which denominator is needed in the integral basis. Using this
    result 'intermediate expression swell' can be avoided.

    The Puiseux expansions generally introduce algebraic extensions. These
    extensions will not appear in the resulting integral basis.",
  paper = "Hoei94.pdf",
  keywords = "axiomref",
  beebe = "vanHoeij:1994:ACI"
}

@misc{Hoei08,
  author = "{van Hoeij}, Mark and Novocin, Andrew",
  title = {{A Reduction Algorithm for Algebraic Function Fields}},
  year = "2008",
  month = "April",
  link = "\url{http://andy.novocin.com/pro/algext.pdf}",
  abstract = "
    Computer algebra systesm often produce large expressions involving
    complicated algebraic numbers. In this paper we study variations of
    the {\tt polred} algorithm that can often be used to find better
    representations for algebraic numbers. The main new algorithm
    presented here is an algorithm that treats the same problem for the
    function field case.",
  paper = "Hoei08.pdf"
}

@phdthesis{Vani96,
  author = "Vaninwegen, Myra",
  title = {{The Machine-Assisted Proof of Programming Language
           Properties}},
  year = "1996",
  school = "University of Pennsylvania",
  link = "\url{https://pdfs.semanticscholar.org/4ba2/6cbd3d1600aa82d556d76ce5531db9e8e940.pdf}",
  abstract =
    "The goals of thie project described in this thesis are
    twofold. First, we wanted to demonstrate that if a programming
    language has a semantics that is complete and rigorous
    (mathematical), but not too complex, then substantial theorems can
    be proved about it. Second, we wanted to assess the utiliity of
    using an automated theorem prover to aid in such proofs. We chose
    SML as the language about which to prove theorems: it has a
    published semantics that is complete and rigorous, and while not
    exactly simple, is comprehensible. We encoded the semantics of
    Core SML into the theorem prover HOL (creating new definitional
    packages for HOL in the process). We proved important theorems
    about evaluation and about the type system. We also proved the
    type preservation theorem, which relates evaluation and typing,
    for a good portion of the language. We were not able to complete
    the proof of type preservation because it is not tur: we found
    counterexamples. These proofs demonstrate that a good semantics
    will allow the proof of programming language properties and allow
    the identification of trouble spotes in the language. The use of
    HOL had its plusses and minuses. On the whole the benefits greatly
    outweigh the drawbacks, enough so that we believe that these
    theorems could ot have been proved in amount of time taken by this
    project had we not use automated help.",
  paper = "Vani96.pdf",
  keywords = "printed"
}

@misc{Voev17,
  author = "Voevodsky, Vladimir and Benedikt, Ahrens and Grayson, Daniel",
  title = {{UniMath: Univalent Mathematics}},
  link = "\url{https://github.com/UniMath/UniMath}",
  year = "2017"
}

@article{Wang89,
  author = "Wang, Dongming",
  title = {{A program for computing the Liapunov functions and Liapunov 
           constants in Scratchpad II}},
  journal = "SIGSAM Bulletin",
  volume = "23",
  number = "4",
  pages = "25-31",
  year = "1989",
  abstract =
    "This report describes the implementation and use of a program for
    computing the Liapunov functions and Liapunov constants for a class
    of differential systems in Scratchpad II",
  paper = "Wang89.pdf",
  keywords = "axiomref",
  beebe = "Wang:1989:PCL"
}

@article{Wang90,
  author = "Wang, Dongming",
  title = {{A Class of Cubic Differential Systems with 6-tuple Focus}},
  journal = "J. Differential Equations",
  publisher = "Academic Press, Inc.",
  volume = "87",
  pages = "305-315",
  year = "1990",
  abstract =
    "This paper presents a class of cubic differential systems with the
    origin as a 6-tuple focus from which 6 limit cycles may be
    constructed. For this class of differential systems the stability of
    the origin is given.",
  paper = "Wang90.pdf",
  keywords = "axiomref"
}

@article{Wang91,
  author = "Wang, Dongming",
  title = {{Mechanical manipulation for a class of differential systems}},
  journal = "Journal of Symbolic Computation",
  volume = "12",
  number = "2",
  pages = "233-254",
  year = "1991",
  comment = "See Wang89 for the implementation code",
  abstract =
    "In this paper we describe a mechanical procedure for computing the
    Liapunov functions and Liapunov constants for a class of differential
    systems. These functions and constants are used for establishing the
    stability criteria, the conditions for the existence of a center and
    for the investigation of limit cycles. Some problems for handling the
    computer constants, which are usually large polynomials in terms of
    the coefficients of the differential system, and an approach towards
    their solution by using computer algebraic methods are proposed. This
    approach has been successfully applied to check some known results
    mechanically. The author has implemented a system DEMS on an HP1000
    and in Scratchpad II on an IBM4341 for computing and manipulating the
    Liapunov functions and Liapunov constants. As examples, two particular
    cubic systems are discussed in detail. The explicit algebraic
    relations between the computed Liapunov constants and the conditions
    given by Saharnikov are established, which leads to a rediscovery of
    the incompleteness of his conditions. A class of cubic systems with
    6-tuple focus is presented to demonstrate the feasibility of the
    approach for finding systems with higher multiple focus.",
  paper = "Wang91.pdf",
  keywords = "axiomref",
  beebe = "Wang:1991:MMC"
}

@misc{Wang95,
  author = "Wang, Dongming",
  title = {{Characteristic Sets and Zero Structure of Polynomial Sets}},
  institution = "Johannes Kepler University",
  comment = "Lecture Notes",
  link = "\url{http://www-polsys.lip6.fr/~wang/papers/CharSet.ps.gz}",
  abtract =
    "This paper provides a tutorial on the theory and method of
    characteristic sets and some relevant topics. The basic algorithms as
    well as their generalization for computing the characteristic set and
    characteristic series of a set of multivariate polynomials are
    presented. The characeristic set, which is of certain triangular form,
    reflects in general the major part of zeros, and the characteristic
    series, which is a sequence of polynomial sets of triangular form,
    furnishes a complete zero decomposition of the given polynomial
    set. Using this decomposition, a complete solution to the algebraic
    decision problem and a method for decomposing any algebraic variety
    into irreducible components are described. Some applications of the
    method are indicated.",
  paper = "Wang95.pdf",
  keywords = "axiomref"
}

@book{Wang01,
  author = "Wang, Dongming",
  title = {{Elimination Methods}},
  publisher = "Springer-Verlag",
  isbn = "978-3-7091-6202-6",
  year = "2001",
  abstract =
    "The development of polynomial-elimination techniques from classical
    theory to modern algorithms has undergone a tortuous and rugged
    path. This can be observed L. van der Waerden's elimination of the
    ``elimination theory'' chapter from from B. his classic Modern Algebra
    in later editions, A. Weil's hope to eliminate ``from algebraic
    geometry the last traces of elimination theory,'' and S. Abhyankar's
    suggestion to ``eliminate the eliminators of elimination theory.''
    The renaissance and recognition of polynomial elimination owe much to
    the advent and advance of modern computing technology, based on
    which effective algorithms are implemented and applied to diverse
    problems in science and engineering. In the last decade, both
    theorists and practitioners have more and more realized the
    significance and power of elimination methods and their underlying
    theories. Active and extensive research has contributed a great deal
    of new developments on algorithms and soft­ ware tools to the subject,
    that have been widely acknowledged. Their applications have taken
    place from pure and applied mathematics to geometric modeling and
    robotics, and to artificial neural networks. This book provides a
    systematic and uniform treatment of elimination algorithms that
    compute various zero decompositions for systems of multivariate 
    polynomials. The central concepts are triangular sets and systems of
    different kinds, in terms of which the decompositions are
    represented. The prerequisites for the concepts and algorithms are
    results from basic algebra and some knowledge of algorithmic
    mathematics.",
  keywords = "axiomref"
}

@InProceedings{Wang02,
  author = "Wang, Dongming",
  title = {{Epsilon: A Library of Software Tools for Polynomial Elimination}},
  booktitle = "Proc. 1st Int. Congress of Mathematical Software",
  series = "ICMS 2002",
  year = "2002",
  location = "Beijing China",
  pages = "379-389",
  link = "\url{https://hal.inria.fr/inria-00107607/file/A02-R-314.pdf}",
  abstract =
    "This article presents a Maple library of functions for decomposing
    systems of multivariate polynomials into triangular systems of
    various kinds (regular, simple, or irreducible), with an application
    package for manipulating and proving geometric theorems.",
  paper = "Wang02.pdf",
  keywords = "axiomref"
}

@misc{Watt86,
  author = "Watt, Stephen M. and Della Dora, J. and Wityak, Sandy (ed)",
  title = {{Algebra Snapshot: Linear Ordinary Differential Operators}},
  source = "Scratchpad II Newsletter: Vol 1 Num 2 (Jan 1986)",
  link = "\url{http://www.csd.uwo.ca/~watt/pub/reprints/1986-snews-lodo.pdf}",
  paper = "Watt86.pdf",
  keywords = "axiomref"
}

@misc{Watt87,
  author = "Watt, Stephen M. and Jenks, Richard D.",
  title = {{Abstract Datatypes, Multiple Views and Multiple Inheritance in
           Scratchpad II}},
  year = "1987",
  link = "\url{https://cs.uwaterloo.ca/~smatt/pub/reprints/1987-itl-spadviews.pdf}",
  abstract =
    "Scratchpad II is an abstract datatype language developed at Yorktown
    Heights for the implementation of a new computer algebra system. It
    provides packages of polymorphic functions and parameterized, abstract
    datatypes with operator overloading and multiple inheritance. To
    express the intricate inter-relationships between the datatypes
    necessary for the description of mathematical objects, a number of
    techniques based on the notion of {\sl category} have been
    used. Categories are used to enforce relationships between type
    parameters and to provide the mechanism for multiple inheritance. They
    also allow the language to be statically type checked and the
    generation of efficient code. This paper describes the role of
    categories in Scratchpad II.",
  paper = "Watt87.pdf",
  keywords = "axiomref"
}

@inproceedings{Watt89,
  author = "Watt, Stephen M.",
  title = {{A fixed point method for power series computation}},
  booktitle = "Proc. ISSAC '88",
  series = "Lecture Notes in Computer Science 358",
  location = "Rome, Italy",
  pages = "206-217",
  isbn = "3-540-51084-2",
  year = "1988",
  abstract =
    "This paper presents a novel technique for manipulating structures
    which represents infinite power series.
    
    When power series are implemented using lazy evaluation, many
    operations can be written as simple recursive procedures. For example,
    the programs to generate the series for the elementary transcendental
    functions are almost transliterations of the defining integral
    equations. However, a naive lazy algorithm provides an implementation
    which may be orders of magnitude slower than a method which
    manipulates the coefficients explicitly.
    
    The technique described here allows a power series to be defined in a
    very natural but computationally inefficient way and transforms it to
    an equivalent, efficient form. This is achieved by using a fixed point
    operator on the delayed part to remove redundant calculations.
    
    This paper describes this fixed point method and the class of problems
    to which it is applicable. It has been used in Scratchpad II to
    improve the performance of a number of operations on infinite series,
    including division, reversion, special functions and the solution of
    linear and non-linear ordinary differential equations.
    
    A few examples are given of the method and of the speed up
    obtained. To illustrate, the computation of the first $n$ terms of
    exp($u$) for a dense, infinite series $u$ is reduced from $O(n^4)$ to
    $O(n^2)$ coefficient operations, the same as required by the standard
    on-line algorithms.",
  keywords = "axiomref",
  beebe = "Watt:1989:FPM"
}

@inproceedings{Watt90,
  author = "Watt, Stephen M. and Jenks, Richard D. and Sutor, Robert S. and
            Trager, Barry M.",
  title = {{The Scratchpad II type system: Domains and Subdomains}},
  booktitle = "Computing Tools for Scientific Problem Solving",
  year = "1990",
  publisher = "Academic Press",
  link = "\url{https://cs.uwaterloo.ca/~smwatt/pub/reprints/1990-miola-spadtypes.pdf}",
  abstract =
    "Scratchpad II is a language developed at Yorktown Heights for the
    implementation of a new computer algebra system. The need to model the
    intricate relationships among the datatypes representing mathematical
    objects has provided a number of challenges in the design of a type
    system for the programming language.
    
    In languages in which a datatype constructor may take multiple
    parameters, ensuring compatibility between them is extremely
    important. Scratchpad II addresses this issue by basing its
    implementation of abstract datatypes on {\sl categories}. Categories
    provide a convenient and useful method for specifying requirements on
    operations from datatypes. These requirements can be very complex when
    modelling mathematics.
    
    We show how categories provide multiple inheritance and how
    inheritance of specification is separated from inheritance of
    implementation. We also present implications of the type system on
    compilation of efficient code and flexibility of a weakly typed
    interactive user interface.
    
    Finally, the mechanisms of Scratchpad II are compared with those of
    traditional abstract datatype and object-oriented programming
    languages.",
  paper = "Watt90.pdf",
  keywords = "axiomref"
}  

@techreport{Watt94,
  author = "Watt, Stephen M. and Broadbery, Peter A. and Dooley, Samuel S.
            and Iglio, Pietro",
  title = {{A First Report on the A\# Compiler (including benchmarks)}},
  institution = "IBM Research",
  year = "1994",
  type = "technical report",
  number = "RC19529 (85075)",
  link = "\url{http://www.aldor.org/docs/reports/i94acomp/i94acomp.pdf}",
  abstract = 
    "The $A^{\#}$ compiler allows users of computer algebra to develop
    programs in a context where multiple programming languages are
    employed. The compiler translates programs written in the $A^{\#}$
    programming language to a low-level intermediate language, Foam,
    from which it can generate stand-alone programs, native object
    libraries to be linked with other applications, or code to be read
    into closed environments. In addition, Foam code may be directly
    executed using an interpreter provided with the $A^{\#}$ compiler.
    
    The $A^{\#}$ programming language provides support for object-oriented
    and functional programming styles. It is ``higher-order'' in the sense
    that both types and functions are first class, and may be manipulated
    in the same ways as any other values. The primary considerations in
    the formulation of the language have been generality, composibility,
    and efficiency. The language has been designed to admit a number of
    important optimizations, allowing compilation to machine code which is
    in many instances of efficiency comparable to that produced by a C or
    Fortran compiler.
    
    The original motivation for $A^{\#}$ comes from the field of computer
    algebra: to provide an improved extension language for the Axiom
    computer algebra system.",
  paper = "Watt94.pdf",
  keywords = "axiomref"
}

@misc{Watt94a,
  author = "Watt, S.M. and Broadbery, P.A. and Dooley, S.S. and  Iglio, P. 
            and Steinbach, J.M. and Morrison, S.C. and Sutor, R.S.",
  title = {{AXIOM Library Compiler Users Guide}},
  publisher = "The Numerical Algorithms Group (NAG) Ltd, 1994",
  year = "1994",
  keywords = "axiomref"
}

@misc{Watt95a,
  author = "Watt, Stephen M.",
  title = {{The A\# Programming Language and Compiler}},
  year = "1995",
  link = "\url{ftp://ftp.inf.ethz.ch/org/cathode/workshops/march93/Watt.tex}",
  paper = "Watt95a.pdf",
  keywords = "axiomref"
}

@misc{Watt01,
  author = "Watt, Stephen M. and  Broadbery, Peter A. and Iglio, Pietro and
            Morrison, Scott C. and Steinbach, Jonathan M",
  title = {{FOAM: A First Order Abstract Machine Version 0.35}},
  year = "2001",
  paper = "Watt01.pdf"
}

@misc{Watt00,
  author = "Watt, Stephen M.",
  title = {{Aldor: The language and recent directions}},
  year = "2000",
  institution = "University of Western Ontario",
  link = "\url{http://www.aldor.org/docs/reports/sa2000/aldortalk-sa2000.pdf}",
  paper = "Watt00.pdf",
  keywords = "axiomref"
}

@misc{Watt00a,
  author = "Watt, Stephen M.",
  title = {{Aldor: An Introduction to the Language}},
  year = "2000",
  institution = "University of Western Ontario",
  link = "\url{http://www.aldor.org/docs/reports/ukqcd-2000/intro1-ukqcd00.pdf}",
  paper = "Watt00a.pdf",
  keywords = "axiomref"
}

@misc{Watt00b,
  author = "Watt, Stephen M.",
  title = {{Aldor: Interfaces}},
  year = "2000",
  institution = "University of Western Ontario",
  link = "\url{http://www.aldor.org/docs/reports/ukqcd-2000/intro2-ukqcd00.pdf}",
  paper = "Watt00b.pdf",
  keywords = "axiomref"
}

@InProceedings{Watt07,
  author = "Watt, Stephen",
  title = {{What Happened to Languages for Symolic Mathematical Computation?}},
  booktitle = "Proc. Prog. Lang. for Mechanized Mathematics",
  series = "PLMMS 07",
  year = "2007",
  location = "RISC-Linz, Austria",
  pages = "81-90",
  link = "\url{http://www.csd.uwo.ca/~watt/pub/reprints/2007-plmms-what\_happened.pdf}",
  abstract =
    "While the state of the art is relatively sophisticated in programming
    language support for computer algebra, there has been less development
    in programming language support for symbolic computation over the past
    two decades. We summarize certain advances in programming languages
    for computer algebra and propose a set of directions and challenges
    for programming languages for symbolic computation.",
  paper = "Watt07.pdf",
  keywords = "axiomref"
}

@phdthesis{Webe93b,
  author = "Weber, Andreas",
  title = {{Type Systems for Computer Algebra}},
  school = "University of Tubingen",
  year = "1993",
  abstract = 
    "We study type systems for computer algebra systems, which frequently
    correspond to the ``pragmatically developed'' typing constructs used
    in AXIOM.
    
    A central concept is that of type classes which correspond to AXIOM
    categories. We will show that types can be syntactically described as
    terms of a regular order-sorted signature if no type parameters are
    allowed. Using results obtained for the functional programming
    language Haskell we will show that the problem of type inference is
    decidable.  This result still holds if higher-order functions are
    present and parametric polymorphism is used.  These additional typing
    constructs are useful for further extensions of existing computer
    algebra systems: These typing concepts can be used to implement
    category theoretic constructs and there are many well known
    constructive interactions between category theory and algebra.
    
    On the one hand we will show that there are well known techniques to
    specify many important type classes algebraically, and we will also
    show that a formal and algorithmically Feasible treatment of the
    interactions of algebraically specified data types and type classes is
    possible.  On the other hand we will prove that there are quite
    elementary examples arising in computer algebra which need very
    ``strong'' formalisms to be specified and are thus hard to handle
    algorithmically.
    
    We will show that it is necessary to distinguish between types and
    elements as parameters of parameterized type classes. The type
    inference problem for the former remains decidable whereas for the
    latter it becomes undecidable.  We will also show that such a
    distinction can be made quite naturally.
    
    Type classes are second-order types.  Although we will show that there
    are constructions used in mathematics which imply that type classes
    have to become first-order types in order to model the examples
    naturally, we will also argue that this does not seem to be the case
    in areas currently accessible for an algebra system. We will only
    sketch some systems that have been developed during the last years in
    which the concept of type classes as first-order types can be
    expressed. For some of these systems the type inference problem was
    proven to be undecidable.

    Another fundamental concept for a type system of a computer algebra
    system — at least for the purpose of a user interface — are coercions.
    We will show that there are cases which can be modeled by coercions
    but not by an ``inheritance mechanism'', i. e. the concept of coercions
    is not only orthogonal to the one of type classes but also to more
    general formalisms as are used in object-oriented languages.  We will
    define certain classes of coercions and impose conditions on important
    classes of coercions which will imply that the meaning of an expression 
    is independent of the particular coercions that are used in order to 
    type it.

    We shall also impose some conditions on the interaction between
    polymorphic operations defined in type classes and coercions that will
    yield a unique meaning of an expression independent of the type which
    is assigned to it — if coercions are present there will very
    frequently be several possibilities to assign types to expressions.

    Often it is not only possible to coerce one type into another but it
    will be the case that two types are actually isomorphic .  We will
    show that isomorphic types have properties that cannot be deduced from
    the properties of coercions and will shortly discuss other possibilities 
    to model type isomorphisms.  There are natural examples of type 
    isomorphisms occurring in the area of computer algebra that have
    a ``problematic'' behavior. So we will prove for a certain example that
    the type isomorphisms cannot be captured by a finite set of coercions
    by proving that the naturally associated equational theory is not
    finitely axiomatizable.

    Up to now few results are known that would give a clear dividing line
    between classes of coercions which have a decidable type inference
    problem and classes for which type inference becomes undecidable.
    We will give a type inference algorithm for some important classes of
    coercions.

    Other typing constructs which are again quite orthogonal to the
    previous ones are those of {\sl partial functions} and of types 
    {\sl depending on elements} . We will link the treatment of {\sl partial 
    functions} in AXIOM to the one used in order-sorted algebras and will 
    show some problems which arise if a seemingly more expressive solution 
    were used. There are important cases in which {\sl types depending on 
    elements} arise naturally.  We will show that not only type inference 
    but even type checking is undecidable for relevant cases occurring in 
    computer algebra.",
  paper = "Webe93b.pdf",
  keywords = "axiomref, printed"
}

@inproceedings{Webe05,
  author = "Weber, Andreas",
  title = {{A Type-Coercion Problem in Computer Algebra}},
  booktitle = "Artificial Intelligence and Symbolic Mathematical Computing",
  series = "Lecture Notes in Computer Science 737",
  year = "2005",
  publisher = "Springer",
  pages = "188-194",
  abstract = 
    "An important feature of modern computer algebra systems is the
    support of a rich type system with the possibility of type inference.
    
    Basic features of such a system are polymorphism and coercion between
    types. Recently the use of order-sorted rewrite systems was proposed
    as a general framework.
    
    We will give a quite simple example of a family of types arising in
    computer algebra whose coercion relations cannot be captured by a
    finite set of first-order rewrite rules.",
  paper = "Webe05.pdf",
  keywords = "axiomref, coercion, printed"
}

@Inproceedings{Webe94,
  author = "Weber, Andreas",
  title = {{Algorithms for Type Inference with Coercions}},
  booktitle = "Proc ISSAC 94",
  series = "ISSAC 94",
  pages = "324-329",
  year = "1994",
  abstract =
    "This paper presents algorithms that perform a type inference for a
    type system occurring in the context of computer algebra. The type
    system permits various classes of coercions between types and the
    algorithms are complete for the precisely defined system, which can be
    seen as a formal description of an important subset of the type system
    supported by the computer algebra program Axiom.

    Previously only algorithms for much more restricted cases of coercions
    have been described or the frameworks used have been so general that
    the corresponding type inference problems were known to be
    undecidable.",
  paper = "Webe94.pdf",
  keywords = "axiomref, coercion"
}

@article{Webe95,
  author = "Weber, Andreas",
  title = {{On coherence in computer algebra}},
  journal = "J. Symb. Comput.",
  volume = "19",
  number = "1-3",
  pages = "25-38",
  year = "1995",
  link = "\url{http://cg.cs.uni-bonn.de/personal-pages/weber/publications/pdf/WeberA/Weber94e.pdf}",
  abstract = 
    "Modern computer algebra systems (e.g. AXIOM) support a rich type
    system including parameterized data types and the possibility of
    implicit coercions between types. In such a type system it will be
    frequently the case that there are different ways of building
    coercions between types. An important requirement is that all
    coercions between two types coincide, a property which is called {\sl
    coherence}. We will prove a coherence theorem for a formal type system
    having several possibilities of coercions covering many important
    examples. Moreover, we will give some informal reasoning why the
    formally defined restrictions can be satisfied by an actual system.",
  paper = "Webe95.pdf",
  keywords = "axiomref, coercion",
  beebe = "Weber:1993:CCA"
}

@misc{Westxx,
  author = "Wester, Michael J.",
  title = {{Computer Algebra Synonyms}},
  link = "\url{http://math.unm.edu/~wester/cas/synonyms.pdf}",
  abstract = 
    "The following is a collection of synonyms for various operations in
    the seven general purpose computer algebra systems {\bf Axiom}, {\bf
    Derive}, {\bf Macsyma}, {\bf Maple}, {\bf Mathematica}, {\bf MuPAD},
    and {\bf Reduce}. This collection does not attempt to be
    comprehensive, but hopefully it will be useful in giving an indication
    of how to translate between the syntaxes used by the different systems
    in many common situations. Note that for a blank entry means that
    there is no exact translation of a particular operation for the
    indicated system, but it may still be possible to work around this
    lack with a related functionality.",
  paper = "Westxx.pdf",
  keywords = "axiomref"
}

@book{West99,
  author = "Wester, Michael J.",
  title = {{Computer Algebra Systems. A practical guide}},
  year = "1999",
  publisher = "Wiley",
  isbn = "0-471-98353-5",
  abstract =
    "In this book some of the most popular general purpose computer
    algebra systems (CAS), such as Mathematica, Maple, Derive, Axiom,
    MuPAD, and Macsyma, are examined. The strengths and weaknesses of
    these programs are compared and contrasted, and tutorial information
    for using these systems in various ways is given. The different
    packages are quantitatively compared using standard test suites,
    giving the possibility to asses the most appropriate for a particular
    user or application. The origins of these systems are revealed and
    many of their behaviors analyzed. This furnishes a feel for where the
    current computer algebra system state of the art stays and what can be
    expected for existing and future systems. The book is organized in
    several chapters written by different authors. Chapters 1,2, and 3 are
    organized as reviews, comparisons, and critiques of CAS
    capabilities. Then more technical issues are discussed considering
    different approaches taken by different CAS: simplifying square roots
    of square roots by denesting (chapter 4), complex number calculation
    (chapter 5), efficiently computing Chebyshev polynomials (chapter 6),
    solving single equations and systems of polynomial equations (chapters
    7, 8), computing limits (chapter 9), multiple integration (chapter
    10), solving ordinary differential equation (chapter 11), integration
    of nonlinear evolution equations (chapter 12), code generation
    (chapter 13), evaluation of expressions and programs in the embedded
    computer algebra programming language (chapter 14), and computer
    algebra in education (chapter 15). Chapter 16 covers the origin of CA,
    and, finally chapter 17 gives a list of most CAS available today.",
  keywords = "axiomref"
}

@misc{Wikixx,
  author = "Unknown",
  title = {{List of open-source software for mathematics}},
  link = "\url{https://en.wikipedia.org/wiki/List\_of\_open-source\_software\_for\_mathematics}",
  keywords = "axiomref"
}

@InProceedings{Will95,
  author = "Williamson, Clifton J.",
  title = {{On the algebraic construction of tri-diagonal matrices with given
           characteristic polynomial}},
  booktitle = "4th Conf. of Canadian Number Theory Association",
  year = "1995",
  location = "Halifax, Nova Scotia, Canada",
  pages = "417-431",
  abstract =
    "Let $K$ be a field of characteristic zero and assume 
    \[f(x)=x^n-s_1x^{n-1}+\cdots+s_n \in L[x]\] 
    where $L=K(s_1,\cdots,s_n)$. Call, for the purposes of this review, an
    $n\times n$ matrix 1-tridiagonal if its entries above and below the
    main diagonal are all 1, the entries on it are $d_1,\cdots,d_n$, and
    all other entries are 0. Under which conditions can 
    $d_1,d_2,\cdots,d_n$ be chosen in a radical extension of $L$ such that 
    the resulting 1-tridiagonal matrix has $f$ as its characteristic 
    polynomial?  The author’s answer: for $n=3$ always. The $d_i,s_i$
    are related by a system of algebraic equations of the form
    $\overline{f}_i(d_1,d_2,d_3)=s_i$. If this is satisfied, then by 
    a resultant- or Groebner basis computation with respect to the 
    lexicographic order, $d_3$ (say) must be a root of a certain 
    irreducible polynomial $\varphi$ of degree 6. Joining this
    with a Galois group computation for $\varphi$ over $L$, it is shown 
    that a solvable and necessarily transitive Gal($\varphi$) will be a 
    necessary and sufficient condition for expressibility of $d_3$ (clear) 
    and, thanks to the form of the Groebner basis, also of $d_1,d_2$ in 
    radicals. Upon a discriminant computation of $\varphi$ and use of 
    results of G. Butler and J. McKay [Commun. Algebra 11,
    863-911 (1983; Zbl 0518.20003)] and J. McKay and L. Soicher [J. Number
    Theory 20, 273-281 (1985; Zbl 0579.12006)], this allows the author to
    deduce the mentioned result. For $n=4$, though the author cannot get as
    complete information about Galois groups etc., he can deduce e.g. that
    if $s_1,\cdots,s_4$ are algebraically independent over $\mathbb{Q}$, 
    then $d_1,\cdots,d_4$ are not expressible in radicals over 
    $\mathbb{Q}(s_1,\cdots,s_4)$. He mentions, however, a solvable subcase.",
  paper = "Will95.pdf",
  keywords = "axiomref"
}

@misc{Wink89,
  author = "Winkler, Franz",
  title = {{Equational Theorem Proving and Rewrite Rule Systems}},
  year = "1989",
  publisher = "Springer-Verlag",
  link = "\url{http://www.risc.jku.at/publications/download/risc_3527/paper_47.pdf}",
  abstract = 
    "Equational theorem proving is interesting both from a mathematical
    and a computational point of view. Many mathematical structures like
    monoids, groups, etc. can be described by equational axioms. So the
    theory of free monoids, free groups, etc. is the equational theory
    defined by these axioms. A decision procedure for the equational
    theory is a solution for the word problem over the associated
    algebraic structure. From a computational point of view, abstract data
    types are basically described by equations. Thus, proving properties
    of an abstract data type amounts to proving theorems in the associated
    equational theory.

    One approach to equational theorem proving consists in associating a
    direction with the equational axioms, thus transforming them into
    rewrite rules. Now in order to prove an equation $a=b$, the rewrite
    rules are applied to both sides, finally yielding reduced versions
    $a^{'}$ and $b^{'}$ of the left and right hand sides, respectively. If
    $a^{'}$ and $b^{'}$ agree syntactically, then the equation holds in
    the equational theory. However, in general this argument cannot be
    reversed; $a^{'}$ and $b^{'}$ might be different even if $a=b$ is a
    theorem. The reason for this problem is that the rewrite system might
    not have the Church-Rosser property. So the goal is to take the
    original rewrite system and transform it into an equivalent one which
    has the desired Church-Rosser property.

    We show how rewrite systems can be used for proving theorems in
    equational and inductive theories, and how an equational specification
    of a problem can be turned into a rewrite program.",
  paper = "Wink89.pdf"
}

@article{Wink93,
  author = "Winkler, Franz",
  title = {{Algebraic Computation in Geometry}},
  year = "1993",
  journal = "IMACS Symposium SC-1993",
  link = "\url{http://www.risc.jku.at/publications/download/risc_3777/paper_35.pdf}",
  abstract =
    "Computation with algebraic curves and surfaces are very well suited
    for being treated with computer algebra. Many aspects of computer
    algebra need to be combined for successfully solving problems in this
    area, e.g. computations with algebraic coefficients, solution of
    algebraic equations and elimination theory, and derivation of power
    series approximations of branches. We will describe the application of
    computer algebra to problems arising in algebraic geometry. The
    program system CASA, which has been developed by the author and a
    group of students will be introduced.",
  paper = "Wink93.pdf"
}

@book{Wink96,
  author = "Winkler, Franz",
  title = {{Polynomial Algorithms in Computer Algebra}},
  year = "1996",
  publisher = "Springer-Verlag",
  isbn = "3-211-82759-5",
  abstract =
    "This book surveys algorithms and results of Computer Algebra that are
    concerned with polynomials. It introduces algorithms from the bottom
    up, starting from very basic problems in computation over the
    integers, and finally leading to, e.g. advanced topics in
    factorization, solution of polynomial equations and constructive
    algebraic geometry. It is not based on a particular computer algebra
    program system.
    
    After two introductory chapters, the book contains six chapters with
    the following respective topics: computation by homomorphic images,
    gcd computation, factorization and decomposition of polynomials,
    linear systems and Hankel systems, Gröbner bases. The last three
    chapters are concerned with applications of polynomial algorithms to
    higher level problems in computer algebra. In particular, a decision
    algorithm in the elementary theory of real closed fields, a
    description of Gosper’s algorithm for solving summation problems, and
    an algorithm for deciding whether an algebraic curve can be
    parametrized by rational functions (and if so for computing such a
    parametrization) is given. Along the way, the complexity of many of
    the algorithms is investigated. Each chapter ends with rich
    bibliographical notes.
    
    The book was originally developed from course material. It can easily
    be used as a textbook on the topic. Most subsections contain
    exercises. Solutions of some of the exercises are provided."
}

@misc{Wrig03,
  author = "Wright, Francis J.",
  title = {{Mathematics and Algorithms for Computer Algebra: Part 1}},
  link = "\url{https://people.eecs.berkeley.edu/~fateman/282/F%20Wright%20notes/week1.pdf}",
  year = "2003",
  comment = "full course. week2=Wrig03a, wee3=Wrig03b,...week8=Wrig3g.pdf",
  abstract =
    "This course will be mainly mathematics, some computer science
    and a little computing. Little or no essential use will be made of
    actual computer languages, although I may occasionally use Pascal, C,
    Lisp or REDUCE for concrete examples. The aim of the course is to
    provide an entry into the current research literature, but not to present
    the most recent research results.
    
    The first half of the course (taught by me) will deal with basic 
    mathematics and algorithms for computer algebra, primarily at the level of
    arithmetic and elementary abstract algebra, including an introduction
    to GCDs and the solution of univariate polynomial equations. This
    leads into the second half of the course (taught by Dr Jim Skea) on
    the more advanced problems of polynomial factorization, indefinite 
    integration, multivariate polynomial equations, etc.
    
    The first week provides an introduction to the computing aspects
    of computer algebra, and contains almost no mathematics. It is 
    intended to show how the later theory can be implemented for practical
    computation. The second week provides a rapid but superficial survey
    of the abstract algebra that is most important for computer algebra.
    The next five weeks will build on this abstract basis to do some more
    concrete mathematics in more details, referring back to the basis 
    setablished in the first two weeks as necessary.
    
    At the end of each set of notes will be exercises, one (or more) of
    which will be assessed.",
  paper = "Wrig03.pdf",
  keywords = "axiomref"
}

@misc{WikiG,
  author = "Wikipedia Authors",
  title = {{Gamma Function}},
  year = "2016",
  link = "\url{https://en.wikipedia.org/wiki/Gamma\_function}",
  algebra = "\newline\refto{package DFSFUN DoubleFloatSpecialFunctions}"
}

@inproceedings{Yunx76,
  author = "Yun, David Y.Y",
  title = {{Algebraic Algorithms using p-adic Constructions}},
  booktitle = "Proc. 1976 Symp. on Symbolic and Algebraic Computation",
  series = "SYMSAC '76",
  publisher = "ACM",
  year = "1976",
  pages = "248-259",
  link = "\url{http://lib.org/by/\_djvu\_Papers/Computer\_algebra/Algebraic\%20numbers}",
  paper = "Yunx76.djvu",
  keywords = "axiomref"
}

@inproceedings{Yunx83,
  author = "Yun, David Y.Y",
  title = {{Computer Algebra and Complex Analysis}},
  booktitle = "Computational Aspects of Complex Analysis",
  pages = "379-393",
  publisher = "D. Reidel Publishing Company",
  year = "1983",
  abstract =
    "Taking complex analysis to mean complex numerical analysis, I
    perceive my mission here to be that of disseminating the algebraic
    approach taken by computer algebraists to many mathematical problems,
    which arise from and are important to complex analysis. In turn,
    complex numerical analysis can be, and have been, providing essential
    theoretical and computational results for computer algebra. The cross
    fertilization should and must continue in order that computational
    mathematics progress with the joint aid of both tools, rather than
    branching into orthogonal pursuits with disparate approaches. First,
    we discuss the different issues and principal concerns of computer
    algebra. Then, the algebraic approach to a long standing problem in
    calculus or complex analysis, indefinite integration in closed form,
    will be motivated and derived through examples. Algorithmic solution
    to the basic, thought provoking, problem of rational function
    integration as well as theoretical foundation underlying the algorithm
    for elementary function integration will be discussed. Further issues
    and approaches will be illustrated through another central (implicitly
    essential) problem of computer algebra, that is simplification of
    symbolic and algebraic expressions. We conclude by showing a set of
    computer executed problems in integration to reveal some of the new
    capabilities added to the arsenal of a mathematician through the
    efforts of computer algebra.",
  keywords = "axiomref"
}

@mastersthesis{Zeng92,
  author = "Zenger, Christoph",
  title = {{Gr\"obnerbasen f\"ur Differentialformen und ihre 
           Implementierung in AXIOM'}},
  type = "Diplomarbeit",
  school = "Universit{\"a}t Karlsruhe",
  address = "Karlsruhe, Germany",
  year = "1992",
  keywords = "axiomref",
  beebe = "Zenger:1992:GFD"
}

@article{Zeng97,
  author = "Zenger, Christoph",
  title = {{Indexed types}},
  journal = "Theor. Comput. Sci.",
  volume = "187",
  numbers = "1-2",
  pages = "147-165",
  year = "1997",
  abstract =
    "A new extension of the Hindley/Milner type system is proposed. The
    type system has algebraic types, that have not only type parameters
    but also value parameters (indices). This allows for example to
    parameterize matrices and vectors by their size and to check size
    compatibility statically. This is especially of interest in computer
    algebra.",
  paper = "Zeng97.pdf",
  keywords = "axiomref"
}

@misc{Zimm95,
  author = "Zimmermann, Paul",
  title = {{Wester's test suite in MuPAD 1.2.2}},
  year = "1995",
  abstract =
    "A few months ago, Michael Wester made a review of the mathematical
    capabilities of different computer algebra systems, namely Axiom,
    Derive, Macsyma, Maple, Mathematica and Reduce. This review, which is
    available by anonymous ftp from math.unm.edu, file pub/cas/Paper.ps,
    consists of 131 tests in different domains of mathematics (arithmetic,
    algebraic equations, differential equations, integration, operator
    computation, series expansions, limits).
    
    We describe in this paper the problems that can be solved with MuPAD
    1.2.2, and how to solve them. The problems marked as [New] are solved
    using new functionalities of the version 1.2.2 with respect to 1.2.1",
  paper = "Zimm95.pdf",
  keywords = "axiomref"
}

@misc{Zimm96,
  author = "Zimmermann, Paul",
  title = {{Wester's test suite in MuPAD 1.3}},
  year = "1996",
  abstract =
    "In December 1994, Michael Wester made a review of the mathematical
    capabilities of different computer algebra systems, namely Axiom,
    Derive, Macsyma, Maple, Mathematica and Reduce. This review, which is
    available by anonymous ftp from math.unm.edu, file pub/cas/Paper.ps,
    consists of 131 tests in different domains of mathematics (arithmetic,
    algebraic equations, differential equations, integration, operator
    computation, series expansions, limits).
    
    We describe in this paper the problems that can be solved with MuPAD
    1.3, and how to solve them. The problems marked as [New] are solved
    using new functionalities of the version 1.3 with respect to 1.2.2",
  paper = "Zimm96.pdf",
  keywords = "axiomref"
}

@book{Zipp92a,
  author = "Zippel, Richard",
  title = {{Algebraic Computation}},
  publisher = "Cornell University",
  comment = "Unpublished",
  year = "1992",
  keywords = "axiomref, printed"
}

@misc{Zipp89,
  author = "Zippel, Richard",
  title = {{The Weyl Computer Algebra Substrate}},
  year = "1989",
  link = "\url{https://ecommons.cornell.edu/bitstream/handle/1813/6917/90-1077.pdf}",
  paper = "Zipp89.pdf",
  keywords = "axiomref, printed"
}

@book{Zipp92,
  author = "Zippel, Richard",
  title = {{Symbolic/Numeric Techniques in Modeling and Simulation}},
  link = "\url{http://www.cs.duke.edu/donaldlab/Books/SymbolicNumericalComputation/323-346.pdf}",
  year = "1992",
  publisher = "Academic Press Limited",
  isbn = "0-12-220535-9",
  abstract =
    "Modeling and simulating collections of physical objects that are
    subject to a wide variety of physical forces and interactions is
    exceedingly difficult.  The construction of a single simulator capable
    of dealing with all possible physical processes is completely
    impractical and, it seems to us, wrong-headed.  Instead, we propose
    to build custom simulators designed for a particular collection of
    physical objects, where a particular set of physical phenomena are
    involved.  For such an approach to be practical, an environment must
    be provided that facilitates the quick construction of these
    simulators.  In this paper we describe the essential features of such
    an environment and describe in some detail how a general
    implementation of the weighted residual method, one of the more
    general classes of numerical integration techniques, can be used.",
  paper = "Zipp92.pdf",
  keywords = "axiomref"
}
  
@inproceedings{Zipp93,
  author = "Zippel, Richard",
  title = {{The Weyl Computer Algebra Substrate}},
  booktitle = "Design and Implementation of Symbolic Computation Systems '93",
  series = "DISCO 93",
  pages = "303=318",
  year = "1993",
  abstract =
    "Weyl is a new type of computer algebra substrate that extends an
    existing, object oriented programming language with symbolic computing
    mechanisms.  Rather than layering a new language on top of an existing
    one, Weyl behaves like a powerful subroutine library, but takes heavy
    advantage of the ability to overload primitive arithmetic operations
    in the base language.  In addition to the usual objects manipulated in
    computer algebra systems (polynomial, rational functions, matrices,
    etc.), domains (e.g., Z, Q[x, y, z]) are also first class objects in
    Weyl.",
  paper = "Zipp93.pdf, printed",
  keywords = "axiomref"
}

@article{Abbo95,
  author = "Abbott, John and van Leeuwen, Andre and Strotmann, Andreas",
  title = {{Objectives of OpenMath}},
  journal = "J. Symbolic Computation",
  volume = "11",
  year = "1995",
  abstract = 
    "OpenMath aims at providing a universal means of communicating
    mathematical information between applications. In this paper we set
    out the objectives and design goals of OpenMath , and sketch the
    framework of a model that meets these requirements. Based upon this
    model, we propose a structured approach for further development and
    implementation of OpenMath. Throughout, emphasis is on
    extensibility and flexibility, so that OpenMath is not confined to any
    particular area of mathematics, nor to any particular
    implementation. We give some example scenarios to motivate and clarify
    the objectives, and include a brief discussion of the parallels
    between this model and the theory of human language perception.",
  paper = "Abbo95.pdf"
}

@article{Abla98,
  author = "Ablamowicz, Rafal",
  title = {{Spinor Representations of Clifford Algebras: A Symbolic Approach}},
  journal = "Computer Physics Communications",
  volume = "115",
  number = "2-3",
  month = "December",
  year = "1998",
  pages = "510-535"
}

@article{Abra06,
  author = "Abramov, Sergey A.",
  title = {{In Memory of Manuel Bronstein}},
  journal = "Programming and Computer Software",
  volume = "32",
  number = "1",
  pages = "56-58",
  publisher = "Pleiades Publishing Inc",
  year = "2006",
  paper = "Abra06.pdf"
}

@book{ADAx83,
  author = "U.S. Government",
  title = {{The Programming Language Ada Reference Manual}},
  publisher = "U.S. Government",
  year = "1983",
  comment = "STD-1815A-1983"
}

@misc{Adam17,
  author = "Adamchik, Victor",
  title = {{Modern Computer Algebra}},
  comment = "Carnegie Mellon SCS 15-355",
  year = "2017",
  keywords = "printed"
}

@book{Altm05,
  author = "Altmann, Simon L.",
  title = {{Rotations, Quaternions, and Double Groups}},
  publisher = "Dover Publications, Inc.",
  year = "2005",
  isbn = "0-486-44518-6"
}

@misc{Arma00,
  author = "Armando, Alessandro and Zini, Daniele",
  title = {{Towards Interoperable Mechanized Reasoning Systems:
            The Logic Broker Architecture}},
  year = "2000",
  abstract =
    "There is a growing interest in the integration of mechanized
    reasoning systems such as automated theorem provers, computer algebra
    systems, and model checkers.  State-of-the-art reasoning systems are
    the result of many man-years of careful development and engineering,
    and usually they provide a high degree of sophistication in their
    respective domain.  Yet they often perform poorly when applied outside
    the domain they have been designed for.  The problem of integrating
    mechanized reasoning systems is therefore being perceived as an
    important issue in automated reasoning.  In this paper we present
    the Logic Broker Architecture, a framework which provides the needed
    infrastructure for making mechanized reasoning systems interoperate.
    The architecture provides location transparency, a way to forward
    requests for logical services to appropriate reasoning systems via a
    simple registration/subscription mechanism, and a translation
    mechanism which ensures the transparent and provably sound exchange of
    logical services.",
  paper = "Arma00.pdf",
  keywords = "printed"
}

@article{Aste02,
  author = "Astesiano, Egidio and Bidoit, Michel and Kirchner, Helene and
            Krieg-Bruckner, Bernd and Mosses, Peter D. and Sannella, Donald
            and Tarlecki, Andrzej",
  title = {{CASL: the Common Algebraic Specification Language}},
  journal = "Theoretical Computer Science",
  volume = "286",
  number = "2",
  pages = "153-196",
  year = "2002",
  abstract =
    "The Common Algebraic Specification Language (CASL) is an expressive
    language for the formal specification of functional requirements and
    modular design of software. It has been designed by COFI, the
    international Common Framework Initiative for algebraic specification
    and development. It is based on a critical selection of features that
    have already been explored in various contexts, including subsorts,
    partial functions, first-order logic, and structured and architectural
    specifications. CASL should facilitate interoperability of many
    existing algebraic prototyping and verification tools.
    
    This paper gives an overview of the CASL design. The major issues that
    had to be resolved in the design process are indicated, and all the
    main concepts and constructs of CASL are briefly explained and
    illustrated — the reader is referred to the CASL Language Summary for
    further details. Some familiarity with the fundamental concepts of
    algebraic specification would be advantageous.",
  paper = "Aste02.pdf"
}

@article{Aubr99,
  author = "Aubry, Phillippe and Lazard, Daniel and {Moreno Maza}, Marc",
  title = {{On the Theories of Triangular Sets}},
  year = "1999",
  pages = "105-124",
  journal = "Journal of Symbolic Computation",
  volume =  "28",
  link = "\url{http://www.csd.uwo.ca/~moreno/Publications/Aubry-Lazard-MorenoMaza-1999-JSC.pdf}",
  papers = "Aubr99.pdf",
  algebra =
    "\newline\refto{category TSETCAT TriangularSetCategory}
     \newline\refto{category RSETCAT RegularTriangularSetCategory}
     \newline\refto{category NTSCAT NormalizedTriangularSetCategory}
     \newline\refto{category SFRTCAT SquareFreeRegularTriangularSetCategory}
     \newline\refto{package LEXTRIPK LexTriangularPackage}
     \newline\refto{package RSDCMPK RegularSetDecompositionPackage}",
  abstract = 
    "Different notions of triangular sets are presented. The relationship
    between these notions are studied. The main result is that four
    different existing notions of {\sl good} triangular sets are
    equivalent."
}

@misc{Aubr96,
  author = "Aubry, Phillippe and Maza, Marc Moreno",
  title = {{Triangular Sets for Solving Polynomial Systems: 
           a Comparison of Four Methods}},
  year = "1996",
  link = "\url{http://www.lip6.fr/lip6/reports/1997/lip6.1997.009.ps.gz}",
  algebra =
    "\newline\refto{category TSETCAT TriangularSetCategory}
     \newline\refto{category RSETCAT RegularTriangularSetCategory}
     \newline\refto{category NTSCAT NormalizedTriangularSetCategory}
     \newline\refto{category SFRTCAT SquareFreeRegularTriangularSetCategory}
     \newline\refto{package LEXTRIPK LexTriangularPackage}
     \newline\refto{package RSDCMPK RegularSetDecompositionPackage}",
  abstract = 
    "Four methods for solving polynomial systems by means of triangular
    sets are presented and implemented in a unified way. These methods are
    those of Wu, Lazard, Kalkbrener, and Wang. They are compared on
    various examples with emphasis on efficiency, conciseness and
    legibility of the outputs.",
  paper = "Aubr96.pdf",
  keywords = "axiomref"
}

@article{Aubr99a,
  author = "Aubry, Phillippe and Maza, Marc Moreno",
  title = {{Triangular Sets for Solving Polynomial Systems: 
           a Comparison of Four Methods}},
  journal = "J. Symb. Comput.",
  volume = "28",
  number = "1-2",
  pages = "125-154",
  year = "1999",
  link = "\url{http://www.csd.uwo.ca/~moreno/Publications/Aubry-MorenoMaza-1999-JSC.pdf}",
  algebra =
    "\newline\refto{category TSETCAT TriangularSetCategory}
     \newline\refto{category RSETCAT RegularTriangularSetCategory}
     \newline\refto{category NTSCAT NormalizedTriangularSetCategory}
     \newline\refto{category SFRTCAT SquareFreeRegularTriangularSetCategory}
     \newline\refto{package RSDCMPK RegularSetDecompositionPackage}",
  abstract = 
    "Four methods for solving polynomial systems by means of triangular
    sets are presented and implemented in a unified way. These methods are
    those of Wu, Lazard, Kalkbrener, and Wang. They are compared on
    various examples with emphasis on efficiency, conciseness and
    legibility of the outputs.",
  paper = "Aubr99a.pdf",
  keywords = "axiomref"
}

@phdthesis{Aubr99b,
  author = "Aubry, Philippe",
  title = {{Ensembles triangulaires de polynomes et resolution de systemes
           algebriques. Implantation en Axiom}},
  school = "l'Universite de Paris VI",
  year = "1999",
  month = "January",
  comment = "French",
  paper = "Aubr99b.pdf"
}

@article{Ausi79,
  author = "Ausiello, Giovanni Francesco Mascari",
  title = {{On the Design of Algebraic Data Structures with the 
           Approach of Abstract Data Types}},
  journal = "LNCS",
  volume = "72",
  year = "1979",
  pages = "514-530",
  abstract =
    "The problem of giving a formal definition of the representation of
    algebraic data structures is considered and developped in the frame
    work of the abstract data types approach. Such concepts as canonical
    form and simplification are formalized and related to properties of
    the abstract specification and of the associated term rewriting
    system.",
  paper = "Ausi79.pdf"
}

@misc{Avig12,
  author = "Avigad, Jeremy",
  title = {{Interactive Theorem Proving, Automated Reasoning, and 
           Mathematical Computation}},
  year = "2012",
  comment = "slides",
  paper = "Avig12.pdf",
  keywords ="CAS-Proof, printed"
}

@misc{Avig14a,
  author = "Avigad, Jeremy",
  title = {{Formal Verification, Interactive Theorem Proving, and
           Automated Reasoning}},
  year = "2014",
  comment = "slides",
  paper = "Avig14a.pdf"
}

@misc{Avig16a,
  author = "Avigad, Jeremy",
  title = {{Interactive Theorem Proving, Automated Reasoning, and Dynamical
           Systems}},
  year = "2016",
  comment = "slides",
  paper = "Avig16a.pdf"
}

@misc{Avig17a,
  author = "Avigad, Jeremy",
  title = {{Formal Methods in Mathematics and the Lean Theorem Prover}},
  year = "2017",
  comment = "slides",
  paper = "Avig17a.pdf"
}

@article{Bail96,
  author = "Bailey, Anthony",
  title = {{Coercion Synthesis in Computer Implementations of 
            Type-Theoretic Frameworks}},
  journal = "LNCS",
  year = "1996",
  pages = "9-27",
  abstract =
    "A coercion is a function that acts on a representation of some object
    in order to alter its type.  The idea is that although applying a
    coercion to an object changes its type, the result still represents
    the ``same'' abject in some sense; perhaps it is some essential
    underlying part of the original object, or a different representation
    of that object.  This paper examines some of the issues involved in
    the computer implementation of systems that allow a user to define
    coercions that may then be left implicit in the syntax of expressions
    and synthesised automatically.  From a type-theoretic perspective,
    coercions are often left implicit in mathematical texts, so they can
    be used to improve the readability of a formalisation, and to
    implement other tricks of syntax if so desired.",
  paper = "Bail96.pdf",
  keywords = "printed"
}

@phdthesis{Bail98,
  author = "Bailey, Anthony",
  title = {{The Machine-Checked Literate Formalisation of Algebra
             in Type Theory}},
  school = "University of Manchester",
  year = "1998",
  link =
  "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.464.1249&rep=rep1&type=pdf}",
  abstract =
    "I present a large-scale formalisation within a type theory of a
    proof of a result from abstract algebra.  The formalisation body
    consists of files that are machine-checked to ensure their
    correctness, and also processed to produce a report on the proof that
    is human-readable.  The resulting presentation is intended to approach
    being a standard informal account of some mathematics.  In addition
    to presenting this proof, the thesis also identifies and examines
    problems in reconciling the formal nature of the development with the
    wish for it to be easy to read.  It presents some tools and methodologies 
    for solving these problems, and discusses the advantages and
    disadvantages of these solutions.  In particular, it addresses the
    implementation and use of implicit coercions within the type theory,
    the styles of proof that can be used, and the borrowing of concepts
    from the literate programming paradigm.  To be more specific, the
    algebra in question is a constructive version of the fundamental
    theorem of Galois Theory. The formalisation is developed within a
    variant of the Unified Theory of Types that is implemented by a
    modified version of the LEGO proof-checker.",
  paper = "Bail98.pdf",
  keywords = "axiomref"
}

@misc{Bake14,
 author = "Baker, Martin",
 title = {{Axiom Architecture}},
 year = "2014",
 link = "\url{http://www.euclideanspace.com/prog/scratchpad/internals/ccode}",
 keywords = "axiomref"
}

@article{Barr96,
  author = "Barras, Bruno",
  title = {{Verification of the Interface of a Small Proof System in Coq}},
  journal = "LNCS",
  volume = "1512",
  pages = "28-45",
  year = "1996",
  abstract =
    "This article describes the formalization of the interface of a
    proof-checker.  The latter is based on a kernel consisting of
    type-checking functions for the Calculus of Constructions, but it
    seems the ideas can generalize to other type systems, as fax as they
    axe based on the proofs- as-terms principle.  We suppose that the
    metatheory of the corresponding type system is proved (up to type
    decidability).  We specify and certify the toplevel loop, the system
    invariant, and the error messages.",
  paper = "Barr96.pdf",
  keywords = "printed"
}

@article{Bart95,
  author = "Barthe, Gilles",
  title = {{Implicit Coercions in Type Systems}},
  journal = "LNCS",
  volume = "1158",
  year = "1995",
  pages = "1-15",
  abstract =
    "We propose a notion of pure type system with implicit coercions. In
    our framework, judgements are extended with a context of coercions Δ
    and the application rule is modified so as to allow coercions to be
    left implicit. The setting supports multiple inheritance and can be
    applied to all type theories with $\Pi$-types. One originality of our
    work is to propose a computational interpretation for implicit
    coercions. In this paper, we demonstrate how this interpretation
    allows a strict control on the logical properties of pure type systems
    with implicit coecions.",
  paper = "Bart95.pdf",
  keywords = "printed"
}

@article{Bart72,
  author = "Barton, D.R. and Fitch, John P.",
  title = {{A Review of Algebraic Manipulative Programs and their 
            Application}},
  journal = "The Computer Journal",
  volume = "15",
  number = "4",
  pages = "362-381",
  year = "1972",
  link = "\url{http://comjnl.oxfordjournals.org/content/15/4/362.full.pdf+html}",
  abstract =
    "This paper describes the applications area of computer programs that
    carry out formal algebraic manipulation. The first part of the paper
    is tutorial and severed typical problems are introduced which can be
    solved using algebraic manipulative systems. Sample programs for the
    solution of these problems using several algebra systems are then
    presented. Next, two more difficult examples are used to introduce the
    reader to the true capabilities of an algebra program and these are
    proposed as a means of comparison between rival algebra systems. A
    brief review of the technical problems of algebraic manipulation is
    given in the final section.",
  paper = "Bart72.pdf",
  keywords = "axiomref"
}

@book{Bart94,
  author = "Barton, John J. and Nackman, Lee R.",
  title = {{Scientific and Engineering C++}},
  publisher = "Pearson",
  year = "1994",
  isbn = "97800201533934"
}

@misc{Batu03,
  author = "Batut, C. and Belabas, K. and Bernardi, D. and Cohen, H. and
            Olivier, M.",
  title = {{User's Guide to PARI/GP}},
  link = "\url{http://math.mit.edu/~brubaker/PARI/PARIusers.pdf}",
  paper = "Batu03.pdf",
  keywords = "axiomref"
}

@article{Baue98,
  author = "Bauer, Andrej and Clarke, Edmund and Zhao, Xudong",
  title = {{Analytica -- An Experiment in Combining Theorem Proving and
           Symbolic Computation}},
  journal = "Journal of Automated Reasoning",
  volume = "21",
  number = "3",
  pages = "295-325",
  year = "1998",
  abstract =
    "Analytica is an automatic theorem prover for theorems in elementary
    analysis. The prover is written in the Mathematica language and runs
    in the Mathematica environment. The goal of the project is to use a
    powerful symbolic computation system to prove theorems that are beyond
    the scope of previous automatic theorem provers. The theorem prover is
    also able to deduce the correctness of certain simplification steps
    that would otherwise not be performed. We describe the structure of
    Analytica and explain the main techniques that it uses to construct
    proofs. Analytica has been able to prove several nontrivial
    theorems. In this paper, we show how it can prove a series of lemmas
    that lead to the Bernstein approximation theorem.",
  paper = "Baue98.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Bees90,
  author = "Beeson, Michael J.",
  title = {{Review of Implementing Mathematics with the Nuprl Proof
            Development System}},
  journal = "J. of Symbolic Logic",
  volume = "55",
  number = "3",
  pages = "1299-1302",
  year = "1990",
  link = "\url{http://www.jstor.org/stable/2274489}",
  paper = "Bees90.pdf"
}

@book{Beez15,
  author = "Judson, Tom and Beezer, Rob",
  title = {{Abstract Algebra: Theory and Applications}},
  year = "2015",
  publisher = "Tom Judson",
  link = "\url{http://abstract.ups.edu/download/aata-20150812-sage-6.8.pdf}",
  paper = "Beez15.pdf"
}

@article{Berg95,
  author = "Berger, U. and Schwichtenberg, H.",
  title = {{The Greatest Common Divisor: A Case Study for Program
            Extraction from Classical Proofs}},
  journal = "LNCS",
  volume = "1158",
  year = "1995",
  pages = "36-46",
  paper = "Berg95.pdf",
  keywords = "printed"
}

@misc{Bern85,
  author = "Berndt, B.C.",
  title = {{Ramanujan's Notebooks, Part I}},
  publisher = "Springer-Verlag",
  year = "1985",
  pages = "25-43"
}

@book{Bern91,
  author = "Bernays, Paul",
  title = {{Axiomatic Set Theory}},
  publisher = "Dover",
  year = "1991"
}

@article{Bert98,
  author = "Bertoli, P. and Calmet, J. and Guinchiglia, F. and Homann, K.",
  title = {{Specification and Integration of Theorem Provers and Computer
           Algebra Systems}},
  journal = "Lecture Notes in Computer Science",
  volume = "1476",
  year = "1998",
  pages = "94-106",
  abstract =
    "Computer algebra systems (CASs) and automated theorem provers (ATPs)
    exhibit complementary abilities. CASs focus on efficiently solving
    domain-specific problems. ATPs are designed to allow for the
    formalization and solution of wide classes of problems within some
    logical framework. Integrating CASs and ATPs allows for the solution
    of problems of a higher complexity than those confronted by each class
    alone. However, most experiments conducted so far followed an ad-hoc
    approach, resulting in tailored solutions to specific problems. A
    structured and principled approach is necessary to allow for the sound
    integration of systems in a modular way. The Open Mechanized Reasoning
    Systems (OMRS) framework was introduced for the specification and
    implementation of mechanized reasoning systems, e.g. ATPs. The
    approach was recasted to the domain of computer algebra systems. In
    this paper, we introduce a generalization of OMRS, named OMSCS (Open
    Mechanized Symbolic Computation Systems). We show how OMSCS can be
    used to soundly express CASs, ATPs, and their integration, by
    formalizing a combination between the Isabelle prover and the Maple
    algebra system. We show how the integrated system solves a problem
    which could not be tackled by each single system alone.",
  paper = "Bert98.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Bert95,
  author = "Bertrand, Laurent",
  title = {{Computing a hyperelliptic integral using arithmetic in the 
           jacobian of the curve}},
  journal = "Applicable Algebra in Engineering, Communication and Computing",
  volume = "6",
  pages = "275-298",
  year = "1995",
  abstract = "
    In this paper, we describe an efficient algorithm for computing an
    elementary antiderivative of an algebraic function defined on a
    hyperelliptic curve. Our algorithm combines B.M. Trager's integration
    algorithm and a technique for computing in the Jacobian of a
    hyperelliptic curve introduced by D.G. Cantor. Our method has been
    implemented and successfully compared to Trager's general algorithm."
}

@inproceedings{Bitt94,
  author = "Bittencourt, G. and Calmet, Jacques and Homann, K. and
            Lulay, A.",
  title = {{MANTRA: A Multi-Level Hybrid Knowledge Representation System}},
  booktitle = "Proc. XI Brazilian Symp. on Artificial Intelligence",
  pages = "493-506",
  year = "1994",
  abstract =
    "The intelligent behavior of a system is based upon its represented
    knowledge and inference capabilities. In this paper we report on a
    knowledge representation and reasoning system, developed at the
    University of Karlsruhe, called Mantra. The system provides four
    different knowledge representation methods -- first-order logic,
    terminological language, semantic networks, and production rules --
    distributed into a three levels architecture. The first three methods
    form the lowest level of the architecture, the epistemological
    level. The supported hybrid inferences as well as the management of
    knowledge bases form the second level, called logical level. Finally,
    the third level, the heuristic level, provides representation of
    procedural knowledge of a domain, and the introduction of ad hoc
    rules. This knowledge is represented in terms of production rules
    which are processed by a Ops5-like rule interpreter. This paper mainly
    describes the introduction of this level into the hybrid system. The
    semantics of the knowledge representation methods of the
    epistemological level is dened according to a four-valued logic
    approach. This denition insures that all inference algorithms are
    sound, complete and decidable. The system has been implemented in
    Common Lisp using the object-oriented extension CLOS, and the
    graphical user interface was implemented in C with XToolkit.",
  paper = "Bitt94.pdf"
}

@book{Boge77,
  author = "Bogen, Richard",
  title = {{MACSYMA Reference Manual, Version 9}},
  publisher = "MIT",
  year = "1977",
  link = "\url{http://bitsavers.informatik.uni-stuttgart.de/pdf/mit/macsyma/MACSYMA_RefMan_V9_Dec77.pdf}",
  paper = "Boge77.pdf"
}

@misc{Bott17,
  author = "Rademacher, Gunther",
  title = {{Railroad Diagram Generator}},
  link = "\url{http://www.bottlecaps.de/rr/ui}",
  year = "2017"
}

@article{Bove02,
  author = "Bove, Ana",
  title = {{General Recursion in Type Theory}},
  journal = "LNCS",
  volume = "2646",
  pages = "39-58",
  year = "2002",
  abstract =
    "In this work, a method to formalise general recursive algorithms in
    constructive type theory is presented throughout examples.  The method
    separates the computational and logical parts of the definitions. As
    a consequence, the resulting type-theoretic algorithms are clear,
    compact and easy to understand. They are as simple as their
    equivalents in a functional programming language, where there is no
    restriction on recursive calls. Given a general recursive algorithm,
    the method consists in defining an inductive special-purpose
    accessibility predicate that characterises the inputs on which the
    algorithm terminates. The type-theoretic version of the algorithm can
    then be defined by structural recursion on the proof that the input
    values satisfy this predicate. When formalising nested algorithms, the
    special-purpose accessibility predicate and the type-theoretic version
    of the algorithm must be defined simultaneously because they depend on
    each other. Since the method separates the computational part from
    the logical part of a definition, formalising partial functions
    becomes also possible.",
  paper = "Bove02.pdf",
  keywords = "printed"
}

@inproceedings{Brad86,
  author = "Bradford, Russell J. and Hearn, Anthony C. and Padget, Julian and
            Schrufer, Eberhard",
  title = {{Enlarging the REDUCE domain of computation}},
  booktitle = "Proc SYMSAC 1986",
  series = "SYMSAC '86",
  publisher = "ACM",
  year = "1986",
  pages = "100-106",
  isbn = "0-89791-199-7",
  abstract =
    "We describe the methods available in the current REDUCE system for
    introducing new mathematical domains, and illustrate these by discussing
    several new domains that significantly increase the power of the overall
    system."
}

@misc{Brad92a,
  author = "Bradford, Russell J.",
  title = {{C78: Computer Algebra Course Lecture Notes}},
  institution = "Univ. of Bath",
  year = "1992"
}

@article{Brad13,
  author = "Brady, Edwin",
  title = {{Idris, a General Purpose Dependently Typed Programming
            Language: Design and Implementation}},
  journal = "J. Functional Programming",
  volume = "23",
  number = "5",
  year = "2013",
  pages = "552-593",
  abstract =
    "Many components of a dependently typed programming language are by
    now well understood, for example, the underlying type theory, type
    checking, unification and evaluation. How to combine these components
    into a realistic and usable high-level language is, however, folklore,
    discovered anew by successive language implementors. In this paper, I
    describe the implementation of Idris, a new dependently typed
    functional programming language. Idris is intended to be a
    general-purpose programming language and as such provides high-level
    concepts such as implicit syntax, type classes and do notation. I
    describe the high-level language and the underlying type theory, and
    present a tactic-based method for elaborating concrete high-level
    syntax with implicit arguments and type classes into a fully explicit
    type theory. Furthermore, I show how this method facilitates the
    implementation of new high-level language constructs.",
  keywords = "printed"
}

@article{Bram02a,
  author = "Braman, Karen and Byers, Ralph and Mathias, Roy",
  title = {{The Multi-Shift QR Algorithm Part I: Maintaining Well Focused 
           Shifts, and Level 3 Performance}},
  journal = "SIAM Journal of Matrix Analysis",
  year = "2002",
  volume = "23",
  pages = "929-947",
  abstract = 
    "This paper presents a small-bulge multishift variation of the
    multishift QR algorithm that avoids the phenomenon of shift blurring,
    which retards convergence and limits the number of simultaneous
    shifts. It replaces the large diagonal bulge in the multishift QR
    sweep with a chain of many small bulges. The small-bulge multishift QR
    sweep admits nearly any number of simultaneous shifts -- even hundreds
    -- without adverse effects on the convergence rate. With enough
    simultaneous shifts, the small-bulge multishift QR algorithm takes
    advantage of the level 3 BLAS, which is a special advantage for
    computers with advanced architectures."

}

@article{Bram02b,
  author = "Braman, Karen and Byers, Ralph and Mathias, Roy",
  title = {{The Multi-Shift QR Algorithm Part II: Aggressive Early Deflation}},
  journal = "SIAM Journal of Matrix Analysis",
  year = "2002",
  volume = "23",
  pages = "948-973",
  abstract = 
    "Aggressive early deflation is a QR algorithm strategy that takes
    advantage of matrix perturbations outside of the subdiagonal entries
    of the Hessenberg QR iterate. It identifies and deflates converged
    eigenvalues long before the classic small-subdiagonal strategy
    would. The new deflation strategy enhances the performance of
    conventional large-bulge multishift QR algorithms, but it is
    particularly effective in combination with the small-bulge multishift
    QR algorithm. The small-bulge multishift QR sweep with aggressive
    early deflation maintains a high rate of execution of floating point
    operations while significantly reducing the number of operations
    required."

}

@misc{Bren10,
  author = "Brent, Richard P. and Zimmermann, Paul",
  title = {{Modern Computer Arithmetic}},
  year = "2010",
  keywords = "printed"
}

@misc{Bron95,
  author = "Bronstein, Manuel",
  title = {{On radical solutions of linear ordinary differential equations}},
  year = "1995",
  link = "\url{ftp://ftp.inf.ethz.ch/org/cathode/workshops/jan95/abstracts/bronstein.ps}",
  algebra =
  "\newline\refto{category OREPCAT UnivariateSkewPolynomialCategory}
   \newline\refto{category LODOCAT LinearOrdinaryDifferentialOperatorCategory}
   \newline\refto{domain AUTOMOR Automorphism}
   \newline\refto{domain ORESUP SparseUnivariateSkewPolynomial}
   \newline\refto{domain OREUP UnivariateSkewPolynomial}
   \newline\refto{domain LODO LinearOrdinaryDifferentialOperator}
   \newline\refto{domain LODO1 LinearOrdinaryDifferentialOperator1}
   \newline\refto{domain LODO2 LinearOrdinaryDifferentialOperator2}
   \newline\refto{package APPLYORE ApplyUnivariateSkewPolynomial}
   \newline\refto{package OREPCTO UnivariateSkewPolynomialCategoryOps}
   \newline\refto{package LODOF LinearOrdinaryDifferentialOperatorFactorizer}
   \newline\refto{package LODOOPS LinearOrdinaryDifferentialOperatorsOps}",
  paper = "Bron95.pdf"
}

@article{Bron90,
  author = "Bronstein, Manuel",
  title = {{The Transcendental Risch Differential Equation}},
  journal = "J. Symbolic Computation",
  volume = "9",
  pages = "49-60",
  year = "1990",
  comment = "IBM Research Report RC13460 IBM Corp. Yorktown Heights, NY",
  algebra =
    "\newline\refto{package RDETR TranscendentalRischDE}
     \newline\refto{package RDETRS TranscendentalRischDESystem}",
  abstract = 
    "We present a new rational algorithm for solving Risch differential
    equations in towers of transcendental elementary extensions. In
    contrast to a recent algorithm by Davenport we do not require a
    progressive reduction of the denominators involved, but use weak
    normality to obtain a formula for the denominator of a possible
    solution. Implementation timings show this approach to be faster than
    a Hermite-like reduction.",
  paper = "Bron90.pdf",
  keywords = "axiomref"
}

@techreport{Bron98,
  author = "Bronstein, Manuel",
  title = {{The lazy hermite reduction}},
  type = "Rapport de Recherche",
  number = "RR-3562",
  year = "1998",
  institution = "French Institute for Research in Computer Science",
  abstract = "
    The Hermite reduction is a symbolic integration technique that reduces
    algebraic functions to integrands having only simple affine
    poles. While it is very effective in the case of simple radical
    extensions, its use in more general algebraic extensions requires the
    precomputation of an integral basis, which makes the reduction
    impractical for either multiple algebraic extensions or complicated
    ground fields. In this paper, we show that the Hermite reduction can
    be performed without {\sl a priori} computation of either a primitive
    element or integral basis, computing the smallest order necessary for
    a particular integrand along the way.",
  paper = "Bron98.pdf"
}

@misc{Bro98b,
  author = "Bronstein, Manuel",
  title = {{Symbolic Integration Tutorial}},
  series = "ISSAC'98",
  year = "1998",
  address = "INRIA Sophia Antipolis",
  link = 
"\url{http://www-sop.inria.fr/cafe/Manuel.Bronstein/publications/issac98.pdf}",

}

@article{Brui94,
  author = "de Bruijn, N.G.",
  title = {{A Survey of the project Automath}},
  journal = "Studies in Logic and the Foundations of Mathematics",
  volume = "133",
  year = "1994",
  pages = "141-161",
  abstract =
    "Thus far, much about Automath has been written in separate
    reports. Most of this work has been made available upon request, but
    only a small part was published in journals, conference proceedings,
    etc. Unfortunately, a general survey in the form of a book is still
    lacking. A short survey was given in [de Bruijn 73c], but the present
    one will be much more extensive. Naturally, this survey will report
    about work that has been done, is going on, or is planned for the
    future. But it will also be used to explain how various parts of the
    project are related. Moreover we shall try to clarify a few points
    which many outsiders consider as uncommon or even wierd. In particular
    we spend quite some attention to our concept of types and the matter
    of ``propositions as types'' (Section 14). Finally the survey will be
    used to ventilate opinions and views in mathematics which are not
    easily set down in more technical reports.",
  paper = "Brui94.pdf",
  keywords = "printed"
}

@incollection{Brui94a,
  author = "de Bruijn, N.G.",
  title = {{The Mathematical Vernacular, a Language for Mathematics
            with Typed Sets}},
  booktitle = "Selected Papers on Automath",
  pages = "865-935",
  year = "1994",
  publisher = "North-Holland",
  link = "\url{https://pure.tue.nl/ws/files/2073504/610209.pdf}",
  paper = "Brui94a.pdf"
}

@article{Buch87,
  author = "Buchberger, Bruno",
  title = {{Applications of Gr\"obner Bases in Non-Linear
           Computational Geometry}},
  journal = "Lecture Notes in Computer Science",
  volume = "296",
  year = "1987",
  paper = "52-80",
  abstract =
    "Groebner bases are certain finite sets of multivariate
    polynomials. Many problems in polynomial ideal theory (algebra
    geometry, non-linear computational geometry) can be solved by easy
    algorithms after transforming the polynomial sets involved in the
    specification of the problems into Groebner basis form. In this paper
    we give gome examples of applying the Groebner bases method to
    problems in non-linear computational geometry (inverse kinematics in
    robot programming, collision detection for superellipsoids,
    implicitization of parametric representations of curves and surfaces,
    inversion problem for parametric representations, automated
    geometrical theorem proving, primary decompsotion of implicitly
    defined geometrical objects). The paper starts with a brief summary of
    the Groebner bases method.",
  paper = "Buch87.pdf",
  keywords = "printed"
}

@inproceedings{Buch97a,
  author = "Buchberger, B. and Jebelean, Tudor and Kriftner, Franz and
            Vasaru, Daniela",
  title = {{A Survey on the Theorema Project}},
  pages = "384-391",
  booktitle = "ISSAC '97",
  year = "1997",
  abstract =
    "The Theorema project aims at extending current computer algebra
    systems by facilities for supporting mathematical proving. The present
    early-prototype version of the Theorema software system is implemented
    in Mathematica 3.0. The system consists of a general higher-order
    predicate logic prover and a collection of special provers that call
    each other depending on the particular proof situations. The
    individual provers imitate the proof style of human mathematicians and
    aim at producing human-readable proofs in natural language presented
    in nested cells that facilitate studying the computer-generated proofs
    at various levels of detail. The special provers are intimately
    connected with the functors that build up the various mathematical
    domains.",
  paper = "Buch97a.pdf",
  keywords = "CAS-Proof"
}

@book{Buch81,
  author = "Buchberger, B. and Lichtenberger, F.",
  title = {{Mathematics for Computer Science I -- The Method of Mathematics}},
  publisher = "Springer",
  year = "1981",
  comment = "German"
}

@book{Buch96,
  author = "Buchberger, B.",
  title = {{Symbolic Computation: Computer Algebra and Logic}},
  booktitle = "Frontiers of Combining Systems",
  publisher = "Kluwer Academic",
  year = "1996",
  pages = "192-220",
  abstract =
    "In this paper we present our personal view of what should be the next
    step in the development of symbolic computation systems. The main
    point is that future systems should integrate the power of algebra and
    logic. We identify four gaps between the future ideal and the systems
    available at present: the logic, the syntax, the mathematics, and the
    prover gap, respectively. We discuss higher order logic without
    extensionality and with set theory as a subtheory as a logic frame for
    future systems and we propose to start from existing computer algebra
    systems and proceed by adding new facilities for closing the syntax,
    mathematics, and the prover gaps. Mathematica seems to be a
    particularly suitable candidate for such an approach. As the main
    technique for structuring mathematical knowledge, mathematical methods
    (including algorithms), and also mathematical proofs, we underline the
    practical importance of functors and show how they can be naturally
    embedded into Mathematica.",
  paper = "Buch96.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Buch14,
  author = "Buchberger, Bruno",
  title = {{Soft Math Math Soft}},
  journal = "LNCS",
  volume = "8592",
  year = "2014",
  pages = "9-15",
  abstract =
    "In this talk we argue that mathematics is essentially software. In
    fact, from the beginning of mathematics, it was the goal of
    mathematics to automate problem solving. By systematic and deep
    thinking, for problems whose solution was difficult in each
    individual instance, systematic procedures were found that allow to
    solve each instance without further thinking. In each round of
    automation in mathematics, the deep thinking on spect ra of problem
    instances is reflected by deep theorems with deep proofs.",
  paper = "Buch14.pdf"
}

@incollection{Bund91,
  author = "Bundy, Alan",
  title = {{A Science of Reasoning (Extended Abstract)}},
  booktitle = "Computational Logic: Essays in Honor of Alan 
               Robinson",
  year = "1991",
  publisher = "MIT Press",
  abstract = 
    "How can we understand reasoning in general and mathematical proofs
    in particular? It is argued that a high-level understanding of proofs
    is needed to complement the low-level understanding provided by
    Logic. A role for computation is proposed to provide this high-level
    understanding, namely by the association of proof plans with
    proofs. Criteria are given for assessing the association of a proof
    plan with a proof.",
  paper = "Bund91.pdf"
}

@article{Burg74,
  author = "William H. Burge",
  title = {{Stream Processing Functions}},
  year = "1974",
  month = "January",
  journal = "IBM Journal of Research and Development",
  volume = "19",
  issue = "1",
  pages = "12-25",
  papers = "Burg74.pdf",
  abstract = "
    One principle of structured programming is that a program should be
    separated into meaningful independent subprograms, which are then
    combined so that the relation of the parts to the whole can be clearly
    established.  This paper describes several alternative ways to compose
    programs. The main method used is to permit the programmer to denote
    by an expression the sequence of values taken on by a variable. The
    sequence is represented by a function called a stream, which is a
    functional analog of a coroutine. The conventional while and for loops
    of structured programming may be composed by a technique of stream
    processing (analogous to list processing), which results in more
    structured programs than the orignals. This technique makes it
    possible to structure a program in a natural way into its logically
    separate parts, which can then be considered independently."
}

@inproceedings{Burs77,
  author = "Burstall, R.M. and Goguen, J.A.",
  title = {{Putting THeories together to make Specifications}},
  booktitle = "IJCAI 77 Volume 2",
  pages = "1045-1058",
  year = "1977",
  paper = "Burs77.pdf"
}

@misc{Busw04,
  author = "Buswell, S. and Caprotti, O. and Carlisle, D.P. and Dewar, M.C.
            and Gaetano, M. and Kohlhase, M.",
  title = {{The OpenMath Standard}},
  year = "2004",
  link = "\url{https://www.openmath.org/standard/om20-2004-06-30/omstd20.pdf}",
  paper = "Busw04.pdf"
}

@inproceedings{Butl96,
  author = "Butler, Greg",
  title = {{Software Architectures for Computer Algebra: A Case Study}},
  booktitle = "DISCO '96",
  pages = "277-286",
  year = "1996",
  abstract =
    "The architectures of the existing computer algebra systems have not
    been discussed sufficiently in the literature. Instead, the focus has
    been on the design of the related programming language, or the design
    of a few key data structures.
    
    We address this deficiency with a case study of the architecture of
    Cayley. Our aim is twofold: to capture this knowledge before the total
    passing of a system now made obsolete by Magma; and to encourage
    others to describe the architecture of the computer algebra systems
    with which they are familiar.
    
    The long-term goal is a better understanding of how to construct
    computer algebra systems in the future.",
  paper = "Butl96.pdf",
  keywords = "axiomref"
}

@book{Calm96,
  author = "Calmet, Jacques and Homann, Karsten",
  title = {{Classification of Communication and Cooperation Mechanisms for
           Logical and Symbolic Computation Systems}},
  booktitle = "Frontiers of Combining Systems",
  publisher = "Kluwer Academic",
  year = "1996",
  pages = "221-234",
  abstract =
    "The combination of logical and symbolic computation systems has
    recently emerged from prototype extensions of stand-alone systems to
    the study of environments allowing interaction among several
    systems. Communication and cooperation mechanisms of systems
    performing any kind of mathematical service enable one to study and
    solve new classes of problems and to perform efficient computation by
    distributed specialized packages. The classification of communication
    and cooperation methods for logical and symbolic computation systems
    given in this paper provides and surveys different methodologies for
    combining mathematical services and their characteristics,
    capabilities, requirements, and differences. The methods are
    illustrated by recent well-known examples. We separate the
    classification into communication and cooperation methods. The former
    includes all aspects of the physical connection, the flow of
    mathematical information, the communication language(s) and its
    encoding, encryption, and knowledge sharing. The latter concerns the
    semantic aspects of architectures for cooperative problem solving.",
  paper = "Calm96.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Cant87,
  author = "Cantor, D.",
  title = {{Computing in the Jacobian of a HyperellipticCurve}},
  journal = "Mathematics of Computation",
  volume = "48",
  number = "177",
  month = "January",
  year = "1987",
  pages = "95-101",
}

@misc{Carl14,
  author = "Carlisle, David and Ion, Patrick and Miner, Robert",
  title = {{Mathematical Markup Language}},
  year = "2014",
  link = "\url{https://www.w3.org/TR/MathML3/}"
}

@article{Carl02,
  author = "Carlstrom, Jesper",
  title = {{Subsets, Quotients, and Partial Functions in Martin-L\"of's
            Type Theory}},
  journal = "LNCS",
  volume = "2646",
  year = "2002",
  pages = "78-94",
  abstract =
    "We treat subsets, equivalence relations, and partial functions,
    with subsets as propositional functions .In order for these three
    notions to work well together, we propose some changes to the theory
    of subsets as propositional functions .The method used is not to make
    any changes to the type theory itself, but to view the new concepts as
    defined ones.",
  paper = "Carl02.pdf",
  keywords = "printed"
}

@book{Char92,
  author = "Char, B.W. and Geddes, K.O. and Gonnet, G.H. and Leong, B.L.
            and Monagan, M.B. and Watt, S.M.",
  title = {{A Tutorial Introduction to Maple V}},
  year = "1992",
  publisher = "Springer-Verlag"
}

@article{Cher80,
  author = "Cherlin, Gregory",
  title = {{Rings of Continuous Functions: Decision Problems}},
  journal = "Lecture Notes in Mathematics",
  volume = "834",
  pages = "44-91",
  year = "1980",
  link = "\url{https://link.springer.com/content/pdf/10.1007/BFb0090160.pdf}",
  paper = "Cher80.pdf"
}

@misc{Chew95,
  author = "Chew, Paul and Constable, Robert L. and Pingali, Keshav and
            Vavasis, Steve and Zippel, Richard",
  title = {{Collaborative Mathematics Environment}},
  link = "\url{http://www.cs.cornell.edu/rz/MathBus95/TechSummary.html}",
  keywords = "axiomref"
}

@article{Chur40,
  author = "Church, Alonzo",
  title = {{A Formulation of the Simple Theory of Types}},
  journal = "J. of Symbolic Logic",
  volume = "5",
  number = "2",
  year = "1940",
  pages = "56-68",
  abstract =
    "The purpose of the present paper is to give a formulation of the 
    simple theory of types which incorporates certain features of the
    calculus of $\lambda$-conversion. A complete incorporation of the
    calculus of $\lambda$-conversion into the theory of types is
    impossible if we require that $\lambda x$ and juxtaposition shall
    retain their respective meanings as an abstraction operator and as
    denoting the application of function to argument. But the present
    partial incorporation has certain advantages from the point of view of
    type theory and is offered as being of interest on this basis
    (whatever may be thought of the finally satisfactory character of the
    theory of types as a foundation for logic and mathematics).",
  paper = "Chur40.pdf"
}

@techreport{Clar92,
  author = "Clarke, Edmund and Zhao, Xudong",
  title = {{Analytica -- An Experiment in Combining Theorem Proving and
           Symbolic Computation}},
  type = "technical report",
  institution = "Carnegie Mellon University",
  number = "CMU-CS-92-147",
  year = "1992",
  abstract =
    "Analytica is an automatic theorem prover for theorems in elementary
    analysis. The prover is written in Mathematica language and runs in
    the Mathematica environment. The goal of the project is to use a
    powerful symbolic computation system to prove theorems that are beyond
    the scope of previous automatic theorem provers. The theorem prover is
    also able to guarantee the correctness of certain steps that are made
    by the symbolic computation system and therefore prevent common errors
    like division by a symbolic expression that could be zero. In this
    paper we describe the structure of Analytica and explain the main
    techniques that it uses to construct proofs. Analytica has been able
    to prove several non-trivial examples including the basic properties
    of the stereographic projection and a series of three lemmas that lead
    to a proof of Weierstrass''s example of a continuous nowhere
    differentiable function. Each of the lemmas in the latter example is
    proved completely automatically.",
  keywords = "CAS-Proof"
}

@techreport{Clar94,
  author = "Clarke, Edmund and Zhao, Xudong",
  title = {{Combining Symbolic Computation and Theorem Proving: Some
           Problems of Ramanujan}},
  year = "1994",
  type = "technical report",
  institution = "Carnegie Mellon University",
  number = "CMU-CS-94-103",
  abstract =
    "One way of building more powerful theorem provers is to use
    techniques from symbolic computation. The challenge problems in this
    paper are taken from Chapter 2 of Ramanujan''s Notebooks. They were
    selected because they are non-trivial and require the use of symbolic
    computation techniques. We have developed a theorem prover based on
    the symbolic computation system Mathematica, that can prove all the
    challenge problems completely automatically. The axioms and inference
    rules for constructing the proofs are also briefly discussed.",
  paper = "Clar94.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Coen07,
  author = "Coen, Claudio Sacerdoti and Tassi, Enrico",
  title = {{Working with Mathematical Structures in Type Theory}},
  journal = "LNCS",
  volume = "4941",
  year = "2007",
  pages = "157-172",
  abstract =
    "We address the problem of representing mathematical structures in a
    proof assistant which: 1) is based on a type theory with dependent
    types, telescopes and a computational version of Leibniz equality; 2)
    implements coercive subtyping, accepting multiple coherent paths 
    between type families; 3) implements a restricted form of higher order
    unification and type reconstruction. We show how to exploit the
    previous quite common features to reduce the ``syntactic'' gap between
    pen and paper and formalised algebra. However, to reach our goal we need
    to propose unification and type reconstruction heuristics that are
    slightly different from the ones usually implemented. We have
    implemented them in Matita.",
  paper = "Coen07.pdf",
  keywords = "printed"
}

@book{Cohn65,
  author = "Cohn, Paul Moritz",
  title = {{Universal Algebra}},
  publisher = "Harper and Row",
  year = "1965"
}

@book{Cohn91,
  author = "Cohn, Paul Moritz",
  title = {{Algebra (2nd Ed.)}},
  publisher = "Wiley",
  isbn = "978-0471101697",
  year = "1991",
  paper = "Cohn91.pdf"
}

@inproceedings{Coll85,
  author = "Collins, George E.",
  title = {{The SAC-2 Computer Algebra System}},
  booktitle = "Proc. Europena Conf. on Computer Algebra",
  year = "1985",
  pages = "34-35",
  paper = "Coll85.pdf"
}

@article{Colm90,
  author = "Colmerauer, Alain",
  title = {{An Introduction to Prolog III}},
  journal = "CACM",
  volume = "33",
  number = "7",
  year = "1990",
  pages = "69-90",
  abstract = 
    "The Prolog III programming language extends Prolog by redefining the
    fundamental process at its heart: unification. This article presents
    the specifications of this new language and illustrates its capabilities.",
  paper = "Colm90.pdf"
}

@article{Coqu98,
  author = "Coquand, Thierry and Persson, Henrik",
  title = {{Groebner Bases in Type Theory}},
  journal = "LNCS",
  volume = "1657",
  year = "1998",
  pages = "33-46",
  abstract =
    "We describe how the theory of Groebner bases, an important part
    of computational algebra, can be developed within Martin-Lof’s
    type theory. In particular, we aim for an integrated development
    of the algorithms for computing Groebner bases: we want to prove,
    constructively in type theory, the existence of Groebner bases and
    from such proofs extract the algorithms. Our main contribution is
    a reformulation of the standard theory of Groebner bases which
    uses generalised inductive definitions. We isolate the main
    non–constructive part, a minimal bad sequence argument, and use
    the open induction principle [Rao88,Coq92] to interpret it by
    induction. This leads to short constructive proofs of Dickson’s
    lemma and Hilbert’s basis theorem, which are used to give an
    integrated development of Buchberger’s algorithm. An important
    point of this work is that the elegance and brevity of the
    original proofs are maintained while the new proofs also have a
    direct constructive content.  In the appendix we present a
    computer formalisation of Dickson’s lemma and an abstract
    existence proof of Groebner bases.",
  paper = "Coqu98.pdf"
}

@article{Corl92,
  author = "Corless, Robert M. and Jeffrey, David J.",
  title = {{Well, it isn't quite that simple}},
  journal = "ACM SIGSAM",
  volume = "26",
  number = "3",
  year = "1992",
  abstract = 
    "Present computer algebra systems base their interactive sessions on a
    very simple model of mathematical discourse. The user's input to the
    system is a line containing a mathematical expression (an operation, a
    formula, a set of equations, etc) and the system's response to the
    user is an output line which contains a mathematical expression
    similar to the input. There are many situations, however, in which
    this is too simple a model of mathematics. Algebra systems should be
    allowed to reply 'Well ... it isn't quite that simple'.",
  paper = "Corl92.pdf"
}

@article{Crai92,
  author = "Craigen, Dan and Kromodimoeljo, Sentot and Meisels, Irwin
            and Pase, Bill",
  title = {{Eves System Description}},
  journal = "Lecture Notes in Computer Science",
  volume = "607",
  pages = "771-775",
  year = "1992",
  paper = "Crai92.pdf"
}

@article{Cunn85,
  author = "Cunningham, R.J. and Dick, A.J.J",
  title = {{Rewrite Systems on a Lattice of Types}},
  journal = "Acta Informatica",
  volume = "22",
  pages = "149-169",
  year = "1985",
  abstract =
    "Re-writing systems for partial algebras are developed by modifying
    the Knuth-Bendix completion algorithm to permit the use of
    lattice-structured domains. Some problems with the original algorithm,
    such as the treatment of division rings, are overcome conveniently by
    this means. The use of a type lattice also gives a natural framework
    for specifying data types in Computer Science without over-specifying
    error situations. The soundness and meaning of the major concepts
    involed in re-writing systems are reviewed when applied to such
    structures.", 
  paper = "Cunn85.pdf",
  keywords = "printed"
}

@misc{Dave80b,
  author = "Davenport, J.H. and Jenks, R.D.",
  title = {{SCRATCHPAD/370: Modes and Domains}},
  year = "1980",
  comment = "private communication",
  keywords = "axiomref"
}

@article{Dave81b,
  author = "Davenport, James H.",
  title = {{Effective Mathematics: The Computer Algebra Viewpoint}},
  journal = "Springer Lecture Notes in Mathematics",
  volume = "873",
  pages = "31-43",
  abstract =
    "This paper is written in an attempt to explain the field of Computer
    Algebra (see e.g. Stoutemyer and Yun, 1980. Recent progress is
    summarised in the proceedings edited by Ng[1979]) to the
    mathematician, and to explain what its mathematical content is, and
    why it raises interesting questions of the effectiveness of various
    common mathematical operations. The fact that effectiveness is not
    always trivial is illustrated by the result of Richardson [1968]
    quoted in the next section.
    
    I hope that it will become clear that many operations that are often
    regarded as effective, are in fact not so; while many operations that
    might not be regarded as effective in fact are.",
  paper = "Dave81b.pdf"
}

@techreport{Deme79,
  author = "Demers, A. and Donahue, J.",
  title = {{Revised Report on RUSSELL}},
  year = "1979",
  type = "technical report",
  institution = "Cornell",
  number = "TR 79-389"
}

@article{Denz94,
  author = "Denzinger, Jorg and Fuchs, Matthias",
  title = {{Goal oriented equational theorem proving using team work}},
  journal = "Lecture Notes in Computer Science",
  volume = "861",
  pages = "343-354",
  year = "1994",
  abstract =
    "The team work method is a concept for distributing automated theorem
    provers by activating several experts to work on a problem. We have
    implemented this for pure equational logic using the unfailing
    Knuth-Bendix completion procedure as basic prover. In this paper we
    present three classes of experts working in a goal oriented
    fashion. In general, goal oriented experts perform their job “unfair”
    and so are often unable to solve a given problem alone. However, as
    team members in the team work method they perform highly efficiently,
    as we demonstrate by examples, some of which can only be proved using
    team work.", 
  paper = "Denz94.pdf"
}

@misc{Dewa00,
  author = "Dewar, Mike",
  title = {{Special Issue on OPENMATH}},
  publisher = "ACM SIGPLAN Bulletin",
  volume = "34",
  number = "2",
  year = "2000"
}

@misc{Dolz96,
  author = "Dolzmann, Andreas and Sturm, Thomas",
  title = {{Redlog User Manual Edition 1.0}},
  year = "1996",
  abstract =
    "REDLOG stands for REDUCE LOGIC system.  It provides an extension
    of the computer algebra system reduce to a ``computer logic system''
    implementing symbolic algorithms on first-order formulas wrt.
    temporarily fixed first-order languages and theories.  Underlying
    theories currently available are ordered fields and discretely 
    valued fields.  Though the focus of the implemented algorithms is on
    effective quantier elimination and simplication of quantier-free
    formulas, REDLOG is intended and designed as an all-purpose system.
    REDLOG is freely available to the scientic community.",
  paper = "Dolz96.pdf",
  keywords = "printed"
}

@article{Dolz97b,
  author = "Dolzmann, Andreas and Sturm, Thomas",
  title = {{REDLOG: Computer Algebra meets Computer Logic}},
  journal = "ACM SIGSAM Bulletin",
  volume = "31",
  number = "2",
  year = "1997",
  pages = "2-9",
  abstract =
    "REDLOG is a package that extends the computer algebra system REDUCE
    to a computer logic system, i.e., a system that provides algorithms
    for the symbolic manipulation of first-order formulas over some
    temporarily fixed language and theory. In contrast to theorem provers,
    the methods applied know about the underlying algebraic theory and
    make use of it. We illustrate some applications of REDLOG, describe
    its functionality as it appears to the user, and explain the design
    issues and implementation techniques. REDLOG is available on the WWW.",
  paper = "Dolz97b.pdf",
  keywords = "CAS-Proof, printed"
}

@misc{blas01,
  author = "Dongarra, Jack and et al.",
  title = {{Basic Linear Algebra Subprograms Technical (BLAST) 
           Forum Standard}},
  year = "2001",
  link = "\url{http://www.netlib.org/blas/blast-forum/blas-report.pdf}",
  paper = "blas01.pdf"
}

@article{Dowe00,
  author = "Dowek, Gilles",
  title = {{Axioms vs Rewrite Rules: From Completeness to Cut Elimination}},
  journal = "Lecture Notes in Computer Science",
  volume = "1794",
  pages = "62-72",
  year = "2000",
  abstract =
    "Combining a standard proof search method, such as resolution or
    tableaux, and rewriting is a powerful way to cut off search space in
    automated theorem proving, but proving the completeness of such
    combined methods may be challenging. It may require in particular to
    prove cut elimination for an extended notion of proof that combines
    deductions and computations. This suggests new interactions between
    automated theorem proving and proof theory.",
  paper = "Dowe00.pdf"
}  

@article{Dowe96,
  author = "Dowek, Gilles",
  title = {{A Type-Free Formalization of Mathematics Where Proofs are
            Objects}},
  journal = "LNCS",
  volume = "1512",
  year = "1996",
  pages = "88-111",
  abstract =
    "We present a first-order type-free axiomatization of mathematics
    where proofs are objects in the sense of Heyting-Kolmogorov functional
    interpretation.  The consistency of this theory is open.",
  paper = "Dowe96.pdf",
  keywords = "printed"
}

@misc{Duns01,
  author = "Dunstan, M.N. and Gottliebsen, H. and Kelsey, T.W. and 
            Martin, U.",
  title = {{Computer Algebra meets Automated Theorem Proving: A 
           Maple-PVS Interface}},
  booktitle = "Proc. Calculemus, 2001",
  year = "2001",
  paper = "Duns01.pdf",
  keywords = "axiomref, CAS-proof, printed"
}

@article{Dute96,
  author = "Dutertre, Bruno",
  title = {{Elements of Mathematical Analysis in PVS}},
  journal = "LNCS",
  volume = "1125",
  pages = "141-156",
  year = "1996",
  abstract =
    "This paper presents the formalization of some elements of
    mathematical analysis using the PVS verification system. Our main
    motivation was to extend the existing PVS libraries and provide means
    of modelling and reasoning about hybrid systems. The paper focuses on
    several important aspects of PVS including recent extensions of the
    type system and discusses their merits and effectiveness. We conclude
    by a brief comparison with similar developments using other theorem
    provers.",
  paper = "Dute96.pdf",
  keywords = "printed"
}

@article{Duva96a,
  author = {Duval, Dominique and Gonz\'alez-Vega, L.},
  title = {{Dynamic Evaluation and Real Closure}},
  journal = "Mathematics and Computers in Simulation",
  volume = "42",
  pages = "551-560",
  year = "1996",
  abstract = "
    The aim of this paper is to present how the dynamic evaluation method
    can be used to deal with the real closure of an ordered field. Two
    kinds of questions, or tests, may be asked in an ordered field:
    equality tests $(a=b?)$ and sign tests $(a > b?)$. Equality tests are
    handled through splittings, exactly as in the algebraic closure of a
    field. Sign tests are handled throug a structure called ``Tarski data
    type''.",
  paper = "Duva96a.pdf"
}

@article{Duva96,
  author = "Duval, D. and Reynaud, Jean-Claude",
  title = {{Sketches and Computations over Fields}},
  journal = "Mathematics and Computers in Simulation",
  volume = "42",
  pages = "363-373",
  year = "1996",
  abstract = 
    "The goal of this short paper is to describe one possible use of
    sketches in computer algebra. We show that sketches are a powerful
    tool for the description of mathematical structures and for the
    description of computations.",
  paper = "Duva96.pdf"
}

@article{Duva94a,
  author = "Duval, D. and Reynaud, J.C.",
  title = {{Sketches and Computation (Part I): 
           Basic Definitions and Static Evaluation}},
  journal = "Mathematical Structures in Computer Science",
  volume = "4",
  pages = "185-238",
  publisher =  "Cambridge University Press",
  year = "1994",
  link = "\url{http://journals.cambridge.org/abstract_S0960129500000438}",
  abstract = 
    "We define a categorical framework, based on the notion of {\sl
    sketch}, for specification and evaluation in the sense of algebraic
    specifications and algebraic programming. This framework goes far
    beyond our initial motivations, which was to specify computation with
    algebraic numbers. We begin by redefining sketches in order to deal
    explicitly with programs. Expressions and terms are carefully defined
    and studied, then {\sl quasi-projective sketches} are introduced. We
    describe {\sl static evaluation} in these sketches: we propose a
    rigorous basis for evaluation in the corresponding structures. These
    structures admit an initial model, but are not necessarily
    equational. In Part II (Duval and Reynaud 1994), we study a more
    general process, called {\sl dynamic evaluation}, for structures that
    may have no initial model.",
  paper = "Duva94a.pdf"
}

@article{Duva94b,
  author = "Duval, Dominique and Reynaud, Jean-Claude",
  title = {{Sketches and Computation (Part II): 
           Dynamic Evaluation and Applications}},
  journal = "Mathematical Structures in Computer Science",
  volume = "4", 
  pages = "239-271",
  publisher = "Cambridge University Press",
  year = "1994",
  link = "\url{http://journals.cambridge.org/abstract_S096012950000044X}",
  abstract = 
    "In the first part of this paper (Duval and Reynaud 1994), we defined a
    categorical framework, based on the notion of {\sl sketch}, for
    specification and evaluation in the senses of algebraic specification
    and algebraic programming. {\sl Static evaluation} in {\sl
    quasi-projective sketches} was defined in Part I; in this paper, {\sl
    dynamic evaluation} is introduced. It deals with more general
    structures, which may have no initial model. Until now, this process
    has not been used in algebraic specification systems, but computer
    algebra systems are beginning to use it as a basic tool. Finally, we
    give some applications of dynamic evaluation to computation in field
    extensions.",
  paper = "Duva94b.pdf"
}

@article{Duva94c,
  author = "Duval, Dominique",
  title = {{Algebraic Numbers: An Example of Dynamic Evaluation}},
  journal = "J. Symbolic Computation",
  volume = "18",
  pages = "429-445",
  year = "1994",
  link = "\url{http://www.sciencedirect.com/science/article/pii/S0747717106000551}",
  abstract = "
    Dynamic evaluation is presented through examples: computations
    involving algebraic numbers, automatic case discussion according to
    the characteristic of a field. Implementation questions are addressed
    too. Finally, branches are presented as ``dual'' to binary functions,
    according to the approach of sketch theory.",
  paper = "Duva94c.pdf",
  keywords = "axiomref"
}

@phdthesis{Elbe98,
  author = "Elbers, Hugo Johannes",
  title = {{Connecting Informal and Formal Mathematics}},
  year = "1998",
  school = "Technische Universiteit Eindhoven",
  paper = "Elbe98.pdf"
}

@inproceedings{Enge65,
  author = "Engelman, C.",
  title = {{A Program for On-Line Assistance in Symbolic Computation}},
  booktitle = "Proc. Fall Joint Comput. Conf. 2",
  year = "1965",
  publisher = "Spartan Books"
}  

@inproceedings{Eras10,
  author = "Erascu, Madalina and Jebelean, Tudor",
  title = {{A Purely Logical Approach to Program Termination}},
  booktitle = "11th Int. Workshop on Termination",
  year = "2010",
  comment = "Extended Abstract",
  link =
  "\url{http://www.risc.jku.at/publications/download/risc_4089/ErascuJebeleanWSTFinal.pdf}",
  paper = "Eras10.pdf"
}

@techreport{Eras10a,
  author = "Erascu, Madalina and Jebelean, Tudor",
  title = {{A Purely Logical Approach to Imperative Program Verification}},
  year = "2010",
  institution = "RISC Linz",
  type = "technical report",
  number = "10-07",
  link =
  "\url{http://www.risc.jku.at/publications/download/risc_4088/techRep.pdf}",
  paper = "Eras10a.pdf"
}

@inproceedings{Eras10b,
  author = "Erascu, Madalina and Jebelean, Tudor",
  title = {{A Purely Logical Approach to Termination of Imperative Loops}},
  booktitle = "Proc. 12th Int. Symp. on SYmbolic and Numeric
               Algorithms for Scientific Computing",
  pages = "142-149",
  year = "2010",
  link = "\url{http://www.risc.jku.at/publications/download/risc_4181/synasc_postproceedings.pdf}",
  abstract =
    "We present and illustrate a method for the generation of the
    termination conditions for nested loops with abrupt termination
    statements.  The conditions are (first-order) formulae obtained by
    certain transformations of the program text.  The loops are treated
    similarly to calls of recursively defined functions.  The program text
    is analyzed on all possible execution paths by forward symbolic
    execution using certain meta-level functions which define the syntax,
    the semantics, the verification conditions for the partial
    correctness, and the termination conditions. The termination
    conditions are expressed as induction principles, however, still in
    first-order logic.  

    Our approach is simpler than others because we use
    neither an additional model for program execution, nor a fixpoint
    theory for the definition of program semantics.  Because the
    meta-level functions are fully formalized in predicate logic, it is
    possible to prove in a purely logical way and at object level that the
    verification conditions are necessary and sufficient for the existence
    and uniqueness of the function implemented by the program.",
  paper = "Eras10b.pdf"
}

@techreport{Eras11,
  author = "Erascu, Madalina",
  title = {{Symbolic Computation and Program Verification. Proving
            Partial Correctness and Synthesizing Optimal Algorithms}}, 
  type = "technical report",
  number = "11-15",
  institution = "RISC Linz",
  year = "2011",
  abstract =
    "We present methods for checking the partial correctness of,
    respectively to optimize, imperative programs, using polynomial
    algebra methods, namely resultant computation and quantifier
    elimination (QE) by cylindrical algebraic decomposition (CAD). The
    result are very promising but also show that there is room for
    improvement of algebraic algorithms.",
  paper = "Eras11.pdf"
}

@inproceedings{Fate90a,
  author = "Fateman, Richard J.",
  title = {{Advances and Trends in the Design of Algebraic
            Manipulation Systems}},
  booktitle = "Proc. ISSAC'90",
  pages = "60-67",
  year = "1990"
}

@misc{Fate08,
  author = "Fateman, Richard J.",
  title = {{Revisiting numeric/symbolic indefinite integration of rational 
           functions, and extensions}},
  link = "\url{http://www.eecs.berkeley.edu/~fateman/papers/integ.pdf}",
  abstract = "
    We know we can solve this problem: Given any rational function
    $f(x)=p(x)/q(x)$, where $p$ and $q$ are univariate polynomials over
    the rationals, compute its {\sl indefinite} integral, using if
    necessary, algebraic numbers. But in many circumstances an approximate
    result is more likely to be of use. Furthermore, it is plausible that
    it would be more useful to solve the problem to allow definite
    integration, or introduce additional parameters so that we can solve
    multiple definite integrations.  How can a computer algebra system
    best answer the more useful questions?  Finally, what if the integrand
    is not a ratio of polynomials, but something more challenging?",
  paper = "Fate08.pdf"
}

@article{Fevr98,
  author = "Fevre, Stephane and Wang, Dongming",
  title = {{Proving Geometric Theorems using Clifford Algebra and Rewrite
           Rules}},
  journal = "LNCS",
  volume = "1421",
  year = "1998",
  pages = "17-32",
  abstract = 
    "We consider geometric theorems that can be stated constructively by
    introducing points, while each newly introduced point may be
    represented in terms of the previously constructed points using
    Clifford algebraic operators. To prove a concrete theorem, one first
    substitutes the expressions of the dependent points into the
    conclusion Clifford polynomial to obtain an expression that involves
    only the free points and parameters. A term-rewriting system is
    developed that can simplify such an expression to 0, and thus prove
    the theorem. A large class of theorems can be proved effectively in
    this coordinate-free manner. This paper describes the method in
    detail and reports on our preliminary experiments.",
  paper = "Fevr98.pdf"
}

@article{Fill98,
  author = "Filliatre, Jean-Christophe",
  title = {{Proof of Imperative Programs in Type Theory}},
  journal = "LNCS",
  volume = "1657",
  year = "1998",
  pages = "78-92",
  abstract = 
    "We present a new approach to certifying functional programs with
    imperative aspects, in the context of Type Theory. The key is a
    functional translation of imperative programs, based on a combination
    of the type and effect discipline and monads. Then an incomplete proof
    of the specification is built in the Type Theory, whose gaps would
    correspond to proof obligations. On sequential imperative programs,
    we get the same proof obligations as those given by Floyd-Hoare
    logic. Compared to the latter, our approach also includes functional
    constructions in a straight-forward way. This work has been
    implemented in the Coq Proof Assistant and applied on non-trivial
    examples.",
  paper = "Fill98.pdf",
  keywords = "printed"
}

@misc{Fitc74,
  author = "Fitch, J.P.",
  title = {{CAMAL Users Manual}},
  institution = "University of Cambridge Computer Laboratory",
  year = "1974"
}

@misc{Flet01,
  author = "Fletcher, John P.",
  title = {{Symbolic processing of Clifford Numbers in C++}},
  year = "2001",
  journal = "Paper 25, AGACSE 2001."
}

@misc{Flet09,
  author = "Fletcher, John P.",
  title = {{Clifford Numbers and their inverses calculated using the matrix 
           representation}},
  publisher = "Chemical Engineering and Applied Chemistry, School of 
    Engineering and Applied Science, Aston University, Aston Triangle, 
    Birmingham B4 7 ET, U. K.",
  link = "\url{http://www.ceac.aston.ac.uk/research/staff/jpf/papers/paper24/index.php}"
}

@article{Floy63,
  author = "Floyd, R. W.",
  title = {{Semantic Analysis and Operator Precedence}},
  journal = "JACM",
  volume = "10",
  number = "3",
  pages = "316-333",
  year = "1963"
}

@book{Fran73,
  author = "Frankel, A.A. and Bar-Hillel, Y. and Levy, A.",
  title = {{Foundations of Set Theory}},
  publisher = "Elsevier Science",
  year = "1973",
  isbn = "978-0720422702"
}

@misc{Fult08,
  author = "Fulton, William",
  title = {{Algebraic Curves: An Introduction to Algebraic Geometry}},
  link = "\url{http://www.math.lsa.umich.edu/~wfulton/CurveBook.pdf}",
  year = "2008",
  algebra = 
    "\newline\refto{package GPAFF GeneralPackageForAlgebraicFunctionField}
\newline\refto{package PAFFFF PackageForAlgebraicFunctionFieldOverFiniteField}
     \newline\refto{package PAFF PackageForAlgebraicFunctionField}",
  paper = "Fult08.pdf"
}

@inproceedings{Garc16,
  author = "Garcia, Ronald and Clark, Alison M. and Tanter, Eric",
  title = {{Abstracting Gradual Typing}},
  booktitle = "POPL 16",
  publisher = "ACM",
  year = "2016",
  pages = "429-442",
  abstract =
    "Language researchers and designers have extended a wide variety
    of type systems to support {\sl gradual typing}, which enables
    languages to seemlessly combine dynamic and static checking. These
    efforts consistently demonstrate that designing a satisfactory
    gradual counterpart to a static type system is challenging, and
    this challenge only increases with the sophistication of the type
    system. Graudal type system designers need more formal tools to
    help them conceptualize, structure, and evaluate their designs.

    In this paper, we propose a new formal foundation for graudal
    typing, drawing on principles from abstract interpretation to give
    gradual types a semantics in terms of pre-existing static
    types. Abstracting Gradual Typing (AGT for short) yields a formal
    account of {\sl consistency} -- one of the cornerstones of the
    gradual typing approach -- that subsumes existing notions of
    consistency, which were developed through intuition and ad hoc
    reasoning. 

    Given a syntax-directed static typing judgment, the AGT approach
    induces a corresponding gradual typing judgment. Then the type
    safety proof for the underlying static discipline induces a
    dynamic semantics for gradual programs defined over
    source-language typing derivations. The AGT approach does not
    resort to an externally justified cast calculus; instead, run-time
    check naturally arise by deducing evidence for consistent
    judgements during proof reduction.

    To illustrate the approach, we develop a novel gradually-typed
    counterpart for a language with record subtyping. Gradual
    languages designed with the AGT approach satisfy {\sl by
    construction} the refined criteria for gradual typing set forth by
    Siek and colleagues.",
  keywords = "printed"
}

@inproceedings{Garc17,
  author = "Garcia, Ronald and Cimini, Matteo",
  title = {{Principal Type Schemes for Gradual Programs}},
  booktitle = "POPL 15",
  comment = "Updated paper",
  year = "2017",
  abstract =
    "Gradual typing is a discipline for integrating dynamic checking
    into a static type system. Since its introduction in functional
    languages, it has been adapted to a variety of type systems,
    including object-oriented, security, and substructural. This work
    studies its application to implictly typed languages based on type
    inference. Siek and Vachharajani designed a gradual type inference
    system and algorithm that infers gradual types but still rejects
    ill-typed static programs. However, the type system requires local
    reasoning about type substitutions, an imperative inference
    algorithm, and a subtle correctness statement.

    This paper introduces a new approach to gradual type inference,
    driven by the principle that gradual inference should only produce
    static types. We present a static implicitly typed language, its
    gradual counterpart, and a type inference procedure. The gradual
    system types the same programs as Siek and Vachharajani, but has a
    modular structure amenable to extension. The language admits
    let-polymorphism, and its dynamics are defined by translation to
    the Polymorphic Blame Calculus (Ahmed et al. 2009, 2011).

    The principal types produced by our initial type system mask the
    distinction between static parametric polymorphism and
    polymorphism that can be attributed to gradual typing. To expose
    this difference, we distinguish static type parameters from
    gradual type parameters and reinterpret gradual type consistency
    accordingly. The resulting extension enables programs to be
    interpreted using either the polymorphic or monomorphic Blame
    Calculi.",
  keywords = "printed"
}

@misc{Gari09,
  author = "Garillot, Francois and Gonthier, Georges and Mahboubi, Assia and
            Rideau, Laurence",
  title = {{Packaging Mathematical Structures}},
  year = "2009",
  abstract =
    "This paper proposes generic design patterns to define and combine
    algebraic structures, using dependent records, coercions and type
    inference, inside the Coq system. This alternative to telescopes in
    particular allows multiple inheritance, maximal sharing of notations
    and theories, and automated structure inference. Our methodology is
    robust enough to support a hierarchy comprising a broad variety of
    algebraic structures, from types with a choice operator to algebraically
    closed fields. Interfaces for the structures enjoy the handiness of a
    classical setting, without requiring any axiom. Finally, we show how
    externally extensible some of these instances are by discussing a
    lemma seminal in defining the discrete logarithm, and a matrix
    decomposition problem.",
  paper = "Gari09.pdf"
}

@book{Gath99,
  author = {{von zur Gathen}, Joachim and Gerhard, J\"urgen},
  title = {{Modern Computer Algebra}},
  publisher = "Cambridge University Press",
  year = "1999",
  isbn = "0-521-64176-4",
  algebra = "\newline\refto{package PGCD PolynomialGcdPackage}"
}

@article{Ghel91,
  author = "Ghelli, Giorgio",
  title = {{Modeling Features of Object-Oriented Languages in Second
            Order Functional Languages with Subtypes}},
  journal = "LNCS",
  volume = "489",
  pages = "311-340",
  year = "1991",
  abstract =
    "Object oriented languages are an important tool to achieve software
    reusability in any kind of application and can increase dramatically
    software productivity in some special fields; they are also considered
    a logical step in the evolution of object oriented languages.  But
    these languages lack a formal foundation, which is needed both to
    develop tools and as a basis for the future evolution of these
    languages; they lack also a strong type system, which would be
    essential to assure that level of reliability which is required to
    large evolving systems.  Recently some researches have tried to
    insulate the basic mechanisms of object oriented languages and to
    embed them in strongly typed functional languages, giving to these
    mechanism a mathematical semantics and a set of strong type
    rules. This is a very promising approach which also allows a
    converging evolution of both the typed functional and the object
    oriented programming paradigms, making it possible a technology
    transfer in both directions.  Most works in this field are very
    technical and deal just with one aspect of object oriented
    programming; many of them use a very similar framework.  In this work
    we describe and exemplify that common framework and we survey some of
    the more recent and promising works on more specific features, using
    the framework introduced.  We describe the resuks achieved and point
    out some problems which are still open, especially those arising from
    the interactions between different mechanisms.",
  paper = "Ghel91.pdf"
}

@inproceedings{Gies05,
  author = "Giese, Martin",
  title = {{A Calculus for Type Predicates and Type Coercion}},
  booktitle = "A Calculus for Type Predicates and Type Coercion",
  series = "LNAI",
  volume = "3702",
  pages = "123-137",
  isbn = "3540289313",
  year = "1985",
  abstract =
    "We extend classical first-order logic with subtyping by type
    predicates and type coercion. Type predicates assert that the value of
    a term belongs to a more special type than the signature guarantees,
    while type coercion allows using terms of a more general type where
    the signature calls for a more special one. These operations are
    important e.g. in the specification and verification of
    object-oriented programs. We present a tableau calculus for this logic
    and prove its completeness.",
  paper = "Gies05.pdf"
}

@book{Gira03,
  author = "Girard, Jean-Yves and Taylor, Paul and Lafont, Yves",
  title = {{Proofs and Types}},
  publisher = "Cambridge University Press",
  link = "\url{http://www.paultaylor.eu/stable/prot.pdf}",
  year = "2003",
  paper = "Gira03.pdf",
  keywords = "printed"  
}

@misc{Gode16,
  author = "Godek, Panicz Maciej",
  title = {{A Pamphlet Against R}},
  month = "February 6",
  year = "2016",
  link = "\url{http://github.com/panicz/pamphlet/raw/master/pamphlet.pdf}",
  abstract = 
   "R is a programming language that has been gaining popularity in the
    domains such as machine learning and artificial intelligence over the
    recent years. It has been praised for its simplicity and elasticity,
    and its creators were even assigned a rockstar status.

    In my pamphlet, I claim that this ``blazing new technology'' is
    actually a step backwards in the development of the domains that it
    tries to tackle, and the better means of expression, or better tools
    for doing the job, were available at least since the 70s, and recently
    they were only getting better.

    This pamphlet explores and expresses various {\sl computational
    intelligence methods} using {\sl Guile}, a very pleasant
    implementation of the {\sl Scheme} programming language.",
  paper = "Gode16.pdf"
}


@techreport{Gont09,
  author = "Gonthier, Georges and Mahboubi, Assia and Tassi, Enrico",
  title = {{A Small Scale Reflection Extension for the Coq System}},
  type = "technical report",
  institution = "Inria Microsoft",
  number = "RR-6455",
  year = "2009",
  abstract =
    "This is the user manual de SSReflect , a set of extensions to the
    proof scripting language of the Coq proof assistant. While these
    extensions were developed to support a particular proof methodology -
    small-scale reflection - most of them actually are of a quite general
    nature, improving the functionality of Coq in basic areas such as
    script layout and structuring, proof context management, and
    rewriting.  Consequently, and in spite of the title of this document,
    most of the extensions described here should be of interest for all
    Coq users, whether they embrace small-scale reflection or not.",
  paper = "Gont09.pdf"

}  

@misc{Grab91a,
  author = "Grabmeier, Johannes",
  title = {{Groups, finite fields and algebras, constructions and 
            calculations}},
  location = "IBM Europe Institute",
  year = "1991"
}

@article{Gray98,
  author = "Gray, Simon and Kajler, Norbert and Wang, Paul S.",
  title = {{Design and Implementation of MP, a Protocol for
            Efficient Exchange of Mathematical Expressions}},
  journal = "J. Symboic Computation",
  volume = "25",
  pages = "213-237",
  year = "1998",
  abstract =
    "The Multi Project is an ongoing research e ort at Kent State
    University aimed at providing an environment for distributed scientific
    computing. An integral part of this environment is the Multi-
    Protocol (MP) which is designed to support ecientific communication of
    mathematical data between scientifically-oriented software tools. MP
    exchanges data in the form of linearized annotated syntax
    trees. Syntax trees provide a simple, flexible and tool-independent
    way to represent and exchange data, and annotations provide a powerful
    and generic expressive facility for transmitting additional
    information.  At a level above the data exchange protocol,
    dictionaries provide definitions for operators and constants, providing
    shared semantics across heterogeneous packages. A clear distinction
    between MP-defined and user-defined entities is enforced. Binary
    encodings are used for efficiency. Commonly used values and blocks of
    homogeneous data are further optimized. The protocol is independent of
    the underlying communication paradigm and can support parallel
    computation, distributed problem-solving environments, and the
    coupling of tools for speci c applications.",
  paper = "Gray98.pdf"
}

@inproceedings{Gris76,
  author = "Griss, Martin L.",
  title = {{The Definition and Use of Data Structures in REDUCE}},
  booktitle = "SYMSAC '76",
  pages = "53-59",
  year = "1976",
  abstract =
    "This paper gives a brief description and motivation of the mode
    analyzing and data-structuring extensions to the algebraic language
    REDUCE. These include generic functions, user defined recursive data
    structures, mode transfer functions and user modifiable automatic
    coercion. A number of examples are given to illustrate the style and
    features of the language, and how it will aid in the construction of
    more efficient and reliable programs."
}

@misc{Grog91,
  author = "Grogono, Peter",
  title = {{Issues in the Design of an Object Oriented Programming
            Language}},
  link = "\url{http://users.encs.concordia.ca/~grogono/Writings/oopissue.pdf}",
  year = "1991",
  comment = "Published in Structured Programming",
  abstract =
    "The object oriented paradigm, which advocates bottom-up program
    development, appears at first sight to run counter to the classical,
    top-down approach of structured programming.  The deep requirement of
    structured programming, however, is that programming should be based
    on well-defined abstractions with clear meaning rather than on
    incidental characteristics of computing machinery. This requirement
    can be met by object oriented programming and, in fact, object
    oriented programs may have better structure than programs obtained by
    functional decomposition.
    
    The definitions of the basic components of object oriented
    programming, object, class, and inheritance, are still sufficiently
    fluid to provide many choices for the designer of an object oriented
    language. Full support of objects in a typed language requires a
    number of features, including classes, inheritance, genericity,
    renaming, and redefinition. Although each of these features is simple
    in itself, interactions between features can become complex. For
    example, renaming and redefinition may interact in unexpected ways.
    In this paper, we show that appropriate design choices can yield a
    language that fulfills the promise of object oriented programming
    without sacrificing the requirements of structured programming.",
  paper = "Grog91.pdf"
}

@article{Gues87,
  author = "Guessarian, Irene and Meseguer, Jose",
  title = {{On the Axiomatization of ``IF-THEN-ELSE''}},
  journal = "SIAM J. Comput.",
  volume = "16",
  numbrrer = "2",
  year = "1987",
  pages = "332-357",
  abstract =
    "The equationally complete proof system for ``if-then-else'' of Bloom
    and Tindell is extended to a complete proof system for many-osrted
    algebras with extra operations, predicates and equations among
    those. We give similar completeness results for continuous algebras
    and program schemes (infinite trees) by the methods of algebraic
    semantics. These extensions provide a purely equational proof system
    to prove properties of functional programs over user-definable data
    types.",
  paper = "Gues87.pdf",
  keywords = "printed"
}

@techreport{Gutt91,
  author = "Guttman, J.D.",
  title = {{A Propoed Interface Logic for Verification Environments}},
  type = "technical report",
  number = "M91-19",
  institution = "The MITRE Corporation",
  year = "1991"
}

@article{Hach95,
  author = "Hach\'e, G. and Le Brigand, D.",
  title = {{Effective construction of algebraic geometry codes}},
  journal = "IEEE Transaction on Information Theory",
  volume = "41",
  number = "6",
  month = "November",
  year = "1995",
  pages = "1615--1628",
  link = "\url{https://hal.inria.fr/inria-00074404/file/RR-2267.pdf}",
  algebra = 
    "\newline\refto{package GPAFF GeneralPackageForAlgebraicFunctionField}
\newline\refto{package PAFFFF PackageForAlgebraicFunctionFieldOverFiniteField}
     \newline\refto{package PAFF PackageForAlgebraicFunctionField}",
  abstract =
    "We intend to show that algebraic geometry codes (AG-codes, introduced
    by Goppa in 1977 [5]) can be constructed easily using blowing-up
    theory. This work is based on a paper by Le Brigand and Risler. Given
    a plane curve, we desingularize the curve by means of blowing-up, and
    then using the desingularisation trees and the monoidal
    transformations associated to the blowing-up morphisms, we compute the
    adjoint divisor of the curve. Finally we show how to use the algorithm
    of Brill-Noether to compute a basis of the vector space associated to
    a divisor of the curve. Two examples of constructions of AG-codes are
    given at the end.",
  paper = "Hach95.pdf",
  keywords = "axiomref"
}

@article{Hach95a,
  author = "Hach\'e, G.",
  title = {{Computation in algebraic function fields for effective 
           construction of algebraic-geometric codes}},
  journal = "Lecture Notes in Computer Science",
  volume = "948",
  year = "1995",
  pages = "262--278"
}

@phdthesis{Hach96,
  author = "Hach\'e, G.",
  title = {{Construction effective des codes g\'eom\'etriques}},
  school = "l'Universit\'e Pierre et Marie Curie (Paris 6)",
  year = "1996",
  month = "Septembre"
}

@article{Haft06,
  author = "Haftmann, Florian and Wenzel, Makarius",
  title = {{Constructive Type Classes in Isabelle}},
  journal = "LNCS",
  volume = "4502",
  year = "2006",
  abstract =
    "We reconsider the well-known concept of Haskell-style type classes
    within the logical framework of Isabelle. So far, axiomatic type
    classes in Isabelle merely account for the logical aspect as
    predicates over types, while the operational part is only a
    convention based on raw overloading. Our more elaborate approach to
    constructive type classes provides a seamless integration with
    Isabelle locales, which are able to manage both operations and logical
    properties uniformly. Thus we combine the convenience of type
    classes and the flexibility of locales. Furthermore, we construct
    dictionary terms derived from notions of the type system. This
    additional internal structure provides satisfactory foundations of
    type classes, and supports further applications, such as code
    generation and export of theories and theorems to environments without
    type classes.",
  paper = "Haft06.pdf",
  keywords = "printed"
}

@misc{Hall17,
  author = "Halleck, John",
  title = {{Logic System Interrelationships}},
  year = "2017",
  link = "\url{http://www.cc.utah.edu/~nahaj/logic/structures/index.html}"
}

@inproceedings{Haml02,
  author = "Hamlet, Dick",
  title = {{Continuity in Software Systems}},
  booktitle = "ISSTA 2002 Int. Symp. on Software Testing and Analysis",
  pages = "196-200",
  year = "2002",
}

@article{Harr06,
  author = "Harrison, John",
  title = {{Proof Style}},
  journal = "LNCS",
  volume = "1512",
  year = "2006",
  pages = "154-172",
  abstract =
    "We are concerned with how computer theorem provers should expect
    users to communicate proofs to them.  There are many stylistic choices
    that still allow the machine to generate a completely formal proof
    object.  The most obvious choice is the amount of guidance required
    from the user, or from the machine perspective, the degree of
    automation provided.  But another important consideration, which we
    consider particularly significant, is the bias towards a
    'procedural' or 'declarative' proof style.  We will explore this
    choice in depth, and discuss the strengths and weaknesses of
    declarative and procedural styles for proofs in pure mathematics and
    for verification applications.  We conclude with a brief summary of
    our own experiments in trying to combine both approaches.",
  paper = "Harr06.pdf",
  keywords = "printed"
}

@inproceedings{Harr07,
  author = "Harrison, John",
  title = {{A Short Survey of Automated Reasoning}},
  booktitle = "Proc. 2nd Int. Conf. on Algebraic Biology",
  pages = "334-349",
  year = "2007",
  publisher = "Springer-Verlag",
  isbn = "978-3-540-73432-1",
  abstract =
    "This paper surveys the field of automated reasoning, giving some
    historical background and outlining a few of the main current research
    themes. We particularly emphasize the points of contact and the
    contrasts with computer algebra. We finish with a discussion of the
    main applications so far.",
  paper = "Harr07.pdf"
}  

@misc{Hath1896,
  author = "Hathway, Arthur S.",
  title = {{A Primer Of Quaternions}},
  year = "1896"
}

@book{Haya05,
  author = "Hayashi, K. and Kangkook, J. and Lascu, O. and Pienaar, H. and 
            Schreitmueller, S. and Tarquinio, T. and Thompson, J.",
  title = {{AIX 5L Practical Performance Tools and Tuning Guide}},
  publisher = "IBM",
  year = "2005",
  link = "\url{http://www.redbooks.ibm.com/redbooks/pdfs/sg246478.pdf}",
  paper = "Haya05.pdf"
}

@inproceedings{Haye74,
  author = "Hayes, Patrick J.",
  title = {{Some Problems and Non-problems in Representation Theory}},
  booktitle = "AI and Simulation of Behaviour Conference",
  year = "1974",
  pages = "63-79"
}

@misc{Hear87,
  author = "Hearn, Anthony",
  title = {{REDUCE User's Manual}},
  version = "3.3",
  institution = "Rand Corporation",
  year = "1987"
}

@misc{Hebi10,
  author = "Hebisch, Waldemar and Rubey, Martin",
  title = {{Extended Rate, more GFUN}},
  year = "2010",
  link = "\url{https://arxiv.org/abs/math/0702086v2}",
  algebra =
    "\newline\refto{package GUESS Guess}
     \newline\refto{package GUESSAN GuessAlgebraicNumber}
     \newline\refto{package GUESSF GuessFinite}
     \newline\refto{package GUESSF1 GuessFiniteFunctions}
     \newline\refto{package GUESSINT GuessInteger}
     \newline\refto{package GUESSP GuessPolynomial}
     \newline\refto{package GUESSUP GuessUnivariatePolynomial}",
  abstract = 
    "We present a software package that guesses formulae for sequences of,
    for example, rational numbers or rational functions, given the first
    few terms. We implement an algorithm due to Bernhard Beckermann and
    George Labahn, together with some enhancements to render our package
    efficient. Thus we extend and complement Christian Krattenthaler's
    program Rate, the parts concerned with guessing of Bruno Salvy and
    Paul Zimmermann's GFUN, the univariate case of Manuel Kauers' Guess.m
    and Manuel Kauers' and Christoph Koutschan's qGeneratingFunctions.m.",
  paper = "Hebi10.pdf"
}

@article{Homa94,
  author = "Homann, Karsten and Calmet, Jacques",
  title = {{Combining Theorem Proving and Symbolic Mathematical Computing}},
  journal = "LNCS",
  volume = "958",
  pages = "18-29",
  year = "1994",
  abstract =
    "An intelligent mathematical environment must enable symbolic
    mathematical computation and sophisticated reasoning techniques on the
    underlying mathematical laws. This paper disscusses different possible
    levels of interaction between a symbolic calculator based on algebraic
    algorithms and a theorem prover. A high level of interaction requires
    a common knowledge representation of the mathematical knowledge of the
    two systems. We describe a model for such a knowledge base mainly
    consisting of type and algorithm schemata, algebraic algorithms and
    theorems.",
  paper = "Homa94.pdf",
  keywords = "axiomref, CAS-Proof, printed"
}

@article{Homa96,
  author = "Homann, Karsten and Calmet, Jacques",
  title = {{Structures for Symbolic Mathematical Reasoning and Computation}},
  journal =  "LNCS",
  volume = "1128",
  year = "1996",
  pages = "216-227",
  abstract =
    "Recent research towards integrating symbolic mathematical reasoning
    and computation has led to prototypes of interfaces and
    environments. This paper introduces computation theories and
    structures to represent mathematical objects and applications of
    algorithms occuring in algorithmic services. The composition of
    reasoning and computation theories and structures provide a formal
    framework for the specification of symbolic mathematical problem
    solving by cooperation of algorithms and theorems.",
  paper = "Homa96.pdf",
  keywords = "CAS-Proof, printed"
}

@book{Hous81,
  author = "Householder, Alston S.",
  title = {{Principles of Numerical Analysis}},
  publisher = "Dover Publications, Mineola, NY",
  year = "1981",
  isbn = "0-486-45312-X"
}

@misc{Hubb,
  author = "Hubbard, John H. and Lundell, Benjamin",
  title = {{A First Look at Differential Algebra}},
  link = "\url{http://www.math.cornell.edu/~hubbard/diffalg1.pdf}",
  algebra = "\newline\refto{category DVARCAT DifferentialVariableCategory}",
  abstract =
    "The object of the paper is to prove that the differential equation
    \[u^{'}(t)=t-[u(t)]^2\] 
    has no solutions which can be written using elementary functions, or
    anti-derivatives of elementary functions, or exponentials of such
    anti-derivatives, or anti-derivative of those, etc. We should note
    that Equation 1 can be solved using power series, integrals which
    depend on a parameter, or Bessel functions of order $1/3$.  However,
    as we will see, none of these methods of solution are ``algebraic'' in
    nature.

    We aim to give a precise definition of ``algebraic'' by developing the
    theory of {\sl differential algebra}, which is largely the work of
    Ritt. Other contributors are Liouville, Picard, Vessoit, Kolchin,
    Rosenlicht, ... The part of differential Galois theory which leads to
    a proof of Abel's celebrated result that a general polynomial equation
    of degree five or higher cannot be solved by radicals. In effort to
    derive these two areas in parallel, we will also explain why the
    polynomial equation
    \[x^5-4x^2-2=0\]
    has no solutions which can be written as radicals of solutions to
    lower degree polyomial equations.",
  paper = "Hubb.pdf"
}

@InProceedings{Hube03,
  author = "Hubert, Evelyne",
  title = {{Notes on Triangular Sets and Triangulation-Decomposition I: 
           Polynomial Systems}},
  booktitle = "Symbolic and Numerical Scientific Computing",
  series = "Lecture Notes in Computer Science 2630",
  year = "2003",
  pages = "1-39",
  link = "\url{http://www-sop.inria.fr/members/Evelyne.Hubert/publications/sncsp.pdf}",
  abstract =
    "This is the first in a series of two tutorial articles devoted to
    triangulation-decomposition algorithms. The value of these notes
    resides in the uniform presentation of triangulation-decomposition
    of polynomial and differential radical ideals with detailed proofs of
    all the presented results.We emphasize the study of the mathematical
    objects manipulated by the algorithms and show their properties in
    independently of those. We also detail a selection of algorithms, one
    for each task.  We address here polynomial systems and some of the
    material we develop here will be used in the second part, devoted to
    differential systems.",
  paper = "Hube03.pdf",
  keywords = "axiomref"
}

@InProceedings{Hube03a,
  author = "Hubert, Evelyne",
  title = {{Notes on Triangular Sets and Triangulation-Decomposition II: 
           Differential Systems}},
  booktitle = "Symbolic and Numerical Scientific Computing",
  series = "Lecture Notes in Computer Science 2630",
  year = "2003",
  pages = "40-87",
  link = "\url{http://www-sop.inria.fr/members/Evelyne.Hubert/publications/sncsd.pdf}",
  abstract =
    "This is the second in a series of two tutorial articles devoted to
    triangulation-decomposition algorithms. The value of these notes
    resides in the uniform presentation of triangulation-decomposition of
    polynomial and differential radical ideals with detailed proofs of all
    the presented results.We emphasize the study of the mathematical
    objects manipulated by the algorithms and show their properties
    independently of those. We also detail a selection of algorithms, one
    for each task. The present article deals with differential systems. It
    uses results presented in the first article on polynomial systems but
    can be read independently.",
  paper = "Hube03a.pdf",
  keywords = "axiomref"
}

@article{Hulz82,
  author = "van Hulzen, J.A.",
  title = {{Computer Algebra Systems Viewed by a Notorious User}},
  journal = "LNCS",
  volume = "144",
  pages = "166-180",
  year = "1982",
  abstract =
    "Are design and use of computer algebra systems disjoint or
    complementary activities? Raising and answering this question are
    equally controversial, since a clear distinction between languages
    features and library facilities is hard to make.  Instead of even
    attempting to answer this rather academic question it is argued why it
    is reasonable to raise related questions: Is SMP a paradox? Is it
    realistic to neglect inaccurate input data? Is a very high level
    programming language instrumental for equal opportunity employment in
    scientific research?",
  paper = "Hulz82.pdf",
  keywords = "axiomref"
}

@misc{IBMx91,
  author = "Computer Algebra Group",
  title = {{The AXIOM Users Guide}},
  publisher = "NAG Ltd., Oxford",
  year = "1991"
}

@article{Ioak94,
  author = "Ioakimidis, N.I.",
  title = {{Symbolic computations for the solution of inverse/design
           problems with Maple}},
  journal = "Comput. Struct.",
  volume = "53",
  number = "1",
  pages = "63-68",
  year = "1994",
  keywords = "axiomref"
}

@book{Iyan60,
  author = "Iyanaga, Shokichi and Iyanaga, Yukiyosi Kawada",
  title = {{Encyclopedia Dictionary of Mathematics}},
  publisher = "Mathematical Society of Japan",
  year = "1960",
  isbn = "978-0262090261",
  algebra = "\newline\refto{category GRALG GradedAlgebra}"
}

@article{Jaco51,
  author = "Jacobson, Nathan",
  title = {{General Representation Theory of Jordan Algebras}},
  journal = "Trans. of the American Mathematical Society",
  volume = "70",
  number = "3",
  year = "1951",
  pages = "509-530",
  link = "\url{http://www.math.uci.edu/~brusso/jacobson1951.pdf}",
  algebra = "\newline\refto{category MONAD Monad}",
  paper = "Jaco51.pdf"
}

@article{Jaco68,
  author = "Jacobson, Nathan",
  title = {{Structure and Representations of Jordan Algebras}},
  journal = "Bull. Amer. Math. Soc",
  volume = "79",
  number = "3",
  year = "1973",
  pages = "509-514",
  link = "\url{http://projecteuclid.org/euclid.bams/1183534656}",
  algebra =
    "\newline\refto{category MONAD Monad},
     \newline\refto{category MONADWU MonadWithUnit}"
}

@techreport{Jenk86c,
  author = "Jenks, Richard D.",
  title = {{A History of the SCRATCHPAD Project (1977-1986)}},
  institution = "IBM Research",
  year = "1986",
  month = "May",
  type = "Scratchpad II Newsletter",
  volume = "1",
  number = "3",
}

@article{Jone96,
  author = "Jones, Alex and Luo, Zhaohui and Soloviev, Sergei",
  title = {{Some Algorithmic and Proof-Theoretical Aspects of
            Coercive Subtyping}},
  journal = "LNCS",
  volume = "1512",
  year = "1996",
  pages = "173-195",
  abstract =
    "Coercive subtyping offers a conceptually simple but powerful
    framework to understand subtyping and subset relationships in type
    theory.  In this paper we study some of its proof-theoretic and
    computational properties.",
  paper = "Jone96.pdf",
  keywords = "printed"
}

@phdthesis{Kalk91,
  author = "Kalkbrener, M.",
  title = {{Three contributions to elimination theory}},
  school = "University of Linz, Austria",
  year = "1991",
  algebra =
    "\newline\refto{category TSETCAT TriangularSetCategory}
     \newline\refto{category RSETCAT RegularTriangularSetCategory}
     \newline\refto{category NTSCAT NormalizedTriangularSetCategory}
     \newline\refto{category SFRTCAT SquareFreeRegularTriangularSetCategory}
     \newline\refto{package RSDCMPK RegularSetDecompositionPackage}"
}

@article{Kalk98,
  author = "Kalkbrener, M.",
  title = {{Algorithmic properties of polynomial rings}},
  journal = "Journal of Symbolic Computation",
  algebra =
    "\newline\refto{category TSETCAT TriangularSetCategory}
     \newline\refto{category RSETCAT RegularTriangularSetCategory}
     \newline\refto{category NTSCAT NormalizedTriangularSetCategory}
     \newline\refto{category SFRTCAT SquareFreeRegularTriangularSetCategory}
     \newline\refto{package RSDCMPK RegularSetDecompositionPackage}",
  year = "1998",
  abstract =
    "In this paper we investigate how algorithms for computing heights,
    radicals, unmixed and primary decompositions of ideals can be lifted
    from a Noetherian commutative ring $R$ to polynomial rings over $R$.",
  paper = "Kalk98.pdf"
}

@misc{Karp14,
  author = "Karpinski, Stefan",
  title = {{Re: Symbolic Math: try a translation of Axiom to Julia?}},
  link = "\url{https://groups.google.com/forum/#!msg/julia-dev/NTfS9fJuIcE/MINQuQuGfoUJ}",
  year = "2016",
  keywords = "axiomref"
}

@book{Kauf00,
  author = "Kaufmann, Matt and Manolios, Panagiotis and Moore, J Strother",
  title = {{Computer-Aided Reasoning: An Approach}},
  publisher = "Kluwer Academic Publishers",
  year = "2000",
  isbn = "0-7923-7744-3"
}

@inproceedings{Khan11a,
  author = "Khan, Muhammad Taimoor and Schreiner, Wolfgang",
  title = {{Towards a Behavioral Analysis of Computer Algebra
            Programs}},
  booktitle = "NWPT'11",
  publisher = "ACM Press",
  pages = "42-44",
  year = "2011",
  paper = "Khan11a.pdf"
}

@techreport{Khan11b,
  author = "Khan, Muhammad Taimoor",
  title = {{Towards a Behavioral Analysis of Computer Algebra
            Programs}},
  type = "technical report",
  institution = "RISC-Linz",
  number = "11-13",
  year = "2011",
  abstract =
    "In this paper, we present our initial results on the behavioral
    analysis of computer algebra programs.  The main goal of our work is
    to find typing and behavioral errors in such programs by static
    analysis of the source code.  This task is more complex for widely
    used computer algebra languages (Maple and Mathematica) as these are
    fundamentally different from classical languages: for example they
    support non-standard types of objects, e.g.  symbols, unevaluated
    expressions, polynomials etc.; moreover they use type information to
    direct the flow of control in the program and have no clear
    difference between declaration and assignment.  For this purpose, we
    have defined the syntax and the formal type system for a substantial
    subset of the computer algebra language Maple, which we call MiniMaple.
    The type system is implemented by a type checker, which verifies
    the type correctness and respectively reports typing errors.  We have
    applied the type checker to the Maple package DifferenceDifferential
    developed at our institute.  Currently we are working on a formal
    specification language of MiniMaple and the specification of this
    package.  The specification language will be used to find errors in
    computer algebra programs with respect to their specifications.",
  paper = "Khan11b.pdf"
}

@techreport{Khan11,
  author = "Khan, Muhammad Taimoor",
  title = {{A Type Checker for MiniMaple}},
  type = "technical report",
  institution = "RISC-Linz",
  number = "11-05",
  year = "2011",
  abstract =
    "In this paper, we present the syntactic definition and the formal type
    system for a substantial subset of the language of the computer
    algebra system Maple, which we call MiniMaple .  The goal of the type
    system is to prevent runtime typing errors by static analysis of the
    source code of MiniMaple programs.  The type system is implemented
    by a type checker, which veries the type correctness of MiniMaple
    programs respectively reports typing errors.",
  paper = "Khan11.pdf"
}

@techreport{Khan12,
  author = "Khan, Muhammad Taimoor",
  title = {{Formal Semantics of MiniMaple}},
  year = "2012",
  type = "technical report",
  number = "12-04",
  institution = "RISC Linz",
  abstract =
    "In this paper, we give the complete definition of a formal
    denotational) semantics of a subset of the language of the
    computer algebra systems Maple which we call MiniMaple .  As a
    next step we will develop a verification calculus for this
    language.  The verification conditions generated by the calculus
    must be sound with respect to the formal semantics.",
  paper = "Khan12.pdf"
}

@techreport{Khan12a,
  author = "Khan, Muhammad Taimoor",
  title = {{Formal Semantics of a Specification Language MiniMaple}},
  year = "2012",
  type = "technical report",
  number = "12-05",
  institution = "RISC Linz",
  abstract =
    "In this paper, we give the complete definition of a formal semantics
    of a specification language for MiniMaple.  This paper is an update
    of the previously reported formal (denotational) semantics of
    MiniMaple.  As a next step we will develop a verification calculus
    for MiniMaple and its specification language.  The verification
    conditions generated by the calculus must be sound with respect to
    both the formal semantics of MiniMaple and its corresponding
    specification language.",
  paper = "Khan12a.pdf"
}

@article{Khan12b,
  author = "Khan, Muhammad Taimoor and Schreiner, Wolfgang",
  title = {{Towards the Formal Semantics and Verification of Maple
            Programs}},
  journal = "LNAI",
  volume = "7362",
  pages = "231-247",
  year = "2012",
  isbn = "978-3-642-31373-8",
  abstract =
    "In this paper, we present our ongoing work and initial results on the
    formal specification and verification of MiniMaple (a substantial
    subset of Maple with slight extensions) programs. The main goal of our
    work is to find behavioral errors in such programs w.r.t. their 
    specifications by static analysis. This task is more complex for widely
    used computer algebra languages like Maple as these are fundamentally
    different from classical languages: they support non-standard types
    of objects such as symbols, unevaluated expressions and polynomials
    and require abstract computer algebraic concepts and objects such as
    rings and orderings etc. As a starting point we have defined and
    formalized a syntax, semantics, type system and specification language
    for MiniMaple.",
  paper = "Khan12b.pdf"
}

@article{Khan12c,
  author = "Khan, Muhammad Taimoor and Schreiner, Wolfgang",
  title = {{On Formal Specification of Maple Programs}},
  journal = "LNAI",
  volume = "7362",
  pages = "442-446",
  year = "2012",
  isbn = "978-3-642-31373-8",
  abstract =
    "This paper is an example-based demonstration of our initial results
    on the formal specification of programs written in the computer
    algebra language MiniMaple (a substantial subset of Maple with slight
    extensions). The main goal of this work is to define a verification
    framework for MiniMaple. Formal specification of MiniMaple programs
    is rather complex task as it supports non-standard types of objects,
    e.g. symbols and unevaluated expressions, and additional functions
    and predicates, e.g. runtime type tests etc. We have used the
    specification language to specify various computer algebra concepts
    respective objects of the Maple package DifferenceDifferential
    developed at our institute.",
  paper = "Khan12c.pdf"
}

@techreport{Khan13,
  author = "Khan, Muhammad Taimoor",
  title = {{On the Formal Verification of Maple Programs}},
  year = "2013",
  type = "technical report",
  number = "13-07",
  institution = "RISC Linz",
  abstract =
    "In this paper, we present an example-based demonstration of our 
    recent results on the formal verification of programs written in the
    computer algebra language (Mini)Maple (a slightly modified subset of
    Maple).  The main goal of this work is to develop a verification
    framework for behavioral analysis of MiniMaple programs.  For
    verification, we translate an annotated MiniMaple program into the
    language Why3ML of the intermediate verification tool Why3 developed
    at LRI, France.  We generate verification conditions by the
    corresponding component of Why3 and later prove the correctness of
    these conditions by various supported by the Why3 back-end automatic
    and interactive theorem provers.  We have used the verification
    framework to verify some parts of the main test example of our
    verification framework, the Maple package DifferenceDifferential 
    developed at our institute to compute bivariate difference-differential
    polynomials using relative Gr ̈obner bases.",
  paper = "Khan13.pdf"
}

@phdthesis{Khan14,
  author = "Khan, Muhammad Taimoor",
  title = {{Formal Specification and Verification of Computer Algebra
           Software}},
  school = "Johannes Kepler University, Linz",
  year = "2014",
  link = 
    "\url{http://www.risc.jku.at/publications/download/risc_4981/main.pdf}",
  abstract = 
    "In this thesis, we present a novel framework for the formal
    specification and verification of computer algebra programs and its
    application to a non-trivial computer algebra package.  The programs
    are written in the language MiniMaple which is a substantial subset of
    the language of the commercial computer algebra system Maple.  The
    main goal of the thesis is the application of light-weight formal
    methods to MiniMaple programs (annotated with types and behavioral
    specifications) for finding internal inconsistencies and violations of
    methods preconditions by employing static program analysis.  This task
    is more complex for a computer algebra language like Maple that for
    conventional programming languages, as Maple supports non-standard
    types of objects and also requires abstract data types to model
    algebraic concepts and notions.  
    
    As a starting point, we have defined
    and formalized a syntax, semantics, type system and specification
    language for MiniMaple .  For verification, we automatically 
    translate the (types and specification) annotated MiniMaple program into a
    behaviorally equivalent program in the intermediate language Why3ML of
    the verification tool Why3; from the translated program, Why3
    generates verification conditions whose correctness can be proved by
    various automated and interactive theorem provers (e.g.  Z3 and Coq).
    Furthermore, we have defined a denotational semantics of MiniMaple and
    its specification language and proved the soundness of the translation
    with respect to the operational semantics of Why3ML. Finally, we
    discuss the application of our verification framework to the Maple
    package DifferenceDifferential developed at our institute to compute
    bivariate difference-differential dimension polynomials using relative
    Groebner bases.",
  paper = "Khan14.pdf",
  keywords = "axiomref"
}

@article{Kies03,
  author = "Kiessling, Robert and Luo, Zhaohui",
  title = {{Coercions in Hindley-Milner Systems}},
  journal = "LNCS",
  volume = "3085",
  year = "2003",
  abstract = 
    "Coercive subtyping is a theory of abbreviation for dependent type
    theories. In this paper, we incorporate the idea of coercive subtyping
    into the traditional Hindley-Milner type systems in functional
    programming languages. This results in a typing system with coercions,
    an extension of the Hindley-Milner type system. A type inference 
    algorithm is developed and shown to be sound and complete with respect 
    to the typing system. A notion of derivational coherence is developed 
    to deal with the problem of ambiguity and the corresponding type
    inference algorithm is shown to be sound and complete.",
  paper = "Kies03.pdf",
  keywords = "printed"
}

@book{Knut92,
  author = "Knuth, Donald E.",
  title = {{Literate Programming}},
  publisher = "Center for the Study of Language and Information, Stanford CA",
  year = "1992",
  isbn = "0-937073-81-4"
} 

@article{Kohl16,
  author = "Kohlhase, Michael and Rabe, Florian",
  title = {{QED Reloaded: Towards a Pluralistic Formal Library of
            Mathematical Knowledge}},
  journal = "Journal of Formalized Reasoning",
  volume = "9",
  number = "1",
  pages = "201-234",
  year = "2016",
  abstract =
   "Proposed in 1994, the ``QED project'' was one of the seminally
   inflnuential initiatives in automated reasoning: It envisioned the
   formalization of ``all of mathematics'' and the assembly of these
   formalizations in a single coherent database. Even though it never led
   to the concrete system, communal resource, or even joint research
   envisioned in the QED manifesto, the idea lives on and shapes the
   research agendas of a significant part of the community This paper
   surveys a decade of work on representation languages and knowledge
   management tools for mathematical knowledge conducted in the KWARC
   research group at Jacobs University Bremen. It assembles the various
   research strands into a coherent agenda for realizing the QED dream
   with modern insights and technologies.",
  paper = "Kohl16.pdf"
}

@article{Kome12,
  author = "Komendantsky, V. and Konovalov, A. and Linton, S.A.",
  title = {{Interfacing Coq + SSReflect with GAP}},
  journal = "Electronic Notes in Theoretical Computer Science",
  year = "2012",
  volume = "295",
  number = "19",
  pages = "17-28",
  abstract =
    "We report on an extendable implementation of the communication
    interface connecting Coq proof assistant to the computational algebra
    system GAP using the Symbolic Computation Software Composability
    Protocol (SCSCP). It allows Coq to issue OpenMath requests to a local
    or remote GAP instances and represent server responses as Coq terms.",
  paper = "Kome12.pdf"
}

@misc{Kono17,
  author = "Konovalov, Alexander and Linton, Steve",
  title = {{Symbolic Computation Software Composability Protocol}},
  version = "2.2.2",
  year = "2017",
  link = "\url{}",
  paper = "Kono17.pdf",
  keywords = "CAS-Proof"
}

@misc{Krei14,
  author = "Kreinovich, Vladik",
  title = {{Constructive Mathematics in St. Petersburg, Russia:
            A (Somewhat Subjective) View from Within}},
  link = "\url{}",
  abstract =
    "In the 1970 and 1980s, logic and constructive mathematics were an
    important part of my life; it’s what I defended in my Master’s thesis,
    it was an important part of my PhD dissertation. I was privileged to
    work with the giants. I visited them in their homes. They were who I
    went to for advice.  And this is my story.",
  paper = "Krei14.pdf"
}

@inproceedings{Kryv13,
  author = "Kryvolap, Andrii and Nikitchenko, Mykola and Schreiner,
            Wolfgang",
  title = {{Extending Floyd-Hoare Logic for Partial Pre- and
            Postconditions}}, 
  booktitle = "ICTERI 2013: 9th Int. Conf. on ICT in Education,
               Research and Industrial Applications",  
  pages = "0-23",
  publisher = "Springer",
  isbn = "978-3-319-03997-8",
  year = "2014",
  abstract =
    "Traditional (classical) Floyd-Hoare logic is defined for a case of
    total pre- and postconditions while programs can be partial. In the
    chapter we propose to extend this logic for partial conditions. To
    do this we first construct and investigate special program algebras of
    partial predicates, functions, and programs. In such algebras
    program correctness assertions are presented with the help of a
    special composition called Floyd-Hoare composition.  This composition
    is monotone and continuous. Considering the class of constructed
    algebras as a semantic base we then define an extended logic – Partial
    Floyd-Hoare Logic – and investigate its properties. This logic has
    rather complicated soundness constraints for inference rules,
    therefore simpler sufficient constraints are proposed. The logic
    constructed can be used for program verification.",
  paper = "Kryv13.pdf"
}

@techreport{Kuts12,
  author = "Kutsia, Temur and Marin, Mircea",
  title = {{Solving, Reasoning, and Programming in Common Logic}},
  type = "technical report",
  institution = "RISC Linz",
  year = "2012",
  abstract =
    "Common Logic (CL) is a recent ISO standard for exchanging logic-based
    information between disparate computer systems.  Sharing and reasoning
    upon knowledge represented in CL require equation solving over terms
    of this language. We study computationally well-behaved fragments of
    such solving problems and show how they can influence reasoning in CL
    and transformations of CL expressions.",
  paper = "Kuts12.pdf"
}

@inproceedings{Kuts12a,
  author = "Kutsia, Temur and Marin, Mircea",
  title = {{Solving, Reasoning, and Programming in Common Logic}},
  booktitle = "SYNASC '12",
  isbn = "978-0-7695-4934-7",
  pages = "119-126",
  year = "2012",
  abstract =
    "Common Logic (CL) is a recent ISO standard for exchanging logic-based
    information between disparate computer systems.  Sharing and reasoning
    upon knowledge represented in CL require equation solving over terms
    of this language. We study computationally well-behaved fragments of
    such solving problems and show how they can influence reasoning in CL
    and transformations of CL expressions.",
  paper = "Kuts12a.pdf"
}

@misc{Lamp95,
  author = "Lamport, Leslie",
  title = {{Types Are Not Harmless}},
  year = "1995",
  paper = "Lamp95.pdf"
}

@book{Lamp86,
  author = "Lamport, Leslie",
  title = {{LaTeX: A Document Preparation System}},
  publisher = "Addison-Wesley Publishing Company, Reading, Massachusetts",
  year = "1986",
  isbn = "0-201-15790-X"
}

@article{Laws79,
  author = "Lawson, C.L. and Hanson R.J. and Kincaid, D.R. and Krogh, F.T.",
  title = {{Algorithm 539: Basic linear algebra subprograms for 
            FORTRAN usage}},
  journal = "ACM Transactions on Mathematical Software",
  volume = "5",
  number = "3",
  month = "September",
  year = "1979",
  pages = "308-323"
}

@article{Laws79a,
  author = "Lawson, C. L. and Hanson, R. J. and Kincaid, D.R. and
            Krogh, F.T.",
  title = {{Basic Linear Algebra Subprograms for Fortran Usage"}},
  journal = "Transactions on Mathematical Software",
  volume = "5",
  number = "3",
  year = "1979",
  month = "September",
  pages = "308-325",
  abstract = 
    "A package of 38 low level subprograms for many of the basic
    operations of numerical linear algebra is presented. The package is
    intended to be used with Fortran. The operations in the package
    include dot product, elementary vector operations, Givens
    transformations, vector copy and swap, vector norm, vector scaling,
    and the determination of the index of the vector component of largest
    magnitude.",
  paper = "Laws79.pdf"
}

@article{Laza91,
  author = "Lazard, Daniel",
  title = {{A new method for solving algebraic systems of positive dimension}},
  journal = "Discrete. Applied. Mathematics",
  volume = "33",
  year = "1991",
  pages = "147-160",
  algebra =
    "\newline\refto{category TSETCAT TriangularSetCategory}
     \newline\refto{category RSETCAT RegularTriangularSetCategory}
     \newline\refto{category NTSCAT NormalizedTriangularSetCategory}
     \newline\refto{category SFRTCAT SquareFreeRegularTriangularSetCategory}
     \newline\refto{package RSDCMPK RegularSetDecompositionPackage}",
  abstract =
    "A new algorithm is presented for solving algebraic systems of
    equations, which is designed from the structure which is wanted for
    the result. This algorithm is not yet implemented; thus technical
    details and proofs are omitted, for emphasising on the relation
    between the algorithm design and a good representation of the
    result. The algorithm is based on a new theorem of decomposition for
    algebraic varieties.",
  paper = "Laza91.pdf"
}

@article{Laza92,
  author = "Lazard, Daniel",
  title = {{Solving Zero-dimensional Algebraic Systems}},
  journal = "J. of Symbolic Computation",
  volume = "13",
  pages = "117-131",
  year = "1992",
  abstract = 
    "It is shown that a good output for a solver of algebraic systems of
     dimension zero consists of a family of ``triangular sets of
     polynomials''. Such an output is simple, readable, and consists
     of all information which may be wanted.

     Different algorithms are described for handling triangular systems
     and obtaining them from Groebner bases. These algorithms are
     practicable, and most of them are polynomial in the number of
     solutions",
  paper = "Laza92.pdf"
}

@article{Laza90,
  author = "Lazard, Daniel and Rioboo, Renaud",
  title = {{Integration of rational functions: Rational computation of the 
           logarithmic part}},
  journal = "Journal of Symbolic Computation",
  volume = "9",
  number = "2",
  year = "1990",
  month = "February",
  pages = "113-115",
  abstract = "
    A new formula is given for the logarithmic part of the integral of a
    rational function, one that strongly improves previous algorithms and
    does not need any computation in an algebraic extension of the field
    of constants, nor any factorisation since only polynomial arithmetic
    and GCD computations are used. This formula was independently found
    and implemented in SCRATCHPAD by B.M. Trager.",
  paper = "Laza90.pdf",
  keywords = "axiomref"
}

@article{LeBr88,
  author =  "Le Brigand, D.; Risler, J.J.",
  title = {{Algorithme de Brill-Noether et codes de Goppa}},
  journal = "Bull. Soc. Math. France",
  volume = "116",
  year = "1988",
  pages = "231--253"
}

@book{Lege11,
  author = "Legendre, George L. and Grazini, Stefano",
  title = {{Pasta by Design}},
  publisher = "Thames and Hudson",
  isbn = "978-0-500-51580-8",
  year = "2011"
}

@article{Lesc87,
  author = "Lescanne, Pierre",
  title = {{Current Trends in Rewriting Techniques and Related Problems}},
  journal = "Lecture Notes in Computer Science",
  volume = "296",
  year = "1987",
  pages = "38-51",
  paper = "Lesc87.pdf",
  keywords = "printed"
}

@misc{Leop03,
  author = "Leopardi, Paul",
  title = {{A quick introduction to Clifford Algebras}},
  publisher = "School of Mathematics, University of New South Wales",
  year = "2003",
  paper = "Leop03.pdf"
}

@misc{Lehn10,
  author = "Lehner, Franz",
  title = {{FriCAS Tutorium}},
  year = "2010",
  link = "\url{https://www.math.tugraz.at/mathc/compmath2/Demo/fricas-tutorium-0.6.pdf}",
  paper = "Lehn10.pdf"
}

@inproceedings{Lest01,
  author = "Lester, D.R.",
  title = {{Effective Continued Fractions}},
  booktitle = "15th IEEE Symp. on Computer Arithmetic",
  publisher = "IEEE Computer Society Press",
  year = "2001",
  pages = "163-170",
  abstract =
    "Only the leading seven terms of a continued fraction are needed to
    perform on-line arithmetic, provided the continued fractions are of
    the correct form.  This forms the basis of a proof that there is an
    effective representation of the computable reals as continued
    fractions: we also demonstrate that the basic arithmetic operations
    are computable using this representation.",
  paper = "Lest01.pdf"
}

@misc{Leve80,
  author = "Levenworth, B.",
  title = {{ADAPT Reference Manual}},
  comment = "IBM Research",
  year = "1980"
}

@article{Lint02,
  author = "Linton, S. and Sebastiani, R.",
  title = {{Editorial: The Integration of Automated Reasoning and Computer
           Algebra Systems}},
  journal = "J. Symbolic Computation",
  volume = "34",
  pages = "239-239",
  year = "2002",
  paper = "Lint02.pdf",
  keywords = "CAS-Proof, printed"
}

@article{Lisk77,
  author = "Liskov, Barbara and Synder, Alan and Atkinson, Russell and
            Schaffert, Craig",
  title = {{Abstraction Mechanisms in CLU}},
  journal = "CACM",
  volume = "20",
  number = "8",
  year = "1977",
  link = "\url{https://www.cs.virginia.edu/~weimer/615/reading/liskov-clu-abstraction.pdf}",
  abstract =
    "CLU is a new programming language designed to support the use of
    abstractions in program construction. Work in programming methodology
    has led to the realization that three kinds of abstractions --
    procedural, control, and especially data abstractions -- are useful in
    the programming process. Of these, only the procedural abstraction is
    supported well by conventional languages, through the procedure or
    subroutine. CLU provides, in addition to procedures, novel linguistic
    mechanisms that support the use of data and control abstractions. This
    paper provides an introduction to the abstraction mechanisms of
    CLU. By means of programming examples, the utility of the three kinds
    of abstractions in program construction is illustrated, and it is
    shown how CLU programs may be written to use and implement
    abstractions. The CLU library, which permits incremental program
    development with complete type checking performed at compile time, is
    also discussed.",
  paper = "Lisk77.pdf"
}

@techreport{Lisk79,
  author = "Liskov, Barbara and Atkinson, Russ and Bloom, Toby and
            Moss, Eliot and Schaffert, Craig and Scheifler, Bob and 
            Snyder, Alan",
  title = {{CLU Reference Manual}},
  institution = "Massachusetts Institute of Technology",
  year = "1979",
  paper = "Lisk79.pdf"
}  

@article{Loba08,
  author = "Lobachev, Oleg and Loogen, Rita",
  title = {{Towards an Implementation of a Computer Algebra System in a
           Functional Language}},
  journal = "LNAI",
  volume = "5144",
  pages = "141-208",
  year = "2008",
  publisher = "Springer-Verlag",
  abstract =
    "This paper discusses the pros and cons of using a functional language
    for implementing a computer algebra system. The contributions of the
    paper are twofold. Firstly, we discuss some language-centered design
    aspects of a computer algebra system -- the ``language unity''
    concept.  Secondly, we provide an implementation of a fast polynomial
    multiplication algorithm, which is one of the core elements of a
    computer algebra system.  The goal of the paper is to test the
    feasibility of an implementation of (some elements of) a computer
    algebra system in a modern functional language.",
  paper = "Loba08.pdf",
  keywords = "axiomref"
}

@misc{Loet09,
  author = "Loetzsch, Martin and Bleys, Joris and Wellens, Pieter",
  title = {{Understanding the Dynamics of Complex Lisp Programs}},
  year = "2009",
  link = "\url{http://www.martin-loetzsch.de/papers/loetzsch09understanding.pdf}",
  paper = "Loet09.pdf"
}

@misc{Loet00,
  author = "Loetzsch, M.",
  title = {{GTFL - A graphical terminal for Lisp}},
  year = "2000",
  link = "\url{http://martin-loetzsch.de/gtfl}"
}

@book{Losc60,
  author = {L\"osch, Friedrich},
  title = {{Tables of Higher Functions}},
  publisher = "McGraw-Hill Book Company",
  year = "1960",
  algebra = "\newline\refto{package DFSFUN DoubleFloatSpecialFunctions}"
}

@book{Luke69a,
  author = "Luke, Yudell L.",
  title = {{The Special Functions and their Approximations}},
  volume = "1",
  publisher = "Academic Press",
  year = "1969",
  booktitle = "Mathematics in Science and Engineering Volume 53-I",
  algebra = "\newline\refto{package DFSFUN DoubleFloatSpecialFunctions}"
}

@book{Luke69b,
  author = "Luke, Yudell L.",
  title = {{The Special Functions and their Approximations}},
  volume = "2",
  publisher = "Academic Press",
  year = "1969",
  booktitle = "Mathematics in Science and Engineering Volume 53-I",
  algebra = "\newline\refto{package DFSFUN DoubleFloatSpecialFunctions}"
}

@book{Luke77,
  author = "Luke, Yudell L.",
  title = {{Algorithms for the Computation of Mathematical Functions}},
  publisher = "Academic Press",
  year = "1977",
  isbn = "978-0124599406"
}

@article{Luox99,
  author = "Luo, Zhaohui and Soloviev, Sergei",
  title = {{Dependent Coercions}},
  journal = "Electr. Notes Theor. Comput. Sci.",
  volume = "29",
  year = "1999",
  pages = "152-168",
  abstract =
    "A notion of dependent coercion is introduced and studied in the
    context of dependent type theories. It extends our earlier work on
    coercive subtyping into a uniform framework which increases the
    expressive power with new applications.  A dependent coercion
    introduces a subtyping relation between a type and a family of types
    in that an object of the type is mapped into one of the types in the
    family.  We present the formal framework, discuss its meta-theory, and
    consider applications such as its use in functional programming with
    dependent types.",
  paper = "Luox99.pdf",
  keywords = "printed"
}

@book{Lutz90,
  author = "Lutzen, Jesper",
  title = {{Joseph Liouville. 1809-1882: Master of Pure and Applied 
           Mathematics}},
  publisher = "Springer",
  year = "1990",
  paper = "Lutz90.pdf"
}

@misc{Lutz90a,
  author = "Lutzen, Jesper",
  title = {{Integration in Finite Terms}},
  publisher = "Springer",
  year = "1990",
  comment = "Chapter IX",
  pages = "351-422",
  paper = "Lutz90a.pdf"
}

@article{Maga00,
  author = "Magaud, Nicolas and Bertot, Yves",
  title = {{Changing Data Structures in Type Theory: A Study of
            Natural Numbers}},
  journal = "LNCS",
  volume = "2277",
  pages = "181-196",
  year = "2000",
  abstract =
    "In type-theory based proof systems that provide inductive
    structures, computation tools are automatically associated to
    inductive definitions. Choosing a particular representation for a
    given concept has a strong influence on proof structure. We
    propose a method to make the change from one representation to
    another easier, by systematically translating proofs from one
    context to another. We show how this method works by using it on
    natural numbers, for which a unary representation (based on Peano
    axioms) and abinary representation are available. This method
    leads to an automatic translation tool that we have implemented in
    Coq and successfully applied to several arithmetical theorems.",
  paper = "Maga00.pdf",
  keywords = "printed"
}

@article{Maga08,
  author = "Magaud, Nicolas and Narboux, Julien and Schreck, Pascal",
  title = {{Formalizing Projective Plane Geometry in Coq}},
  journal = "LNCS",
  volume = "6301",
  pages = "141-162",
  year = "2008",
  abstract =
    "We investigate how projective plane geometry can be formalized in a
    proof assistant such as Coq. Such a formalization increases the
    reliability of textbook proofs whose details and particular cases are
    often overlooked and left to the reader as exercises. Projective plane
    geometry is described through two different axiom systems which are
    formally proved equivalent. Usual properties such as decidability of
    equality of points (and lines) are then proved in a constructive
    way. The duality principle as well as formal models of projective
    plane geometry are then studied and implemented in Coq. Finally, we
    formally prove in Coq that Desargues’ property is independent of the
    axioms of projective plane geometry.",
  paper = "Maga08.pdf"
}

@misc{Magu17,
  author = "Maguire, Camm and Schelter, William",
  title = {{Gnu Common Lisp}},
  link = "\url{https://savannah.gnu.org/projects/gcl}"
}

@incollection{Mark62,
  author = "Markov, Andrei .A.",
  title = {{On Constructive Mathematics}},
  booktitle = "Problems of the Constructive Direction in Mathematics:
                Part 2 -- Constructive Mathematical Analysis",
  publisher = "Academy of Science USSR",
  comment = "In Russian",
  link = "\url{http://www.mathnet.ru/links/4fe363fcbbf9aeaad8dc9baed1c7d1c8/tm1756.pdf}",
  year = "1962"
}

@misc{Mars07,
  author = "Marshak, U.",
  title = {{HT-AJAX - AJAX framework for Hunchentoot}},
  year = "2007",
  link = "\url{http://common-lisp.net/project/ht-ajax/ht-ajax.html}"
}

@inproceedings{Matt10,
  author = "Matthews, David C.J. and Wenzel, Makarius",
  title = {{Efficient Parallel Programming in Poly/ML and Isabelle/ML}},
  booktitle = "Proc. 5th ACM SIGPLAN workshop on Declarative aspects of
               multicore programming",
  pages = "53-62",
  year = "2010",
  publisher = "ACM",
  isbn = "978-1-60558-859-9",
  abstract =
    "The ML family of languages and LCF-style interactive theorem proving
    have been closely related from their beginnings about 30 years
    ago. Here we report on a recent project to adapt both the Poly/ML
    compiler and the Isabelle theorem prover to current multicore
    hardware. Checking theories and proofs in typical Isabelle application
    takes minutes or hours, and users expect to make efficient use of
    ``home machines'' with 2-8 cores, or more.
    
    Poly/ML and Isabelle are big and complex software systems that have
    evolved over more than two decades. Faced with the requirement to
    deliver a stable and efficient parallel programming environment, many
    infrastructure layers had to be reworked: from low-level system
    threads to high-level principles of value-oriented programming. At
    each stage we carefully selected from the many existing concepts for
    parallelism, and integrated them in a way that fits smoothly into the
    idea of purely functional ML with the addition of synchronous
    exceptions and asynchronous interrupts.
    
    From the Isabelle/ML perspective, the main concept to manage parallel
    evaluation is that of ``future values''. Scheduling is implicit, but it
    is also possible to specify dependencies and priorities. In addition,
    block-structured groups of futures with propagation of exceptions
    allow for alternative functional evaluation (such as parallel search),
    without requiring user code to tackle concurrency. Our library also
    provides the usual parallel combinators for functions on lists, and
    analogous versions on prover tactics.
    
    Despite substantial reorganization in the background, only minimal
    changes are occasionally required in user ML code, and none at the
    Isabelle application level (where parallel theory and proof processing
    is fully implicit). The present implementation is able to address more
    than 8 cores effectively, while the earlier version of the official
    Isabelle2009 release works best for 2-4 cores. Scalability beyond 16
    cores still poses some extra challenges, and will require further
    improvements of the Poly/ML runtime system (heap management and
    garbage collection), and additional parallelization of Isabelle
    application logic.",
  paper = "Matt10.pdf"
}

@misc{Maxi16a,
  author = "Maxima",
  title = {{Symbolic Integration: The Algorithms}},
  link = "\url{http://maxima.sourceforge.net/docs/tutorial/en/gaertner-tutorial-revision/Pages/SI001.htm}"
}

@phdthesis{Mayr09,
  author = "Mayrhofer, Gunther",
  title = {{Symbolic COmputation Prover with Induction}},
  year = "2009",
  link =
    "\url{http://www.risc.jku.at/publications/download/risc_3910/Thesis.pdf}",
  school = "RISC Linz",
  abstract =
    "An important task in automated theorem proving is computing with
    ``arbitrary but fixed'' constants. This kind of highschool proving
    occurs in the main part of most proofs. The current master's thesis
    presents an automated prover that focuses on such computations with
    symbols. The prover uses equalities and equivalences in the knowledge
    base to rewrite a goal formula. In all formulae there may be universal
    quantifiers and some selected logical connectives. Special syntax
    elements support case distinctions and sequence variables. The prover
    uses a specialized method for proving equalities and an important
    feature is proving by cases. An extension allows induction over some
    predefined domains. Additionally to the prover implementation in
    Mathematica, there is a tracer that prints a protocol of the proof
    flow. Since the code for this tracer is separated from the prover,
    there may be more than one tracer with different output. But more
    important is that a user can inspect the code of prover without being
    confused by technical details for generating some nice output. The
    main motivation for this prover is a new extension of the Theorema
    system. The aim is an environment for defining new prover in the same
    language as theorems. The advantage is clear, existing prover may
    prove facts of a new one, for example the correctness. Using this it
    is possible to build up a hierarchy of formally checked provers. For
    such reasoning about reasoners a starting point needs induction on the
    structure of terms and formulae. A first prover in the hierarchy will
    need computations with symbols in many proof branches. This may be
    done by the current Symbolic Computation Prover.",
  paper = "Mayr09.pdf"
}

@misc{Maye17,
  author = "Mayero, Micaela and Delahaye, David",
  title = {{A Maple Mode for Coq}},
  link = "\url{https://github.com/coq-contribs/maple-mode}",
  year = "2017",
  abstract =
    "This contribution is an interface between Coq and Maple. In
    particular, this exports the functions simplify/factor/expand/normal
    giving the corresponding tactics Simplify/Factor/Expand/Normal. The
    manipulations carried out by these tactics are proved thanks to the
    tactic Field. These tactics can be also used as functions by means of
    the Eval ... In command. "
}

@article{Maza95,
  author = "Maza, Marc Moreno and Rioboo, Renaud",
  title = {{Polynomial Gcd Computations over Towers of Algebraic Extensions}},
  year = "1995",
  journal = "Proceedings of AAECC11",
  algebra =
    "\newline\refto{category TSETCAT TriangularSetCategory}
     \newline\refto{category RSETCAT RegularTriangularSetCategory}
     \newline\refto{category NTSCAT NormalizedTriangularSetCategory}
     \newline\refto{category SFRTCAT SquareFreeRegularTriangularSetCategory}
     \newline\refto{package LEXTRIPK LexTriangularPackage}
     \newline\refto{package RSDCMPK RegularSetDecompositionPackage}",
  abstract =
    "Some methods for polynomial system solving require efficient
    techniques for computing univariate polynomial gcd over algebraic
    extensions of a field. Currently used techniques compute {\sl generic}
    univariate polynomial gcd before {\sl specializing} the result using
    algebraic relations in the ring of coefficients. This strategy
    generates very big intermediate data and fails for many problems. We
    present here a new approach which takes permanently into account those
    algebraic relations. It is based on a property of subresultant
    remainder sequences and leads to a great increase of the speed of
    computations and thus the size of accessible systems.",
  paper = "Maza95.pdf",
  keywords = "axiomref"
}

@phdthesis{Maza97,
  author = "Maza, Marc Moreno",
  title = {{Calculs de pgcd au-dessus des tours d'extensions simples et 
           resolution des systemes d'equations algebriques}},
  school = "Universite P.etM. Curie",
  year = "1997",
  algebra =
    "\newline\refto{category TSETCAT TriangularSetCategory}
     \newline\refto{category RSETCAT RegularTriangularSetCategory}
     \newline\refto{category NTSCAT NormalizedTriangularSetCategory}
     \newline\refto{category SFRTCAT SquareFreeRegularTriangularSetCategory}
     \newline\refto{package RSDCMPK RegularSetDecompositionPackage}",
  link = "\url{http://www.csd.uwo.ca/~moreno//Publications/MorenoMaza-Thesis-1997.ps.gz}",
  abstract = 
    "This thesis is dedicated to polynomial system solving by means of
    triangular sets. A first prt presents two algorithms to compute
    polynomial gcds over tower of simple extensions. The first one was
    designed by Renaud Rioboo and applies to algebraic towers. The second
    one is a generalization of the previous one to the most general case
    of seperable towers. These algorithms lead to an efficient
    implementation of two methods suggested by Daniel Lazard to solve
    polynomial systems by means of triangular sets. These programs solved
    problems that were previously unreachable. The second method was only
    sketched by its author. So, a second part of this thesis presents the
    necessary developements to describe a right implementation. Moreover,
    a theorecal and unified presentation, together with an experimental
    comparison with similar methods due to Wu Wen-Tsun, Dongming Wang and
    Michael Kalkbrener were realized by Philippe Aubry and are reported in
    a third part of this document.",
  paper = "Maza97.pdf",
  keyword = "axiomref"
}

@techreport{Maza98,
  author = "Maza, M. Moreno",
  title = {{A new algorithm for computing triangular decomposition of 
           algebraic varieties}},
  institution = "Numerical Algorithms Group (NAG)",
  algebra =
    "\newline\refto{category TSETCAT TriangularSetCategory}
     \newline\refto{category RSETCAT RegularTriangularSetCategory}
     \newline\refto{category NTSCAT NormalizedTriangularSetCategory}
     \newline\refto{category SFRTCAT SquareFreeRegularTriangularSetCategory}
     \newline\refto{package RSDCMPK RegularSetDecompositionPackage}",
  year = "1998"
}

@article{Melh02,
  author = "Melham, Thomas F.",
  title = {{PROSPER An Investigation into Software Architecture for Embedded
           Proof Engines}},
  journal = "LNCS",
  volume = "2309",
  pages = "193-206",
  year = "2002",
  abstract =
    "Prosper is a recently-completed ESPRIT Framework IV research project
    that investigated software architectures for component-based, embedded
    formal verification tools. The aim of the project was to make
    mechanized formal analysis more accessible in practice by providing a
    framework for integrating formal proof tools inside other software
    applications. This paper is an extended abstract of an invited
    presentation on Prosper given at FroCoS 2002. It describes the vision
    of the Prosper project and provides a summary of the technical
    approach taken and some of the lessons learned.",
  paper = "Melh02.pdf",
  keywords = "CAS-Proof, printed"
}

@book{Meye92,
  author = "Meyer, Bertrand",
  title = {{Eiffel: The Language}},
  publisher = "Prentice Hall",
  year = "1992",
  isbn = "0-13-247925-7"
}

@article{Mill68,
  author = "Millen, J. K.",
  title = {{CHARYBDIS: A LISP program to display mathematical expressions 
           on typewriter-like devices}},
  year = "1968",
  journal = "Interactive Systems for Experimental and Applied Mathematics",
  publisher = "M. Klerer and J. Reinfelds, eds., Academic Press, New York",
  pages = "79--90",
  abstract = "
    CHARYBDIS (from CHARacter-composed sYmBolic DISplay) is a LISP program
    to display mathematical expressions on typewriter-like devices such as
    line printers, teletypes, and scopes which display lines of characters,
    as well as typewriters.",
  paper = "Mill68.pdf",
  keywords = "printed"
}

@book{Mine88,
  author = "Mines, Ray and Richman, Fred and Ruitenburg, Wim",
  title = {{A Course in Constructive Algebra}},
  year = "1988",
  publisher = "Universitext",
  abstract =
    "The constructive approach to mathematics has enjoyed a renaissance,
    caused in large part by the appearance of Errett Bishop's book
    Foundations of constructive analysis in 1967, and by the subtle
    influences of the proliferation of powerful computers. Bishop
    demonstrated that pure mathematics can be developed from a
    constructive point of view while maintaining a continuity with
    classical terminology and spirit; much more of classical mathematics
    was preserved than had been thought possible, and no classically false
    theorems resulted, as had been the case in other constructive schools
    such as intuitionism and Russian constructivism. The computers created
    a widespread awareness of the intuitive notion of an effecti ve
    procedure, and of computation in principle, in addi tion to
    stimulating the study of constructive algebra for actual
    implementation, and from the point of view of recursive function
    theory. In analysis, constructive problems arise instantly because we
    must start with the real numbers, and there is no finite procedure for
    deciding whether two given real numbers are equal or not (the real
    numbers are not discrete) . The main thrust of constructive
    mathematics was in the direction of analysis, although several
    mathematicians, including Kronecker and van der waerden, made
    important contributions to construc­ tive algebra. Heyting, working in
    intuitionistic algebra, concentrated on issues raised by considering
    algebraic structures over the real numbers, and so developed a
    handmaiden'of analysis rather than a theory of discrete algebraic
    structures."
}

@article{Moss93,
  author = "Mosses, Peter D.",
  title = {{The Use of Sorts in Algebraic Specification}},
  journal = "Lecture Notes in Computer Science",
  volume = "655",
  pages = "66-91",
  year = "1993",
  abstract =
    "Algebraic specification frameworks exploit a variety of sort
    disciplines. The treatment of sorts has a considerable influence on
    the ease with which such features as partiality and polymorphism can
    be specified. This survey gives an accessible overview of various
    frameworks, focusing on their sort disciplines and assessing their
    strengths and weaknesses for practical applications. Familiarity with
    the basic notions of algebraic specification is assumed.",
  paper = "Moss93.pdf, printed"
}

@article{Muld97,
  author = "Mulders, Thom",
  title = {{A Note on Subresultants and the Lazard/Rioboo/Trager Formula in 
           Rational Function Integration}},
  journal = "Journal of Symbolic Computation",
  year = "1997",
  volume = "24",
  number = "1",
  month = "July",
  pages = "45-50",
  abstract = "
    An ambiguity in a formula of Lazard, Rioboo and Trager, connecting
    subresultants and rational function integration, is indicated and
    examples of incorrect interpretations are given.",
  paper = "Muld97.pdf"
}

@article{Nara99,
  author = "Naraschewski, Wolfgang and Nipkow, Tobias",
  title = {{Type Inference Verified: Algorithm W in Isabelle/HOL}},
  journal = "LNCS",
  volume = "1512",
  year = "1999",
  pages = "317-332",
  paper = "Nara99.pdf",
  keywords = "printed"
}

@article{Naud98,
  author = "Naudin, Patrice and Quitte, Claude",
  title = {{Univariate polynomial factorization over finite fields}},
  journal = "Theor. Comput. Sci.",
  volume = "191",
  number = "1-2",
  pages = "1-36",
  year = "1998",
  abstract =
    "This paper is a tutorial introduction to univariate polynomial
    factorization over finite fields. The authors recall the classical
    methods that induced most factorization algorithms (Berlekamp’s and
    the Cantor-Zassenhaus ones) and some refinements which can be applied
    to these methods. Explicit algorithms are presented in a form suitable
    for almost immediate implementation. They give a detailed description
    of an efficient implementation of the Cantor-Zassenhaus algorithm used
    in the release 2 of the Axiom computer algebra system.",
  paper = "Naud98.pdf"
}

@misc{Neup12,
  author = "Neuper, Walther",
  title = {{Automated Generation of User Guidance by Combining
            Computation and Deduction}},
  year = "2012",
  publisher = "Open Publishing Association",
  pages = "82-101",
  abstract =
    "Herewith, a fairly old concept is published for the first time and
    named ”Lucas Interpretation”. This has been implemented in a
    prototype, which has been proved useful in educational practice and
    has gained academic relevance with an emerging generation of
    educational mathematics assistants (EMA) based on Computer Theorem
    Proving (CTP).  
    
    Automated Theorem Proving (ATP), i.e.  deduction, is
    the most reliable technology used to check user input. However ATP is
    inherently weak in automatically generating solutions for arbitrary
    problems in applied mathematics. This weakness is crucial for EMAs:
    when ATP checks user input as incorrect and the learner gets stuck
    then the system should be able to suggest possible next steps.  
    
    The key idea of Lucas Interpretation is to compute the steps of a
    calculation following a program written in a novel CTP-based
    programming language, i.e. computation provides the next steps.  User
    guidance is generated by combining deduction and computation: the
    latter is performed by a specific language interpreter, which works
    like a debugger and hands over control to the learner at breakpoints,
    i.e.  tactics generating the steps of calculation.  The interpreter
    also builds up logical contexts providing ATP with the data required
    for checking user input, thus combining computation and deduction.
    
    The paper describes the concepts underlying Lucas Interpretation so
    that open questions can adequately be addressed, and prerequisites for
    further work are provided.",
  paper = "Neup12.pdf"
}

@article{Neup13a,
  author = "Neuper, Walther",
  title = {{On the Emergence of TP-based Educational Math
            Assistants}},
  journal = "The Electronic Journal of Mathematics and Technology",
  volume = "1",
  number = "1",
  year = "2013",
  link = "\url{http://www.ist.tugraz.at/projects/isac/publ/newgen.pdf}",
  abstract =
    "Presently Computer Algebra Systems, Dynamic Geometry Systems and
    Spreadsheets cover most of e-learning in high-school mathematics and
    as well are used for education in formal parts of science.  Recently
    and largely unnoticed in public, the academic discipline of
    interactive and automated Theorem Proving (TP) has become of major
    importance for mathematics and computer science.  
    
    This paper considers the promises of TP technology for education in
    science, technology, engineering and mathematics at the full range of
    levels from high-school to university.
    
    Starting from prototypes of TP-based educational mathematics systems,
    conceptual foundations are considered: TP-based software which
    implements reasoning as an essential part of mathematical thinking
    technology.  Educational objectives for the development of TP-based
    systems are discussed and concluded with some predictions on possible
    impact of TP-based educational mathematics assistants.
    
    The final conclusion suggests to announce the emergence of a new,
    TP-based generation of educational mathematics software.",
  paper = "Neup13a.pdf"
}  

@book{Nils76,
  author = "Nilsson, Nils J.",
  title = {{Principles of Artificial Intelligence}},
  publisher = "Morgan Kaufmann",
  year = "1976",
  abstract =
    "A classic introduction to artificial intelligence intended to bridge
    the gap between theory and practice, Principles of Artificial
    Intelligence describes fundamental AI ideas that underlie applications
    such as natural language processing, automatic programming, robotics,
    machine vision, automatic theorem proving, and intelligent data
    retrieval. Rather than focusing on the subject matter of the
    applications, the book is organized around general computational
    concepts involving the kinds of data structures used, the types of
    operations performed on the data structures, and the properties of the
    control strategies used. Principles of Artificial Intelligenceevolved
    from the author's courses and seminars at Stanford University and
    University of Massachusetts, Amherst, and is suitable for text use in
    a senior or graduate AI course, or for individual study."
}

@article{Nipk02,
  author = "Nipkow, Tobias",
  title = {{Structured Proofs in Isar/HOL}},
  journal = "LNCS",
  volume = "2646",
  year = "2002",
  pages = "259-278",
  abstract =
    "Isar is an extension of the theorem prover Isabelle with a language
    for writing human-readable structured proofs. This paper is an
    introduction to the basic constructs of this language.",
  paper = "Nipk02.pdf",
  keywords = "printed"
}

@misc{OCAM14,
  author = "unknown",
  title = {{The OCAML website}},
  link = "\url{http://ocaml.org}"
}

@misc{Ostr1845,
  author = "Ostrogradsky. M.W.",
  title = {{De l'int\'{e}gration des fractions rationelles.}},
  journal = "Bulletin de la Classe Physico-Math\'{e}matiques de 
    l'Acae\'{e}mie Imp\'{e}riale des Sciences de St. P\'{e}tersbourg,",
  volume = "IV",
  pages = "145-167,286-300",
  year = "1845"
}

@misc{Paga17,
  author = "Pagani, Kurt",
  title = {{trigonometric simplification}},
  year = "2017",
  link = "\url{https://groups.google.com/forum/#topic/fricas-devel/kteSoeR_Iyg}"
}

@book{Paul81,
  author = "Paul, Richard",
  title = {{Robot Manipulators}},
  year = "1981",
  publisher = "MIT Press",
  isbn = "0-262-16082-X"
}

@article{Paul90,
  author = "Paulson, Lawrence C.",
  title = {{A Formulation of the Simple Theory of Types (for
            Isabelle)}},
  journal = "LNCS",
  volume = "417",
  pages = "246-274",
  year = "1990",
  abstract =
    "Simple type theory is formulated for use with the generic theorem
    prover Isabelle. This requires explicit type inference rules. There
    are function, product, and subset types, which may be
    empty. Descriptions (the eta-operator) introduce the Axiom of
    Choice. Higher-order logic is obtained through reflection between
    formulae and terms of type bool. Recursive types and functions can be
    formally constructed. Isabelle proof procedures are described. The
    logic appears suitable for general mathematics as well as
    computational problems.",
  paper = "Paul90.pdf",
  keywords = "printed"
}

@book{Pear56,
  author = "Pearcey, T.",
  title = {{Table of the Fresnel Integral}},
  publisher = "Cambridge University Press",
  year = "1956",
  algebra = "\newline\refto{package DFSFUN DoubleFloatSpecialFunctions}"
}

@misc{Pfei12,
  author = "Pfeil, Greg",
  title = {{Common Lisp Type Hierarchy}},
  year = "2012",
  link = "\url{http://sellout.github.io/2012/03/03/common-lisp-type-hierarchy}"
}

@inproceedings{Pfen89a,
  author = "Pfenning, Frank",
  title = {{Elf: A Language for Logic Definition and Verified
           Metaprogramming}},
  booktitle = "4th Symp. on Logic in Computer Science",
  pages = "313-322",
  isbn = "0-8186-1954-6",
  year = "1989",
  abstract = 
    "We describe Elf, a metalanguage for proof manipulation environments
    that are independent of any particular logic system. Elf is
    intended for meta-programs such as theorem provers, proof
    transformers, or type inference programs for programming languages
    with complex type systems. Elf unifies logic definition (in the
    style of LF, the Edinburgh Logical Framework) with logic
    programming (in the style of $\lambda$Prolog). It achieves this
    unification by giving {\sl types} an operational interpretation,
    much the same way that Prolog gives certain formulas
    (Horn-clauses) an operational interpretation. Novel features of
    Elf include: (1) the Elf search process automatically constructs
    terms that can represent object-logic proofs, and thus a program
    need not construct them explicitly, (2( the partial correectness
    of meta-programs with respect to a given logic can be expressed
    and proved in Elf itself, and (3) Elf exploits Elliott's
    unification algorithm for a $\lambda$-calculus with dependent
    types.",
  paper = "Pfen89a.pdf"
}

@article{Pfen92b,
  author = "Pfenning, Frank and Rohwedder, Ekkehard",
  title = {{Implementing the Meta-Theory of Deductive Systems}},
  journal = "Lecture Notes in Computer Science",
  volume = "607",
  pages = "771-775",
  year = "1992",
  abstract =
    "We exhibit a methodology for formulating and verifying meta-theorems
    about deductive systems in the Elf language, an implementation of the
    LF Logical Framework with an operational semantics in the spirit of
    logic programming.  It is based on the mechanical verification of
    properties of transformations between deductions, which relies on type
    reconstruction and schema-checking.  The latter is justified by
    induction principles for closed LF objects, which can be constructed
    over a given signature.  We illustrate our technique through several
    examples, the most extensive of which is an interpretation of
    classical logic in minimal logic through a continuation-passing-style
    transformation on proofs.",
  paper = "Pfen92b.pdf"
}

@misc{Pfen97,
  author = "Pfenning, Frank",
  title = {{Computation and Deduction}},
  year = "1997",
  link = "\url{http://www.cs.cmu.edu/~twelf/notes/cd.pdf}",
  paper = "Pfen97.pdf"
}

@phdthesis{Phil02,
  author = "Philippe, M. Trebuchet",
  title = {{Toward a fast and numerically stable algebraic equation solving}},
  comment = "Vers une resolution stable et rapide des equations algebriques",
  school = "l'Universite de Paris 6",
  year = "2002",
  month = "December",
  abstract = 
    "Polynomial systems can be found in many industrial applications. They
    are also in the heart of effective algebraic geometry. A fundamental
    tool for studying them is the Groebner bases. The knowledge of this
    paricular base of the ideal generated by the polynomials composing the
    system allows us to compute in $A=K[x_1,\ldots,x_n]/I$, the quotient
    algebra, and this is necessary when we try to solve. Nevertheless,
    Groebner bases computations rely heavily on the introduction of
    monomial ordering. This introduces a certain rigidity in the
    computation and thus numerical instability. We propose a new algorithm
    that tries to remedy that problem. It generalises Groebner bases
    computation and is much less numerically instable. To do this, we
    decrease the requirement of monomial ordering, and use a new normal
    form criterion. We then give an algorithm and prove its termination
    and correctness when the input polynomial system is
    0-dimensional. After, we compare it with the previously known methods
    and show how it can be seen as a generalisation of them. Next, we
    detail how we implemented it in C++ using the Synaps library. We also
    describe the sparse matrix elimination algorithm we used in or
    program. Finally, we present some of the experiments we have done with
    our program in domains like computer vision, algorithmic geometry,
    robotics, or pharmacology.",
  paper = "Phil02.pdf"
}

@misc{Pier17,
  author = "Pierce, Benjamin",
  title = {{DeepSpec Summer School, Coq Intensive, Part 1 (July 13,2017)}},
  year = "2017",
  link = "\url{https://www.youtube.com/watch?v=jG61w5pOc2A}"
}

@book{Prie12,
  author = "Priest, Graham",
  title = {{An Introduction to Non-Classical Logic}},
  year = "2012",
  publisher = "Cambridge University Press"
}

@inproceedings{Popo09,
  author = "Popov, Nikolaj and Jebelean, Tudor",
  title = {{Functional Program Verification in Theorema Soundness and
           Completeness}},
  booktitle = "Proc. 15th Biennial Workshop on Programmiersprachen und
               Grundlagen der Programmierung KPS'09",
  year = "2009",
  pages = "221-229",
  link = 
    "\url{http://www.risc.jku.at/publications/download/risc_3913/PopJeb.pdf}",
  abstract =
    "We present a method for verifying recursive functional programs. We
    define a Verification Condition Generator (VCG) which covers the most
    frequent types of recursive programs. These programs may operate on
    arbitrary domains. Soundness and Completeness of the VCG are proven on
    the meta level, and this provides a warranty that any system based on
    our results will be sound.",
  paper = "Popo09.pdf"
}

@inproceedings{Popo09a,
  author = "Popov, Nikolaj and Jebelean, Tudor",
  title = {{A Complete Method for Algorithm Validation}},
  booktitle = 
    "Proc. Workshop on Automated Math Theory Exploration AUTOMATHEO'09",
  pages = "21-25",
  year = "2009",
  link = "\url{http://www.risc.jku.at/publications/download/risc_3915/PopJeb-AUTOMATHEO.pdf}",
  abstract =
    "We present some novel ideas for proving total correctness of
    recursive functional programs and we discuss how they may be used for
    algorithm validation.  As usual, correctness (validation) is
    transformed into a set of first-order predicate logic
    formulae—verification conditions.  As a distinctive feature of our
    method, these formulae are not only sufficient, but also necessary
    for the correctness. We demonstrate our method on the Nevilles
    algorithm for polynomial interpolation and show how it may be
    validated automatically. In fact, even if a small part of the
    specification is missing—in the literature this is often a case --
    the correctness cannot be proven. Furthermore, a relevant 
    counterexample may be constructed automatically.",
  paper = "Popo99a.pdf"
}

@inproceedings{Prat73,
  author = "Pratt, Vaughan R.",
  title = {{Top down operator precedence}},
  booktitle = "Proc. 1st annual ACM SIGACT-SIGPLAN Symposium on Principles 
               of Programming Languages",
  series = "POPL'73",
  pages = "41-51",
  year = "1973",
  link = "\url{http://hall.org.ua/halls/wizzard/pdf/Vaughan.Pratt.TDOP.pdf}",
  paper = "Prat73.pdf",
  keywords = "axiomref, printed"
}

@book{Pres07,
  author = "Press, William H. and Teukolsky, Saul A. and 
            Vetterling, William T. and Flannery, Brian P.",
  title = {{Numerical Recipes (3rd Edition)}},
  year = "2007",
  publisher = "Cambridge University Press",
  isbn = "978-0-521-88068-8",
}

@misc{Puff09,
  author = "Puffinware LLC",
  title = {{Singular Value Decomposition (SVD) Tutorial}},
  link = "\url{http://www.puffinwarellc.com/p3a.htm}"
}

@article{Rabe13,
  author = "Rabe,Florian and Kohlhase, Michael",
  title = {{A Scalable Module System}},
  journal = "Information and Computation",
  volume = "230",
  pages = "1-54",
  year = "2013",
  abstract =
   "Symbolic and logic computation systems ranging from computer algebra
   systems to theorem provers are finding their way into science,
   technology, mathematics and engineering. But such systems rely on
   explicitly or implicitly represented mathematical knowledge that needs
   to be managed to use such systems effectively.
   
   While mathematical knowledge management (MKM) ``in the small'' is
   well-studied, scaling up to large, highly interconnected corpora
   remains difficult. We hold that in order to realize MKM “in the
   large”, we need representation languages and software architectures
   that are designed systematically with large-scale processing in mind.
   
   Therefore, we have designed and implemented the Mmt language – a
   module system for mathematical theories. Mmt is designed as the
   simplest possible language that combines a module system, a
   foundationally uncommitted formal semantics, and web-scalable
   implementations. Due to a careful choice of representational
   primitives, Mmt allows us to integrate existing representation
   languages for formal mathematical knowledge in a simple, scalable
   formalism. In particular, Mmt abstracts from the underlying
   mathematical and logical foundations so that it can serve as a
   standardized representation format for a formal digital
   library. Moreover, Mmt systematically separates logic-dependent and
   logic-independent concerns so that it can serve as an interface layer
   between computation systems and MKM systems.",
  paper = "Rabe13.pdf"
}

@article{Rabe13a,
  author = "Rabe, Florian",
  title = {{The MMT API: A Generic MKM System}},
  journal = "LNCS",
  volume = "7961",
  pages = "339-343",
  year = "2013",
  abstract =
   "The Mmt language has been developed as a scalable representation and
   interchange language for formal mathematical knowledge. It permits
   natural representations of the syntax and semantics of virtually all
   declarative languages while making Mmt-based MKM services easy to
   implement. It is foundationally unconstrained and can be instantiated
   with specific formal languages.
   
   The Mmt API implements the Mmt language along with multiple backends
   for persistent storage and frontends for machine and user
   access. Moreover, it implements a wide variety of Mmt-based knowledge
   management services. The API and all services are generic and can be
   applied to any language represented in Mmt. A plugin interface permits
   injecting syntactic and semantic idiosyncrasies of individual formal
   languages.",
  paper = "Rabe13a.pdf, printed"
}

@article{Rabe16,
  author = "Rabe, Florian",
  title = {{The Future of Logic: Foundation-Independence}},
  journal = "Logica Universalis",
  volume = "10",
  number = "1",
  pages = "1-20",
  year = "2016",
  abstract =
   "Throughout the twentieth century, the automation of formal logics in
   computers has created unprecedented potential for practical
   applications of logic—most prominently the mechanical verification of
   mathematics and software. But the high cost of these applications
   makes them infeasible but for a few flagship projects, and even those
   are negligible compared to the ever-rising needs for verification. One
   of the biggest challenges in the future of logic will be to enable
   applications at much larger scales and simultaneously at much lower
   costs. This will require a far more efficient allocation of
   resources. Wherever possible, theoretical and practical results must
   be formulated generically so that they can be instantiated to
   arbitrary logics; this will allow reusing results in the face of
   today’s multitude of application-oriented and therefore diverging
   logical systems. Moreover, the software engineering problems
   concerning automation support must be decoupled from the theoretical
   problems of designing logics and calculi; this will allow researchers
   outside or at the fringe of logic to contribute scalable
   logic-independent tools. Anticipating these needs, the author has
   developed the Mmt framework. It offers a modern approach towards
   defining, analyzing, implementing, and applying logics that focuses on
   modular design and logic-independent results. This paper summarizes
   the ideas behind and the results about Mmt. It focuses on showing how
   Mmt. provides a theoretical and practical framework for the future of
   logic.",
  paper = "Rabe16.pdf, printed",
}

@article{Rabe17,
  author = "Rabe, Florian",
  title = {{How to Identify, Translate, and Combine Logics?}},
  journal = "J. of Logic and Computation",
  volume = "27",
  number = "6",
  pages = "1753-1798",
  year = "2017",
  abstract =
   "We give general definitions of logical frameworks and
   logics. Examples include the logical frameworks LF and Isabelle and
   the logics represented in them. We apply this to give general
   definitions for equivalence of logics, translation between logics and
   combination of logics. We also establish general criteria for the
   soundness and completeness of these. Our key messages are that the
   syntax and proof systems of logics are theories; that both semantics
   and translations are theory morphisms; and that combinations are
   colimits. Our approach is based on the Mmt language, which lets us
   combine formalist declarative representations (and thus the associated
   tool support) with abstract categorical conceptualizations.",
  paper = "Rabe17.pdf, printed"
}

@phdthesis{Rave91,
  author = "Ravenscroft, Robert Andrews Jr.",
  title = {{Generating Function Algorithms for Symbolic Computation}},
  school = "Brown University",
  year = "1991",
  file = "Rave91.pdf",
  abstract = 
    "Various algebraic models have been used to implement symbolic
    procedures for solving summations and recurrences. Surprisingly, one
    model that has received little attention in the development of
    symbolic algorithms is generating functions. Generating function
    methods for discrete mathematics provide an ad hoc collection of
    techniques for solving summations and recurrences. In this work we
    consider their systematic application to symbolic computation.
     
    To demonstrate the feasibility and capabilities of generating
    functions as an algebraic model of computation, we develop and
    implement algorithms for manipulating generating functions and the
    sequences that they encode. In particular, we concentrate on
    algorithms for solving summations and recurrences. Techniques are
    developed to handle special functions and hybrid terms, which are
    terms involving two or more factors.
     
    Given various classes of summations and recurrences we develop a
    theory to characterize the form of their solution. This allows us to
    demonstrate the ability of our algorithms to solve problems of these
    classes. This knowledge also assists us in developing algorithms that
    know where to look for a solution rather than doing an ad hoc search
    for a solution."

}

@book{Redf98,
  author = "Redfern, D.",
  title = {{The Maple Handbook: Maple V Release 5}},
  publisher = "Springer",
  year = "1998"
}

@article{Redf27,
  author = "Redfield, J. Howard",
  title = {{The Theory of Group-Reduced Distributions}},
  journal = "American J. Math.",
  volume = "49",
  number = "3",
  year = "1927",
  pages = "433-455"
}

@article{Rich69,
  author = "Richardson, Daniel",
  title = {{Some Undecidable Problems involving Elementar Functions of
            a Real Variable}},
  journal = "J. Symbolic Logic",
  volume = "33",
  number = "4",
  year = "1969",
  pages = "514-520",
  abstract =
    "Let $E$ be a set of expressions representing real, single valued,
    partially defined functions of one real variable. $E^*$ will be the
    set of functions represented by expressions in $E$. If $A$ is an
    expression in $E$, $A(x)$ is the function denoted by $AA$. It is
    assumed that $D^*$ contains the identity function and the rational
    numbers as constant functions and that $E^*$ is closed under addition,
    subtraction, multiplication, and composition"
}

@inproceedings{Rich94,
  author = "Richardson, Dan and Fitch, John P.",
  title = {{The identity problem for elementary functions and constants}},
  booktitle = "ACM Proc. of ISSAC 94",
  pages = "285-290",
  isbn = "0-89791-638-7",
  year = "1994",
  abstract =
    "A solution for a version of the identify problem is proposed for a
    class of functions including the elementary functions. Given f(x),
    g(x), defined at some point $\beta$ we decide whether or not 
    $f(x) \equiv g(x)$
    in some neighbourhood of $\beta$. This problem is first reduced to a
    problem about zero equivalence of elementary constants. Then a semi
    algorithm is given to solve the elementary constant problem. This semi
    algorithm is guaranteed to give the correct answer whenever it
    terminates, and it terminates unless the problem being considered
    contains a counterexample to Schanuel's conjecture.",
  paper = "Rich94.pdf"

}

@article{Ritt50,
  author = "Ritt, Joseph Fels",
  title = {{Differential Algebra}},
  journal = "AMS Colloquium Publications",
  year = "1950",
  volume = "33",
  isbn = "978-0-8218-4638-4",
  algebra = "\newline\refto{category DVARCAT DifferentialVariableCategory}",
  paper = "Ritt50.pdf"
}

@InProceedings{Rube06,
  author = "Rubey, Martin",
  title = {{Extented rate, more GFUN}},
  booktitle = "4th Colloquium on mathematics and computer science",
  series = "DMTCS",
  year = "2006",
  location = "Nancy, France",
  pages = "431-434",
  link = "\url{http://mathinfo06.iecn.u-nancy.fr/papers/dmAG431-434.pdf}",
  algebra =
    "\newline\refto{package GUESS Guess}
     \newline\refto{package GUESSAN GuessAlgebraicNumber}
     \newline\refto{package GUESSF GuessFinite}
     \newline\refto{package GUESSF1 GuessFiniteFunctions}
     \newline\refto{package GUESSINT GuessInteger}
     \newline\refto{package GUESSP GuessPolynomial}
     \newline\refto{package GUESSUP GuessUnivariatePolynomial}",
  abstract =
    "We present a software package that guesses formulas from sequences
    of, for example, rational numbers or rational functions, given the
    first few terms. Thereby we extend and complement C. Krattenthaler's
    program RATE [RATE: a Mathematics guessing machine] and the relevant
    parts of B. Salvy and P. Zimmermann's GFUN.",
  paper = "Rube06.pdf"
}

@article{Rump88,
  author = "Rump, Siegfried M.",
  title = {{Algebraic Computation, Numerical Computation and Verified
            Inclusions}},
  journal = "LNCS",
  volume = "296",
  pages = "177-197",
  year = "1988",
  abstract = 
    "The three different types of computation -- the algebraic
    manipulation, the numerical computation and the computation of
    verified results -- are aiming on different problems and deliver
    qualitatively different results, each method having its specific
    advantages for specific classes of problems. The following remarks
    give some thoughts on possible combinations of all three methods to
    obtain algorithms benefitting from the specific strength of either
    method.",
  paper = "Rump88.pdf",
  keywords = "printed"
}

@inproceedings{Rush00,
  author = "Rushby, John",
  title = {{Disappearing Formal Methods}},
  booktitle = "High Assurance Systems Engineering, 5th Int. Symp.",
  pages = "95-96",
  year = "2000",
  publisher = "ACM",
  paper = "Rush00.pdf"
}  

@book{Saun79,
  author = "MacLane, Saunders and Birkhoff, Garrett",
  title = {{Algebra, Second Edition}},
  publisher = "MacMillan",
  year = "1979",
  algebra = "\newline\refto{category GRMOD GradedModule}"
}

@misc{Salo16,
  author = "Salomone, Matthew",
  title = {{Exploring Abstract Algebra II}},
  year = "2016",
  link = "\url{https://www.youtube.com/watch?v=RNpdUG_yH_s\&list=PLL0ATV5XYF8DTGAPKRPtYa4E8rOLcw88y\&index=1}"
}

@article{Scha61,
  author = "Schafer, R.D.",
  title = {{An Introduction to Nonassociative Algebras}},
  year = "1961",
  algebra = "\newline\refto{category NARNG NonAssociativeRng}",
  link = "\url{http://www.gutenberg.org/ebooks/25156}",
  journal = "Advanced Subject Matter Institute",
  abstract =
    "These are notes for my lectures in July, 1961, at the Advanced
    Subject Matter Institute in Algebra which was held at Oklahoma State
    University in the summer of 1961.

    Students at the Institute were provided with reprints of my paper,
    {\sl Structure and representation of nonassociate algebras} (Bulletin
    of the American Mathematical Society, vol. 61 (1955), pp469-484),
    together with copies of a selective bibliography of more recent papers
    on non-associative algebras. These notes supplement the 1955 Bulletin
    article, bringing the statements there up to date and providing
    detailed proofs of a selected group of theorems. The proofs illustrate
    a number of important techniques used in the study of nonassociative
    algebras.",
  paper = "Scha61.pdf"
}

@article{Scha86,
  author = "Schaffert, C. and Cooper, T.",
  title = {{An Introduction to Trellis/Owl}},
  journal = "SIGPLAN Notices",
  volume = "21",
  number = "11",
  publisher = "ACM",
  year = "1986",
  pages = "9-16"
}

@book{Scha66,
  author = "Schafer, R.D.",
  title = {{An Introduction to Nonassociative Algebras}},
  year = "1966",
  publisher = "Academic Press, New York",
  algebra =
    "\newline\refto{category NARNG NonAssociativeRng}
     \newline\refto{category NASRING NonAssociativeRing}
     \newline\refto{domain ALGSC AlgebraGivenByStructuralConstants}" 
}

@book{Scha10,
  author = "Schafer, R.D.",
  title = {{An Introduction to Nonassociative Algebras}},
  year = "2010",
  publisher = "Benediction Classics",
  algebra = "\newline\refto{category NARNG NonAssociativeRng}",
  isbn = "978-1849025904",
  abstract =
    "Concise study presents in a short space some of the important ideas
    and results in the theory of non-associative algebras, with particular
    emphasis on alternative and (commutative) Jordan algebras. Written as
    an introduction for graduate students and other mathematicians meeting
    the subject for the first time."
}

@book{Sche01,
  author = "Schelter, William F.",
  title = {{Maxima Manual Version 5.41.0}},
  year = "2001",
  publisher = "Sourceforge",
  paper = "Sche01.pdf"
}

@techreport{Schr09,
  author = "Schreiner, Wolfgang",
  title = {{How to Write Postconditions with Nultiple Cases}},
  year = "2009",
  institution = "RISC Linz",
  abstract =
    "We investigate and compare the two major styles of writing program
    function postconditions with multiple cases: as conjunctions of
    implications or as disjunctions of conjunctions. We show that both
    styles not only have different syntax but also different semantics and
    pragmatics and give recommendations for their use.",
  paper = "Schr09.pdf"
}

@inproceedings{Schr00,
  author = "Schreiner, Wolfgang and Danielczyk-Landerl, Werner and
            Marin, Mircea and Stocher, Wolfgang",
  title = {{A Generic Programming Environment for High-Performance
           Mathematical Libraries}},
  booktitle = "Int. Seminar on Generic Programming",
  publisher = "Springer-Verlag",
  year = "2000",
  pages = "256-268",
  isbn = "3-540-41090-2",
  abstract =
    "We report on a programming environment for the development of
    generic mathematical libraries based on functors (parameterized
    modules) that have rigorously specified but very abstract
    interfaces. We focus on the combination of the functor-based
    programming style with software engineering principles in large
    development projects. The generated target code is highly efficient
    and can be easily embedded into foreign application environments.",
  paper = "Schr00.pdf"
}

@techreport{Schr14,
  author = "Schreiner, Wolfgang",
  title = {{Some Lessons Learned on Writing Predicate Logic Proofs in
           Isabelle/Isar}},
  year = "2014",
  institution = "RISC Linz",
  abstract =
    "We describe our experience with the use of the proving assistant
    Isabelle and its proof development language Isar for formulating and
    proving formal mathematical statements. Our focus is on how to use
    classical predicate logic and well established proof principles for
    this purpose, bypassing Isabelle’s meta-logic and related technical
    aspects as much as possible.  By a small experiment on the proof of
    (part of a) verification condition for a program, we were able to
    identify a number of important patterns that arise in such proofs
    yielding to a workflow with which we feel personally comfortable; the
    resulting guidelines may serve as a starting point for a the
    application of Isabelle / Isar for the “average” mathematical user
    (i.e, a mathematical user who is not interested in Isabelle / Isar per
    se but just wants to use it as a tool for computer-supported formal
    theory development).",
  paper = "Schr14.pdf"
}

@article{Schw85,
  author = "Schwarz, Fritz",
  title = {{An Algorithm for Determining Polynomial First Integrals of 
            Autonomous Systems of Ordinary Differential Equations}},
  journal = "J. Symbolic Computation",
  volume = "1",
  year = "1985",
  pages = "229-233",
  paper = "Schw85.pdf"
}

@article{Schw88,
  author = "Schwarz, Fritz",
  title = {{Symmetries of Differential Equations: From Sophus Lie to
            Computer Algebra}},
  journal = "SIAM Review",
  volume = "30",
  number = "3",
  year = "1988",
  abstract =
    "The topic of this article is the symmetry analysis of
    differential equations and the applications of computer algebra to
    th extensive analytical calculations which are usually involved in
    it. The whole area naturally decomposes into two parts depending
    on whether ordinary or partial differential equations are
    considered. We show how a symmetry may be applied to lower the
    order of an ordinary differential equation or to obtain similarity
    solutions of partial differential equations. The computer algebra
    packages SODE and SPDE, respectively, which have been developed to
    perform almost all algebraic manipulations necessary to determine
    the symmetry group of a given differential equation, are
    presented. Futhermore it is argued that the application of
    computer algebra systems has qualitatively changed this area of
    applied mathematics",
  keywords = "axiomref, printed"
}

@misc{SCSCP,
  author = "The SCSCP development team",
  title = {{Symbolic Computation Software Composability Protocol}},
  link = "\url{https://gap-packages.github.io/scscp/}",
  year = "2017"
}

@book{Segg93,
  author = "{von Seggern}, David Henry",
  title = {{CRC Standard Curves and Surfaces}},
  publisher = "CRC Press",
  year = "1993",
  isbn = "0-8493-0196-3"
}

@misc{Simo97,
  author = "Simon, Barry",
  title = {{The PC Is Now Axiomatic}},
  publisher = "PC Mag",
  year = "1997",
  month = "March",
  day = "25",
  keywords = "axiomref"
}

@misc{Sorg96,
  author = "Sorge, Volker",
  title = {{Integration eines Computeralgebrasystems in eine logische
            Beweisumgebung }},
  school = "Univ. des Saarlandes",
  year = "1996",
  comment = "Master's thesis"
}

@article{Sorg00,
  author = "Sorge, Volker",
  title = {{Non-trivial Symbolic Computations in Proof Planning}},
  journal = "LNCS",
  volume = "1794",
  pages = "121-135",
  year = "2000",
  abstract =
    "We discuss a pragmatic approach to integrate computer algebra into
    proof planning. It is based on the idea to separate computation and
    verification and can thereby exploit the fact that many elaborate
    symbolic computations are trivially checked. In proof planning the
    separation is realized by using a powerful computer algebra system
    during the planning process to do non-trivial symbolic
    computations. Results of these computations are checked during the
    refinement of a proof plan to a calculus level proof using a small,
    self-implemented system that gives us protocol information on its
    calculation. This protocol can be easily expanded into a checkable
    low-level calculus proof ensuring the correctness of the
    computation. We demonstrate our approach with the concrete
    implementation in the $\Omega$MEGA system.",
  paper = "Sorg00.pdf",
  keywords = "CAS-Proof, printed"
}

@book{Stee90,
  author = "Steele, Guy L.",
  title = {{Common Lisp, The Language}},
  year = "1990",
  link = "\url{http://daly.axiom-developer.org/clm.pdf}",
  publisher = "Digital Equipment Corporation",
  isbn = "1-555558-041-6",
  algebra = "\newline\refto{package DFSFUN DoubleFloatSpecialFunctions}",
  paper = "Stee90.pdf"
}

@book{Stur93,
  author = "Sturmfels, Bernd.",
  title = {{Algorithms in Invariant Theory}},
  year = "1993",
  publisher = "Springer",
  paper = "Stur93.pdf"
}

@misc{Stic93,
  author = "Stichtenoth, H.",
  title = {{Algebraic function fields and codes}},
  publisher = "Springer-Verlag",
  year = "1993"
}

@article{Supp77,
  author = "Suppes, Patrick and Smith, Robert and Beard, Marian",
  title = {{University-Level Computer-Assisted Instruction at Stanford: 1975}},
  journal = "Instructional Science",
  volume = "6",
  year = "1977",
  pages = "151-185",
  abstract =
    "This article provides an overview of current work on university-level
    computer-assisted instruction at Stanford University.  Brief
    descriptions are given of the main areas of current interest.  The
    main emphasis is on the courses now being used: Introduction to Logic,
    Axiomatic Set Theory, Old Church Slavonic, History of the Russian
    Literary Language, Introduction to Bulgarian, Introduction to BASIC,
    Introduction to LISP, and various courses in music.",
  paper = "Supp77.pdf"
}

@article{Supp89,
  author = "Suppes, P. and Takahashi, S.",
  title = {{An Interactive Calculus Theorem-prover for Continuity
            Properites}},
  journal = "J. Symbolic Computation",
  volume = "7",
  number = "6",
  year = "1989",
  pages = "573-590",
  abstract =
    "he work reported concerns the development of an interactive
    theorem-prover for the foundationsof the differential and integral
    calculus. The main tools are a resolution theorem-prover VERIFY,
    previously developed for interactive proofs in set theory, and the
    symbolic computation program REDUCE. The use of REDUCE in a
    theorem-proving context is described in detail. Sample proofs are
    given with data on computation time per step on an IBM-4381.",
  paper = "Supp89.pdf",
  keywords = "printed"
}

@techreport{Swee86,
  author = "Sweedler, Moss E.",
  title = {{Typing in Scratchpad II}},
  institution = "IBM Research",
  year = "1986",
  month = "January",
  type = "Scratchpad II Newsletter",
  volume = "1",
  number = "2",
}

@book{Tait1890,
  author = "Tait, P.G.",
  title = {{An Elementary Treatise on Quaternions}},
  publisher = "C.J. Clay and Sons, Cambridge University Press Warehouse, 
               Ave Maria Lane",
  year = "1890"
}

@article{That82,
  author = "Thatcher, J.W. and Wagner, E.G. and Wright, J.B.",
  title = {{Data Type Specification: Parameterization and the Power of 
           Specification Techniques}},
  journal = "ACM TOPLAS",
  volume = "4",
  number = "4",
  pages = "711-732",
  year = "1982",
  abstract = 
    "Our earlier work on abstract data types is extended by the answers to
    a number of questions on the power and limitations of algebraic
    specification techniques and by an algebraic treatment of
    parameterized data types like {\bf sets-of()} and 
    {\bf stacks-of-()}. The ``hidden function'' problem (the need to include
    operations in specifications which are wanted hidden from the user) is
    investigated; the relative power of conditional specifications and
    equational specifications is investigated; the relative power of
    conditional specifications and equational specifications is
    investigated; and it is shown that parameterized specifications must
    contain ``side conditions'' (e.g. that {\bf finite-sets-of-d} requires
    an equality predicate on {\bf d}).",
  paper = "That82.pdf"
}

@article{Thur94,
  author = "Thurston, William P.",
  title = {{On Proof and Progress in Mathematics}},
  journal = "Bulletin AMS",
  volume = "30",
  number = "2",
  month = "April",
  year = "1994",
  link = "\url{http://www.ams.org/journals/bull/1994-30-02/S0273-0979-1994-00502-6/S0273-0979-1994-00502-6.pdf}",
  paper = "Thur94.pdf"
}

@misc{Tray10,
  author = "Traytel, Bmytro",
  title = {{Extensions of a Type Inference Algorithm with Coerce Subtyping}},
  school = "Der Technischen Universitat Munchen",
  link = "\url{https://www21.in.tum.de/~traytel/bscthesis.pdf}",
  year = "2010",
  abstract =
    "Subtyping with coercion semantics allows a type inference system to
    correct some ill-typed programs by the automatic insertion of
    implicit type conversions at run-time. This simplifies programmer’s
    life but has its price: the general typability problem for given base
    type subtype dependencies is NP-complete.  Nevertheless, if the given
    coercions define an order on types with certain properties, the
    problem behaves in a sane way in terms of complexity.  This thesis
    presents an algorithm that can be used to extend Hindley-Milner type
    inference with coercive subtyping assuming a given partial order on
    base types. Especially, we discuss restrictions on the subtype
    dependencies that are necessary to achieve an efficient
    implementation. Examples of problems that occur if these restrictions
    are not met are given. The result of these considerations is that the
    algorithm is complete if the given base type poset is a disjoint union
    of lattices. Furthermore, the interaction of subtyping with type
    classes is addressed.  The algorithm that is extended to deal with
    type classes requires even a stronger restriction to assure
    completeness.  An ML-implementation of the presented algorithm is used
    in the generic proof assistant Isabelle.",
  paper = "Tray10.pdf",
  keywords = "printed"
}

@misc{Tryb02,
  author = "Trybulec, Andrzej",
  title = {{Towards Practical Formalizaiton of Mathematics}},
  comment = "abstract",
  link =
  "\url{http://www.macs.hw.ac.uk/~fairouz/forest//events/automath2002/abstracts/Andrzej.abst.html}",
  abstract =
    "In sixties the opinion that mathematics cannot be practically
    formalized was well established. So the main achievement of de Bruijn,
    among many others, is the decision that the problem must be reconsider
    and probably positively solved. And I believe that 1967 will be
    treated by future historians of mathematics as a turning point.

    It is not easy to say precisely what we learnt in the meantime, if we
    learnt anything at all. I believe that the most important results are:
    \begin{enumerate}
    \item we need experiments with much more advanced mathematics than
    already done
    \item a system for practical formalization of mathematics probably will
    not be a simple system based on small number of primitive notions
    \item integration with a computer algebra systems may be necessary or at
    least a feasible system must have bigger computational power. 
    \end{enumerate}

    Still we should me more concerned with the original ideas of de
    Bruijn, the most important that eventually we have to be able to
    formalize new mathematical results, published in mathematical
    newspapers in this century."
}

@InProceedings{Mart99,
  author = "Martin, Ursula",
  title = {{Computers, reasoning and mathematical practice}},
  booktitle = "Computational Logic",
  publisher = "Springer",
  year = "1999",
  location = "Marktoberdorf, Germany",
  pages = "301-346",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.50.2061}",
  abstract =
    "We identify three main objectives for computer aided reasoning
    enhancing the techniques available for mathematical experimentation,
    developing community standards for experiment and modelling and
    developing methods which will make computation more acceptable as part
    of a proof. We discuss three areas of research which address these:
    the use of theorem proving techniques to enhance or extend
    mathematical software systems, support for formal methods techniques
    to increase the reliability of such systems, and the use of computer
    aided formal reasoning in support of mathematical practice. This last
    includes activities such as formalizing systems for computational
    mathematics or visualization so that they can still be used informally
    but generate a formal development, and developing techniques to
    provide assistance in the initial stages of developing a new theory.

    By mathematics here we mean the activities of working research
    mathematicians, producing new results in pure or applied mathematics,
    although we touch briefly on some questions concerning the
    applications of computational mathematics and simulation in research
    science and engineering. We have left out several other related areas
    entirely: logical questions of decidability, soundness or
    completeness, theoretical computer science issues of semantics,
    computability or complexity, foundational issues such as
    constructivity, computer aided proofs about software and hardware, and
    the use of computers in mathematical education at all levels and in
    heuristic discovery. In particular foundational questions about
    computation have transformed mathematical logic, computation has made
    constructive proof feasible, and effective notions of practice for
    proofs about hardware and software are by no means well understood.
    However these matters fall outside the scope of this paper.",
  paper = "Mart99.pdf",
  keywords = "axiomref"
}

@misc{Vogl07,
  author = "Vogler, Doctor",
  title = {{Genus of a Plane Curve}},
  year = "2007",
  link = "\url{http://mathforum.org/library/drmath/view/71229.html}",
  algebra = 
    "\newline\refto{package GPAFF GeneralPackageForAlgebraicFunctionField}
\newline\refto{package PAFFFF PackageForAlgebraicFunctionFieldOverFiniteField}
     \newline\refto{package PAFF PackageForAlgebraicFunctionField}"
}

@techreport{Vuil87,
  author = "Vuillemin, J.",
  title = {{Exact Real Computer Arithmetic with Continued Fractions}},
  institution = "Institut National de Recherche en Informatique et en
                 Automatique",
  year = "1987",
  number = "760",
  abstract =
    "We discuss a representation of the {\sl computable real numbers} by
    continued fractions. This deals with the subtle points of undecidable
    and integer division, as well as representing the infinite
    $\infty=1/0$ and undefined $\bot=0/0$ numbers. Two general algorithms
    for performing arithmetic operations are introduced. The 
    {\sl algebraic algorithm}, which computes sums and products of
    continued fractions as a special case, basically operates in a
    positional manner, producing one term of output for each term of
    input. The {\sl transcendental algorithm} uses a general formula of
    Gauss to compute the continued fractions of exponentials, logarithms,
    trigonometric functions, as well as a wide class of special
    functions. This work has been implemented in Le\_Lisp and the
    performance of these algorithms appears to be quite good; however, no
    competing system has been available for comparison",
  paper = "Vuil87.pdf"
}

@book{Walk78,
  author = "Walker, Robert J.",
  title = {{Algebraic Curves}},
  year = "1978",
  publisher = "Princeton University Press",
  isbn = "978-0-387-90361-3",
  algebra = 
    "\newline\refto{package GPAFF GeneralPackageForAlgebraicFunctionField}
\newline\refto{package PAFFFF PackageForAlgebraicFunctionFieldOverFiniteField}
     \newline\refto{package PAFF PackageForAlgebraicFunctionField}"
}

@misc{Watt03,
  author = "Watt, Stephen",
  title = {{Aldor}},
  link = "\url{http://www.aldor.org}",
  year = "2003"
}

@article{Wegb74,
  author = "Wegbreit, Ben",
  title = {{The Treatment of Data Types in EL1}},
  journal = "Communications of the ACM",
  volume = "17",
  number = "5",
  year = "1974",
  pages = "251-264",
  abstract = 
    "In constructing a general purpose programming language, a key issue
    is providing a sufficient set of data types and associated operations
    in a manner that permits both natural problem-oriented notation and
    efficient implementation. The EL1 language contains a number of
    features specifically designed to simultaneously satisfy both
    requirements. The resulting treatment of data types includes provision
    for programmer-defined data types and generaic routines, programmer
    control over type conversion, and very flexible data type behavior, in
    a context that allows efficient compiled code and compact data
    representation.",
  paper = "Wegb74.pdf"
}

@misc{Weil71,
  author = "Weil, Andr\'{e}",
  title = {{Courbes alg\'{e}briques et vari\'{e}t\'{e}s Abeliennes}},
  year = "1971"
}

@misc{Weit03,
  author = "Weitz, E.",
  title = {{CL-WHO -Yet another Lisp markup language}},
  year = "2003",
  link = "\url{http://www.weitz.de/cl-who/}"
}

@misc{Weit06,
  author = "Weitz, E.",
  title = {{HUNCHENTOOT - The Common Lisp web server formerly known as TBNL}},
  year = "2006",
  link = "\url{http://www.weitz.de/hunchentoot}"
}

@article{Wied03,
  author = "Wiedijk, Freek",
  title = {{Formal Proof Sketches}},
  journal = "LNCS",
  volume = "3085",
  year = "2003",
  pages = "378-393",
  abstract = 
    "Formalized mathematics currently does not look much like informal
    mathematics. Also, formalizing mathematics currently seems far too
    much work to be worth the time of the working mathematician. To
    address both of these problems we introduce the notion of a formal
    proof sketch . This is a proof representation that is in between a
    fully checkable formal proof and a statement without any proof at
    all. Although a formal proof sketch is too high level to be checkable
    by computer, it has a precise notion of correctness (hence the
    adjective formal ).  We will show through examples that formal proof
    sketches can closely mimic already existing mathematical
    proofs. Therefore, although a formal proof sketch contains gaps in
    the reasoning from a formal point of view (which is why we call it a
    sketch ), a mathematician probably would call such a text just a
    ‘proof’.",
  paper = "Wied03.pdf",
  keywords = "printed"
}

@misc{Wiki14a,
  author = "ProofWiki",
  title = {{Euclidean Algorithm}},
  link = "\url{http://proofwiki.org/wiki/Euclidean_Algorithm}"
}

@misc{Wiki14b,
  author = "ProofWiki",
  title = {{Division Theorem}},
  link = "\url{http://proofwiki.org/wiki/Division_Theorem}"
}

@misc{Wiki16,
  author = "Anonymous",
  title = {{Levenshtein distance}},
  year = "2016",
  link = "\url{https://en.wikipedia.org/wiki/Levenshtein\_distance}"
}

@misc{Wink95,
  author = "Winkler, Franz",
  title = {{Computer Algebra -- Problems and Developments}},
  year = "1995",
  abstract = 
    "Recent developments in computer algebra are discussed using simple
    but illustrative examples",
  paper = "Wink95.pdf"
}

@article{Wosx92,
  author = "Wos, Larry",
  title = {{The Impossibility of the Automation of Logical Reasoning}},
  journal = "Lecture Notes in Computer Science",
  volume = "607",
  year = "1992",
  pages = "1-3",
  paper = "Wosx92.pdf"
}

@book{Wuxx94,
  author = "Wu, Wen-tsun",
  title = {{Mechanical Theorem Proving in Geometries}},
  isbn = "978-3-7091-6639-0",
  publisher = "Springer",
  year = "1994"
}

@article{Wulf76,
  author = "Wulf, William A. and London, Ralph L. and Shaw, Mary",
  title = {{An Introduction to the Construction and Verification of 
           Alphard Programs}},
  journal = "IEEE Tr. Software Engineering",
  volume = "SE-2",
  number = "4",
  year = "1976",
  pages = "253-265",
  abstract =
    "The programming language Alphard is designed to provide support for
    both the methodologies of ``well-structured'' programming and the
    techniques of formal program verification.  Language constructs allow
    a programmer to isolate an abstraction, specifying its behavior
    publicly while localizing.  knowledge about its implementation.  The
    verification of such an abstraction consists of showing that its
    implementation behaves in accordance with its public specifications;
    the abstraction can then be used with confidence in constructing
    other programs, and the verification of that use employs only the
    public specifications.  This paper introduces Alphard by developing
    and verifying a data structure definition and a program that uses it.
    It shows how each language construct contributes to the development of
    the abstraction and discusses the way the language design and the
    verification methodology wete tailored to each other.  It serves not
    only as an introduction to Alphard, but also as an example of the
    symbiosis between verification and methodology in language design.
    The strategy of program structuring, illustrated for Alphard, is
    also applicable to most of the ``data abstraction'' mechanisms now
    appearing.",
  paper = "Wulf76.pdf"
}

@misc{Zdan14,
  author = "Zdancewic, Steve and Martin, Milo M.K.",
  title = {{Vellvm: Verifying the LLVM}},
  link = "\url{http://www.cis.upenn.edu/~stevez/vellvm}"
}

@phdthesis{Zinn04,
  author = "Zinn, Claus",
  title = {{Understanding Informal Mathematical Discourse}},
  school = "Universitat Erlangen Nurnberg",
  year = "2004",
  abstract =
    "Automated reasoning is one of the most established disciplines in
    informatics and artificial intelligence, and formal methods become
    increasingly employed in practical applications. However, for the most
    part, such applications seem to be limited to informatics-specific
    areas (e.g., the verification of correctness properties of software
    and hardware specifications) and areas close to informatics such as
    computational linguistics (e.g., the computation of consistence and
    informativeness properties of semantic representations). Naturally,
    there is also a potential for practical applications in the the area
    of mathematics: the generation of proofs for mathematically
    interesting and motivated theorems and, quite associated, the
    computer-supported formalisation of (parts of elementary)
    mathematics. It is a matter of fact, however, that mathematicians
    rarely use computer-support for the construction and verification of
    proofs. This is mainly caused by the “unnaturalness” of the language
    and the reasoning that such proof engines support.

    In the past, researchers in the area of automated reasoning have
    focused their work on formalisms and algorithms that allow the
    construction and verification of proofs that are written in a
    formal-logical language and that only use a limited number of
    inference rules. For the computer scientist, such formal proofs have
    the advantage of a simple and ambiguous-free syntax, which can thus be
    easily processed. Moreover, the limited number of inference rules has
    a direct impact on the complexity of the search space that needs to be
    conquered during the process of constructing proofs. The verification
    of given formal proofs is greatly facilitated by the complete
    explicitness of their logical argumentation where no reasoning step is
    left out.  For mathematicians, however, such formal proofs are usually
    hard to understand. For them, they are written in an unfamiliar and
    artificial language and much too detailed. Moreover, the sheer number
    of inference steps, while logically relevant, describe only trivial
    mathematical details and make it difficult to follow a proof’s main
    underlying argumentation line. In practise, thus, mathematicians use
    proof generation engines rather seldom, if at all. The same is true
    with regard to proof verification tools. The amount of work that is
    required to verify a mathematical proof with such tools is
    considerable, if not prohibitive. Since proof verification systems
    only accept formal proofs as input, the mathematician’s first task is
    to manually translate the mathematical proof into the formal language
    that is accepted by the verifier. This in turn includes the
    translation of the proof’s underlying mathematical argumentation into
    inference rules that are supported by the proof engine. Such
    translations and refinements are usually very time consuming, tedious,
    and prone to error themselves. Hence, how the proof verifier then
    judges the result of proof translation and proof refinement is only
    of limited relevance to the original mathematical proof. From the
    mathematician’s point of view, there is thus a need for a proof
    verification system that is capable of processing mathematical proofs
    automatically, at least with regard to translating the mathematicians’
    expert language into the system’s artificial formal language. Such a
    system would have an enormous potential in the community of
    mathematics, and this potential has been recognised early. In the
    beginning of the 1960s, John McCarthy, one of the pioneers of
    artificial intelligence, remarked that ``[c]hecking mathematical proofs
    is potentially one of the most interesting and useful applications of
    automatic computers'' [111]. More than forty years thereafter, a tool
    that supports mathematicians with the verification of mathematical
    proofs is more science fiction than reality. Its realisation is
    associated with research questions within the disciplines of automated
    reasoning and computational linguistics that are still only partially
    answered.

    This thesis aims at contributing towards the realisation of a
    verifier for mathematical proofs. It attempts to provide a general
    framework as well as an implementation for such a proof
    engine. The dissertation’s objects of study are short and simple
    proofs that were taken from textbooks on elementary number
    theory. Fig. 1 depicts a proof of the mathematical truth that
    every positive integer greater than 1 can be represented as a
    product of one or more primes. The proof consists of only a few
    lines and little mathematical knowledge is necessary to follow the
    proof author’s argumentation. It is this kind of short proof that
    we attempt to check automatically.",
  paper = "Zinn04.pdf"
}

@misc{OCon08,
  author = "O'Connor, Christine",
  title = {{Christine Jeanne O'Connor Obituary}},
  year = "2008",
  link = "\url{http://www.cargainfuneralhomes.com/home/index.cfm/obituaries/view/fh_id/10350/id/183842}"
}

@misc{Hale13,
  author = "Hales, Thomas C.",
  title = {{Mathematics in the Age of the Turing Machine}},
  year = "2013",
  link = "\url{http://arxiv.org/pdf/1302.2898v1}",
  abstract = "
    This article gives a survey of mathematical proofs that rely on
    computer calculations and formal proofs",
  paper = "Hale13.pdf"
}

@misc{Martxx,
  author = "Martin, W.A. and Fateman, R.J.",
  title = {{The Macsyma System}},
  link = "\url{http://groups.csail.mit.edu/mac/classes/symbolic/spring13/readings/simplification/martin-fateman-macsyma.pdf}",
  abstract = "
    MACSYMA is a system for symbolic manipulation of algebraic expressions
    which is being developed at Project MAC, M.I.T. This paper discusses
    its philosophy, goals, and current achievements.

    Drawing on the past work of Maring, Moses, and Engelman, it extends
    the capabilities of automated algebraic manipulation systems in
    several areas, including

    a) limit calculations
    b) symbolic integration
    c) solution of equations
    d) canonical simplification
    e) user-level pattern matching
    f) user-specified expression manipulation
    g) programming and bookkeeping assistance

    MACSYMA makes extensive use of the power of its rational function
    subsystem. The facilities derived from this are discussed in
    considerable detail.

    An appendix briefly notes some highlights of the overall system.",
  paper = "Martxx.pdf"

}

@misc{Andr00,
  author = "Andrews, George E. and Knopfmacher, Arnold and Paule, Peter
            and Zimmermann, Burkhard",
  title = {{Engel Expansions of q-Series by Computer Algebra}},
  year = "2000",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.207}",
  abstract = "
    The $q$-Engle Expansion is an algorithm that leads to unique series
    expansions of $q$-series. Various examples related to classical
    partition theorems, including the Rogers-Ramanujan identities together
    with the elegant generalization found by Garrett, Ismail and Stanton,
    have been described recently. The object of this paper is to present
    the computer algebra package Engel, written in Mathematics, that has
    already played a signiicant role in this work. The package now is made
    freely available via the web and should help to intensify research in
    this new branch of $q$-series theory. Among various illustrative
    examples we present a new infinite Rogers-Ramanujan type family that
    has been discovered by using the package.",
  paper = "Andr00.pdf"

}

@misc{Abra99,
  author = "Abramov, Sergei A. and van Hoeij, Mark",
  title = {{Integration of Solutions of Linear Functional Equations}},
  year = "1999",
  link = "\url{http://www.math.fsu.edu/~hoeij/papers/itsf99/ab_final.pdf}",
  abstract = "
    We introduce the notion of the adjoint Ore ring and give a definition
    of adjoint polynomial, operator and equation. We apply this for
    integrating solutions of Ore equations.",
  paper = "Abra99.pdf"

}

@misc{Bail97,
  author = "Bailey, David and Borwein, Peter and Plouffe, Simon",
  title = {{On the Rapid Computation of Various Polylogarithmic Constants}},
  year = "1997",
  link = "\url{http://www.ams.org/journals/mcom/1997-66-218/S0025-5718-97-00856-9/S0025-5718-97-00856-9.pdf}",
  abstract = "
    We give algorithms for the computation of the $d$-th digit of certain
    transcendental numbers in various bases. These algorithms can be
    easily implemented (multiple precision arithmetic is not needed),
    require virtually no memeory, and feature run times that scale nearly
    linearly with the order of the digit desired. They make it feasible to
    compute, for example, the billionth binary digit of log(2) or $\pi$ on
    a modest work station in a few hours run time.

    We demonstrate this technique by computing the ten billionth
    hexadecimal digit of $\pi$, the billionth hexadecimal digits of
    $\pi^2$, log(2), and log${}^2$(2), and the ten billionth decimal digit
    of log(9/10).

    These calculations rest on the observation that very special types of
    identities exist for certain numbers like $\pi$, $\pi^2$, log(2) and
    log${}^2$. These are essentially polylogarithmic ladders in an integer
    base. A number of these identities that we derive in this work appear
    to be new, for example the critical identity for $\pi$:

    \[\pi=\sum_{i=0}^\infty{\frac{1}{16^i}\left(
    \frac{4}{8i+1}-\frac{2}{8i+4}-\frac{1}{8i+5}-\frac{1}{8i+6}\right)}\]",
  paper = "Bail97.pdf"

}

@misc{Thie15,
  author = "Thiery, Nicolas M.",
  title = {{Open Digital Research Environment Toolkit for the 
            Advancement of Mathematics}},
  year = "2015",
  link = "\url{http://opendreamkit.org}",
  abstract =
   "OpenDreamKit will deliver a flexible toolkit enabling research groups
    to set up Virtual Research Environments, customised to meet the varied
    needs of research projects in pure mathematics and applications, and
    supporting the full research life-cycle from exploration, through
    proof and publication, to archival and sharing of data and code.

    OpenDreamKit will be built out of a sustainable ecosystem of
    community-developed open software, databases, and services,
    including popular tools such as LINBOX, MPIR, SAGE (sagemath.org),
    GAP, PARI/GP, LMFDB, and SINGULAR. We will extend the JUPYTER Notebook
    environment to provide a flexible user interface. By improving and
    unifying existing building blocks, OpenDreamKit will maximise both
    sustainability and impact, with beneficiaries extending to scientific
    computing, physics, chemistry, biology and more, and including
    researchers, teachers, and industrial practitioners.

    We will define a novel component-based VRE architecture and adapt
    existing mathematical software, databases, and user interface
    components to work well within it on varied platforms. Interfaces to
    standard HPC and grid services will be built in. Our architecture will
    be informed by recent research into the sociology of mathematical
    collaboration, so as to properly support actual research practice. The
    ease of set up, adaptability and global impact will be demonstrated in
    a variety of demonstrator VREs.

    We will ourselves study the social challenges associated with
    large-scale open source code development and publications based on
    executable documents, to ensure sustainability.

    OpenDreamKit will be conducted by a Europe-wide steered by demand
    collaboration, including leading mathematicians, computational
    researchers, and software developers with a long track record of
    delivering innovative open source software solutions for their
    respective communities. All produced code and tools will be open
    source.",
  paper = "Thie15.pdf"
}

