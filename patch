books/bookvolbib add additional references

Goal: Proving Axiom Correct

\index{Sterling, Jonathan}
\index{Harper, Robert}
\begin{chunk}{axiom.bib}
@misc{Ster17,
  author = "Sterling, Jonathan and Harper, Robert",
  title = "Algebraic Foundations of Proof Refinement",
  link = "\url{http://www.cs.cmu.edu/~rwh/papers/afpr/afpr.pdf}",
  year = "2017",
  abstract =
    "We contribute a general apparatus for {\sl dependent} tactic-based
    proof refinement in the LCF tradition, in which the statements of
    subgoals may express a dependency on the proofs of other subgoals;
    this form of dependency is extremely useful and can serve as an
    {\sl algorithmic} alternative to extensions of LCF based on non-local
    instantiation of schematic variables. Additionally, we introduce a
    novel behavioral distinction between {\sl refinement rules} and
    {\sl tactics} based on naturality. Our framework, called Dependent
    LCF, is already deployed in the nascent RedPRL proof assistant for
    computational cubical type theory.",
  paper = "Ster17.pdf"
}

\end{chunk}

\index{Plotkin, G.D.}
\begin{chunk}{axiom.bib}
@article{Plot77,
  author = "Plotkin, G.D.",
  title = "LCF Considered as a Programming Language",
  journal = "Theoretical Computer Science",
  volume = "5",
  year = "1977",
  pages = "223-255",
  link = "\url{http://homepages.inf.ed.ac.uk/gdp/publications/LCF.pdf}",
  abstract =
    "The paper studies connections between denotational and operational
    semantics for a simple programming language based on LCF. It begins
    with the connection between the behaviour of a program and its 
    denotation. It turns out that a program denotes $\bot$ in any of
    several possible semantics iff it does not terminate. From this it 
    follows that if two terms hae the same denotation in one of these
    semantics, they have the same behaviour in all contexts. The converse
    fails for all the semantics. If, however, the language is extended to
    allow certain parallel facilities behavioural equivalence does coincide
    with denotational equivalence in one of the semantics considered, which
    may therefore be called ``fully abstract''. Next a connection is given
    which actually determines the semantics up to isomorphism from the
    behaviour alone. Conversely, by allowing further parallel facilities, 
    every r.e. element of the fully abstract semantics becomes definable,
    thus characterising the programming language, up to interdefinability,
    from the set of r.e. elements of the domains of the semantics.",
  paper = "Plot77.pdf"
}

\end{chunk}

\index{Milner, Robin}
\begin{chunk}{axiom.bib}
@techreport{Miln72,
  author = "Milner, Robert",
  title = "Logic for Computable Functions: Description of a Machine
           Implementation",
  year = "1972",
  institution = "Stanford Artificial Intelligence Project",
  number = "STAN-CS-72-288",
  link =
 "\url{http://i.standford.edu/pub/cstr/reports/cs/tr/72/288/CS-TR-72-288.pdf}",
  abstract =
    "LCF is based on a logic by Dana Scott, proposed by him at Oxford in the
    fall of 1969, for reasoning about computable functions.",
  paper = "Miln72.pdf"
}

\end{chunk}

\index{Gordon, Mike}
\begin{chunk}{axiom.bib}
@misc{Gord96,
  author = "Gordon, Mike",
  title = "From LCF to HOL: a short history",
  year = "1996",
  link = "\url{http://www.cl.cam.ac.uk/~mjcg/papers/HolHistory.pdf}",
  paper = "Gord96.pdf"
}

\end{chunk}

\index{Trostle, Anne}
\begin{chunk}{axiom.bib}
@misc{Tros13,
  author = "Trostle, Anne",
  title = "An Algorithm for the Greatest Common Divisor",
  link = "\url{http://www.nuprl.org/MathLibrary/gcd/}",
  year = "2013"
}

\end{chunk}

\index{Babovsky, Hans}
\index{Grabmeier, Johannes}
\begin{chunk}{axiom.bib}
@inproceedings{Babo16,
  author = "Babovsky, Hans and Grabmeier, Johannes",
  title = "Calculus and design of discrete velocity models using
           computer algebra",
  booktitle = "AIP Conference Proceedings",
  volume = "1786",
  isbn = "978-0-7354-1448-8",
  link = "\url{http://aip.scitation.org/doi/pdf/10.1063/1.4967672}",
  year = "2016",
  abstract =
    "In [2,3], a framework for a calculus with Discrete Velocity Models
    (DVM) has been derived. The rotational symmetry of the discrete
    velocities can be modelled algebraically by the action of the cyclic
    group $C_4$ -- or including reflections of the dihedral group $D_4$.
    Taking this point of view, the linearized collision operator can be
    represented in a compact form as a matrix of elements in the group
    algebra. Or in other words, by choosing a special numbering it
    exhibits a certain block structure which lets it appear as a matrix
    with entries in a certain polynomial ring. A convenient way for
    approaching such a structure is the use of a computer algebra system
    able to treat these (predefined) algebraic structures. We use the
    computer algebra system FriCAS/AXIOM [4,5] for the generation of the
    velocity and the collision sets and for the analysis of the structure
    of the collision operator. Concerning the fluid dynamic limit, the
    system provides the characterization of sets of collisions and their
    contribution to the flow parameters. It allows the design of
    rotationally invariant symmetric models for prescribed Prandtl
    numbers.  The implementation in FriCAS/AXIOM is explained and its
    results for a 25-velocity model are presented.",
  paper = "Babo16.pdf",
  keywords = "axiomref"
}

\end{chunk}

\index{Hebisch, Waldemer}
\begin{chunk}{axiom.bib}
@article{Hebi15,
  author = "Hebisch, Waldemer",
  title = "Integration in terms of exponential integrals and incomplete
           gamma functions",
  year = "2015",
  journal = "ACM Communications in Computer Algebra",
  volume = "49",
  Issue = "3",
  pages = "98-100",
  abstract =
    "Indefinite integration means that given $f$ in some set we want to
    find $g$ from possibly larger set such that $f = g^\prime$. When $f$
    and $g$ are required to be elementary functions due to work of among
    others Risch, Rothstein, Trager, Bronstein (see [1] for references)
    integration problem is now solved at least in theory. In his thesis
    Cherry gave algorithm to integrate transcendental elementary functions
    in terms of exponential integrals. In [2] he gave algorithm to
    integrate transcendental elementary functions in so called reduced
    fields in terms of error functions. Knowles [3] and [4] extended this
    allowing also liovillian integrands and weakened restrictions on the
    field containing integrands. We extend previous results allowing
    incomplete gamma function $\Gamma(a, x)$ with rational $a$. Also, our
    theory can handle algebraic extensions and is complete jointly (and
    not only separately for Ei and erf). In purely transcendental case our
    method should be more efficient and easier to implement than [2]. In
    fact, it seems that no system currently implements algorithm from [2],
    while partial implementation of our method in FriCAS works well enough
    to be turned on by default. With our approach non-reduced case from
    [2] can be handled easily. We hope that other classes of special
    functions can be handled in a similar way, in particular irrational
    case of incomplete gamma function and polylogarithms (however
    polylogarithms raise tricky theoretical questions)."
}

\end{chunk}

\index{Knowles, Paul}
\begin{chunk}{axiom.bib}
@article{Know93,
  author = "Knowles, Paul",
  title = "Integration of a Class of Transcendental Liouvillian Functions 
           with Error-Functions, Part I",
  journal = "Journal of Symbolic Computation",
  volume = "13",
  number = "5",
  pages = "525-543",
  year = "1993",
  abstract =
    "This paper gives a decision-procedure for the symbolic integration of
    a certain class of transcendental Liouvillian functions in terms of
    elementary functions and error-functions. An example illustrating the
    use of the decision-procedure is given.",
  paper = "Know93.pdf"
}

\end{chunk}

\index{Knowles, Paul}
\begin{chunk}{axiom.bib}
@article{Know93a,
  author = "Knowles, Paul",
  title = "Integration of a Class of Transcendental Liouvillian Functions 
           with Error-Functions, Part II",
  journal = "Journal of Symbolic Computation",
  volume = "16",
  number = "3",
  year = "1993",
  pages = "227-239",
  abstract =
    "This paper extends the decision procedure for the symbolic
    integration of a certain class of transcendental Liouvillian functions
    in terms of elementary functions and error-functions given in Knowles
    (1992) to allow a much larger class of integrands. Examples
    illustrating the use of the decision procedure are given.",
  paper = "Know93a.pdf"
}

\end{chunk}

\index{Cherry, G.W.}
\begin{chunk}{axiom.bib}
@article{Cher85,
  author = "Cherry, G.W.",
  title = "Integration in Finite Terms with Special Functions: 
           The Error Function",
  journal = "J. Symbolic Computation",
  year = "1985",
  volume = "1",
  pages = "283-302",
  abstract = 
    "A decision procedure for integrating a class of transcendental
    elementary functions in terms of elementary functions and error
    functions is described. The procedure consists of three mutually
    exclusive cases. In the first two cases a generalised procedure for
    completing squares is used to limit the error functions which can
    appear in the integral of a finite number.  This reduces the problem
    to the solution of a differential equation and we use a result of
    Risch (1969) to solve it.  The third case can be reduced to the
    determination of what we have termed $\sum$-decompositions. The resutl
    presented here is the key procuedure to a more general algorithm which
    is described fully in Cherry (1983).",
  paper = "Cher85.pdf"
}

\end{chunk}

\index{Lisonek, Petr}
\index{Paule, Peter},
\index{Strehl, Volker}
\begin{chunk}{axiom.bib}
@article{Liso93,
  author = "Lisonek, Petr and Paule, Peter and Strehl, Volker",
  title = "Improvement of the Degree Setting in Gosper's Algorithm",
  journal = "J. Symbolic Computation",
  volume = "16",
  year = "1993",
  pages = "243-258",
  link = 
   "\url{http://www.sciencedirect.com/science/article/pii/S0747717183710436}",
  abstract =
    "A detailed study of the degree setting for Gosper's algorithm for
    indefinite hypergeometric summation is presented. In particular, we
    discriminate between rational and proper hypergeometric input. As a
    result, the critical degree bound can be improved in the former case.",
  paper = "Liso93.pdf"
}

\end{chunk}

\index{Man, Yiu-Kwong}
\begin{chunk}{axiom.bib}
@article{Manx93,
  author = "Man, Yiu-Kwong",
  title = "On Computing Closed Forms for Indefinite Summations",
  journal = "J. Symbolic Computation",
  volume = "16",
  pages = "335-376",
  year = "1993",
  link = 
   "\url{http://www.sciencedirect.com/science/article/pii/S0747717183710539}",
  abstract =
    "A decision procedure for finding closed forms for indefinite
    summation of polynomials, rational functions, quasipolynomials and
    quasirational functions is presented. It is also extended to deal with
    some non-hypergeometric sums with rational inputs, which are not
    summable by means of Gosper's algorithm. Discussion of its
    implementation, analysis of degree bounds and some illustrative
    examples are included.",
  paper = "Manx93.pdf"
}

\end{chunk}

\index{Paule, Peter}
\begin{chunk}{axiom.bib}
@article{Paul95,
  author = "Paule, Peter",
  title = "Greatest Factorial Factorization and Symbolic Summation",
  journal = "Journal of Symbolic Computation",
  year = "1995",
  volume = "20",
  pages = "235-268",
  link = 
   "\url{http://www.sciencedirect.com/science/article/pii/S0747717185710498}",
  abstract = 
    "The greatest factorial factorization (GFF) of a polynomial provides
    an analogue to square-free factorization but with respect to integer
    shifts instead to multiplicities. We illustrate the fundamental role
    of that concept in the context of symbolic summation. Besides a
    detailed discussion of the basic GFF notions we present a new approach
    to the indefinite rational summation problem as well as to Gosper's
    algorithm for summing hypergeometric sequences.",
  paper = "Paul95.pdf"
}

\end{chunk}

\index{Petkovsek, Marko}
\begin{chunk}{axiom.bib}
@article{Petk92,
  author = "Petkovsek, Marko",
  title = "Hypergeometric solutions of linear recurrences with
           polynomial coefficients",
  journal = "J. Symbolic Computation",
  volume = "14",
  pages = "243-264",
  year = "1992",
  link =
   "\url{http://www.sciencedirect.com/science/article/pii/0747717192900386}",
  abstract =
    "We describe algorithm Hyper which can be used to find all
    hypergeometric solutions of linear recurrences with polynomial
    coefficients.",
  paper = "Petk92.pdf"
}  

\end{chunk}

\index{Grabm\"uller, Martin}
\begin{chunk}{axiom.bib}
@misc{Grab17,
  author = {Grabm\"uller, Martin},
  title = "Algorithm W",
  year = "2017",
  link = "\url{https://github.com/mgrabmueller/AlgorithmW}",
  abstract =
    "In this paper we develop a complete implementation of Algorithm W for 
     Hindley-Milner polyhmorphic type inference in Haskell",
  paper = "Grab17.pdf"
}
\end{chunk}

\index{Heeren, Bastiaan}
\index{Hage, Jurriaan},
\index{Swierstra, Doaitse}
\begin{chunk}{axiom.bib}
@misc{Heer02,
  author = "Heeren, Bastiaan and Hage, Jurriaan and Swierstra, Doaitse",
  title = "Generalizing Hindley-Milner Type Inference Algorithms",
  year = "2002",
  link = "\url{https://pdfs.semanticscholar.org/8983/233b3dff2c5b94efb31235f62bddc22dc899.pdf}",
  abstract =
    "Type inferencing according to the standard algorithms $W$ and $M$
    often yields uninformative error messages. Many times, this is a
    consequence of a bias inherent in the algorithms. The method
    developed here is to first collect constraints from the program, and
    to solve these afterwards, possibly under the influence of a
    heuristic.  We show the soundness and completeness of our algorithm.
    The algorithms $W$ and $M$ turn out to be deterministic instances of our
    method, giving the correctness for $W$ and $M$ with respect to the
    Hindley-Milner typing rules for free.  We also show that our algorithm
    is more flexible, because it naturally allows the generation of
    multiple messages",
  paper = "Heer02.pdf"
}

\end{chunk}

\index{Milner, Robin}
\begin{chunk}{axiom.bib}
@article{Miln78,
  author = "Milner, Robin",
  title = "A Theory of Type Polymorphism in Programming",
  year = "1978",
  journal = "Journal of Computer and System Sciences",
  volume = "17",
  pages = "348-375",
  link = "\url{https://courses.engr.illinois.edu/cs421/sp2013/project/milner-polymorphism.pdf}",
  abstract =
    "The aim of this work is largely a practical one.  A widely employed
    style of programming, particularly in structure-processing languages
    which impose no discipline of types, entails defining procedures which
    work well on objects of a wide variety.  We present a formal type
    discipline for such polymorphic procedures in the context of a simple
    programming language, and a compile time type-checking algorithm $W$
    which enforces the discipline.  A Semantic Soundness Theorem (based on
    a formal semantics for the language) states that well-type programs
    cannot “go wrong” and a Syntactic Soundness Theorem states that if $W$
    accepts a program then it is well typed.  We also discuss extending
    these results to richer languages; a type-checking algorithm based on
    $W$ is in fact already implemented and working, for the metalanguage ML
    in the Edinburgh LCF system,",
  paper = "Miln78.pdf"
}

\end{chunk}

\index{Wadler, Philip}
\index{Blott, Stephen}
\begin{chunk}{axiom.bib}
@inproceedings{Wadl88,
  author = "Wadler, Philip and Blott, Stephen",
  title = "How to make ad-hoc polymorphism less ad hoc",
  booktitle = "Proc 16th ACM SIGPLAN-SIGACT Symp. on Princ. of Prog. Lang",
  isbn = "0-89791-294-2",
  pages = "60-76",
  year = "1988",
  link = "\url{http://202.3.77.10/users/karkare/courses/2010/cs653/Papers/ad-hoc-polymorphism.pdf}",
  abstract =
    "This paper presents {\sl type classes}, a new approach to {\sl ad-hoc}
    polymorphism. Type classes permit overloading of arithmetic operators
    such as multiplication, and generalise the ``eqtype variables'' of
    Standard ML Type classes extend the Hindley/Milner polymorphic type
    system, and provide a new approach to issues that arise in object-oriented
    programming, bounded type quantification, and abstract data types. This 
    paper provides an informal introduction to type classes, and defines them
    formally by means of type inference rules",
  paper = "Wadl88.pdf"
}

\end{chunk}
