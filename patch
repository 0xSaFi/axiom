books/bookvolbib add references

Goal: Proving Axiom Correct

\index{Kohlhase, Michael}
\index{De Feo, Luca}
\index{Muller, Dennis}
\index{Pfeiffer, Markus}
\index{Rabe, Florian}
\index{Thiery, Nicolas M.}
\index{Vasilyev, Victor}
\index{Wesing, Tom}
\begin{chunk}{axiom.bib}
@inproceedings{Kohl17,
  author = "Kohlhase, Michael and De Feo, Luca and Muller, Dennis and
            Pfeiffer, Markus and Rabe, Florian and Thiery, Nicolas M.
            and Vasilyev, Victor and Wesing, Tom",
  title = {{Knowledge-Based Interoperability for Mathematical Software
            Systems}},
  booktitle = "7th Int. Conf. on Mathematical Aspects of Computer and
               Information Sciences",
  publisher = "Springer",
  year = "2017",
  pages = "195-210",
  isbn = "9783319724539",
  abstract = 
    "There is a large ecosystem of mathematical software systems. 
    Individually, these are optimized for particular domains and
    functionalities, and together they cover many needs of practical and
    theoretical mathematics. However, each system specializes on one area,
    and it remains very difficult to solve problems that need to involve
    multiple systems. Some integrations exist, but the are ad-hoc and have
    scalability and maintainability issues. In particular, there is not
    yet an interoperability layer that combines the various systems into a
    virtual research environment (VRE) for mathematics.
    
    The OpenDreamKit project aims at building a toolkit for such VREs. It
    suggests using a central system-agnostic formalization of mathematics
    (Math-in-the-Middle, MitM) as the needed interoperability layer. In
    this paper, we conduct the first major case study that instantiates
    the MitM paradigm for a concrete domain as well as a concrete set of
    systems. Specifically, we integrate GAP, Sage, and Singular to perform
    computation in group and ring theory. 
    
    Our work involves massive practical efforts, including a novel
    formalization of computational group theory, improvements to the
    involved software systems, and a novel mediating system that sits at
    the center of a star-shaped integration layout between mathematical
    software systems.",
  paper = "Kohl17.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Dehaye, Paul-Olivier}
\index{Iancu, Mihnea}
\index{Kohlhase, Michael}
\index{Konovalov, Alexander}
\index{Lelievre, Samuel}
\index{Muller, Dennis}
\index{Pfeiffer, Markus}
\index{Rabe, Florian}
\index{Thiery, Nicolas M.}
\index{Wiesling, Tom}
\begin{chunk}{axiom.bib}
@inproceedings{Deha16,
  author = "Dehaye, Paul-Olivier and Iancu, Mihnea and Kohlhase, Michael
            and Konovalov, Alexander and Lelievre, Samuel and 
            Muller, Dennis and Pfeiffer, Markus and Rabe, Florian and
            Thiery, Nicolas M. and Wiesling, Tom",
  title = {{Interoperability in the OpenDreamKit project: The
           Math-in-the-Middle Approach}},
  booktitle = "Intelligent Computer Mathematics",
  pages = "117-131",
  year = "2016",
  isbn = "9783319425467",
  publisher = "Springer",
  abstract =
    "OpenDreamKit - 'Open Digital Research Environment Toolkit for the
    Advancement of Mathematics' - is an H2020 EU Research Infrastructure
    project that aims at supporting, over the period 2015-2019, the
    ecosystem of open-source mathematical software systems. OpenDreamKit
    will deliver a flexible toolkit enabling research groups to set up
    Virtual Research Environments, customised to meet the varied needs of
    research projects in pure mathematics and applications. 

    An important step in the OpenDreamKit endeavor is to foster the
    interoperability between a variety of systems, ranging from
    computer algebra systems over mathematical databases to
    front-ends. This is the mission of the integration work
    package. We report on experiments and future plans with the
    Math-in-the-Middle approach. This architecture consists of a
    central mathematical ontology that documents the domain and fixes a
    joint vocabulary, or even a language, going beyond existing
    systems such as OpenMath, combined with specifications of the
    functionalities of the various systems. Interaction between
    systems can then be enriched by pivoting around this architecture.",
  paper = "Deha16.pdf",
  keywords = "printed, axiomref"
}

\end{chunk}

\index{de Bruijn, N.G.}
\begin{chunk}{axiom.bib}
@techreport{Brui68a,
  author = "de Bruijn, N.G.",
  title = {{AUTOMATH, A Language for Mathematics}},
  year = "1968",
  type = "technical report",
  number = "68-WSK-05",
  institution = "Technische Hogeschool Eindhoven",
  paper = "Brui68a.pdf"
}

\end{chunk}

\index{Farmer, William M.}
\begin{chunk}{axiom.bib}
@article{Farm07a,
  author = "Farmer, William M.",
  title = {{Chiron: A Multi-Paradigm Logic}},
  journal = "Studies in Logic, Grammar and Rhetoric",
  volume = "10",
  number = "23",
  year = "2007",
  abstract =
    "Chiron is a derivative of von-Neumann-Bernays-GÌˆodel ( NBG ) set
    theory that is intended to be a practical, general-purpose logic for
    mechanizing mathematics. It supports several reasoning paradigms by
    integrating NBG set theory with elements of type theory, a scheme for
    handling undefinedness, and a facility for reasoning about the syntax
    of expressions.  This paper gives a quick, informal presentation of
    the syntax and semantics of Chiron and then discusses some of the
    benefits Chiron provides as a multi-paradigm logic.",
  paper = "Farm07a.pdf",
  keywords = "axiomref, printed"
}

\end{chunk}

\index{Farmer, William M.}
\begin{chunk}{axiom.bib}
@techreport{Farm13,
  author = "Farmer, William M.",
  title = {{Chiron: A Set Theory with Types, Undefinedness, Quotation,
            and Evaluation}},
  type = "technical report",
  institution = "McMaster University",
  number = "SQRL Report No. 38",
  year = "2013",
  link = "\url{https://arxiv.org/pdf/1305.6206.pdf}",
  abstract =
    "Chiron is a derivative of von-Neumann-Bernays-Godel (NBG) set
    theory that is intended to be a practical, general-purpose logic for
    mechanizing mathematics. Unlike traditional set theories such as
    Zermelo-Fraenkel (ZF) and NBG, Chiron is equipped with a type system,
    lambda notation, and definite and indefinite description. The type
    system includes a universal type, dependent types, dependent function
    types, subtypes, and possibly empty types. Unlike traditional logics
    such as first-order logic and simple type theory, Chiron admits
    undefined terms that result, for example, from a function applied to
    an argument outside its domain or from an improper definite or
    indefinite description. The most noteworthy part of Chiron is its
    facility for reasoning about the syntax of expressions. Quotation is
    used to refer to a set called a construction that represents the
    syntactic structure of an expression, and evaluation is used to refer
    to the value of the expression that a construction represents. Using
    quotation and evaluation, syntactic side conditions, schemas,
    syntactic transformations used in deduction and computation rules, and
    other such things can be directly expressed in Chiron. This paper
    presents the syntax and semantics of Chiron, some definitions and
    simple examples illustrating its use, a proof system for Chiron, and a
    notion of an interpretation of one theory of Chiron in another.",
  paper = "Farm13.pdf"
}

\end{chunk}

\index{Kripke, Saul}
\begin{chunk}{axiom.bib}
@article{Krip75,
  author = "Kripke, Saul",
  title = {{Outline of a Theory of Truth}},
  journal = "Journal of Philosophy",
  volume = "72",
  number = "19",
  year = "1975",
  pages = "690-716",
  paper = "Krip75.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Kleene, Stephen Cole}
\begin{chunk}{axiom.bib}
@book{Klee52,
  author = "Kleene, Stephen Cole",
  title = {{Introduction to MetaMathematics}},
  year = "1952",
  publisher = "Ishi Press International",
  isbn = "978-0923891572",
  paper = "Klee52.pdf"
}

\end{chunk}

\index{Smith, Peter}
\begin{chunk}{axiom.bib}
@misc{Smit15,
  author = "Smith, Peter",
  title = {{Some Big Books on Mathematical Logic}},
  year = "2015",
  link = "\url{}",
  paper = "Smit15.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Gonthier, Georges}
\begin{chunk}{axiom.bib}
@article{Gont09a,
  author = "Gonthier, Georges",
  title = {{Software Engineering for Mathematics}},
  journal = "LNCS",
  volume = "5625",
  year = "2009",
  pages = "27-27",
  abstract = 
    "Despite its mathematical origins, progress in computer assisted
    reasoning has mostly been driven by applications in computer science,
    like hardware or protocol security verification. Paradoxically, it has
    yet to gain widespread acceptance in its original domain of
    application, mathematics; this is commonly attributed to a 'lack of
    libraries': attempts to formalize advanced mathematics get bogged down
    into the formalization of an unwieldly large set of basic resuts.

    This problem is actually a symptom of a deeper issue: the main
    function of computer proof systems, checking proofs down to their
    finest details, is at odds with mathematical practice, which ignores
    or defers details in order to apply and combine abstractions in
    creative and elegant ways. Mathematical texts commonly leave logically
    important parts of proofs as 'exercises to the reader', and are rife
    with 'abuses of notation that make mathematics tractable' (according
    to Bourbaki). This (essential) flexibility cannot be readily
    accomodated by the narrow concept of 'proof library' used by most
    proof assistants and based on 19th century first-order logic: a
    collection of constants, definitions, and lemmas.  

    This mismatch is familiar to software engineers, who have been
    struggling for the past 50 years to reconcile the flexibility needed
    to produce sensible user requirements with the precision needed to
    implement them correctly with computer code. Over the last 20 years
    object and components have replaced traditional data and procedure
    libraries, partly bridging this gap and making it possible to build
    significantly larger computer systems.  

    These techniques can be implemented in compuer proof systems by
    exploiting advances in mathematical logic. Higher-order logics allow
    the direct manipulation of functions; this can be used to assign
    behaviour, such as simplification rules, to symbols, similarly to
    objects. Advanced type systems can assign a secondary, contextual
    meaning to expressions, using mechanisms such as type classes, 
    similarly to the metadata in software components. The two can be combined
    to perform reflection, where an entire statement gets quoted as
    metadata and then proved algorithmically by some decision procedure.

    We propose to use a more modest, small-scale form of reflection, to
    implement mathematical components. We use the type-derived metadata to
    indicate how symbols, definitions and lemmas should be used in other
    theories, and functions to implement this usage â€” roughly, formalizing
    some of the exercize section of a textbook. We have applied
    successfully this more engineered approch to computer proofs in our
    past work on the Four Color Theorem, the Cayley-Hamilton Theorem, and
    our ongoing long-term effort on the Odd Order Theorem, which is the
    starting point of the proof of the Classification of Finite Simple
    Groups (the famous 'monster theorem' whose proof spans 10,000 pages in
    400 articles).",
  paper = "Gont09a.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Carette, Jacques}
\begin{chunk}{axiom.bib}
@article{Care10,
  author = "Carette, Jacques",
  title = {{Mechanized Mathematics}},
  journal = "LNCS",
  volume = "6167",
  year = "2010",
  pages = "157-157",
  abstract =
    "In the 50 years since McCarthyâ€™s 'Recursive Functions of Symbolic
    Expressions and Their Computation by Machine', what have we
    learned about the realization of Leibnizâ€™s dream of just being
    able to utter 'Calculemus!' (Let us calculate!) when faced with a
    mathematical dilemma?  

    In this talk, I will first present what I see as the most
    important lessons from the past which need to be heeded by modern
    designers. From the present, I will look at the context in which
    computers are used, and derive further requirements. In
    particular, now that computers are no longer the exclusive
    playground for highly educated scientists, usability is now more
    important than ever, and justifiably so.  

    I will also examine what I see as some principal failings of
    current systems, primarily to understand some major mistakes to
    avoid. These failings will be analyzed to extract what seems to be
    the root mistake, and I will present my favourite solutions.

    Furthermore, various technologies have matured since the creation
    of many of our systems, and whenever appropriate, these should be
    used. For example, our understanding of the structure of
    mathematics has significantly increased, yet this is barely
    reflected in our libraries. The extreme focus on efficiency by the
    computer algebra community, and correctness by the (interactive)
    theorem proving community should no longer be considered viable
    long term strategies.  But how does one effectively bridge that
    gap?  

    I personally find that a number of (programming) language-based
    solutions are particularly effective, and I will emphasize
    these. Solutions to some of these problems will be illustrated
    with code from a prototype of MathScheme 2.0, the system I am
    developing with Bill Farmer and our research group.",
  paper = "Care10.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Zeilberger, Doron}
\begin{chunk}{axiom.bib}
@article{Zeil10,
  author = "Zeilberger, Doron",
  title = {{Against Rigor}},
  journal = "LNCS",
  volume = "6167",
  year = "2010",
  pages = "262-262",
  abstract =
    "The ancient Greeks gave (western) civilization quite a few
    gifts, but we should beware of Greeks bearing gifts. The gifts of theatre
    and democracy were definitely good ones, and perhaps even the gift of
    philosophy, but the 'gift' of the so-called 'axiomatic method' and the
    notion of 'rigorous' proof did much more harm than good. If we want
    to maximize Mathematical Knowledge, and its Management, we have to
    return to Euclid this dubious gift, and give-up our fanatical insistence
    on perfect rigor. Of course, we should not go to the other extreme, of
    demanding that everything should be non-rigorous. We should encourage
    diversity of proof-styles and rigor levels, and remember that nothing is
    absolutely sure in this world, and there does not exist an absolutely
    rigorous proof, nor absolute certainty, and 'truth' has many shades and
    levels.",
  paper = "Zeil10.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Pfenning, Frank}
\begin{chunk}{axiom.bib}
@phdthesis{Pfen87,
  author = "Pfenning, Frank",
  title = {{Proof Transformations in Higher-Order Logic}},
  year = "1987",
  school = "Carnegie Mellon University",
  link = "\url{http://www-cgi.cs.cmu.edu/~fp/papers/thesis87.pdf}",
  abstract =
    "We investigate the problem of translating between different styles of
    proof systems in higher-order logic: analytic proofs which are well
    suited for automated theorem proving, and non-analytic deductions
    which are well suited for the mathematician. Analytic proofs are
    represented as expansion proofs, $H$ , a form of the sequent calculus we
    define, non-analytic proofs are represented by natural deductions. A
    non-deterministic translation algorithm between expansion proofs and 
    $H$-deductions is presented and its correctness is proven. We also
    present an algorithm for translation in the other direction and prove
    its correctness. A cut-elimination algorithm for expansion proofs is
    given and its partial correctness is proven. Strong termination of
    this algorithm remains a conjecture for the full higher-order
    system, but is proven for the first-order fragment. We extend the
    translations to a non-analytic proof system which contains a primitive
    notion of equality, while leaving the notion of expansion proof
    unaltered. This is possible, since a non-extensional equality is
    definable in our system of type theory. Next we extend analytic and
    non-analytic proof systems and the translations between them to
    include extensionality.  Finally, we show how the methods and notions
    used so far apply to the problem of translating expansion proofs into
    natural deductions. Much care is taken to specify this translation in
    a modular way (through tactics) which leaves a large number of choices
    and therefore a lot of room for heuristics to produce elegant natural
    deductions. A practically very useful extension, called symmetric
    simplification, produces natural deductions which make use of lemmas
    and are often much more intuitive than the normal deductions which
    would be created by earlier algorithms.",
  paper = "Pfen87.pdf"
}

\end{chunk}

\index{Comon, Hubert}
\begin{chunk}{axiom.bib}
@incollection{Como01,
  author = "Comon, Hubert",
  title = {{Inductionless Induction}},
  booktitle = "Handbook of Automated Reasoning",
  comment = "Chapter 14",
  publisher = "Elsevier",
  year = "2001",
  paper = "Como01.ps"
}

\end{chunk}

\index{Schmitt, P.H.}
\begin{chunk}{axiom.bib}
@article{Schm86,
  author = "Schmitt, P.H.}",
  title = {{Computational Aspects of Three-Valued Logic}},
  journal = "LNCS",
  volume = "230",
  year = "1986",
  abstract =
    "This paper investigates a three-valued logic $L_3$, that has been
    introduced in the study of natural language semantics. A complete
    proof system based on a three-valued analogon of negative resolution
    is presented.  A subclass of $L_3$ corresponding to Horn clauses in
    two-valued logic is defined. Its model theoretic properties are
    studied and it is shown to admit a PROLOG-style proof procedure.",
  paper = "Schm86.pdf"
}

\end{chunk}

\index{Common, Hubert}
\index{Lescanne, Pierre}
\begin{chunk}{axiom.bib}
@article{Como89,
  author = "Common, Hubert and Lescanne, Pierre",
  title = {{Equational Problems and Disunification}},
  journal = "J. Symbolic Computation",
  volume = "7",
  number = "3-4",
  year = "1989",
  pages = "371-425",
  abstract =
    "Roughly speaking, an equational problem is a first order formula
    whose only predicate symbol is $=$. We propose some rules for the
    transformation of equational problems and study their correctness in
    various models. Then, we give completeness results with respect to
    some 'simple' problems called solved forms. Such completeness results
    still hold when adding some control which moreover ensures
    termination. The termination proofs are given for a 'weak' control and
    thus hold for the (large) class of algorithms obtained by restricting
    the scope of the rules. Finally, it must be noted that a by-product of
    our method is a decision procedure for the validity in the Herbrand
    Universe of any first order formula with the only predicate symbol $=$.",
  paper = "Como89.pdf"
}  

\end{chunk}

\index{Jones, Simon > Peyton}
\begin{chunk}{axiom.bib}
@inproceedings{Jone96,
  author = "Jones, Simon > Peyton",
  title = {{Compiling Haskell by Program Transformation: A Report from
            the Trenches}},
  booktitle = "Proc. European Symposium on Programming",
  year = "1996",
  publisher = "Eurographics",
  abstract =
    "Many compilers do some of their work by means of
    correctness-preseving, and hopefully performance-improving, program
    transformations. The Glasgow Haskell Compiler (GHC) takes this idea
    of 'compilation by transformation' as its war-cry, trying to express
    as much as possible of the compilation process in the form of
    program transformations.

    This paper reports on our practical experience of the
    transformational approach to compilation, in the context of a
    substantial compiler.",
  paper = "Jone96.pdf"
}

\end{chunk}

\index{Hanus, Michael}
\begin{chunk}{axiom.bib}
@article{Hanu90,
  author = "Hanus, Michael",
  title = {{Compiling Logic Programs with Equality}},
  journal = "LNCS",
  volume = "456",
  year = "1990",
  pages = "387-401",
  abstract =
    "Horn clause logic with equality is an amalgamation of functional and
    logic programming languages. A sound and complete operational
    semantics for logic programs with equality is based on resolution to
    solve literals, and rewriting and narrowing to evaluate functional
    expressions. This paper proposes a technique for compiling programs
    with these inference rules into programs of a low-level abstract
    machine which can be efficiently executed on conventional
    architectures. The presented approach is based on an extension of the
    Warren abstract machine (WAM). In our approach pure logic programs
    without function definitions are compiled in the same way as in the
    WAM-approach, and for logic programs with function definitions
    particular instructions are generated for occurrences of functions
    inside clause bodies. In order to obtain an efficient implementation
    of functional computations, a stack of occurrences of function symbols
    in goals is managed by the abstract machine. The compiler generates
    the necessary instructions for the efficient manipulation of the
    occurrence stack from the given equational logic programs.",
  paper = "Hanu90.pdf"
}

\end{chunk}

\index{Ballerin, Clemens}
\begin{chunk}{axiom.bib}
@phdthesis{Ball99a,
  author = "Ballerin, Clemens",
  title = {{Computer Algebra and Theorem Proving}},
  school = "Darwin College, University of Cambridge",
  year = "1999",
  abstract =
    "Is the use of computer algebra technology beneficial for
    mechanised reasoining in and about mathematical domains? Usually
    it is assumed that it is. Many works in this area, however, either
    have little reasoning conent, or use symbolic computation only to
    simplify expressions. In work that has achieved more, the used
    methods do not scale up. They trust the computer algebra system
    either too much or too little.

    Computer algebra systems are not as rigorous as many provers. They
    are not logically sound reasoning systems, but collections of
    algorithms. We classify soundness problems that occur in computer
    algebra systems. While many algorithms and their implementations
    are perfectly trustworthy, the semantics of symbols is often
    unclear and leads to errors. On the other hand, more robust
    approaches to interface external reasoners to provers are not
    always practical because the mathematical depth of proofs
    algorithms in computer algebra are based on can be enormous.

    Our own approach takes both trustworthiness of the overall system
    and efficiency into account. It relies on using only reliable
    parts of a computer algebra system, which can be achieved by
    choosing a suitable library, and deriving specifications for these
    algorithms from their literature.

    We design and implement an interface between the prover Isabelle
    and the computer algebra library Sumit and use it to prove
    non-trivial theorems from coding theory. This is based on the
    mechanisation of the algebraic theories of rings and
    polynomials. Coding theory is an area where proofs do have a
    substantial amount of computational content. Also, it is realistic
    to assume that the verification of an encoding or decoding device
    could be undertaken in, and indeed, be simplified by, such a
    system. 

    The reason why semantics of symbols is often unclear in current
    computer algebra systems is not mathematical difficulty, but the
    design of those systems. For Gaussian elimination we show how the
    soundness problem can be fixed by a small extension, and without
    losing efficiency. This is a prerequisite for the efficient use of
    the algorithm in a prover.",
  paper = "Ball99a.pdf",
  keywords = "axiomref, printed"
}

\end{chunk}

\index{Nederpelt, Rob}
\index{Geuvers, Nerman}
\begin{chunk}{axiom.bib}
@book{Nede14,
  author = "Nederpelt, Rob and Geuvers, Nerman",
  title = {{Type Theory and Formal Proof}},
  year = "2014",
  publisher = "Cambridge University Press",
  isbn = "978-1-107-03650-5"
}

\end{chunk}

\index{Robinson, Alan}
\index{Voronkov, Andrei}
\begin{chunk}{axiom.bib}
@book{Robi01,
  author = "Robinson, Alan and Voronkov, Andrei",
  title = {{Handbook of Automated Reasoning (2 Volumes)}},
  year = "2001",
  publisher = "MIT Press",
  isbn = "0-262-18223-8"
}

\end{chunk}

\index{Barendregt, Henk}
\index{Wiedijk, Freek}
\begin{chunk}{axiom.bib}
@article{Bare05,
  author = "Barendregt, Henk and Wiedijk, Freek",
  title = {{The Challenge of Computer Mathematics}},
  journal = "Phil. Trans. Royal Society",
  volume = "363",
  pages = "2351-2375",
  year = "2005",
  abstract =
    "Progress in the foundations of mathematics has made it possible to
    formulate all thinkable mathematical concepts, algorithms and proofs
    in one language and in an impeccable way. This is not in spite of, but
    partially based on the famous results of Godel and Turing. In this way
    statements are about mathematical objects and algorithms, proofs show
    the correctness of statements and computations, and computations are
    dealing with objects and proofs. Interactive computer systems for a
    full integration of defining, computing and proving are based on
    this. The human defines concepts, constructs algorithms and provides
    proofs, while the machine checks that the definitions are well formed
    and the proofs and computations are correct. Results formalized so far
    demonstrate the feasibility of this â€˜computer mathematicsâ€™. Also there
    are very good applications. The challenge is to make the systems more
    mathematician-friendly, by building libraries and tools. The eventual
    goal is to help humans to learn, develop, communicate, referee and
    apply mathematics.",
  paper = "Bare05.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Klaeren, Herbert A.}
\begin{chunk}{axiom.bib}
@article{Klae84,
  author = "Klaeren, Herbert A.",
  title = {{A Constructive Method for Abstract Algebraic Software
            Specification}},
  journal = "Theoretical Computer Science",
  vollume = "30",
  year = "1984",
  pages = "139-204",
  abstract =
    "A constructive method for abstract algebraic software
    specification is presented, where the operations are not
    implicitly specified by equations but by an explicit recursion on
    the generating operations of an algebra characterizing the
    underlying data structure. The underlying algebra itself may be
    equationally specified since we cannot assume that all data
    structures will correspond to free algebras. This implies that we
    distinguish between generating and defined operations and that the
    underlying algebra has a mechanism of well-founded decomposition
    w.r.t. the generating operations. We show that the explicit
    specification of operations using so-called structural recursive
    schemata offers advantages over purely equational specifications,
    especially concerning the safeness of enrichments, the ease of
    semantics description and the separation between the underlying
    data structure and the operations defined on it.",
  keywords = "printed"
}

\end{chunk}

\index{Pfenning, Frank}
\begin{chunk}{axiom.bib}
@misc{Pfen04,
  author = "Pfenning, Frank",
  title = {{Automated Theorem Proving}},
  year = "2004",
  comment = "Draft of Spring 2004",
  keywords = "printed"
}

\end{chunk}

\index{Coquuand, Thierry}
\index{Huet, Gerard}
\begin{chunk}{axiom.bib}
@incollection{Coqu88,
  author = "Coquuand, Thierry and Huet, Gerard",
  title = {{The Calculus of Constructions}},
  booktitle = "Information and Computation, Volume 76",
  year = "1988",
  publisher = "Academic Press",
  paper = "Coqu88.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Okasaki, Chris}
\begin{chunk}{axiom.bib}
@phdthesis{Okas96,
  author = "Okasaki, Chris",
  title = {{Purely Functional Data Structures}},
  school = "Carnegie Mellon University",
  year = "1996",
  link = "\url{}",
  comment = "CMU-CS-96-177",
  isbn = "978-0521663502",
  abstract =
    "When a C programmer needs an efficient data structure for a
    particular problem, he or she can often simply look one up in any of
    a number of good textbooks or handbooks.  Unfortunately, programmers
    in functional languages such as Standard ML or Haskell do not have
    this luxury.  Although some data structures designed for imperative
    languages such as C can be quite easily adapted to a functional
    setting, most cannot, usually because they depend in crucial ways on
    assignments, which are disallowed, or at least discouraged, in
    functional languages.  To address this imbalance, we describe several
    techniques for designing functional data structures, and numerous
    original data structures based on these techniques, including multiple
    variations of lists, queues, double-ended queues, and heaps, many
    supporting more exotic features such as random access or efficient
    catenation.

    In addition, we expose the fundamental role of lazy evaluation in
    amortized functional data structures.  Traditional methods of
    amortization break down when old versions of a data structure, not
    just the most recent, are available for further processing.  This
    property is known as persistence , and is taken for granted in
    functional languages.  On the surface, persistence and amortization
    appear to be incompatible, but we show how lazy evaluation can be used
    to resolve this conflict, yielding amortized data structures that are
    efficient even when used persistently.  Turning this relationship
    between lazy evaluation and amortization around, the notion of
    amortization also provides the first practical techniques for
    analyzing the time requirements of non-trivial lazy programs.

    Finally, our data structures offer numerous hints to programming
    language designers, illustrating the utility of combining strict and
    lazy evaluation in a single language, and providing non-trivial
    examples using polymorphic recursion and higher-order, recursive
    modules.",
  paper = "Okas96.pdf"
}

\end{chunk}

\index{Letouzey, Pierre}
\begin{chunk}{axiom.bib}
@inproceedings{Leto04,
  author = "Letouzey, Pierre",
  title = {{A New Extraction for Coq}},
  booktitle = "Types for Proofs and Programs. TYPES 2002",
  publisher = "Springer",
  pages = "617-635",
  year = "2004",
  abstract =
    "We present here a new extraction mechanism for the Coq proof
    assistant. By extraction, we mean automatic generation of
    functional code from Coq proofs, in order to produce certified
    programs. In former versions of Coq, the extraction mechanism
    suffered several limitations and in particular worked only with a
    subset of the language. We first discuss difficulties encountered
    and solutions proposed to remove these limitations. Then we give a
    proof of correctness for a theoretical model of the new
    extraction. Finally we describe the actual implementation.",
  paper = "Leto04.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Abrams, Marshall D.}
\index{Zelkowitz, Marvin V.}
\begin{chunk}{axiom.bib}
@article{Abra95,
  author = "Abrams, Marshall D. and Zelkowitz, Marvin V.",
  title = {{Striving for Correctness}},
  journal = "Computers and Security",
  volume = "14",
  year = "1995",
  pages = "719-738",
  paper = "Abra95.pdf"
}

\end{chunk}

\index{Parnas, David L.}
\begin{chunk}{axiom.bib}
@article{Parn72,
  author = "Parnas, David L.",
  title = {{A Technique for Software Module Specification with 
            Examples}},
  journal = "CACM",
  volume = "15",
  number = "5",
  year = "1972",
  pages = "330-336",
  abstract =
    "This paper presents an approach to writing specifications for
    parts of software systems. The main goal is to provide
    specifications sufficiently precise and complete that other pieces
    of software can be written to interact with the piece specified
    without additional information. The secondary goal is to include
    in the specification no more information than necessary to meet
    the first goal. The technique is illustrated by means of a variety
    of examples from a tutorial system.",
  paper = "Parn72.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Johnson, C.W.}
\begin{chunk}{axiom.bib}
@article{John96,
  author = "Johnson, C.W.",
  title = {{Literate Specifications}},
  journal = "Software Engineering Journal",
  volume = "11",
  number = "4",
  year = "1996",
  pages = "225-237",
  paper = "John96.pdf",
  abstract =
    "The increasing scale and complexity of software is imposing serious
    burdens on many industries. Formal notations, such as Z, VDM and
    temporal logic, have been developed to address these problems. There
    are, however, a number of limitations that restrict the use of
    mathematical specifications far large-scale software development. Many
    projects take months or years to complete. This creates difficulties
    because abstract mathematical requirements cannot easily be used by
    new members of a development team to understand past design
    decisions. Formal specifications describe what software must do, they
    do not explain why it must do it. In order to avoid these limitations,
    a literate approach to software engineering is proposed. This
    technique integrates a formal specification language and a semi-formal
    design rationale.  The Z schema calculus is usecl to represent what a
    system must do. In contrast, the Questions, Options and Criteria
    notation is used to represent the justifications that lie behind
    development decisions. Empirical evidence is presented that suggests
    the integration of these techniques provides significant benefits over
    previous approaches to mathematical analysis and design techniques. A
    range of tools is described that have been developed to support our
    literate approach to the specification of large-scale sohare systems.",
  keywords = "printed"
}

\end{chunk}

\index{Finney, Kate}
\begin{chunk}{axiom.bib}
@article{Finn96,
  author = "Finney, Kate",
  title = {{Mathematical Notation in Formal Specifications: Too
            Difficult for the Masses?}},
  journal = "IEEE Trans. on Software Engineering",
  volume = "22",
  number = "2",
  year = "1996",
  pages = "158-159",
  abstract =
    "The phrase 'not much mathematics required' can imply a
    variety of skill levels. When this phrase is applied to computer
    scientists, software engineers, and clients in the area of formal
    specification, the word 'much' can be widely misinterpreted with
    disastrous consequences. A small experiment in reading
    specifications revealed that students already trained in discrete
    mathematics and the specification notation performed very poorly;
    much worse than could reasonably be expected if formal methods
    proponents are to be believed.",
  paper = "Finn96.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Moore, Andrew P.}
\index{Payne Jr., Charles N.}
\begin{chunk}{axiom.bib}
@inproceedings{Moor96,
  author = "Moore, Andrew P. and Payne Jr., Charles N.",
  title = {{Increasing Assurance with LIterate Programming Techniques}},
  booktitle = "Comf. on Computer Assurance COMPASS'96",
  publisher = "National Institute of Standards and Technology",
  year = "1996",
  pages = "187-198",
  abstract =
    "The assurance argument that a trusted system satisfies its
    information security requirements must be convincing, because the
    argument supports the accreditation decision to allow the computer to
    process classified information in an operational
    environment. Assurance is achieved through understanding, but some
    evidence that supports the assurance argument can be difficult to
    understand. This paper describes a novel applica- tion of a technique,
    called literate programming [ll], that significantly improves the
    readability of the assur- ance argument while maintaining its
    consistency with formal specifications that are input to specification
    and verification systems. We describe an application c,f this
    technique to a simple example and discuss the lessons learned from
    this effort.",
  paper = "Moor96.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Nissanke, Nimal}
\begin{chunk}{axiom.bib}
@book{Niss99,
  author = "Nissanke, Nimal",
  title = {{Formal Specification}},
  publisher = "Springer",
  year = "1999",
  isbn = "978-1-85233-002-6",
  paper = "Niss99.pdf"
}

\end{chunk}

\index{Vienneau, Robert L.}
\begin{chunk}{axiom.bib}
@misc{Vien93,
  author = "Vienneau, Robert L.",
  title = {{A Review of Formal Methods}},
  year = "1993",
  paper = "Vien93.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Murthy, S.G.K}
\index{Sekharam, K. Raja}
\begin{chunk}{axiom.bib}
@article{Murt09,
  author = "Murthy, S.G.K and Sekharam, K. Raja",
  title = {{Software Reliability through Theorem Proving}},
  journal = "Defence Science Journal",
  volume = "59",
  number = "3",
  year = "2009",
  pages = "314-317",
  abstract =
    "Improving software reliability of mission-critical systems is
    widely recognised as one of the major challenges.  Early detection
    of errors in software requirements, designs and implementation,
    need rigorous verification and validation techniques. Several
    techniques comprising static and dynamic testing approaches are
    used to improve reliability of mission critical software; however
    it is hard to balance development time and budget with software
    reliability. Particularly using dynamic testing techniques, it is
    hard to ensure software reliability, as exhaustive testing is not
    possible. On the other hand, formal verification techniques
    utilise mathematical logic to prove correctness of the software
    based on given specifications, which in turn improves the
    reliability of the software.  Theorem proving is a powerful formal
    verification technique that enhances the software reliability for
    mission- critical aerospace applications. This paper discusses the
    issues related to software reliability and theorem proving used to
    enhance software reliability through formal verification
    technique, based on the experiences with STeP tool, using the
    conventional and internationally accepted methodologies, models,
    theorem proving techniques available in the tool without proposing
    a new model.",
  paper = "Murt09.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Paulson, Lawrence C.}
\begin{chunk}{axiom.bib}
@incollection{Paul90a,
  author = "Paulson, Lawrence C.",
  title = {{Isabelle: The Next 700 Theorem Provers}},
  booktitle = "Logic and Computer Science",
  publisher = "Academic Press",
  pages = "361-386",
  year = "1990",
  paper = "Paul90a.pdf"
}

\end{chunk}

\index{Beeson, M.}
\begin{chunk}{axiom.bib}
@article{Bees16,
  author = "Beeson, M.",
  title = {{Mixing Computations and Proofs}},
  journal = "J. of Formalized Reasoning",
  volume = "9",
  number = "1",
  pages = "71-99",
  year = "2016",
  abstract =
    "We examine the relationship between proof and computation in
    mathematics, especially in formalized mathematics.  We compare the
    various approaches to proofs with a significant computational
    component, including (i) verifying the algorithms, (ii) verifying the
    results of the unverified algo- rithms, and (iii) trusting an external
    computation.",
  paper = "Bees16.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Beeson, Michael}
\begin{chunk}{axiom.bib}
@misc{Bees14,
  author = "Beeson, M.",
  title = {{Mixing Computations and Proofs}},
  comment = "slides",
  year = "2014",
  link = "\url{http://www.cs.ru.nl/qed20/slides/beeson-qed20.pdf}",
  paper = "Bees14.pdf"
}

\end{chunk}

\index{Gunter, Elsa L.}
\begin{chunk}{axiom.bib}
@inproceedings{Gunt89a,
  author = "Gunter, Elsa L.",
  booktitle = "Int. Workshop on Extensions of Logic Programming",
  publisher = "Springer",
  year = "1989",
  pages = "223-244",
  abstract =
    "In this article, we discuss several possible extensions to
    traditional logic programming languages. The specific extensions
    proposed here fall into two categories: logical extensions and the
    addition of constructs to allow for increased control.  There is a
    unifying theme to the proposed logical extensions, and that is the
    scoped introduction of extensions to a programming context. More
    specifically, these extensions are the ability to introduce variables
    whose scope is limited to the term in which they occur (i.e. A-bound
    variables within A-terms), the ability to intro~iuce into a goal a
    fresh constant whose scope is limited to the derivation of that goal,
    and the ability to introduce into a goal a program clause whose scope
    is limited to the derivation of that goal. The purpose of the
    additions for increased control is to facilitate the raising and
    handling of failures.
    
    To motivate these various extensions, we have repeatedly appealed to
    examples related to the construction of a generic theorem prover. It
    is our thesis that this problem domain is specific enough to lend
    focus when one is considering various language constructs, and yet
    complex enough to encompass many of the general difficulties found in
    other areas of symbolic computation.",
  paper = "Gunt89a.pdf"
}

\end{chunk}

\index{Paulson, Lawrence C.}
\index{Smith, Andrew W.}
\begin{chunk}{axiom.bib}
@inproceedings{Paul89,
  author = "Paulson, Lawrence C. and Smith, Andrew W.",
  title = {{Logic Programming, Functional Programming, and 
            Inductive Definitions}},
  booktitle = "Int. Workshop on Extensions of Logic Programming",
  publisher = "Springer",
  year = "1989",
  pages = "283-309",
  abstract =
    "The unification of logic and functional programming, like the Holy
    Grail, is sought by countless people [6, 14]. In reporting our
    attempt, we first discuss the motivation. We argue that logic
    programming is still immature, compared with functional programming,
    because few logic programs are both useful and pure.  Functions can
    help to purify logic programming, for they can eliminate certain uses
    of the cut and can express certMn negations positively.

    More generally, we suggest that the traditional paradigm -- logic
    programming as first-order logic -- is seriously out of step with
    practice. We offer an alternative paradigm. We view the logic program
    as an inductive definition of sets and relations. This view explains
    certain uses of Negation as Failure, and explains why most attempts to
    extend PROLO G to larger fragments of first-order logic have not been
    successful. It suggests a logic language with functions, 
    incorporating equational unification.

    We have implemented a prototype of this language. It is very slow,
    but complete, and appear to be faster than some earlier
    implementations of narrowing.  Our experiments illustrate the
    programming style and shed light on the further development of such
    languages.",
  paper = "Paul89.pdf"
}

\end{chunk}

\index{Weirich, Jim}
\begin{chunk}{axiom.bib}
@misc{Weir12,
  author = "Weirich, Jim",
  title = {{Y Not? -- Adventures in Functional Programming}},
  year = "2012",
  link = "\url{https://www.infoq.com/presentations/Y-Combinator}",
  abstract = "Explains the Y-Combinator"
}

\end{chunk}

\index{Beeson, Michael}
\begin{chunk}{axiom.bib}
@article{Bees06,
  author = "Beeson, Michael",
  title = {{Mathematical Induction in Otter-Lambda}},
  journal = "J. Automated Reasoning",
  volume = "36",
  number = "4",
  pages = "311'344",
  abstract =
    "Otter-lambda is Otter modified by adding code to implement an
    algorithm for lambda unification. Otter is a resolution-based,
    clause-language first-order prover that accumulates deduced clauses
    and uses strategies to control the deduction and retention of
    clauses. This is the first time that such a first-order prover has
    been combined in one program with a unification algorithm capable of
    instantiating variables to lambda terms to assist in the
    deductions. The resulting prover has all the advantages of the
    proof-search algorithm of Otter (speed, variety of inference rules,
    excellent handling of equality) and also the power of lambda
    unification. We illustrate how these capabilities work well together
    by using Otter-lambda to find proofs by mathematical induction. Lambda
    unification instantiates the induction schema to find a useful
    instance of induction, and then Otter's first-order reasoning can be
    used to carry out the base case and induction step. If necessary,
    induction can be used for those, too. We present and discuss a variety
    of examples of inductive proofs found by Otter-lambda: some in pure
    Peano arithmetic, some in Peano arithmetic with defined predicates,
    some in theories combining algebra and the natural numbers, some
    involving algebraic simplification (used in the induction step) by
    simplification code from MathXpert, and some involving list induction
    instead of numerical induction. These examples demonstrate the
    feasibility and usefulness of adding lambda unification to a
    first-order prover.",
  papers = "Bees06.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Barendregt, Henk}
\index{Barendsen, Erik}
\begin{chunk}{axiom.bib}
@article{Bare02,
  author = "Barendregt, Henk and Barendsen, Erik",
  title = {{Autorkic Computations in Formal Proofs}},
  journal = "J. Automated Reasoning",
  volume = "28",
  pages = "321-336",
  year = "2002",
  abstract =
    "Formal proofs in mathematics and computer science are being studied
    because these objects can be verified by a very simple computer
    program. An important open problem is whether these formal proofs can
    be generated with an effort not much greater than writing a
    mathematical paper in, say, LATEX. Modern systems for proof
    development make the formalization of reasoning relatively
    easy. However, formalizing computations in such a manner that the
    results can be used in formal proofs is not immediate. In this paper
    we show how to obtain formal proofs of statements such as Prime(61) in
    the context of Peano arithmetic or $(x + 1)(x + 1) = x 2 + 2x + 1$ in
    the context of rings. We hope that the method will help bridge the gap
    between the efficient systems of computer algebra and the reliable
    systems of proof development.",
  paper = "Bare02.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Beeson, Michael}
\begin{chunk}{axiom.bib}
@article{Bees95,
  author = "Beeson, Michael",
  title = {{Using Nonstandard Analysis to Ensure the Correctness of
            Symbolic Computations}},
  journal = "Int. J. of Foundations of Computer Science",
  volume = "6",
  number = "3",
  pages = "299-338",
  year = "1995",
  abstract =
    "Nonstandard analysis has been put to use in a theorem-prover,
    where it assists in the analysis of formulae involving limits. The
    theorem-prover in question is used in the computer program
    Mathpert to ensure the correctness of calculations in
    calculus. Although non-standard analysis is widely viewed as
    non-constructive, it can alternately be viewed as a method of
    reducing logical manipulation (of formulae with quantifiers) to
    coputation (with rewrite rules). We give a logical theory of
    nonstandard analysis which is implemented in Mathpert. We describe
    a procedure for 'elimination of infinitesimals' (also implemented
    in Mathpert) and prove its correctness.",
  paper = "Bees95.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Beeson, Michael}
\index{Wiedijk, Freek}
\begin{chunk}{axiom.bib}
@article{Bees02,
  author = "Beeson, Michael and Wiedijk, Freek",
  title = {{The Meaning of Infinity in Calculus and Computer Algebra
            Systems}},
  journal = "LNCS",
  volume = "2385",
  year = "2002",
  abstract =
    "We use filters of open sets to provide a semantics justifying the
    use of infinity in informal limit calculations in calculus, and in
    the same kind of calculations in computer algebra. We compare the
    behavior of these filters to the way Mathematica behaves when
    calculating with infinity.

    We stress the need to have a proper semantics for computer algebra
    expressions, especially if one wants to use results and methods
    from computer algebra in theorem provers. The computer algebra
    method under discussion in this paper is the use of rewrite rules
    to evaluate limits involving infinity.",
  paper = "Bees02.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Zimmer, Jurgen}
\index{Dennis, Louise A.}
\begin{chunk}{axiom.bib}
@article{Zimm02,
  author = "Zimmer, Jurgen and Dennis, Louise A.",
  title = {{Inductive Theorem Proving and Computer Algebra in the
            MathWeb Software Bus}},
  journal = "LNCS",
  volume = "2385",
  year = "2002",
  abstract =
    "Reasoning systems have reached a high degree of maturity in the last
    decade. However, even the most successful systems are usually not
    general purpose problem solvers but are typically specialised on 
    problems in a certain domain. The MathWeb Software Bus (MathWeb-SB) is a
    system for combining reasoning specialists via a common software bus.
    We describe the integration of the $\lambda$-Clam system, a reasoning
    specialist for proofs by induction, into the MathWeb-SB. Due to this
    integration, $\lambda$-Clam now offers its theorem proving expertise 
    to other systems in the MathWeb-SB. On the other hand,
    $\lambda$-Clam can use the
    services of any reasoning specialist already integrated. We focus on
    the latter and describe first experiments on proving theorems by
    induction using the computational power of the Maple system within
    $\lambda$-Clam.",
  paper = "Zimm02.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Campbell, J.A.}
\begin{chunk}{axiom.bib}
@article{Camp02,
  author = "Campbell, J.A.",
  title = {{Indefinite Integration as a Testbed for Developments in
            Multi-agent Systems}},
  journal = "LNCS",
  volume = "2385",
  year = "2002",
  abstract =
    "Coordination of multiple autonomous agents to solve problems that
    require each of them to contribute their limited expertise in the
    construction of a solution is often ensured by the use of numerical
    methods such as vote-counting, payoff functions, game theory and 
    economic criteria. In areas where there are no obvious numerical methods
    for agents to use in assessing other agentsâ€™ contributions, many 
    questions still remain open for research. The paper reports a study of
    one such area: heuristic indefinite integration in terms of agents with
    different single heuristic abilities which must cooperate in finding
    indefinite integrals. It examines the reasons for successes and lack
    of success in performance, and draws some general conclusions about
    the usefulness of indefinite integration as a field for realistic
    tests of methods for multi-agent systems where the usefulness of
    'economic' criteria is limited. In this connection, the role of
    numerical taxonomy is emphasised.",
  paper = "Camp02.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Wiedijk, Freek}
\begin{chunk}{axiom.bib}
@misc{Wied18,
  author = "Wiedijk, Freek",
  title = {{Formaizing 100 Theorems}},
  link = "\url{http://www.cs.ru.nl/~freek/100/}",
  year = "2018"
}

\end{chunk}

\index{Wiedijk, Freek}
\begin{chunk}{axiom.bib}
@misc{Wied08,
  author = "Wiedijk, Freek",
  title = {{Formai Proof -- Geting Started}},
  link = "\url{https://www.ams.org/notices/200811/tx081101408p.pdf}",
  year = "2008",
  paper = "Wied08.pdf"
}

\end{chunk}

\index{Kaliszyk, Cezary}
\begin{chunk}{axiom.bib}
@article{Kali08,
  author = "Kaliszyk, Cezary",
  title = {{Automating Side Conditions in Formalized Partial Functions}},
  journal = "LNCS",
  volume = "5144",
  year = "2008",
  abstract =
    "Assumptions about the domains of partial functions are necessary in
    state-of-the-art proof assistants. On the other hand when
    mathematicians write about partial functions they tend not to
    explicitly write the side conditions. We present an approach to
    formalizing partiality in real and complex analysis in total
    frameworks that allows keeping the side conditions hidden from the
    user as long as they can be computed and simplified
    automatically. This framework simplifies defining and operating on
    partial functions in formalized real analysis in HOL Light. Our
    framework allows simplifying expressions under partiality conditions
    in a proof assistant in a manner that resembles computer algebra
    systems.", 
  paper = "Kali08.pdf"
}

\end{chunk}


\index{Kaliszyk, Cezary}
\index{Wiedijk, Freek}
\begin{chunk}{axiom.bib}
@article{Kali08a,
  author = "Kaliszyk, Cezary and Wiedijk, Freek",
  title = {{Merging Procedural and Declarative Proof}},
  journal = "LNCS",
  volume = "5497",
  year = "2008",
  abstract =
    "There are two different styles for writing natural deduction proofs:
    the 'Gentzen' style in which a proof is a tree with the conclusion at
    the root and the assumptions at the leaves, and the 'Fitch' style
    (also called â€˜flagâ€™ style) in which a proof consists of lines that are
    grouped together in nested boxes.  
    
    In the world of proof assistants these two kinds of natural deduction
    correspond to procedural proofs (tactic scripts that work on one or
    more subgoals, like those of the Coq, HOL and PVS systems), and
    declarative proofs (like those of the Mizar and Isabelle/Isar
    languages).  
    
    In this paper we give an algorithm for converting tree style proofs to
    flag style proofs. We then present a rewrite system that simplifies
    the results.  
    
    This algorithm can be used to convert arbitrary procedural proofs to
    declarative proofs. It does not work on the level of the proof terms
    (the basic inferences of the system), but on the level of the
    statements that the user sees in the goals when constructing the
    proof.  
    
    The algorithm from this paper has been implemented in the ProofWeb
    interface to Coq. In ProofWeb a proof that is given as a Coq proof
    script (even with arbitrary Coq tactics) can be displayed both as a
    tree style and as a flag style proof.",
  paper = "Kali08a.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Akbarpour, Behzad}
\index{Paulson, Lawrence C.}
\begin{chunk}{axiom.bib}
@article{Akba08a,
  author = "Akbarpour, Behzad and Paulson, Lawrence C.",
  title = {{MetiTarski: An Automatic Prover for the Elementary Functions}},
  journal = "LNCS",
  volume = "5144",
  year = "2008",
  abstract = 
    "Many inequalities involving the functions ln, exp, sin, cos, etc.,
    can be proved automatically by MetiTarski: a resolution theorem prover
    (Metis) modified to call a decision procedure (QEPCAD) for the theory
    of real closed fields. The decision procedure simplifies clauses by
    deleting literals that are inconsistent with other algebraic facts,
    while deleting as redundant clauses that follow algebraically from
    other clauses.  MetiTarski includes special code to simplify
    arithmetic expressions.",
  paper = "Akba08a.pdf"
}

\end{chunk}

\index{Cramer, Marcos}
\begin{chunk}{axiom.bib}
@misc{Cram14,
  author = "Cramer, Marcos",
  title = {{Modelling the usage of partial functions and undefined
            terms using presupposition theory}},
  year = "2014",
  isbn = "978-1-84890-130-8",
  publisher = "College Publications",
  pages = "71-88",
  abstract =
    "We describe how the linguistic theory of presuppositions can be used
    to analyse and model the usage of partial functions and undefined
    terms in mathematical texts. We compare our account to other accounts
    of partial functions and undefined terms, showing how our account
    models the actual usage of partial functions and undefined terms more
    faithfully than existing accounts. The model described in this paper
    has been developed for the Naproche system, a computer system for
    proof-checking mathematical texts written in controlled natural
    language, and has largely been implemented in this system.",
  paper = "Cram14.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Freundt, Sebastian}
\index{Horn, Peter}
\index{Konovalov, Alexander}
\index{Linton, Steve}
\index{Roozemond, Dan}
\begin{chunk}{axiom.bib}
@article{Freu08,
  author = "Freundt, Sebastian and Horn, Peter and Konovalov, Alexander
            and Linton, Steve and Roozemond, Dan",
  title = {{Symbolic Computation Software Composability}},
  journal = "LNCS",
  volume = "5144",
  year = "2008",
  abstract =
    "We present three examples of the composition of Computer Algebra
    Systems to illustrate the progress on a composability infrastructure
    as part of the SCIEnce (Symbolic Computation Infrastructure for
    Europe) project 1 . One of the major results of the project so far is
    an OpenMath based protocol called SCSCP (Symbolic Computation Software
    Composability Protocol). SCSCP enables the various software packages
    for example to exchange mathematical objects, request calcula- tions,
    and store and retrieve remote objects, either locally or accross the
    internet. The three examples show the current state of the GAP, KANT,
    and MuPAD software packages, and give a demonstration of exposing
    Macaulay using a newly developed framework.",
  paper = "Freu08.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Kozen, Dexter}
\index{Palsberg, Jens}
\index{Schwartzbach, Michael I.}
\begin{chunk}{axiom.bib}
@article{Koze93,
  author = "Kozen, Dexter and Palsberg, Jens and Schwartzbach, Michael I.",
  title = {{Efficient Recursive Subtyping}},
  journal = "Mathematical Structures in Computer Science",
  volume = "5",
  number = "1",
  pages = "113-125",
  year = "1995",
  abstract =
    "Subtyping in the presence of recursive types for the
    $\lambda$-calculus was studied by Amadio and Cardelli in 1991. In that
    paper they showed that the problem of deciding whether one recursive
    type is a subtype of another is decidable in exponential time.  In
    this paper we give an $O(n^2)$ algorithm.  Our algorithm is based on a
    simplification of the definition of the subtype relation, which allows
    us to reduce the problem to the emptiness problem for a certain finite
    automaton with quadratically many states.  It is known that equality
    of recursive types and the covariant BÌˆohm order can be decided
    efficiently by means of finite automata, since they are just language
    equality and language inclusion, respectively.  Our results extend the
    automata-theoretic approach to handle orderings based on
    contravariance.",
  paper = "Koze93.pdf"
}

\end{chunk}

\index{Martelli, Alberto}
\index{Montanari, Ugo}
\begin{chunk}{axiom.bib}
@article{Mart82,
  author = "Martelli, Alberto and Montanari, Ugo",
  title = {{An Efficient Unification Algorithm}},
  journal = "ACM TOPLAS",
  volume = "4",
  number = "2",
  pages = "258-282",
  year = "1982",
  abstract =
    "The unification problem in first-order predicate calculus is
    described in general terms as the solution of a system of equations,
    and a nondeterministic algorithm is given.  A new unification
    algorithm, characterized by having the acyclicity test efficiently
    embedded into it, is derived from the nondeterministic one, and a
    PASCAL implementation is given.  A comparison with other well-known
    unification algorithms shows that the algorithm described here
    performs well in all cases.",
  paper = "Mart82.pdf"
}

\end{chunk}

\index{Pottier, Francois}
\begin{chunk}{axiom.bib}
@phdthesis{Pott98,
  author = "Pottier, Francois",
  title = {{Type Inference in the Presence of Subtyping: From Theory
            to Practice}},
  school = "Universite Paris 7",
  year = "1988",
  comment = "INRIA Research Report RR-3483",
  abstract =
    "From a purely theoretical point of view, type inference for a
    functional language with parametric polymorphism and subtyping
    poses little difficulty. Indeed, it suffices to generalize the
    inference algorithm used in the ML language, so as to deal with
    type inequalities, rather than equalities. However, the number of
    such inequalities is linear in the program size -- whence, from a
    practical point of view, a serious efficiency and readability
    problem. 

    To solve this problem, one must simplify the inferred
    constraints. So, after studying the logical properties of
    subtyping constraints, this work proposes several simplifcation
    algorithms. They combine seamlessly, yielding a homogeneous, fully
    formal framework, which directly leads to an efficient
    implementation. Although this theoretical study is performed in a
    simplified setting, numerous extensions are possible. Thus, this
    framework is realistic and should allow a practical appearance of
    subtyping in languages with type inference.",
  paper = "Pott98.pdf"
}

\end{chunk}

\index{McCarthy, John}
\begin{chunk}{axiom.bib}
@article{Mcca60,
  author = "McCarthy, John",
  title = {{Recursive Functions of Symbolic Expressions and Their
            Computation by Machine, Part I}},
  journal = "CACM",
  volume = "3",
  pages = "184-195",
  year = "1960",
  paper = "Mcca60.pdf",
  keywords = "printed"
}

\end{chunk}  

\index{Barendregt, Henk}
\begin{chunk}{axiom.bib}
@incollection{Bare92,
  author = "Barendregt, Henk",
  title = {{Lambda Calculi with Types}},
  booktitle = "Handbook of Logic in Computer Science (vol 2)",
  publisher = "Oxford University Press",
  year = "1992",
  paper = "Bare92.pdf"
}

\end{chunk}

\index{Church, Alonzo}
\begin{chunk}{axiom.bib}
@article{Chur28,
  author = "Church, Alonzo",
  title = {{On the Law of the Excluded Middle}},
  journal = "Bull. of the American Mathematical Society",
  volume = "34",
  number = "1",
  pages = "75-78",
  year = "1928",
  paper = "Chur28.pdf"
}

\end{chunk}

\index{Mitchell, John}
\begin{chunk}{axiom.bib}
@inproceedings{Mitc84
  author = "Mitchell, John",
  title = {{Type Inference and Type Containment}},
  booktitle = "Proc. Int. Symp. on Semantics of Data Types",
  publisher = "Springer",
  isbn = "3-540-13346-1",
  year = "1984"
  pages = "257-277",
  abstract =
    "Type inference, the process of assigning types to untyped
    expressions, may be motivated by the design of a typed language or
    semantical considerations on the meeaaings of types and expressions.
    A typed language GR with polymorphic functions leads to the GR
    inference rules. With the addition of an 'oracle' rule for equations
    between expressions, the GR rules become complete for a general class
    of semantic models of type inference. These inference models are
    models of untyped lambda calculus with extra structure similar to
    models of the typed language GR. A more specialized set of type
    inference rules, the GRS rules, characterize semantic typing when the
    functional type $a~$ , is interpreted as all elements of the model that
    map a to $~r$ and the polymorphic type $Vt,~0$ is interpreted as the
    intersection of all $o'(r)$. Both inference systems may be reformulated
    using rules for deducing containments between types. The advantage of
    the type inference rules based on containments is thatproofs
    correspond more closely to the structure of terms.",
  paper = "Mitc84.pdf"
}

\end{chunk}

\index{Griesmer, James}
\begin{chunk}{axiom.bib}
@artcile{Grie11,
  author = "Griesmer, James",
  title = {{James Griesmer 1929--2011}},
  journal = "ACM Communications in Computer Algebra",
  volume = "46",
  number = "1/2",
  year = "2012",
  pages = "10-11",
  comment = "Obituary",
  paper = "Grie11.pdf"
}

\end{chunk}

\index{Daly, Timothy}
\index{Kastner, John}
\index{Mays, Eric}
\begin{chunk}{axiom.bib}
@inproceedings{Daly88,
  author = "Daly, Timothy and Kastner, John and Mays, Eric",
  title = {{Integrating Rules and Inheritance Networks in a Knowlege-based
            Financial Marketing Consutation System}},
  booktitle = "Proc. HICSS 21",
  volume = "3",
  number = "5-8",
  pages = "496-500",
  year = "1988",
  comment = "KROPS",
  abstract =
    "This paper describes the integrated use of rule-bascd inference and
    an object-centered knowledge representation (inheritance network) in a
    financial marketing consultation system. The rules provide a highly
    flexible pattern match capability and inference cycle for control. The
    inheritance network provides a convenient way to represent the
    conceptual structure of the domain. By merging the two techniques, our
    financial computation can he shared at the most general level, and
    rule inference is carried out at any appropriate Ievel of
    generalization.  Since domain knowledge is representcd independently
    from control knowledge, knowledge ahout a particular problcm solving
    technique is decoupled from the conditions for its invocation. A Iarge
    financial marketing system has been built and examples are given to
    show our combined use of rules and inheritance networks.",
  paper = "Daly88.pdf"
}  

\end{chunk}
