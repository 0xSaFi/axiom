books/bookvolbib add references

Goal: Proving Axiom Correct

\index{Coer, Claudio Sacerdoti}
\index{Tassi, Enrico}
\begin{chunk}{axiom.bib}
@inproceedings{Aspe11a,
  author = "Coer, Claudio Sacerdoti and Tassi, Enrico",
  title = {{Nonuniform Coercions via Unification Hints}},
  booktitle = "Proc. Types for Proofs and Programs",
  volume = "53",
  pages = "19-26",
  year = "2011",
  abstract =
    "We introduce the notion of nonuniform coercion, which is the
    promotion of a value of one type to an enriched value of a different
    type via a nonuniform procedure. Nonuniform coercions are a
    generalization of the (uniform) coercions known in the literature and
    they arise naturally when formalizing mathematics in an higher order
    interactive theorem prover using convenient devices like canonical
    structures, type classes or unification hints. We also show how
    nonuniform coercions can be naturally implemented at the user level in
    an interactive theorem prover that allows unification hints.",
  paper = "Aspe11a.pdf"
}

\end{chunk}

\index{Asperti, Andrea}
\index{Ricciotti, Wilmer}
\index{Coer, Claudio Sacerdoti}
\index{Tassi, Enrico}
\begin{chunk}{axiom.bib}
@article{Aspe09a,
  author = "Asperti, Andrea and Ricciotti, Wilmer and Coer, Claudio
            Sacerdoti and Tassi, Enrico",
  title = {{Hints in Unification}},
  journal = "LNCS",
  volume = "5674",
  pages = "84-98",
  year = "2009",
  isbn = "978-3-642-03358-2",
  abstract =
    "Several mechanisms such as Canonical Structures, Type Classes, or
    Pullbacks have been recently introduced with the aim to improve the
    power and flexibility of the type inference algorithm for interactive
    theorem provers. We claim that all these mechanisms are particular
    instances of a simpler and more general technique, just consisting in
    providing suitable hints to the unification procedure underlying type
    inference. This allows a simple, modular and not intrusive
    implementation of all the above mentioned techniques, opening at the
    same time innovative and unexpected perspectives on its possible
    applications.",
  paper = "Aspe09a.pdf"
}

\end{chunk}

\index{Asperti, Andrea}
\index{Tassi, Enrico}
\begin{chunk}{axiom.bib}
@article{Aspe10,
  author = "Asperti, Andrea and Tassi, Enrico",
  title = {{Smart Matching}},
  journal = "LNCS",
  volume = "6167",
  pages = "263-277",
  year = "2010",
  isbn = "978-3-642-14128-7",
  abstract =
    "One of the most annoying aspects in the formalization of mathematics
    is the need of transforming notions to match a given, existing
    result. This kind of transformations, often based on a conspicuous
    background knowledge in the given scientific domain (mostly expressed
    in the form of equalities or isomorphisms), are usually implicit in
    the mathematical discourse, and it would be highly desirable to obtain
    a similar behavior in interactive provers. The paper describes the
    superposition-based implementation of this feature inside the Matita
    interactive theorem prover, focusing in particular on the so called
    smart application tactic, supporting smart matching between a goal and
    a given result.",
  paper = "Aspe10.pdf"
}

\end{chunk}

\index{Asperti, Andrea}
\index{Ricciotti, Wilmer}
\index{Coer, Claudio Sacerdoti}
\index{Tassi, Enrico}
\begin{chunk}{axiom.bib}
@inproceedings{Aspe09b,
  author = "Asperti, Andrea and Ricciotti, Wilmer and Coer, Claudio
            Sacerdoti and Tassi, Enrico",
  title = {{A New Type for Tactics}},
  booktitle = "SIGSAM PLMMS 2009",
  publisher = "ACM",
  year = "2009",
  isbn = "978-1-60558-735-6",
  abstract =
    "The type of tactics in all (procedural) proof assistants is (a
    variant of) the one introduced in LCF. We discuss why this is
    inconvenient and we propose a new type for tactics that 1) allows the
    implementation of more clever tactics; 2) improves the implementation
    of declarative languages on top of procedural ones; 3) allows for
    better proof structuring; 4) improves proof automation; 5) allows
    tactics to rearrange and delay the goals to be proved (e.g. in case of
    side conditions or PVS subtyping judgements).",
  paper = "Aspe09b.pdf"
}

\end{chunk}

\index{Asperti, Andrea}
\index{Tassi, Enrico}
\begin{chunk}{axiom.bib}
@inproceedings{Aspe07,
  author = "Asperti, Andrea and Tassi, Enrico",
  title = {{Higher Order Proof Reconstruction from
            Paramodulation-based Refutations: The Unit Equality Case}},
  booktitle = "MKM 2007",
  year = "2007",
  abstract =
    "In this paper we address the problem of reconstructing a higher
    order, checkable proof object starting from a proof trace left by a
    first order automatic proof searching procedure, in a restricted
    equational framework. The automatic procedure is based on
    superposition rules for the unit equality case. Proof transformation
    techniques aimed to improve the readability of the final proof are
    discussed.",
  paper = "Aspe07.pdf"
}

\end{chunk}

\index{Asperti, Andrea}
\index{Coen, Claudio Sacerdoti}
\index{Tassi, Enrico}
\index{Zacchiroli, Stefano}
\begin{chunk}{axiom.bib}
@inproceedings{Aspe06,
  author = "Asperti, Andrea and Coen, Claudio Sacerdoti and 
            Tassi, Enrico and Zacchiroli, Stefano",
  title = {{Crafting a Proof Assistant}},
  booktitle = "Proc. Types 2006: Conf. of the Types Project",
  year = "2006",
  abstract =
    "Proof assistants are complex applications whose develop- ment has
    never been properly systematized or documented. This work is a
    contribution in this direction, based on our experience with the
    devel- opment of Matita: a new interactive theorem prover based—as
    Coq—on the Calculus of Inductive Constructions (CIC). In particular,
    we analyze its architecture focusing on the dependencies of its
    components, how they implement the main functionalities, and their
    degree of reusability. The work is a first attempt to provide a ground
    for a more direct com- parison between different systems and to
    highlight the common functionalities, not only in view of
    reusability but also to encourage a more systematic comparison of
    different softwares and architectural solutions.",
  paper = "Aspe06.pdf"
}

\end{chunk}

\index{Asperti, Andrea}
\index{Coen, Claudio Sacerdoti}
\index{Tassi, Enrico}
\index{Zacchiroli, Stefano}
\begin{chunk}{axiom.bib}
@article{Aspe07a,
  author = "Asperti, Andrea and Coen, Claudio Sacerdoti and Tassi, Enrico
            and Zacchiroli, Stefano",
  title = {{User Interaction with the Matita Proof Assistant}},
  journal = "J. of Automated Reasoning",
  volume = "39",
  number = "2",
  pages = "109-139",
  abstract =
    "Matita is a new, document-centric, tactic-based interactive theorem
    prover. This paper focuses on some of the distinctive features of the
    user interaction with Matita, characterized mostly by the organization
    of the library as a searchable knowledge base, the emphasis on a
    high-quality notational rendering, and the complex interplay between
    syntax, presentation, and semantics.",
  paper = "Aspe07a.pdf"
}

\end{chunk}

\index{Coen, Claudio Sacerdoti}
\index{Tassi, Enrico}
\index{Zacchiroli, Stefano}
\begin{chunk}{axiom.bib}
@inproceedings{Coen06,
  author = "Coen, Claudio Sacerdoti and Tassi, Enrico and 
            Zacchiroli, Stefano",
  title = {{Tinycals: Step by Step Teacticals}},
  booktitle = "Proc. User Interfaces for Theorem Provers",
  year = "2006",
  pages = "125-142",
  abstract =
    "Most of the state-of-the-art proof assistants are based on procedural
    proof languages, scripts, and rely on LCF tacticals as the primary
    tool for tactics composition. In this paper we discuss how these
    ingredients do not interact well with user interfaces based on the
    same interaction paradigm of Proof General (the de facto standard in
    this field), identifying in the coarse-grainedness of tactical
    evaluation the key problem. We propose tinycals as an alternative to a
    subset of LCF tacticals, showing that the user does not experience the
    same problem if tacticals are evaluated in a more fine-grained
    manner. We present the formal operational semantics of tinycals as
    well as their implementation in the Matita proof assistant.",
  paper = "Coen06.pdf"
}

\end{chunk}

\index{Padovani, Luca}
\index{Zacchiroli, Stefano}
\begin{chunk}{axiom.bib}
@inproceedings{Pado06,
  author = "Padovani, Luca and Zacchiroli, Stefano",
  title = {{From Notation to Semantics: There and Back Again}},
  booktitle = "5th Conf. on Mathematical Knowledge Management",
  year = "2006",
  abstract =
    "Mathematical notation is a structured, open, and ambiguous
    language. In order to support mathematical notation in MKM
    applications one must necessarily take into account presentational as
    well as semantic aspects. The former are required to create a
    familiar, comfortable, and usable interface to interact with. The
    latter are necessary in order to process the information
    meaningfully. In this paper we investigate a framework for dealing
    with mathematical notation in a meaningful, extensible way, and we
    show an effective instantiation of its architecture to the field of
    interactive theorem proving. The framework builds upon well-known
    concepts and widely-used technologies and it can be easily adopted by
    other MKM applications.",
  paper = "Pado06.pdf"
}

\end{chunk}

\index{Asperti, Andrea}
\index{Guidi, Ferruccio}
\index{Coen, Claudio Sacerdoti}
\index{Tassi, Enrico}
\index{Zacchiroli, Stefano}
\begin{chunk}{axiom.bib}
@article{Aspe04,
  author = "Asperti, Andrea and Guidi, Ferruccio and Coen, Claudio Sacerdoti
            and Tassi, Enrico and Zacchiroli, Stefano",
  title = {{A Content Based Mathematical Search Engine: Whelp}},
  journal = "LNCS",
  volume = "3839",
  year = "2004",
  pages = "17-32",
  isbn = "3-540-31428-8",
  abstract =
    "The prototype of a content based search engine for mathematical
    knowledge supporting a small set of queries requiring matching and/or
    typing operations is described. The prototype, called Whelp, exploits
    a metadata approach for indexing the information that looks far more
    flexible than traditional indexing techniques for structured
    expressions like substitution, discrimination, or context trees. The
    prototype has been instantiated to the standard library of the Coq
    proof assistant extended with many user contributions.",
  paper = "Aspe04.pdf"
}

\end{chunk}

\index{Coen, Claudio Sacerdoti}
\index{Zacchiroli, Stefano}
\begin{chunk}{axiom.bib}
@article{Coen04,
  author = "Coen, Claudio Sacerdoti and Zacchiroli, Stefano",
  title = {{Efficient Ambiguous Parsing of Mathematical Formulae}},
  journal = "LNCS",
  volume = "3119",
  pages = "347-362",
  year = "2004",
  isbn = "3-540-23029-7",
  abstract =
    "Mathematical notation has the characteristic of being ambiguous:
    operators can be overloaded and information that can be deduced is
    often omitted. Mathematicians are used to this ambiguity and can
    easily disambiguate a formula making use of the context and of their
    ability to find the right interpretation.
    
    Software applications that have to deal with formulae usually avoid
    these issues by fixing an unambiguous input notation. This solution is
    annoying for mathematicians because of the resulting tricky syntaxes
    and becomes a show stopper to the simultaneous adoption of tools
    characterized by different input languages.
    
    In this paper we present an efficient algorithm suitable for ambiguous
    parsing of mathematical formulae. The only requirement of the
    algorithm is the existence of a 'validity' predicate over abstract
    syntax trees of incomplete formulae with placeholders. This
    requirement can be easily fulfilled in the applicative area of
    interactive proof assistants, and in several other areas of
    Mathematical Knowledge Management.",
  paper = "Coen04.pdf"
}

\end{chunk}

\index{Grabm\"uller, Martin}
\begin{chunk}{axiom.bib}
@misc{Grab06a,
  author = "Grabmuller, Martin",
  title = {{Algorithm W Step by Step}},
  year = "2006",
  link = "\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7733&rep=rep1&type=pdf}",
  abstract =
    "In this paper we develop a complete implementation of the classic
    algoirhtm W for Hinley-Milner polymorphic type inference in Haskell",
  paper = "Grab06a.pdf"
}

\end{chunk}

\index{Hoare, C.A.R}
\begin{chunk}{axiom.bib}
@article{Hoar87,
  author = "Hoare, C.A.R",
  title = {{An Overview of Some Formal Methods for Program Design}},
  journal = "Computer",
  year = "1987",
  volume = "20",
  number = "9",
  paper = "Hoar87.pdf"
}

\end{chunk}

\index{Bowen, Jonathan P.}
\index{Hinchey, Michael G.}
\begin{chunk}{axiom.bib}
@article{Bowe95,
  author = "Bowen, Jonathan P. and Hinchey, Michael G.",
  title = {{Seven More Myths of Formal Methods}},
  journal = "IEEE Software",
  volume = "12",
  number = "4",
  pages = "34-41",
  year = "1995", 
  abstract =
    "New myths about formal methods are gaining tacit acceptance both
    outside and inside the system-development community. The authors
    address and dispel these myths based on their observations of
    industrial projects. The myths include: formal methods delay the
    development process; they lack tools; they replace traditional
    engineering design methods; they only apply to software; are
    unnecessary; not supported; and formal methods people always use
    formal methods.",
  paper = "Bowe95.pdf",
  keywords = "printed"
}

\end{chunk}

\index{McBride, Conor}
\begin{chunk}{axiom.bib}
@phdthesis{Mcbr99,
  author = "McBride, Conor",
  title = {{Dependently Typed Functional Programs and their Proofs}},
  school = "University of Edinburgh",
  year = "1999",
  link = "\url{http://strictlypositive.org/thesis.pdf}",
  abstract =
    "Research in dependent type theories has, in the past, concentrated on
    its use in the presentation of theorems and theorem-proving. This
    thesis is concerned mainly with the exploitation of the computational
    aspects of type theory for programming, in a context where the
    properties of programs may readily be specified and established. In
    particular, it develops technology for programming with dependent
    inductive families of datatypes and proving those programs correct. It
    demonstrates the considerable advantage to be gained by indexing data
    structures with pertinent characteristic information whose soundness
    is ensured by typechecking, rather than human effort.
    
    Type theory traditionally presents safe and terminating computation on
    inductive datatypes by means of elimination rules which serve as
    induction principles and, via their associated reduction behaviour,
    recursion operators. In the programming language arena, these appear
    somewhat cumbersome and give rise to unappealing code, complicated by
    the inevitable interaction between case analysis on dependent types
    and equational reasoning on their indices which must appear explicitly
    in the terms. Thierry Coquand's proposal to equip type theory directly
    with the kind of pattern matching notation to which functional
    programmers have become used to over the past three decades offers a
    remedy to many of these difficulties. However, the status of pattern
    matching relative to the traditional elimination rules has until now
    been in doubt. Pattern matching implies the uniqueness of identity
    proofs, which Martin Hofmann showed underivable from the conventiaonal
    definition of equality. This thesis shows that the adoption of this
    uniqueness as axiomatic is sufficient to make pattern matching
    admissible.
    
    A datatype's elimination rule allows abstraction only over the whole
    inductively defined family. In order to support pattern matching, the
    application of such rules to specific instances of dependent families
    has been systematised. The underlying analysis extends beyond
    datatypes to other rules of a similar second order character,
    suggesting they may have other roles to play in the specification,
    verification and, perhaps, derivation of programs. The technique
    developed shifts the specificity from the instantiation of the type's
    indices into equational constraints on indices freely chosen, allowing
    the elimination rule to be applied.
    
    Elimination by this means leaves equational hypotheses in the
    resulting subgoals, which must be solved if further progress is to be
    made. The first-order unification algorithm for constructor forms in
    simmple types presented in [McB96] has been extended to cover
    dependent datatypes as well, yielding completely automated solution of
    a class of problems which can be syntactically defined.
    
    The justification and operation of these techniques requires the
    machine to construct and exploit a standardised collection of
    auxiliary lemmas for each datatype. This is greatly facilitated by two
    technical developments of interest in their own right:
    \begin{itemize}
    \item a more convenient definition of equality, with a relaxed
    formulation rule allowing elements of different types to be compared,
    but nonetheless equivalent to the usual equality plus the axiom of
    uniqueness
    \item a type theory, OLEG, which incorporates incomplete objects,
    accounting for their 'holes' entirely within the typing judgments and,
    novelly, not requiring any notion of explicit substitution to manage
    their scopes.
    \end{itemize}
    
    A substantial prototype has been implemented, extending the proof
    assistant LEGO. A number of programs are developed by way of
    example. Chiefly, the increased expressivity of dependent datatypes is
    shown to capture a standard first-order unification algorithm within
    the class of structurally recursive programs, removing any need for a
    termination argument. Furthermore, the use of elimination rules in
    specifying the components of the program simplifies significantly its
    correctness proof.",
  paper = "Mcbr99.pdf",
  keywords = "printed"
}

\end{chunk}
