books/bookvolbib add references

Goal: Proving Axiom Sane

\index{Al-hassy, Musa}
\index{Carette, Jacques}
\index{Kahl, Wolfram}
\begin{chunk}{axiom.bib}
@misc{Alha19,
  author = "Al-hassy, Musa and Carette, Jacques and Kahl, Wolfram",
  title = {{A Language Feature to Unbundle Data at Will}},
  year = "2019",
  link = "\url{http://www.cas.mcmaster.ca/~carette/publications/gpce19main-p33.pdf}",
  abstract =
    "Programming languages with sufficiently expressive type systems
    provide users with different means of data 'bundling'. 
    Specifically, in dependently-typed languages such as Agda, Coq,
    Lean and Idris, one can choose to encode information in a record
    either as a parameter or a field. For example, we can speak of
    graphs over a particular vertex set, or speak of arbitrary graphs
    where the vertex set is a component. These create isomorphic
    types, but differ with respect to intended use. Traditionally, a
    library designer would make this choice (between parameters and
    fields); if a user wants a different variant, they are forced to
    build cnversion utilities, as well as duplicate functionality. For
    a graph data type, if a library only provides a Haskell-like
    typeclass view of graphs over a vertex set, yet a user wishes to
    work with the category of graphs, they must now package a vertex
    set as a component in a record along with a graph over that set. 

    We design and implement a language feature that allows both the
    library designer and the user to make the choice of information
    exposure only when necessary, and otherwise leave the
    distinguishing line between parameters and fields unspecified.
    Our language feature is currently implemented as a prototype
    meta-program incorporated into Agda's Emacs ecosystem, in a what
    that is unobtrusive to Agda users.",
  paper = "Alha19.pdf"
}

\end{chunk}

\index{Kahan, William}
\begin{chunk}{axiom.bib}
@misc{Kaha04,
  author = "Kahan, William",
  title = {{On the Cost of Floating-Point Computation Without
            Extra-Precise Arithmetic}},
  year = "2004",
  link = "\url{https://people.eecs.berkeley.edu/~wkahan/Qdrtcs.pdf}",
  abstract =
    "Current benchmarks give the impression that computation costs no
    more than the time consumed, and perhaps the memory occupied, only
    because we can measure these so easily. What about the costs of
    maintenance (when hardware or operating systems change), of
    development (algorithm design, tests, proofs of correctness), and
    of misleading results? Solving a quadratic equation provides a
    relatively easily understood case study of the way all costs get
    inflated when arithmetic precision rather higher than the
    precision of the data and the accuracy demanded of results is
    unavailable or, worse, unusable because of lapses in the design
    and/or implementation of widely used programming languages. Then
    costs are inflated by the trickery required to devise portable
    programs that compute roots at least about as accurately as the
    data deserve, and to prove that they are that accurate, and to
    test them. This trickery will be illustrated by a MATLAB program
    designed to get results of high quality from diverse versions of
    MATLAB on the two most popular kinds of hardware, PCs and
    Macs. Included is a test program and some of its results. Only a
    small part of the program needs higher precision arithmetic not
    too slow. There it would cost far less than the consequences of
    doing without.",
  paper = "Kaha04.pdf"
}

\end{chunk}

\index{Dominus, Mark}
\begin{chunk}{axiom.bib}
@misc{Domi19,
  author = "Dominus, Mark",
  title = {{The Least Common Divisor and the Greatest Common Multiple}},
  link = "\url{https://blog.plover.com/math/gcm.html}",
  year = "2019"
}

\end{chunk}

\index{Carette, Jacques}
\index{O'Connor, Russell}
\index{Sharoda, Yasmine}
\begin{chunk}{axiom.bib}
@misc{Care19,
  author = "Carette, Jacques and O'Connor, Russell and Sharoda, Yasmine",
  title = {{Building on the Diamonds between Theories: Theory
            Presentation Combinators}},
  year = "2019",
  abstract =
    "To build a large library of mathematics, it seems more efficient
    to take advantage of the inherent structure of mathematical
    theories. Various theory presentation combinators have been
    proposed, and some have been implemented, in both legacy and
    current systems. Surprisingly, the 'standard library' of most
    systems do not make pervasive use of these combinators.

    We present a set of combinators optimized for reuse, via the tiny
    theory approach. Our combinators draw their power from the
    inherent structure already present in the {\sl category of
    contexts} associated to a dependently typed language. The current
    work builds on ideas originating in CLEAR and Specware and their
    descendents (both direct and intellectual). Driven by some design
    criteria for user-centric library design, our library-building
    experience via the systematic use of combinators has fed back into
    the semantics of these combinators, and later into an updated
    syntax for them.",
  paper = "Care19.pdf"
}

\end{chunk}

\index{Steel, Allan K.}
\begin{chunk}{axiom.bib}
@article{Stee10,
  author = "Steel, Allan K.",
  title = {{Computing with Agebraically Closed Fields}},
  journal = "J. of Symbolic Computation",
  volume = "45",
  number = "3",
  year = "2010",
  pages = "342-372",
  abstract = 
    "A practical computational system is described for computing with
    an algebraic closure of a field. The system avoids factorization
    of polynomials over extension fields, but gives the illusion of a
    genuine field to the user. All roots of an arbitrary polynomial
    defined over such an algebraically closed field can be constructed
    and are easily distinguished within the system. The difficult case
    of inseparable extensions of function fields of positive
    characteristic is also handled properly by the system. A technique
    of modular evaluation into a finite field critically ensures that
    a unique genuine field is simulated by the system but also
    provides fast optimizations for some fundamental operations. Fast
    matrix techniques are also used for several non-trivial
    operations. The system has been successfully implemented within
    the Magma Computer Algebra System, and several examples are
    presented, using this implementation.",
  paper = "Stee10.pdf"
}

\end{chunk}
