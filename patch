books/bookvolbib add Ahre15

Goal: Axiom Numerics

@techreport{Ahre15,
  author = "Ahrens, Peter and Nguyen, Hong Diep and Demmel, James",
  title = "Efficient Reproducible Floating Point Summation and BLAS",
  institution = "University of California, Berkeley",
  year = "2015",
  month = "December",
  type = "technical report",
  number = "229",
  paper = "Ahre15.pdf",
  url = "http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-229.pdf",
  abstract =
    "We define reproducibility to mean getting bitwise identical results
    from multiple runs of the same program, perhaps with different
    hardware resources or other changes that should ideally not change the
    answer. Many users depend on reproducibility for debugging or
    correctness. However, dynamic scheduling of parallel computing
    resources, combined with nonassociativity of floating point addition,
    makes attaining reproducibility a challenge even for simple operations
    like summing a vector of numbers, or more complicated operations like
    Basic Linear Algebra Subprograms (BLAS). We describe an algorithm that
    computes a reproducible sum of floating point numbers independent of
    the order of summation. The algorithm depends only on a subset of the
    IEEE Floating Point Standard 754-2008. It is communication-optimal, in
    the sense that it does just one pass over the data in the sequential
    case, or one reduction operation in the parallel case, requiring an
    ``accumulator'' represented by just 6 floating point words (more can
    be used if higher precision is desired). Th arithmetic code with a
    6-word accumulator is $7n$ floating point additions to sum $n$ words,
    and (in IEEEE double precision) the final error bound can be up to
    $10^8$ times smaller than the error bound for conventional
    summation. We describe the basic summation algorithm, the software
    infrastructure used to build reproducible BLAS (ReproBLAS), and
    performance results. For example, when computing the dot product of
    4096 double precision floating point numbers, we get a $4x$ slowdown
    compared to Intel Math Kernel Library (MKL) running on an Intel Core
    i7-2600 CPU operating at 3.4 GHz and 256 KB L2 Cache."
}

