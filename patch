books/bookvolbib add references

Goal: Proving Axiom Sane

\index{Alturki, Musab A.}
\index{Moore, Brandon}
\begin{chunk}{axiom.bib}
@misc{Altu19,
  author = "Alturki, Musab A. and Moore, Brandon",
  title = {{K vs Coq as Language Verification Frameworks}},
  year = "2019",
  link = "\url{https://runtimeverification.com/blog/k-vs-coq-as-language-verification-frameworks-part-1-of-3/}"
}

\end{chunk}

\index{Rosu, Grigore}
\index{Lucanu, Dorel}
\index{Guth, Dwight}
\begin{chunk}{axiom.bib}
@misc{Rosu20,
  author = "Rosu, Grigore and Lucanu, Dorel and Guth, Dwight",
  title = {{The K Framework}},
  year = "2020",
  link = "\url{http://www.kframework.org/index.php/Main_Page}"
}

\end{chunk}

\index{Moore, Brandon}
\index{Pena, Lucas}
\index{Rosu, Grigore}
\begin{chunk}{axiom.bib}
@inproceedings{Moor18,
  author = "Moore, Brandon and Pena, Lucas and Rosu, Grigore",
  title = {{Program Verification by Coinduction}},
  booktitle = "Programming Languages and Systems",
  publisher = "Springer",
  year = "2018",
  pages = "589-618",
  link = "\url{https://link.springer.com/content/pdf/10.1007%2F978-3-319-89884-1.pdf}",
  abstract =
    "We present a novel program verification approach based on
    coinduction, which takes as input an operational semantics. No
    intermediates like program logics or verifcation condition
    generators are needed. Specifications can be written using any
    state predicates. We implement our approach in Coq, giving a
    certifying language-independent verification framework. Our proof
    system is implemented as a single module imported unchanged into
    language-specific proofs. Automation is reached by instantiating a
    generic heuristic with language-specific tactics. Manual
    assistance is also smoothly allowed at points the automation
    cannot handle. We demonstrate the power and versatility of our
    approach by verifying algoirthms as complicated as Schorr-Waite
    graph marking and instantiating our framework for object languages
    in several styles of semantics. Finally, we show that our
    coinductive approach subsumes reachability logic, a recent
    language-independent soudn and (relatively) complete logic for
    program verification that has been instantiated with operational
    semantics of languages as complex as C, Java and Javascript.",
  paper = "Moor18.pdf"
}

\end{chunk}

\index{Carneiro, Mario}
\begin{chunk}{axiom.bib}
@misc{Carn19a,
  author = "Carneiro, Mario",
  title = {{The Type Theory of Lean}},
  year = "2019",
  link = "\url{https://github.com/digama0/lean-type-theory/releases/v1.0}",
  abstract =
    "This thesis is a presentation of dependent type theory with
    inductive types, a hierarchy of universes, with an impredicative
    universe of propositions, proof irrelevance, and subsingleton
    elimination, along with axioms for propositional extensionality,
    quotient types, and the axiom of choice. This theory is notable
    for being the axiomatic framework of the Lean theorem prover. The
    axiom system is given here in complete detail, including
    ``optional'' features of the type system such as {\bf let} binders
    and definitions. We provide a reduction of the theory to a
    finitely axiomatized fragment utilizing a fixed set of inductive
    types (the {\bf W}-type plus a few others), to ease the study of
    this framework.

    The metatheory of this theory (which we will Lean) is studied. In
    particular, we prove unique typing of the definitional equality,
    and use this to construct the expected set-theoretic model, from
    which we derive consistency of Lean relative to {\bf ZFC+} \{there
    are $n$ inaccessible cardinals $\vert~n<\omega$\} (a relatively
    weak large cardinal assumption). As Lean supports models of 
    {\bf ZFC} with $n$ inaccessible cardinals, this is optimal.

    We also show a number of negative results, where the theory is
    less nice than we would like. In particular, type checking is
    undecidable, and the type checking as implemented by the Lean
    theorem prover is a decideable non-transitive underapproximation
    of the typing judgment. Non-transitivity also leads to lack of
    subject reduction, and the reduction relation does not satisfy the
    Church-Rosser property, so reduction to a normal form does not
    produce a decision procedure for definitional equality. However, a
    modified reduction relation allows us to restore the Church-Rosser
    property at the expense of guaranteed termination, so that unique
    typing is shown to hold.",
  paper = "Carn19a.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Carneiro, Mario}
\begin{chunk}{axiom.bib}
@misc{Carn19b,
  author = "Carneiro, Mario",
  title = {{The Type Theory of Lean (slides)}},
  year = "2019",
  link = "\url{https://github.com/digama0/lean-type-theory/releases/v1.0}",
  paper = "Carn19b.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Milner, Robin}
\begin{chunk}{axiom.bib}
@techreport{Miln73,
  author = "Milner, Robin",
  title = {{Models of LCF}},
  type = "technical report",
  institution = "Stanford Artificial Intelligence Laboratory",
  number = "STAN-CS-73-332",
  year = "1973",
  link = "\url{http://i.stanford.edu/pub/cstr/reports/cs/tr/73/332/CS-TR-73-332.pdf}",
  abstract = 
    "LCF is a deductive system for computable functions proposed by
    D. Scott in 1969 in an unpublished memorandum. The purpose of the
    present paper is to demonstrate the soundness of the system with
    respect to certain models, which are partially ordered domains of
    continuous functions. This demonstration was supplied by Scott in
    his memorandum; the present paper is merely intended to make this
    work more accessible.",
  paper = "Miln73.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Jenkins, Kris}
\begin{chunk}{axiom.bib}
@misc{Jenk18,
  author = "Jenkins, Kris",
  title = {{Communicating with Types}},
  year = "2018",
  link = "\url{https://vimeo.com/302682323}"
}

\end{chunk}

\index{Taylor, Sophie}
\begin{chunk}{axiom.bib}
@misc{Tayl16,
  author = "Taylor, Sophie",
  title = {{A working (class) Introduction to Homotopy Type Theory}},
  year = "2016",
  link = "\url{https://www.youtube.com/watch?v=xv6SwPn1dlc}",
  keywords = "DONE"
}

\end{chunk}

\index{Voevodsky, Vladimir}
\begin{chunk}{axiom.bib}
@misc{Voev16,
  author = "Voevodsky, Vladimir",
  title = {{Univalent Foundations of Mathematics}},
  year = "2016",
  link = "\url{https://www.youtube.com/watch?v=9f4sS9s-X2A}"
}

\end{chunk}

\index{Zach, Richard}
\begin{chunk}{axiom.bib}
@book{Zach20,
  author = "Zach, Richard",
  title = {{The Open Logic Text}},
  year = "2020",
  publisher = "The Open Logic Project",
  link = "\url{http://builds.openlogicproject.org/open-logic-complete.pdf}",
  paper = "Zach20.pdf"
}

\end{chunk}

\index{Bezhanishvili, Nick}
\index{de Jongh, Dick}
\begin{chunk}{axiom.bib}
@misc{Bezh05,
  author = "Bezhanishvili, Nick and de Jongh, Dick",
  title = {{Intuitionistic Logic}},
  year = "2005",
  link = "\url{https://www.cs.le.ac.uk/people/nb118/Publications/ESSLLI'05.pdf}",
  paper = "Bezh05.pdf"
}

\end{chunk}

\index{Milne, Peter}
\begin{chunk}{axiom.bib}
@article{Miln94,
  author = "Milne, Peter",
  title = {{Classical Harmony: Rules of Inference and the Meaning of
            the Logical Constants}},
  journal = "Synthese",
  volume = "100",
  number = "1",
  pages = "49-94",
  year = "1994",
  abstract = 
    "The thesis that, in a system of natural deduction, the meaning of
    a logical constant is given by some or all of its introduction and
    elimination rules has been developed recently by Dummett, Prawitz,
    Tennant, and others, by the addition of harmony constraints. 
    Introduction and elimination rules for a logical constant must be
    in harmony. By deploying harmony constraints, these authors have
    arrived at logics no stronger than intuitionistic propositional
    logic. Classical logic, they maintain, cannot be justified from
    this proof-theoretic perspective. This paper argues that, while
    classical logic can be formulated so as to satisfy a number of
    harmony constraints, the meanings of the standard logical
    constants cannot all be given by their introduction and/or
    elimination rules; negation, in particular, comes under close
    scrutiny.",
  paper = "Miln94.pdf"
}  

\end{chunk}

\index{Tennant, Neil}
\begin{chunk}{axiom.bib}
@article{Tenn79,
  author = "Tennant, Neil",
  title = {{Entailment and Proofs}},
  journal = "Proc. of the Aristotelian Society",
  volume = "79",
  pages = "167-189",
  year = "1979",
  paper = "Tenn79.pdf"
}

\end{chunk}

\index{Montanaro, Ashley}
\begin{chunk}{axiom.bib}
@misc{Mont12,
  author = "Montanaro, Ashley",
  title = {{Computational Complexity Lecture Notes}},
  year = "2012",
  link = "\url{https://people.maths.bris.ac.uk/~csxam/teaching/cc-lecturenotes.pdf}",
  paper = "Mont12.pdf"
}

\end{chunk}

\index{Grayson, Daniel R.}
\begin{chunk}{axiom.bib}
@misc{Gray18a,
  author = "Grayson, Daniel R.",
  title = {{The Mathematical Work of Vladimir Voevodsky}},
  year = "2018",
  link = "\url{https://www.youtube.com/watch?v=BanMgvdKP8E}",
  keywords = "DONE"
}

\end{chunk}

\index{Gentzen, G.}
\begin{chunk}{axiom.bib}
@misc{Gent35,
  author = "Gentzen, G.",
  title = {{Untersuchungen uber das logische Schliessen I and II}},
  booktitle = "Mathematische Zeitschrift",
  year = "1935",
  publisher = "Springer"
  paper = "Gent35.pdf"
}

\end{chunk}

\index{Vapnik, Vladimir}
\begin{chunk}{axiom.bib}
@misc{Vapn20,
  author = "Vapnik, Vladimir",
  title = {{Predicates, Invariants, and the Essence of Intelligence}},
  year = "2020",
  link = "\url{https://lexfridman.com/vladimir-vapnik-2}",
  keywords = "DONE"
}

\end{chunk}

\index{Baruch, Robert}
\begin{chunk}{axiom.bib}
@misc{Baru19,
  author = "Baruch, Robert",
  title = {{Very Basic Introduction to Formal Verification}},
  year = "2019",
  link = "\url{https://www.youtube.com/watch?v=9e7F1XhjhKw}",
  keywords = "DONE"
}

\end{chunk}

\index{Carneiro, Mario}
\begin{chunk}{axiom.bib}
@misc{Carn19c,
  author = "Carneiro, Mario",
  title = {{Specifying Verified x86 Software from Scratch}},
  year = "2019",
  link = "\url{https://arxiv.org/pdf/1907.01283.pdf}",
  paper = "Carn19c.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Bendersky, Eli}
\begin{chunk}{axiom.bib}
@misc{Bend10,
  author = "Bendersky, Eli",
  title = {{Top Down Operator Precedence}},
  year = "2010",
  link = "\url{https://eli.thegreenplace.net/2010/01/02/top-down-operator-precedence-parsing}",
  keywords = "DONE"
}

\end{chunk}

\index{Baruch, Robert}
\begin{chunk}{axiom.bib}
@misc{Baru19a,
  author = "Baruch, Robert",
  title = {{Cmod A7 Reference Manual}},
  year = "2019",
  link = "\url{https://reference.digilentinc.com/_media/reference/programmable-logic/cmod-a7/cmod_a7_rm.pdf}",
  paper = "Baru19a.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Baruch, Robert}
\begin{chunk}{axiom.bib}
@misc{Baru19b,
  author = "Baruch, Robert",
  title = {{Cmod A7 Schematic}},
  year = "2019",
  link = "\url{https://reference.digilentinc.com/_media/cmod-a7/cmod_a7_sch.pdf}",
  paper = "Baru19b.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Reis, Leonardo Vieira dos Santos}
\index{Dilorio, Oliveira}
\index{Bigonha, Roberto S.}
\begin{chunk}{axiom.bib}
@inproceedings{Reis14,
  author = "Reis, Leonardo Vieira dos Santos and Dilorio, Oliveira and
            Bigonha, Roberto S.",
  title = {{A Mixed Approach for Building Extensible Parsers}},
  booktitle = "Programming Language",
  publisher = "Springer",
  volume = "LNCS 8771",
  pages = "1-15",
  year = "2014",
  abstract = 
    "For languages whose syntax is fixed, parsers are usually built
    with a static structure. The implementation of features like macor
    mechanisms or extensible languages requires the use of parsers
    that may be dynamically extended. In this work, we discuss a mixed
    approach for building efficient top-down dynamically extensible
    parsers. Our view is based on the fact that a large part of the
    parser code can be statically compiled and only the parts that are
    dynamic should be interpreted for a more efficient processing. We
    propose the generation of code for the base parser, in which hooks
    are included to allow efficient extension of the underlying
    grammar and activation of a grammar interpreter whenever it is
    necessary to use an extended syntax. As a proof of concept, we
    present a prototype implementation of a parser generator using
    Adaptable Parsing Expression Grammars (APEG) as the underlying
    method for syntax definition. We shos that APEG has features which
    allow an efficient implementation using the proposed mixed
    approach.",
  paper = "Reis14.pdf"
}

\end{chunk}

\index{Ford, Bryan}
\begin{chunk}{axiom.bib}
@article{Ford04,
  author = "Ford, Bryan",
  title = {{Parsing Expression Grammars: A Recognition-Based Syntactic
            Foundation}}, 
  journal = "SIGPLAN Notices",
  volume = "39",
  number = "1",
  pages = "111-122",
  year = "2004",
  abstract =
    "For decades we have been using Chomsky's generative system of
    grammars, particularly context-free grammars (CFGs) and regular
    expressions (REs), to express the syntax of programming languages
    and protocols. The power of generative grammars to express
    ambiguity is crucial to their original purpose of modelling
    natural languages, but this very power makes it unnecessarily
    difficult both to express and to parse machine-oriented languages
    using CFGs. Parsing Expression Grammars (PEGs) provide an
    alternative, recognition-based formal foundation for describing
    machine-oriented syntax, which solves the ambiguity problem by not
    introducing ambiguity in the first place. Where CFGs express
    nondeterministic choice between alternatives, PEGs instead use 
    {\sl prioritized choice}. PEGs address frequently felt
    expressiveness limitations of CFGs and REs, simplifying syntax
    definitions and making it unnecessary to separate their lexical
    and hierarchical components. A linear-time parser can be built for
    any PEG, avoiding both the complexity and fickleness of LR parsers
    and the inefficiency of generalized CFG parsing. While PEGs
    provide a rich set of operators for constructing grammars, they
    are reducible to two minimal recognition schemas developed around
    1970, TS/TDPL and gTS/GTDPL, which are here proven equivalent in
    effective recognition power.",
  paper = "Ford04.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Gueneau, Armael}
\begin{chunk}{axiom.bib}
@phdthesis{Guen19,
  author = "Gueneau, Armael",
  title = {{Mechanized Verification of the Correctness and Asymptotic
            Complexity of Programs}},
  school = "University of Paris",
  year = "2019",
  link = "\url{http://gallium.inria.fr/~agueneau/phd/manuscript.pdf}",
  abstract =
    "This dissertation is concerned with the question of formally
    verifying that the implementation of an algorithm is not only
    functionally correct (it always returns the right result), but
    also has the right asymptotic complexity (it reliably computes the
    result in the expected amount of time).

    In the algorithms literature, it is standard practice to
    characterize the performance of an algorithm by indicating its
    asymptotic time complexity, typically using Landau's ``big-O''
    notation. We first argue informally that asymptotic complexity
    bounds are equally useful as formal specifications, because they
    enable modular reasoning: a $O$ bound abstracts over the concrete
    cost expression of a program, and therefore abstracts over the
    specifics of its implementation. We illustrate -- with the help of
    small illustrative examples -- a number of challenges with the use
    of the $O$ notation, in particular in the multivariate case, that
    might be overlooked when reasoning informally.

    We put these considerations into practice by formalizing the $O$
    notation in the Coq proof assistant, and by extending an existing
    program verification framework, CFML, with support for a
    methodology enabling robust and modular proofs of asymptotic
    complexity bounds. We extend the existing framework of Separation
    Logic with Time Credits, which allows to reason at the same time
    about correctness and time complexity, and introduce negative time
    credits. Negative time credits increase the expressiveness of the
    logic, and enable convenient reasoning principles as well as
    elegant specifications. At the level of specifications, we show
    how asymptotic complexity specifications using $O$ can be
    integrated and composed within Separation Logic with Time
    Credits. Then, in order to establish such specifications, we
    develop a methodology that allows proofs of complexity in
    Separation Logic to be robust and carried out at a relatively high
    level of abstraction, by relying on two key elements: a mechanism
    for collecting and deferring constraints during the proof, and a
    mechanism for semi-automatically synthesizing cost expressions
    without loss of generality.

    We demonstrate the usefulness and practicality of our approach on
    a number of increasingly challenging case studies. These include
    algorithms whose complexity analysis is relatively simple (such as
    binary search, which is nonetheless out of the scope of many
    automated complexity analysis tools) and data structures (such as
    Okasaki's binary random access lists). In our most challenging
    case study, we establish the correctness and amortized complexity
    of a state-of-the-art incremental cycle detection algorithm: our
    methodology scales up to highly non-trivial algorithms whose
    complexity analysis intimately depends on subtle functional
    invariants, and furthermore makes it possible to formally verify
    OCaml code which can then actually be used as part of real world
    programs.", 
  paper = "Guen19.pdf"
}

\end{chunk}

\begin{chunk}{axiom.bib}
@misc{Fade17,
  author = "Unknown",
  title = {{Strongly Connected and Completely Specified Moore
            Equivalent of a Mealy Machine}},
  year = "2017",
  link = "\url{https://cs.stackexchange.com/questions/81127/strongly-connected-and-completely-specified-moore-equivalent-of-a-mealy-machine}",
  keywords = "DONE"
}

\end{chunk}

\index{Hoare, C.A.R}
\begin{chunk}{axiom.bib}
@misc{Hoar04,
  author = "Hoare, C.A.R",
  title = {{Unified Theories of Programming}},
  year = "2004",
  link = "\url{https://fi.ort.edu.uy/innovaportal/file/20124/1/04-hoard_unified_theories.pdf}",
  abstract =
    "Professional practice in a mature engineering discipline is based
    on relevant scientific theories, usually expressed in the language
    of mathematics. A mathematical theory of programming aims to
    provide a similar basis for specification, design and
    implementation of computer programs. The theory can be presented
    in a variety of styles, including
    \begin{enumerate}
    \item Denotational, relating a program to a specification of its
    observable properties and behaviour
    \item Algebraic, providing equations and inequalities for
    comparison, transformation and optimisation of designs and
    programs.
    \item Operational, describing individual steps of a possible
    mechanical implementation
    \end{enumerate}
    
    This paper presents simple theories of sequential
    non-deterministic programming in each of these three styles; by
    deriving each presentation from its predecessor in a cyclic
    fashion, mutual consistency is assured.",
  paper = "Hoar04.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Davis, Ernest}
\begin{chunk}{axiom.bib}
@misc{Davi19,
  author = "Davis, Ernest",
  title = {{The Use of Deep Learning for Symbolic Integration}},
  year = "2019",
  link = "\url{https://arxiv.org/pdf/1912.05752.pdf}",
  abstract =
    "Lample and Charton (2019) describe a system that uses deep
    learning technology to compute symbolic, indefinite integrals, and
    to find symbolic solutions to first- and second-order ordinary
    differential equations, when the solutions are elementary
    functions. They found that, over a particular test set, the system
    could find solutions more successfully than sophisticated packages
    for symbolic mathematics such as Mathematica run with a long
    time-out. This is an impressive accomplishment, as far as it
    goes. However, the system can handle only a quite limited subset
    of the problems that Mathematica deals with, and the test set has
    significant built-in biases. Therefore the claim that this
    outperforms Mathematica on symbolic integration needs to be very
    much qualified.",
  paper = "Davi19.pdf",
  keywords = "printed, DONE"
}

\end{chunk}

\index{Pierce, Benjamin}
\index{Appel, Andrew W.}
\index{Weirich, Stephanie}
\index{Zdancewic, Steve}
\index{Shao, Zhong}
\index{Chlipala, Adam}
\begin{chunk}{axiom.bib}
@misc{Pier16,
  author = "Pierce, Benjamin and Appel, Andrew W. and Weirich, Stephanie 
            and Zdancewic, Steve and Shao, Zhong and Chlipala, Adam",
  title = {{The Science of Deep Specification}},
  year = "2016",
  link = "\url{https://www.cis.upenn.edu/~bcpierce/papers/deepspec-hcss2016-slides.pdf}",
  paper = "Pier16.pdf"
}

\end{chunk}

\index{Pierce, Benjamin}
\begin{chunk}{axiom.bib}
@misc{Pier18,
  author = "Pierce, Benjamin"
  title = {{The Science of Deep Specification}},
  year = "2018",
  link = "\url{https://www.cis.upenn.edu/~bcpierce/papers/pierce-etaps2018.pdf}",
  paper = "Pier18.pdf"
}

\end{chunk}

\index{Chlipala, Adam}
\index{Arvind}
\index{Sherman, Benjamin}
\index{Choi, Joonwon}
\index{Vijayaraghavan, Murali}
\begin{chunk}{axiom.bib}
@misc{Chli17b,
  author = "Chlipala, Adam and Arvind and Sherman, Benjamin and
            Choi, Joonwon and Vijayaraghavan, Murali",
  title = {{Kami: A Platform for High-Level Parametric Hardware
            Specification and its Modular Verification}},
  year = "2017",
  abstract =
    "It has become fairly standard in the programming languages
    research world to verify functional programs in proof assistants
    using induction, algebraic simplification, and rewriting. In this
    paper, we introduce Kami, a Coq library that uses labelled
    transition systems to enable similar expressive and modular
    reasoning for hardware designs expressed in the style of the
    Bluespec language. We can specify, implement, and verify realistic
    designs entirely within Coq, ending with automatic extraction into
    a pipeline that bottoms out in FPGAs. Our methodology has been
    evaluated in a case study verifying an infinite family fo
    multicore systems, with cache-coherent shared memory and pipelined
    cores implementing (the base integer subset of) the RISC-V
    instruction set.",
  paper = "Chli19b.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Appel, Andrew W.}
\index{Beringer, Lennart}
\index{Chlipala, Adam}
\index{Pierce, Benjamin C.}
\index{Shao, Zhong}
\index{Weirich, Stephanie}
\index{Zdancewic, Steve}
\begin{chunk}{axiom.bib}
@article{Appe17a,
  author = "Appel, Andrew W. and Beringer, Lennart and Chlipala, Adam
            and Pierce, Benjamin C. and Shao, Zhong and Weirich, Stephanie
            and Zdancewic, Steve",
  title = {{Position Paper: The Science of Deep Specification}},
  journal = "Philosophical Transactions of the Royal Society",
  volume = "375",
  year = "2017",
  link = "\url{https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2016.0331}",
  abstract =
    "We introduce our efforts within the project ``The Science of Deep
    Specification'' to work out the key formal underpinnings of
    inductrial-scale formal specifications of software and hardware
    components, anticipating a world where large verified systems are
    routinely built out of smaller verified components that are also
    used by many other projects. We identify an important class of
    specification that has already been used in a few experiments that
    connect strong component-correctness theorems across the work of
    different teams. To help popularize the unique advantages of that
    style, we dub it {\sl deep specification}, and we say that it
    encompasses specifications that are {\sl rich}, {\sl two-sided}, 
    {\sl formal}, and {\sl live} (terms that we define in the
    article). Our core team is developing a proof-of-concept ssystem
    (based on teh Coq proof assistant) whose specification and
    verification work is divided across argely decoupled subteams at
    our four institutions, encompassing hardware microarchitecture,
    compilers, operating systems and applications, along with
    cross-cutting principles and tools for effective specification. We
    also aim to catalyse interest in the approach, not just by basic
    researchers but also by users in industry.

    This article is part of the themed issue ``Verified trustworthy
    software systems''",
  paper = "Appe17a.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Braibant, Thomas}
\index{Chlipala, Adam}
\begin{chunk}{axiom.bib}
@inproceedings{Brai13,
  author = "Braibant, Thomas and Chlipala, Adam",
  title = {{Formal Verification of Hardware Synthesis}},
  booktitle = "Computer Aided Verification (CAV'13)",
  publisher = "unknown",
  year = "2013",
  abstract = 
    "We report on the implmentation of a certified compier for a
    high-level hardware description language (HDL) called Fe-Si
    (FEatherweight Synthesis). Fe-Si is a simplified version of
    Bluespec, an HDL based on a notion of guarded atomic
    actions. Fe-Si is defined as a dependently typed deep embedding in
    Coq. The target language of the compiler corresponds to a
    synthesisable subset of Verilog or VHDL. A key aspect of our
    approach is that input programs to the compiler can be defined and
    proved correct inside Coq. The, we use extraction and a Verilog
    back-end (written in OCaml) to get a certified version of a
    hardware design.",
  paper = "Brai13.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Timany, Amin}
\index{Sozeau, Mattieu}
\begin{chunk}{axiom.bib}
@inproceedings{Tima18,
  author = "Timany, Amin and Sozeau, Mattieu",
  title = {{Cumulative Inductive Types in Coq}},
  booktitle = "3rd Conf. on Formal Structures for Computation and
               Deduction",
  publisher = "HAL",
  link = "\url{https://hal.inria.fr/hal-01952037/document}",
  abstract =
    "In order to avoid well-known paradoxes associated with
    self-referential definitions, high-order dependent type theories
    stratify the theory using a countably infinite hierarchy of
    universes (also known as sorts), $Type_0 : Type_1,\ldots$ Such
    type systems are called cumulative if for any type $A$ we have
    that $A:Type_i$ implies $A:Type_{i+1}$. The Predicative Calculus
    of Inductive Constructions (pCIC) which forms the basis of the Coq
    proof assistant, is one such system. In this paper we present the
    Predicative Calculus of Cumulative Inductive Constructions
    (pCUIC) which extends the cumulativity relation to inductive
    types. We discuss cumulative inductive types as present in Coq 8.7
    and their application to formalization and definitional
    translations.", 
  paper = "Tima18.pdf"
}

\end{chunk}

\index{Gilbert, Gaetan}
\begin{chunk}{axiom.bib}
@phdthesis{Gilb19,
  author = "Gilbert, Gaetan",
  title = {{A Type Theory with Definitional Proof-Irrelevance}},
  school = "Ecole Nationale Superieure Mines-Telecom Alantique.",
  year = "2019",
  link = "\url{https://gitlab.com/SkySkimmer/thesis/~/jobs/artifacts/master/download?job=build",
  paper = "Gilb19.zip"
}
 
\end{chunk}

\index{Awodey, Steve}
\begin{chunk}{axiom.bib}
@misc{Awod16,
  author = "Awodey, Steve",
  title = {{Univalence as a Principle of Logic}},
  year = "2016",
  link = "\url{https://www.andrew.cmu.edu/user/awodey/preprints/ualp.pdf}",
  abstract =
    "It is sometmes convenient or useful in mathematics to treat
    isomorphic structures as the same. The recently proposed
    Univalence Axiom for the foundations of mathematics elevates this
    idea to a foundational principle in the setting of Homotopy Type
    Theory. It states, roughly, that isomorphic structures can be
    identified. We explore the motivations and consequences, both
    mathematical and philosophical, of making such a new logical
    postulate.", 
  paper = "Awod16.pdf",
  keywords = "printed"
}

\end{chunk}

\index{McCorduck, Pamela}
\begin{chunk}{axiom.bib}
@book{Mcco79,
  author = "McCorduck, Pamela",
  tilte = {{Machines Who Think}},
  year = "1979",
  publisher = "Freeman",
  keywords = "shelf, DONE"
}

\end{chunk}

\index{Gabriel, Richard}
\begin{chunk}{axiom.bib}
@book{Gabr96,
  author = "Gabriel, Richard",
  title = {{Patterns of Software}},
  link = "\url{https://www.dreamsongs.com/Files/PatternsOfSoftware.pdf}",
  year = "1996",
  publisher = "Oxford University Press",
  paper = "Gabr96.pdf"

}

\end{chunk}

\index{Gabriel, Richard}
\begin{chunk}{axiom.bib}
@book{Gabr85,
  author = "Gabriel, Richard",
  title = {{Performance and Evaluation of Lisp Systems}},
  link = "\url{https://www.dreamsongs.com/Files/Timrep.pdf}",
  year = "1985",
  publisher = "MIT Press",
  paper = "Gabr85.pdf"

}

\end{chunk}

\index{Lucas, J.R.}
\begin{chunk}{axiom.bib}
@misc{Luca03,
  author = "Lucas, J.R.",
  title = {{Minds, Machines and Godel}},
  year = "2003",
  link =
  "\url{https://pdfs.semanticscholar.org/bde3/b731bf73ef6052e34c4465e57718c03b13f8.pdf}",
  paper = "Luca03.pdf"
}

\end{chunk}

\index{Lem, Stanislaw}
\begin{chunk}{axiom.bib}
@book{Lemx68,
  author = "Lem, Stanislaw",
  title = {{His Master's Voice}},
  publisher = "MIT Press",
  year = "1968"
}

\end{chunk}
